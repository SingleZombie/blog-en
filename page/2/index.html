<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:type" content="website">
<meta property="og:title" content="周弈帆的博客">
<meta property="og:url" content="https://zhouyifan.net/page/2/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhou Yifan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/23/20240114-SD-LoRA/" class="post-title-link" itemprop="url">LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-23 19:42:10" itemprop="dateCreated datePublished" datetime="2024-01-23T19:42:10+08:00">2024-01-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>如果你一直关注 Stable Diffusion (SD) 社区，那你一定不会对 “LoRA” 这个名词感到陌生。社区用户分享的 SD LoRA 模型能够修改 SD 的画风，使之画出动漫、水墨或像素等风格的图片。但实际上，LoRA 不仅仅能改变 SD 的画风，还有其他的妙用。在这篇文章中，我们会先简单学习 LoRA 的原理，再认识科研中 LoRA 的三种常见应用：1） 还原单幅图像；2）风格调整；3）训练目标调整，最后阅读两个基于 Diffusers 的 SD LoRA 代码实现示例。</p>
<h2 id="LoRA-的原理"><a href="#LoRA-的原理" class="headerlink" title="LoRA 的原理"></a>LoRA 的原理</h2><p>在认识 LoRA 之前，我们先来回顾一下迁移学习的有关概念。迁移学习指在一次新的训练中，复用之前已经训练过的模型的知识。如果你自己动手训练过深度学习模型，那你应该不经意间地使用到了迁移学习：比如你一个模型训练了 500 步，测试后发现效果不太理想，于是重新读取该模型的参数，又继续训练了 100 步。之前那个被训练过的模型叫做预训练模型（pre-trained model），继续训练预训练模型的过程叫做微调（fine-tune）。</p>
<p>知道了微调的概念，我们就能来认识 LoRA 了。LoRA 的全称是 Low-Rank Adaptation (低秩适配)，它是一种 Parameter-Efficient Fine-Tuning (参数高效微调，PEFT) 方法，即在微调时只训练原模型中的部分参数，以加速微调的过程。相比其他的 PEFT 方法，LoRA 之所以能脱颖而出，是因为它有几个明显的优点：</p>
<ul>
<li>从性能上来看，使用 LoRA 时，只需要存储少量被微调过的参数，而不需要把整个新模型都保存下来。同时，LoRA 的新参数可以和原模型的参数合并到一起，不会增加模型的运算时间。</li>
<li>从功能上来看，LoRA 维护了模型在微调中的「变化量」。通过用一个介于 0~1 之间的混合比例乘变化量，我们可以控制模型的修改程度。此外，基于同一个原模型独立训练的多个 LoRA 可以同时使用。</li>
</ul>
<p>这些优点在 SD LoRA 中的体现为：</p>
<ul>
<li>SD LoRA 模型一般都很小，一般只有几十 MB。</li>
<li>SD LoRA 模型的参数可以合并到 SD 基础模型里，得到一个新的 SD 模型。</li>
<li>可以用一个 0~1 之间的比例来控制 SD LoRA 新画风的程度。</li>
<li>可以把不同画风的 SD LoRA 模型以不同比例混合。</li>
</ul>
<p>为什么 LoRA 能有这些优点呢？LoRA 名字中的 「低秩」又是什么意思呢？让我们从 LoRA 的优点入手，逐步揭示它原理。</p>
<p>上文提到过，LoRA 之所以那么灵活，是因为它维护了模型在微调过程中的变化量。那么，假设我们正在修改模型中的一个参数 $W \in \mathbb{R}^{d \times d}$，我们就应该维护它的变化量 $\Delta W \in \mathbb{R}^{d \times d}$，训练时的参数用 $W + \Delta W$ 表示。这样，想要在推理时控制模型的修改程度，只要添加一个 $\alpha \in [0, 1]$，令使用的参数为 $W + \alpha \Delta W$即可。</p>
<p>可是，这样做我们还是要记录一个和原参数矩阵一样大的参数矩阵 $\Delta W$，这就算不上是参数<strong>高效</strong>微调了。为此，LoRA 的作者提出假设：模型参数在微调时的变化量中蕴含的信息没有那么多。为了用更少的信息来表示参数的变化量$\Delta W$，我们可以把$\Delta W$拆解成两个低秩矩阵的乘积：</p>
<script type="math/tex; mode=display">
\Delta W = BA</script><p>其中，$A \in \mathbb{R}^{r \times d}$, $B \in \mathbb{R}^{d \times r}$，$d$ 是一个比 $r$ 小得多的数。这样，通过用两个参数量少得多的矩阵 $A, B$ 来维护变化量，我们不仅提高了微调的效率，还保持了使用变化量来描述微调过程的灵活性。这就是 LoRA 的全部原理，它十分简单，用 $\Delta W = BA$ 这一行公式足以表示。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/20240114-SD-LORA/1.jpg" alt></p>
<p>了解了 LoRA 的原理，我们再回头看前文提及的 LoRA 的四项优点。LoRA 模型由许多参数量较少的矩阵 $A, B$ 来表示，它可以被单独存储，且占用空间不大。由于 $\Delta W = BA$ 维护的其实是参数的变化量，我们既可以把它与预训练模型的参数加起来得到一个新模型以提高推理速度，也可以在线地用一个混合比例来灵活地组合新旧模型。LoRA 的最后一个优点是各个基于同一个原模型独立训练出来的 LoRA 模型可以混合使用。LoRA 甚至可以作用于被其他方式修改过的原模型，比如 SD LoRA 支持带 ControlNet 的 SD。这一点其实来自于社区用户的实践。一个可能的解释是，LoRA 用低秩矩阵来表示变化量，这种低秩的变化量恰好与其他方法的变化量「错开」，使得 LoRA 能向着一个不干扰其他方法的方向修改模型。</p>
<p>我们最后来学习一下 LoRA 的实现细节。LoRA 有两个超参数，除了上文中提到的$r$，还有一个叫$\alpha$的参数。LoRA 的作者在实现 LoRA 模块时，给修改量乘了一个 $\frac{\alpha}{r}$ 的系数，即对于输入$x$，带了 LoRA 模块后的输出为 $Wx + \frac{\alpha}{r}BAx$。作者解释说，调这个参数几乎等于调学习率，一开始令$\alpha=r$即可。在我们要反复调超参数$r$时，只要保持$\alpha$不变，就不用改其他超参数了（因为不加$\alpha$的话，改了$r$后，学习率等参数也得做相应调整以维持同样的训练条件）。当然，实际运用中，LoRA 的超参数很好调。一般令$r=4, 8, 16$即可。由于我们不怎么会变$r$，总是令$\alpha=r$就够了。</p>
<p>为了使用 LoRA，除了确定超参数外，我们还得指定需要被微调的参数矩阵。在 SD 中使用 LoRA 时，大家一般会对 SD 的 U-Net 的所有多头注意力模块的所有参数矩阵做微调。即对于多头注意力模块的四个矩阵 $W_Q, W_K, W_V, W_{out}$ 进行微调。</p>
<h2 id="LoRA-在-SD-中的三种运用"><a href="#LoRA-在-SD-中的三种运用" class="headerlink" title="LoRA 在 SD 中的三种运用"></a>LoRA 在 SD 中的三种运用</h2><p>LoRA 在 SD 的科研中有着广泛的应用。按照使用 LoRA 的动机，我们可以把 LoRA 的应用分成：1） 还原单幅图像；2）风格调整；3）训练目标调整。通过学习这些应用，我们能更好地理解 LoRA 的本质。</p>
<h3 id="还原单幅图像"><a href="#还原单幅图像" class="headerlink" title="还原单幅图像"></a>还原单幅图像</h3><p>SD 只是一个生成任意图片的模型。为了用 SD 来编辑一张给定的图片，我们一般要让 SD 先学会生成一张一模一样的图片，再在此基础上做修改。可是，由于训练集和输入图片的差异，SD 或许不能生成完全一样的图片。解决这个问题的思路很简单粗暴：我们只用这一张图片来微调 SD，让 SD 在这张图片上过拟合。这样，SD 的输出就会和这张图片非常相似了。</p>
<p>较早介绍这种提高输入图片保真度方法的工作是 Imagic，只不过它采取的是完全微调策略。后续的 DragDiffusion 也用了相同的方法，并使用 LoRA 来代替完全微调。近期的 DiffMorpher 为了实现两幅图像间的插值，不仅对两幅图像单独训练了 LoRA，还通过两个 LoRA 间的插值来平滑图像插值的过程。</p>
<h3 id="风格调整"><a href="#风格调整" class="headerlink" title="风格调整"></a>风格调整</h3><p>LoRA 在 SD 社区中最受欢迎的应用就是风格调整了。我们希望 SD 只生成某一画风，或者某一人物的图片。为此，我们只需要在一个符合我们要求的训练集上直接训练 SD LoRA 即可。</p>
<p>由于这种调整 SD 风格的方法非常直接，没有特别介绍这种方法的论文。稍微值得一提的是基于 SD 的视频模型 AnimateDiff，它用 LoRA 来控制输出视频的视角变换，而不是控制画风。</p>
<p>由于 SD 风格化 LoRA 已经被广泛使用，能否兼容 SD 风格化 LoRA 决定了一个工作是否易于在社区中传播。</p>
<h3 id="训练目标调整"><a href="#训练目标调整" class="headerlink" title="训练目标调整"></a>训练目标调整</h3><p>最后一个应用就有一点返璞归真了。LoRA 最初的应用就是把一个预训练模型适配到另一任务上。比如 GPT 一开始在大量语料中训练，随后在问答任务上微调。对于 SD 来说，我们也可以修改 U-Net 的训练目标，以提升 SD 的能力。</p>
<p>有不少相关工作用 LoRA 来改进 SD。比如 Smooth Diffusion 通过在训练目标中添加一个约束项并进行 LoRA 微调来使得 SD 的隐空间更加平滑。近期比较火的高速图像生成方法 LCM-LoRA 也是把原本作用于 SD 全参数上的一个模型蒸馏过程用 LoRA 来实现。</p>
<h3 id="SD-LoRA-应用总结"><a href="#SD-LoRA-应用总结" class="headerlink" title="SD LoRA 应用总结"></a>SD LoRA 应用总结</h3><p>尽管上述三种 SD LoRA 应用的设计出发点不同，它们本质上还是在利用微调这一迁移学习技术来调整模型的数据分布或者训练目标。LoRA 只是众多高效微调方法中的一种，只要是微调能实现的功能，LoRA 基本都能实现，只不过 LoRA 更轻便而已。如果你想微调 SD 又担心计算资源不够，那么用 LoRA 准没错。反过来说，你想用 LoRA 在 SD 上设计出一个新应用，就要去思考微调 SD 能够做到哪些事。</p>
<h2 id="Diffusers-SD-LoRA-代码实战"><a href="#Diffusers-SD-LoRA-代码实战" class="headerlink" title="Diffusers SD LoRA 代码实战"></a>Diffusers SD LoRA 代码实战</h2><p>看完了原理，我们来尝试用 Diffusers 自己训一训 LoRA。我们会先学习 Diffusers 训练 LoRA 的脚本，再学习两个简单的 LoRA 示例： SD 图像插值与 SD 图像风格迁移。</p>
<p>项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA">https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA</a></p>
<h3 id="Diffusers-脚本"><a href="#Diffusers-脚本" class="headerlink" title="Diffusers 脚本"></a>Diffusers 脚本</h3><p>我们将参考 Diffusers 中的 SD LoRA 文档 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/training/lora">https://huggingface.co/docs/diffusers/training/lora</a> ，使用官方脚本 <code>examples/text_to_image/train_text_to_image_lora.py</code> 训练 LoRA。为了使用这个脚本，建议直接克隆官方仓库，并安装根目录和 <code>text_to_image</code> 目录下的依赖文件。本文使用的 Diffusers 版本是 0.26.0，过旧的 Diffusers 的代码可能和本文展示的有所出入。目前，官方文档也描述的是旧版的代码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/huggingface/diffusers</span><br><span class="line">cd diffusers</span><br><span class="line">pip install .</span><br><span class="line">cd examples/text_to_image</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>这份代码使用 accelerate 库管理 PyTorch 的训练。对同一份代码，只需要修改 accelerate 的配置，就能实现单卡训练或者多卡训练。默认情况下，用 <code>accelerate launch</code> 命令运行 Python 脚本会使用所有显卡。如果你需要修改训练配置，请参考相关文档使用 <code>accelerate config</code> 命令配置环境。</p>
<p>做好准备后，我们来开始阅读 <code>examples/text_to_image/train_text_to_image_lora.py</code> 的代码。这份代码写得十分易懂，复杂的地方都有注释。我们跳过命令行参数部分，直接从 <code>main</code> 函数开始读。</p>
<p>一开始，函数会配置 accelerate 库及日志记录器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">args = parse_args()</span><br><span class="line">logging_dir = Path(args.output_dir, args.logging_dir)</span><br><span class="line"></span><br><span class="line">accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator(</span><br><span class="line">    gradient_accumulation_steps=args.gradient_accumulation_steps,</span><br><span class="line">    mixed_precision=args.mixed_precision,</span><br><span class="line">    log_with=args.report_to,</span><br><span class="line">    project_config=accelerator_project_config,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> args.report_to == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_wandb_available():</span><br><span class="line">        <span class="keyword">raise</span> ImportError(<span class="string">&quot;Make sure to install wandb if you want to use it for logging during training.&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> wandb</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make one log on every process with the configuration for debugging.</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">)</span><br><span class="line">logger.info(accelerator.state, main_process_only=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">if</span> accelerator.is_local_main_process:</span><br><span class="line">    datasets.utils.logging.set_verbosity_warning()</span><br><span class="line">    transformers.utils.logging.set_verbosity_warning()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_info()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    datasets.utils.logging.set_verbosity_error()</span><br><span class="line">    transformers.utils.logging.set_verbosity_error()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_error()</span><br></pre></td></tr></table></figure>
<p>随后的代码决定是否手动设置随机种子。保持默认即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If passed along, set the training seed now.</span></span><br><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    set_seed(args.seed)</span><br></pre></td></tr></table></figure>
<p>接着，函数会创建输出文件夹。如果我们想把模型推送到在线仓库上，函数还会创建一个仓库。我们的项目不必上传，忽略所有 <code>args.push_to_hub</code> 即可。另外，<code>if accelerator.is_main_process:</code> 表示多卡训练时只有主进程会执行这段代码块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Handle the repository creation</span></span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        os.makedirs(args.output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        repo_id = create_repo(</span><br><span class="line">            repo_id=args.hub_model_id <span class="keyword">or</span> Path(args.output_dir).name, exist_ok=<span class="literal">True</span>, token=args.hub_token</span><br><span class="line">        ).repo_id</span><br></pre></td></tr></table></figure>
<p>准备完辅助工具后，函数正式开始着手训练。训练前，函数会先实例化好一切处理类，包括用于维护扩散模型中间变量的 <code>DDPMScheduler</code>，负责编码输入文本的 <code>CLIPTokenizer, CLIPTextModel</code>，压缩图像的VAE <code>AutoencoderKL</code>，预测噪声的 U-Net <code>UNet2DConditionModel</code>。参数 <code>args.pretrained_model_name_or_path</code> 是 Diffusers 在线仓库的地址（如<code>runwayml/stable-diffusion-v1-5</code>），或者本地的 Diffusers 模型文件夹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load scheduler, tokenizer and models.</span></span><br><span class="line">noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;scheduler&quot;</span>)</span><br><span class="line">tokenizer = CLIPTokenizer.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;tokenizer&quot;</span>, revision=args.revision</span><br><span class="line">)</span><br><span class="line">text_encoder = CLIPTextModel.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;text_encoder&quot;</span>, revision=args.revision</span><br><span class="line">)</span><br><span class="line">vae = AutoencoderKL.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;vae&quot;</span>, revision=args.revision, variant=args.variant</span><br><span class="line">)</span><br><span class="line">unet = UNet2DConditionModel.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;unet&quot;</span>, revision=args.revision, variant=args.variant</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>函数还会设置各个带参数模型是否需要计算梯度。由于我们待会要优化的是新加入的 LoRA 模型，所有预训练模型都不需要计算梯度。另外，函数还会根据 accelerate 配置自动设置这些模型的精度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># freeze parameters of models to save more memory</span></span><br><span class="line">unet.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">vae.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">text_encoder.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze the unet parameters before adding adapters</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> unet.parameters():</span><br><span class="line">    param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to half-precision</span></span><br><span class="line"><span class="comment"># as these weights are only used for inference, keeping weights in full precision is not required.</span></span><br><span class="line">weight_dtype = torch.float32</span><br><span class="line"><span class="keyword">if</span> accelerator.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.float16</span><br><span class="line"><span class="keyword">elif</span> accelerator.mixed_precision == <span class="string">&quot;bf16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.bfloat16</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move unet, vae and text_encoder to device and cast to weight_dtype</span></span><br><span class="line">unet.to(accelerator.device, dtype=weight_dtype)</span><br><span class="line">vae.to(accelerator.device, dtype=weight_dtype)</span><br><span class="line">text_encoder.to(accelerator.device, dtype=weight_dtype)</span><br></pre></td></tr></table></figure></p>
<p>把预训练模型都调好了后，函数会配置 LoRA 模块并将其加入 U-Net 模型中。最近，Diffusers 更新了添加 LoRA 的方式。Diffusers 用 Attention 处理器来描述 Attention 的计算。为了把 LoRA 加入到 Attention 模块中，早期的 Diffusers 直接在 Attention 处理器里加入可训练参数。现在，为了和其他 Hugging Face 库统一，Diffusers 使用 PEFT 库来管理 LoRA。我们不需要关注 LoRA 的实现细节，只需要写一个 <code>LoraConfig</code> 就行了。</p>
<blockquote>
<p>PEFT 中的 LoRA 文档参见 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/peft/conceptual_guides/lora">https://huggingface.co/docs/peft/conceptual_guides/lora</a></p>
</blockquote>
<p><code>LoraConfig</code> 中有四个主要参数: <code>r, lora_alpha, init_lora_weights, target_modules</code>。 <code>r, lora_alpha</code> 的意义我们已经在前文中见过了，前者决定了 LoRA 矩阵的大小，后者决定了训练速度。默认配置下，它们都等于同一个值 <code>args.rank</code>。<code>init_lora_weights</code> 表示如何初始化训练参数，<code>gaussian</code>是论文中使用的方法。<code>target_modules</code> 表示 Attention 模块的哪些层需要添加 LoRA。按照通常的做法，会给所有层，即三个输入变换矩阵 <code>to_k, to_q, to_v</code> 和一个输出变换矩阵 <code>to_out.0</code> 加 LoRA。</p>
<p>创建了配置后，用 <code>unet.add_adapter(unet_lora_config)</code> 就可以创建 LoRA 模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unet_lora_config = LoraConfig(</span><br><span class="line">    r=args.rank,</span><br><span class="line">    lora_alpha=args.rank,</span><br><span class="line">    init_lora_weights=<span class="string">&quot;gaussian&quot;</span>,</span><br><span class="line">    target_modules=[<span class="string">&quot;to_k&quot;</span>, <span class="string">&quot;to_q&quot;</span>, <span class="string">&quot;to_v&quot;</span>, <span class="string">&quot;to_out.0&quot;</span>],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">unet.add_adapter(unet_lora_config)</span><br><span class="line"><span class="keyword">if</span> args.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> unet.parameters():</span><br><span class="line">        <span class="comment"># only upcast trainable parameters (LoRA) into fp32</span></span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            param.data = param.to(torch.float32)</span><br></pre></td></tr></table></figure>
<p>更新完了 U-Net 的结构，函数会尝试启用 <code>xformers</code> 来提升 Attention 的效率。PyTorch 在 2.0 版本也加入了类似的 Attention 优化技术。如果你的显卡性能有限，且 PyTorch 版本小于 2.0，可以考虑使用 <code>xformers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.enable_xformers_memory_efficient_attention:</span><br><span class="line">  <span class="keyword">if</span> is_xformers_available():</span><br><span class="line">      <span class="keyword">import</span> xformers</span><br><span class="line"></span><br><span class="line">      xformers_version = version.parse(xformers.__version__)</span><br><span class="line">      <span class="keyword">if</span> xformers_version == version.parse(<span class="string">&quot;0.0.16&quot;</span>):</span><br><span class="line">          logger.warn(</span><br><span class="line">              ...</span><br><span class="line">          )</span><br><span class="line">      unet.enable_xformers_memory_efficient_attention()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;xformers is not available. Make sure it is installed correctly&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>做完了 U-Net 的处理后，函数会过滤出要优化的模型参数，这些参数稍后会传递给优化器。过滤的原则很简单，如果参数要求梯度，就是待优化参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lora_layers = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, unet.parameters())</span><br></pre></td></tr></table></figure>
<p>之后是优化器的配置。函数先是配置了一些细枝末节的训练选项，一般可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.gradient_checkpointing:</span><br><span class="line">    unet.enable_gradient_checkpointing()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable TF32 for faster training on Ampere GPUs,</span></span><br><span class="line"><span class="comment"># cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices</span></span><br><span class="line"><span class="keyword">if</span> args.allow_tf32:</span><br><span class="line">    torch.backends.cuda.matmul.allow_tf32 = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>然后是优化器的选择。我们可以忽略其他逻辑，直接用 <code>AdamW</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the optimizer</span></span><br><span class="line"><span class="keyword">if</span> args.use_8bit_adam:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">import</span> bitsandbytes <span class="keyword">as</span> bnb</span><br><span class="line">    <span class="keyword">except</span> ImportError:</span><br><span class="line">        <span class="keyword">raise</span> ImportError(</span><br><span class="line">            <span class="string">&quot;...&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    optimizer_cls = bnb.optim.AdamW8bit</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    optimizer_cls = torch.optim.AdamW</span><br></pre></td></tr></table></figure>
<p>选择了优化器类，就可以实例化优化器了。优化器的第一个参数是之前准备好的待优化 LoRA 参数，其他参数是 Adam 优化器本身的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizer_cls(</span><br><span class="line">    lora_layers,</span><br><span class="line">    lr=args.learning_rate,</span><br><span class="line">    betas=(args.adam_beta1, args.adam_beta2),</span><br><span class="line">    weight_decay=args.adam_weight_decay,</span><br><span class="line">    eps=args.adam_epsilon,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>准备了优化器，之后需要准备训练集。这个脚本用 Hugging Face 的 datasets 库来管理数据集。我们既可以读取在线数据集，也可以读取本地的图片文件夹数据集。在本文的示例项目中，我们将使用图片文件夹数据集。稍后我们再详细学习这样的数据集文件夹该怎么构建。相关的文档可以参考 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder">https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</a> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.dataset_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># Downloading and loading a dataset from the hub.</span></span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        args.dataset_name,</span><br><span class="line">        args.dataset_config_name,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">        data_dir=args.train_data_dir,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_files = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> args.train_data_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        data_files[<span class="string">&quot;train&quot;</span>] = os.path.join(args.train_data_dir, <span class="string">&quot;**&quot;</span>)</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        <span class="string">&quot;imagefolder&quot;</span>,</span><br><span class="line">        data_files=data_files,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># See more about loading custom images at</span></span><br><span class="line">    <span class="comment"># https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</span></span><br></pre></td></tr></table></figure>
<p>训练 SD 时，每一个数据样本需要包含两项信息：图像数据与对应的文本描述。在数据集 <code>dataset</code> 中，每个数据样本包含了多项属性。下面的代码用于从这些属性中取出图像与文本描述。默认情况下，第一个属性会被当做图像数据，第二个属性会被当做文本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets.</span></span><br><span class="line"><span class="comment"># We need to tokenize inputs and targets.</span></span><br><span class="line">column_names = dataset[<span class="string">&quot;train&quot;</span>].column_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Get the column names for input/target.</span></span><br><span class="line">dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, <span class="literal">None</span>)</span><br><span class="line"><span class="keyword">if</span> args.image_column <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    image_column = dataset_columns[<span class="number">0</span>] <span class="keyword">if</span> dataset_columns <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> column_names[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    image_column = args.image_column</span><br><span class="line">    <span class="keyword">if</span> image_column <span class="keyword">not</span> <span class="keyword">in</span> column_names:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;--image_column&#x27; value &#x27;<span class="subst">&#123;args.image_column&#125;</span>&#x27; needs to be one of: <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(column_names)&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line"><span class="keyword">if</span> args.caption_column <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    caption_column = dataset_columns[<span class="number">1</span>] <span class="keyword">if</span> dataset_columns <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> column_names[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    caption_column = args.caption_column</span><br><span class="line">    <span class="keyword">if</span> caption_column <span class="keyword">not</span> <span class="keyword">in</span> column_names:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;--caption_column&#x27; value &#x27;<span class="subst">&#123;args.caption_column&#125;</span>&#x27; needs to be one of: <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(column_names)&#125;</span>&quot;</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>准备好了数据集，接下来要定义数据预处理流程以创建 <code>DataLoader</code>。函数先定义了一个把文本标签预处理成 token ID 的 token 化函数。我们不需要修改它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_captions</span>(<span class="params">examples, is_train=<span class="literal">True</span></span>):</span></span><br><span class="line">    captions = []</span><br><span class="line">    <span class="keyword">for</span> caption <span class="keyword">in</span> examples[caption_column]:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(caption, <span class="built_in">str</span>):</span><br><span class="line">            captions.append(caption)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(caption, (<span class="built_in">list</span>, np.ndarray)):</span><br><span class="line">            <span class="comment"># take a random caption if there are multiple</span></span><br><span class="line">            captions.append(random.choice(caption) <span class="keyword">if</span> is_train <span class="keyword">else</span> caption[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Caption column `<span class="subst">&#123;caption_column&#125;</span>` should contain either strings or lists of strings.&quot;</span></span><br><span class="line">            )</span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        captions, max_length=tokenizer.model_max_length, padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> inputs.input_ids</span><br></pre></td></tr></table></figure>
<p>接着，函数定义了图像数据的预处理流程。该流程是用 <code>torchvision</code> 中的 <code>transforms</code> 实现的。如代码所示，处理流程中包括了 resize 至指定分辨率 <code>args.resolution</code>、将图像长宽均裁剪至指定分辨率、随机翻转、转换至 tensor 和归一化。</p>
<p>经过这一套预处理后，所有图像的长宽都会被设置为 <code>args.resolution</code> 。统一图像的尺寸，主要的目的是对齐数据，以使多个数据样本能拼接成一个 batch。注意，数据预处理流程中包括了随机裁剪。如果数据集里的多数图片都长宽不一致，模型会倾向于生成被裁剪过的图片。为了解决这一问题，要么自己手动预处理图片，使训练图片都是分辨率至少为 <code>args.resolution</code> 的正方形图片，要么令 batch size 为 1 并取消掉随机裁剪。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets.</span></span><br><span class="line">train_transforms = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize(</span><br><span class="line">            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),</span><br><span class="line">        transforms.CenterCrop(</span><br><span class="line">            args.resolution) <span class="keyword">if</span> args.center_crop <span class="keyword">else</span> transforms.RandomCrop(args.resolution),</span><br><span class="line">        transforms.RandomHorizontalFlip() <span class="keyword">if</span> args.random_flip <span class="keyword">else</span> transforms.Lambda(<span class="keyword">lambda</span> x: x),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>定义了预处理流程后，函数对所有数据进行预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_train</span>(<span class="params">examples</span>):</span></span><br><span class="line">    images = [image.convert(<span class="string">&quot;RGB&quot;</span>) <span class="keyword">for</span> image <span class="keyword">in</span> examples[image_column]]</span><br><span class="line">    examples[<span class="string">&quot;pixel_values&quot;</span>] = [</span><br><span class="line">        train_transforms(image) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br><span class="line">    examples[<span class="string">&quot;input_ids&quot;</span>] = tokenize_captions(examples)</span><br><span class="line">    <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> accelerator.main_process_first():</span><br><span class="line">    <span class="keyword">if</span> args.max_train_samples <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        dataset[<span class="string">&quot;train&quot;</span>] = dataset[<span class="string">&quot;train&quot;</span>].shuffle(</span><br><span class="line">            seed=args.seed).select(<span class="built_in">range</span>(args.max_train_samples))</span><br><span class="line">    <span class="comment"># Set the training transforms</span></span><br><span class="line">    train_dataset = dataset[<span class="string">&quot;train&quot;</span>].with_transform(preprocess_train)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>之后函数用预处理过的数据集创建 <code>DataLoader</code>。这里要注意的参数是 batch size <code>args.train_batch_size</code> 和读取数据的进程数 <code>args.dataloader_num_workers</code> 。这两个参数的用法和一般的 PyTorch 项目一样。<code>args.train_batch_size</code> 决定了训练速度，一般设置到不爆显存的最大值。如果要读取的数据过多，导致数据读取成为了模型训练的速度瓶颈，则应该提高 <code>args.dataloader_num_workers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">examples</span>):</span></span><br><span class="line">    pixel_values = torch.stack([example[<span class="string">&quot;pixel_values&quot;</span>]</span><br><span class="line">                                <span class="keyword">for</span> example <span class="keyword">in</span> examples])</span><br><span class="line">    pixel_values = pixel_values.to(</span><br><span class="line">        memory_format=torch.contiguous_format).<span class="built_in">float</span>()</span><br><span class="line">    input_ids = torch.stack([example[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> example <span class="keyword">in</span> examples])</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;pixel_values&quot;</span>: pixel_values, <span class="string">&quot;input_ids&quot;</span>: input_ids&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataLoaders creation:</span></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(</span><br><span class="line">    train_dataset,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=collate_fn,</span><br><span class="line">    batch_size=args.train_batch_size,</span><br><span class="line">    num_workers=args.dataloader_num_workers,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>如果想用更大的 batch size，显存又不够，则可以使用梯度累计技术。使用这项技术时，训练梯度不会每步优化，而是累计了若干步后再优化。<code>args.gradient_accumulation_steps</code> 表示要累计几步再优化模型。实际的 batch size 等于输入 batch size 乘 GPU 数乘梯度累计步数。下面的代码维护了训练步数有关的信息，并创建了学习率调度器。我们按照默认设置使用一个常量学习率即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scheduler and math around the number of training steps.</span></span><br><span class="line">overrode_max_train_steps = <span class="literal">False</span></span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line"><span class="keyword">if</span> args.max_train_steps <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch</span><br><span class="line">    overrode_max_train_steps = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    args.lr_scheduler,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,</span><br><span class="line">    num_training_steps=args.max_train_steps * accelerator.num_processes,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare everything with our `accelerator`.</span></span><br><span class="line">unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(</span><br><span class="line">    unet, optimizer, train_dataloader, lr_scheduler</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We need to recalculate our total training steps as the size of the training dataloader may have changed.</span></span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line"><span class="keyword">if</span> overrode_max_train_steps:</span><br><span class="line">    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch</span><br><span class="line"><span class="comment"># Afterwards we recalculate our number of training epochs</span></span><br><span class="line">args.num_train_epochs = math.ceil(</span><br><span class="line">    args.max_train_steps / num_update_steps_per_epoch)</span><br></pre></td></tr></table></figure>
<p>在准备工作的最后，函数会用 accelerate 库记录配置信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    accelerator.init_trackers(<span class="string">&quot;text2image-fine-tune&quot;</span>, config=<span class="built_in">vars</span>(args))</span><br></pre></td></tr></table></figure>
<p>终于，要开始训练了。训练开始前，函数会准备全局变量并记录日志。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train!</span></span><br><span class="line">total_batch_size = args.train_batch_size * \</span><br><span class="line">    accelerator.num_processes * args.gradient_accumulation_steps</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;***** Running training *****&quot;</span>)</span><br><span class="line">...</span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line">first_epoch = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>此时，如果设置了 <code>args.resume_from_checkpoint</code>，则函数会读取之前训练过的权重。一般继续训练时可以把该参数设为 <code>latest</code>，程序会自动找最新的权重。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Potentially load in the weights and states from a previous save</span></span><br><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint:</span><br><span class="line">    <span class="keyword">if</span> args.resume_from_checkpoint != <span class="string">&quot;latest&quot;</span>:</span><br><span class="line">        path = ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the most recent checkpoint</span></span><br><span class="line">        path = ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        args.resume_from_checkpoint = <span class="literal">None</span></span><br><span class="line">        initial_global_step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        accelerator.load_state(os.path.join(args.output_dir, path))</span><br><span class="line">        global_step = <span class="built_in">int</span>(path.split(<span class="string">&quot;-&quot;</span>)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        initial_global_step = global_step</span><br><span class="line">        first_epoch = global_step // num_update_steps_per_epoch</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    initial_global_step = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>随后，函数根据总步数和已经训练过的步数设置迭代器，正式进入训练循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">progress_bar = tqdm(</span><br><span class="line">    <span class="built_in">range</span>(<span class="number">0</span>, args.max_train_steps),</span><br><span class="line">    initial=initial_global_step,</span><br><span class="line">    desc=<span class="string">&quot;Steps&quot;</span>,</span><br><span class="line">    <span class="comment"># Only show the progress bar once on each machine.</span></span><br><span class="line">    disable=<span class="keyword">not</span> accelerator.is_local_main_process,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(first_epoch, args.num_train_epochs):</span><br><span class="line">    unet.train()</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        <span class="keyword">with</span> accelerator.accumulate(unet):</span><br></pre></td></tr></table></figure>
<p>训练的过程基本和 LDM 论文中展示的一致。一开始，要取出图像<code>batch[&quot;pixel_values&quot;]</code> 并用 VAE 把它压缩进隐空间。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert images to latent space</span></span><br><span class="line">latents = vae.encode(batch[<span class="string">&quot;pixel_values&quot;</span>].to(</span><br><span class="line">    dtype=weight_dtype)).latent_dist.sample()</span><br><span class="line">latents = latents * vae.config.scaling_factor</span><br></pre></td></tr></table></figure><br>再随机生成一个噪声。该噪声会套入扩散模型前向过程的公式，和输入图像一起得到 <code>t</code> 时刻的带噪图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sample noise that we&#x27;ll add to the latents</span></span><br><span class="line">noise = torch.randn_like(latents)</span><br></pre></td></tr></table></figure>
<p>下一步，这里插入了一个提升扩散模型训练质量的小技巧，用上它后输出图像的颜色分布会更合理。原理见注释中的链接。<code>args.noise_offset</code> 默认为 0。如果要启用这个特性，一般令 <code>args.noise_offset = 0.1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.noise_offset:</span><br><span class="line">    <span class="comment"># https://www.crosslabs.org//blog/diffusion-with-offset-noise</span></span><br><span class="line">    noise += args.noise_offset * torch.randn(</span><br><span class="line">        (latents.shape[<span class="number">0</span>], latents.shape[<span class="number">1</span>], <span class="number">1</span>, <span class="number">1</span>), device=latents.device</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>然后是时间戳的随机生成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bsz = latents.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># Sample a random timestep for each image</span></span><br><span class="line">timesteps = torch.randint(</span><br><span class="line">    <span class="number">0</span>, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)</span><br><span class="line">timesteps = timesteps.long()</span><br></pre></td></tr></table></figure></p>
<p>时间戳和前面随机生成的噪声一起经 DDPM 的前向过程得到带噪图片 <code>noisy_latents</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add noise to the latents according to the noise magnitude at each timestep</span></span><br><span class="line"><span class="comment"># (this is the forward diffusion process)</span></span><br><span class="line">noisy_latents = noise_scheduler.add_noise(</span><br><span class="line">    latents, noise, timesteps)</span><br></pre></td></tr></table></figure>
<p>再把文本 <code>batch[&quot;input_ids&quot;]</code> 编码，为之后的 U-Net 前向传播做准备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the text embedding for conditioning</span></span><br><span class="line">encoder_hidden_states = text_encoder(batch[<span class="string">&quot;input_ids&quot;</span>])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>在 U-Net 推理开始前，函数这里做了一个关于 U-Net 输出类型的判断。一般 U-Net 都是输出预测的噪声 <code>epsilon</code>，可以忽略这段代码。当 U-Net 是想预测噪声时，要拟合的目标是之前随机生成的噪声 <code>noise</code> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the target for loss depending on the prediction type</span></span><br><span class="line"><span class="keyword">if</span> args.prediction_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># set prediction_type of scheduler if defined</span></span><br><span class="line">    noise_scheduler.register_to_config(</span><br><span class="line">        prediction_type=args.prediction_type)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> noise_scheduler.config.prediction_type == <span class="string">&quot;epsilon&quot;</span>:</span><br><span class="line">    target = noise</span><br><span class="line"><span class="keyword">elif</span> noise_scheduler.config.prediction_type == <span class="string">&quot;v_prediction&quot;</span>:</span><br><span class="line">    target = noise_scheduler.get_velocity(</span><br><span class="line">        latents, noise, timesteps)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">f&quot;Unknown prediction type <span class="subst">&#123;noise_scheduler.config.prediction_type&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>之后把带噪图像、时间戳、文本编码输入进 U-Net，U-Net 输出预测的噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict the noise residual and compute loss</span></span><br><span class="line">model_pred = unet(noisy_latents, timesteps,</span><br><span class="line">                  encoder_hidden_states).sample</span><br></pre></td></tr></table></figure>
<p>有了预测值，下一步是算 loss。这里又可以选择是否使用一种加速训练的技术。如果使用，则 <code>args.snr_gamma</code> 推荐设置为 5.0。原 DDPM 的做法是直接算预测噪声和真实噪声的均方误差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.snr_gamma <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    loss = F.mse_loss(model_pred.<span class="built_in">float</span>(),</span><br><span class="line">                      target.<span class="built_in">float</span>(), reduction=<span class="string">&quot;mean&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>训练迭代的最后，要用 accelerate 库来完成梯度计算和反向传播。在更新梯度前，可以通过设置 <code>args.max_grad_norm</code> 来裁剪梯度，以防梯度过大。<code>args.max_grad_norm</code> 默认为 1.0。代码中的 <code>if accelerator.sync_gradients:</code> 可以保证所有 GPU 都同步了梯度再执行后续代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Backpropagate</span></span><br><span class="line">accelerator.backward(loss)</span><br><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    params_to_clip = lora_layers</span><br><span class="line">    accelerator.clip_grad_norm_(</span><br><span class="line">        params_to_clip, args.max_grad_norm)</span><br><span class="line">optimizer.step()</span><br><span class="line">lr_scheduler.step()</span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>一步训练结束后，更新和步数相关的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    global_step += <span class="number">1</span></span><br><span class="line">    accelerator.log(&#123;<span class="string">&quot;train_loss&quot;</span>: train_loss&#125;, step=global_step)</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>脚本默认每 <code>args.checkpointing_steps</code> 步保存一次中间结果。当需要保存时，函数会清理多余的 checkpoint，再把模型状态和 LoRA 模型分别保存下来。<code>accelerator.save_state(save_path)</code> 负责把模型及优化器等训练用到的所有状态存下来，后面的 <code>StableDiffusionPipeline.save_lora_weights</code> 负责存储 LoRA 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> global_step % args.checkpointing_steps == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">        <span class="comment"># _before_ saving state, check if this save would set us over the `checkpoints_total_limit`</span></span><br><span class="line">        <span class="keyword">if</span> args.checkpoints_total_limit <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            checkpoints = ...</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(checkpoints) &gt;= args.checkpoints_total_limit:</span><br><span class="line">                <span class="comment"># remove ckpt</span></span><br><span class="line">                ...</span><br><span class="line"></span><br><span class="line">        save_path = os.path.join(</span><br><span class="line">            args.output_dir, <span class="string">f&quot;checkpoint-<span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">        accelerator.save_state(save_path)</span><br><span class="line"></span><br><span class="line">        unwrapped_unet = accelerator.unwrap_model(unet)</span><br><span class="line">        unet_lora_state_dict = convert_state_dict_to_diffusers(</span><br><span class="line">            get_peft_model_state_dict(unwrapped_unet)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        StableDiffusionPipeline.save_lora_weights(</span><br><span class="line">            save_directory=save_path,</span><br><span class="line">            unet_lora_layers=unet_lora_state_dict,</span><br><span class="line">            safe_serialization=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        logger.info(<span class="string">f&quot;Saved state to <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>训练循环的最后，函数会更新进度条上的信息，并根据当前的训练步数决定是否停止训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logs = &#123;<span class="string">&quot;step_loss&quot;</span>: loss.detach().item(</span><br><span class="line">), <span class="string">&quot;lr&quot;</span>: lr_scheduler.get_last_lr()[<span class="number">0</span>]&#125;</span><br><span class="line">progress_bar.set_postfix(**logs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> global_step &gt;= args.max_train_steps:</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>训完每一个 epoch 后，函数会进行验证。默认的验证方法是新建一个图像生成 pipeline，生成一些图片并保存。如果有其他验证方法，如计算某一指标，可以自行编写这部分的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">   <span class="keyword">if</span> args.validation_prompt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> epoch % args.validation_epochs == <span class="number">0</span>:</span><br><span class="line">       logger.info(</span><br><span class="line">           <span class="string">f&quot;Running validation... \n Generating <span class="subst">&#123;args.num_validation_images&#125;</span> images with prompt:&quot;</span></span><br><span class="line">           <span class="string">f&quot; <span class="subst">&#123;args.validation_prompt&#125;</span>.&quot;</span></span><br><span class="line">       )</span><br><span class="line">       pipeline = DiffusionPipeline.from_pretrained(...)</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure>
<p>所有训练结束后，函数会再存一次最终的 LoRA 模型权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save the lora layers</span></span><br><span class="line">accelerator.wait_for_everyone()</span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    unet = unet.to(torch.float32)</span><br><span class="line"></span><br><span class="line">    unwrapped_unet = accelerator.unwrap_model(unet)</span><br><span class="line">    unet_lora_state_dict = convert_state_dict_to_diffusers(</span><br><span class="line">        get_peft_model_state_dict(unwrapped_unet))</span><br><span class="line">    StableDiffusionPipeline.save_lora_weights(</span><br><span class="line">        save_directory=args.output_dir,</span><br><span class="line">        unet_lora_layers=unet_lora_state_dict,</span><br><span class="line">        safe_serialization=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>函数还会再测试一次模型。具体方法和之前的验证是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Final inference</span></span><br><span class="line"><span class="comment"># Load previous pipeline</span></span><br><span class="line"><span class="keyword">if</span> args.validation_prompt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>运行完了这里，函数也就结束了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerator.end_training()</span><br></pre></td></tr></table></figure>
<p>为了方便使用，我把这个脚本改写了一下：删除了部分不常用的功能，并且配置参数能通过配置文件而不是命令行参数传入。新的脚本为项目根目录下的 <code>train_lora.py</code>，示例配置文件在 <code>cfg</code> 目录下。</p>
<p>以 <code>cfg</code> 中的某个配置文件为例，我们来回顾一下训练脚本主要用到的参数：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;log_dir&quot;</span>: <span class="string">&quot;log&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;ckpt&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;data_dir&quot;</span>: <span class="string">&quot;dataset/mountain&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;ckpt_name&quot;</span>: <span class="string">&quot;mountain&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;pretrained_model_name_or_path&quot;</span>: <span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;rank&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="attr">&quot;enable_xformers_memory_efficient_attention&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span>,</span><br><span class="line">    <span class="attr">&quot;adam_beta1&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="attr">&quot;adam_beta2&quot;</span>: <span class="number">0.999</span>,</span><br><span class="line">    <span class="attr">&quot;adam_weight_decay&quot;</span>: <span class="number">1e-2</span>,</span><br><span class="line">    <span class="attr">&quot;adam_epsilon&quot;</span>: <span class="number">1e-08</span>,</span><br><span class="line">    <span class="attr">&quot;resolution&quot;</span>: <span class="number">512</span>,</span><br><span class="line">    <span class="attr">&quot;n_epochs&quot;</span>: <span class="number">200</span>,</span><br><span class="line">    <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;dataloader_num_workers&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;lr_scheduler_name&quot;</span>: <span class="string">&quot;constant&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;resume_from_checkpoint&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">&quot;noise_offset&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="attr">&quot;max_grad_norm&quot;</span>: <span class="number">1.0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要关注的参数：<code>output_dir</code> 为输出 checkpoint 的文件夹，<code>ckpt_name</code> 为输出 checkpoint 的文件名。<code>data_dir</code> 是训练数据集所在文件夹。<code>pretrained_model_name_or_path</code> 为 SD 模型文件夹。<code>rank</code> 是决定 LoRA 大小的参数。<code>learning_rate</code> 是学习率。<code>adam</code> 打头的是 AdamW 优化器的参数。<code>resolution</code> 是训练图片的统一分辨率。<code>n_epochs</code> 是训练的轮数。<code>checkpointing_steps</code> 指每过多久存一次 checkpoint。<code>train_batch_size</code> 是 batch size。<code>gradient_accumulation_steps</code> 是梯度累计步数。</p>
<p>要修改这个配置文件，要先把文件夹的路径改对，填上训练时的分辨率，再通过 <code>gradient_accumulation_steps</code> 和 <code>train_batch_size</code> 决定 batch size，接着填 <code>n_epochs</code> (一般训 10~20 轮就会过拟合)。最后就可以一边改 LoRA 的主要超参数 <code>rank</code> 一边反复训练了。</p>
<h3 id="SD-图像插值"><a href="#SD-图像插值" class="headerlink" title="SD 图像插值"></a>SD 图像插值</h3><p>在这个示例中，我们来实现 DiffMorpher 工作的一小部分，完成一个简单的图像插值工具。在此过程中，我们将学会怎么在单张图片上训练 SD LoRA，以验证我们的训练环境。</p>
<p>这个工具的原理很简单：我们对两张图片分别训练一个 LoRA。之后，为了获取两张图片的插值，我们可以对两张图片 DDIM Inversion 的初始隐变量及两个 LoRA 分别插值，用插值过的隐变量在插值过的 SD LoRA 上生成图片就能得到插值图片。</p>
<p>该示例的所有数据和代码都已经在项目文件夹中给出。首先，我们看一下该怎么在单张图片上训 LoRA。训练之前，我们要准备一个数据集文件夹。数据集文件夹及包含所有图片及一个描述文件 <code>metadata.jsonl</code>。比如单图片的数据集文件夹的结构应如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">├── mountain</span><br><span class="line">│       ├── metadata.jsonl</span><br><span class="line">│       └── mountain.jpg</span><br></pre></td></tr></table></figure>
<p><code>metadata.jsonl</code> 元数据文件的每一行都是一个 json 结构，包含该图片的路径及文本描述。单图片的元数据文件如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>如果是多图片，就应该是：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain_up.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>我们可以运行项目目录下的数据集测试文件 <code>test_dataset.py</code> 来看看 datasets 库的数据集对象包含哪些信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;imagefolder&quot;</span>, data_dir=<span class="string">&quot;dataset/mountain&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>].column_names)</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>][<span class="string">&#x27;image&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>][<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>其输出大致为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Generating train split: 1 examples [00:00, 66.12 examples/s]</span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [&#x27;image&#x27;, &#x27;text&#x27;],</span><br><span class="line">        num_rows: 1</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br><span class="line">[&#x27;image&#x27;, &#x27;text&#x27;]</span><br><span class="line">[&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F0400246670&gt;]</span><br><span class="line">[&#x27;mountain&#x27;]</span><br></pre></td></tr></table></figure></p>
<p>这说明数据集对象实际上是一个词典。默认情况下，数据集放在词典的 <code>train</code> 键下。数据集的 <code>column_names</code> 属性可以返回每项数据有哪些属性。在我们的数据集里，数据的 <code>image</code> 是图像数据，<code>text</code> 是文本标签。训练脚本默认情况下会把每项数据的第一项属性作为图像，第二项属性作为文本标签。我们的这个数据集定义与训练脚本相符。</p>
<p>认识了数据集，我们可以来训练模型了。用下面的两行命令就可以分别在两张图片上训练 LoRA。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python train_lora.py cfg/mountain.json</span><br><span class="line">python train_lora.py cfg/mountain_up.json</span><br></pre></td></tr></table></figure>
<p>如果要用所有显卡训练，则应该用 accelerate。当然，对于这个简单的单图片训练，不需要用那么多显卡。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch train_lora.py cfg/mountain.json</span><br><span class="line">accelerate launch train_lora.py cfg/mountain_up.json</span><br></pre></td></tr></table></figure>
<p>这两个 LoRA 模型的配置文件我们已经在前文见过了。相比普通的风格化 LoRA，这两个 LoRA 的训练轮数非常多，有 200 轮。设置较大的训练轮数能保证模型在单张图片上过拟合。</p>
<p>训练结束后，项目的 <code>ckpt</code> 文件夹下会多出两个 LoRA 权重文件: <code>mountain.safetensor</code>, <code>mountain_up.safetensor</code>。我们可以用它们来做图像插值了。</p>
<p>图像插值的脚本为 <code>morph.py</code>，它的主要内容为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> inversion_pipeline <span class="keyword">import</span> InversionPipeline</span><br><span class="line"></span><br><span class="line">lora_path = <span class="string">&#x27;ckpt/mountain.safetensor&#x27;</span></span><br><span class="line">lora_path2 = <span class="string">&#x27;ckpt/mountain_up.safetensor&#x27;</span></span><br><span class="line">sd_path = <span class="string">&#x27;runwayml/stable-diffusion-v1-5&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pipeline: InversionPipeline = InversionPipeline.from_pretrained(</span><br><span class="line">    sd_path).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path, adapter_name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path2, adapter_name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img1_path = <span class="string">&#x27;dataset/mountain/mountain.jpg&#x27;</span></span><br><span class="line">img2_path = <span class="string">&#x27;dataset/mountain_up/mountain_up.jpg&#x27;</span></span><br><span class="line">prompt = <span class="string">&#x27;mountain&#x27;</span></span><br><span class="line">latent1 = pipeline.inverse(img1_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">latent2 = pipeline.inverse(img2_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">n_frames = <span class="number">10</span></span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_frames + <span class="number">1</span>):</span><br><span class="line">    alpha = i / n_frames</span><br><span class="line">    pipeline.set_adapters([<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>], adapter_weights=[<span class="number">1</span> - alpha, alpha])</span><br><span class="line">    latent = slerp(latent1, latent2, alpha)</span><br><span class="line">    output = pipeline(prompt=prompt, latents=latent,</span><br><span class="line">                      guidance_scale=<span class="number">1.0</span>).images[<span class="number">0</span>]</span><br><span class="line">    images.append(output)</span><br></pre></td></tr></table></figure>
<p>对于每一个 Diffusers 的 Pipeline 类实例，都可以用 <code>pipeline.load_lora_weights</code> 来读取 LoRA 权重。如果我们在同一个模型上使用了多个 LoRA，为了区分它们，我们要加上 <code>adapter_name</code> 参数为每个 LoRA 命名。稍后我们会用到这些名称。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipeline.load_lora_weights(lora_path, adapter_name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path2, adapter_name=<span class="string">&#x27;b&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>读好了文件，使用已经写好的 DDIM Inversion 方法来得到两张图片的初始隐变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img1_path = <span class="string">&#x27;dataset/mountain/mountain.jpg&#x27;</span></span><br><span class="line">img2_path = <span class="string">&#x27;dataset/mountain_up/mountain_up.jpg&#x27;</span></span><br><span class="line">prompt = <span class="string">&#x27;mountain&#x27;</span></span><br><span class="line">latent1 = pipeline.inverse(img1_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">latent2 = pipeline.inverse(img2_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>最后开始生成不同插值比例的图片。根据混合比例 <code>alpha</code>，我们可以用 <code>pipeline.set_adapters([&quot;a&quot;, &quot;b&quot;], adapter_weights=[1 - alpha, alpha])</code> 来融合 LoRA 模型的比例。随后，我们再根据 <code>alpha</code> 对隐变量插值。用插值隐变量在插值 SD LoRA 上生成图片即可得到最终的插值图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_frames = <span class="number">10</span></span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_frames + <span class="number">1</span>):</span><br><span class="line">    alpha = i / n_frames</span><br><span class="line">    pipeline.set_adapters([<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>], adapter_weights=[<span class="number">1</span> - alpha, alpha])</span><br><span class="line">    latent = slerp(latent1, latent2, alpha)</span><br><span class="line">    output = pipeline(prompt=prompt, latents=latent,</span><br><span class="line">                      guidance_scale=<span class="number">1.0</span>).images[<span class="number">0</span>]</span><br><span class="line">    images.append(output)</span><br></pre></td></tr></table></figure>
<p>下面两段动图中，左图和右图分别是无 LoRA 和有 LoRA 的插值结果。可见，通过 LoRA 权重上的插值，图像插值的过度会更加自然。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/output.gif" alt></p>
<h3 id="图片风格迁移"><a href="#图片风格迁移" class="headerlink" title="图片风格迁移"></a>图片风格迁移</h3><p>接下来，我们来实现最流行的 LoRA 应用——风格化 LoRA。当然，训练一个每张随机输出图片都质量很高的模型是很困难的。我们退而求其次，来实现一个能对输入图片做风格迁移的 LoRA 模型。</p>
<p>训练风格化 LoRA 对技术要求不高，其主要难点其实是在数据收集上。大家可以根据自己的需求，准备自己的数据集。我在本文中会分享我的实验结果。我希望把《弹丸论破》的画风——一种颜色渐变较多的动漫画风——应用到一张普通动漫画风的图片上。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/2.png" alt></p>
<p>由于我的目标是拟合画风而不是某一种特定的物体，我直接选取了 50 张左右的游戏 CG 构成训练数据集，且没有对图片做任何处理。训风格化 LoRA 时，文本标签几乎没用，我把所有数据的文本都设置成了游戏名 <code>danganronpa</code>。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;1.png&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;danganronpa&quot;</span>&#125;</span><br><span class="line">...</span><br><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;59.png&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;danganronpa&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>我的配置文件依然和前文的相同，LoRA rank 设置为 8。我一共训了 100 轮，但发现训练后期模型的过拟合很严重，其实令 <code>n_epochs</code> 为 10 到 20 就能有不错的结果。50 张图片训 10 轮最多几十分钟就训完。</p>
<p>由于训练图片的内容不够多样，且图片预处理时加入了随机裁剪，我的 LoRA 模型随机生成的图片质量较低。于是我决定在图像风格迁移任务上测试该模型。具体来说，我使用了 ControlNet Canny 加上图生图 （SDEdit）技术。相关的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableDiffusionControlNetImg2ImgPipeline, ControlNetModel</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lora_path = <span class="string">&#x27;...&#x27;</span></span><br><span class="line">sd_path = <span class="string">&#x27;runwayml/stable-diffusion-v1-5&#x27;</span></span><br><span class="line">controlnet_canny_path = <span class="string">&#x27;lllyasviel/sd-controlnet-canny&#x27;</span></span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&#x27;1 man, look at right, side face, Ace Attorney, Phoenix Wright, best quality, danganronpa&#x27;</span></span><br><span class="line">neg_prompt = <span class="string">&#x27;longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, &#123;multiple people&#125;&#x27;</span></span><br><span class="line">img_path = <span class="string">&#x27;...&#x27;</span></span><br><span class="line">init_image = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">init_image = init_image.resize((<span class="number">768</span>, <span class="number">512</span>))</span><br><span class="line">np_image = np.array(init_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get canny image</span></span><br><span class="line">np_image = cv2.Canny(np_image, <span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line">np_image = np_image[:, :, <span class="literal">None</span>]</span><br><span class="line">np_image = np.concatenate([np_image, np_image, np_image], axis=<span class="number">2</span>)</span><br><span class="line">canny_image = Image.fromarray(np_image)</span><br><span class="line">canny_image.save(<span class="string">&#x27;tmp_edge.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">controlnet = ControlNetModel.from_pretrained(controlnet_canny_path)</span><br><span class="line">pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(</span><br><span class="line">    sd_path, controlnet=controlnet</span><br><span class="line">)</span><br><span class="line">pipe.load_lora_weights(lora_path)</span><br><span class="line"></span><br><span class="line">output = pipe(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    negative_prompt=neg_prompt,</span><br><span class="line">    strength=<span class="number">0.5</span>,</span><br><span class="line">    guidance_scale=<span class="number">7.5</span>,</span><br><span class="line">    controlnet_conditioning_scale=<span class="number">0.5</span>,</span><br><span class="line">    num_inference_steps=<span class="number">50</span>,</span><br><span class="line">    image=init_image,</span><br><span class="line">    cross_attention_kwargs=&#123;<span class="string">&quot;scale&quot;</span>: <span class="number">1.0</span>&#125;,</span><br><span class="line">    control_image=canny_image,</span><br><span class="line">).images[<span class="number">0</span>]</span><br><span class="line">output.save(<span class="string">&quot;tmp.png&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><code>StableDiffusionControlNetImg2ImgPipeline</code> 是 Diffusers 中 ControlNet 加图生图的 Pipeline。使用它生成图片的重要参数有：</p>
<ul>
<li><code>strength</code>：0~1 之间重绘比例。越低越接近输入图片。</li>
<li><code>controlnet_conditioning_scale</code>： 0~1 之间的 ControlNet 约束比例。越高越贴近约束。</li>
<li><code>cross_attention_kwargs=&#123;&quot;scale&quot;: scale&#125;</code>：此处的 <code>scale</code> 是 0~1 之间的 LoRA 混合比例。越高越贴近 LoRA 模型的输出。</li>
</ul>
<p>这里贴一下输入图片和两张编辑后的图片。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/3.png" alt><br><img src="/2024/01/23/20240114-SD-LoRA/4.png" alt><br><img src="/2024/01/23/20240114-SD-LoRA/5.png" alt></p>
<p>可以看出，输出图片中人物的画风确实得到了修改，颜色渐变更加丰富。我在几乎没有调试 LoRA 参数的情况下得到了这样的结果，可见虽然训练一个高质量的随机生成新画风的 LoRA 难度较高，但只是做风格迁移还是比较容易的。</p>
<p>尽管实验的经历不多，我还是基本上了解了 SD LoRA 风格化的能力边界。LoRA 风格化的本质还是修改输出图片的分布，数据集的质量基本上决定了生成的质量，其他参数的影响不会很大（包括训练图片的文本标签）。数据集最好手动裁剪至 512x512。如果想要生成丰富的风格化内容而不是只生成人物，就要丰富训练数据，减少人物数据的占比。训练时，最容易碰到的机器学习上的问题是过拟合问题。解决此问题的最简单的方式是早停，即不用最终的训练结果而用中间某一步的结果。如果你想实现改变输出数据分布以外的功能，比如精确生成某类物体、向模型中加入一些改变画风的关键词，那你应该使用更加先进的技术，而不仅仅是用最基本的 LoRA 微调。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LoRA 是当今深度学习领域中常见的技术。对于 SD，LoRA 则是能够编辑单幅图片、调整整体画风，或者是通过修改训练目标来实现更强大的功能。LoRA 的原理非常简单，它其实就是用两个参数量较少的矩阵来描述一个大参数矩阵在微调中的变化量。Diffusers 库提供了非常便利的 SD LoRA 训练脚本。相信读完了本文后，我们能知道如何用 Diffusers 训练 LoRA，修改训练中的主要参数，并在简单的单图片 LoRA 编辑任务上验证训练的正确性。利用这些知识，我们也能把 LoRA 拓展到风格化生成及其他应用上。</p>
<p>本文的项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA">https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/23/20230713-SD3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/23/20230713-SD3/" class="post-title-link" itemprop="url">Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-23 19:42:00" itemprop="dateCreated datePublished" datetime="2024-01-23T19:42:00+08:00">2024-01-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>看完了Stable Diffusion的论文，在最后这篇文章里，我们来学习Stable Diffusion的代码实现。具体来说，我们会学习Stable Diffusion官方仓库及Diffusers开源库中有关采样算法和U-Net的代码，而不会学习有关训练、VAE、text encoder (CLIP) 的代码。如今大多数工作都只会用到预训练的Stable Diffusion，只学采样算法和U-Net代码就能理解大多数工作了。</p>
<blockquote>
<p>建议读者在阅读本文之前了解DDPM、ResNet、U-Net、Transformer。</p>
<p>本文用到的Stable Diffusion版本是v1.5。Diffusers版本是0.25.0。为了提升可读性，本文对源代码做了一定的精简，部分不会运行到的分支会被略过。</p>
</blockquote>
<h2 id="算法梳理"><a href="#算法梳理" class="headerlink" title="算法梳理"></a>算法梳理</h2><p>在正式读代码之前，我们先用伪代码梳理一下Stable Diffusion的采样过程，并回顾一下U-Net架构的组成。实现Stable Diffusion的代码库有很多，各个库之间的API差异很大。但是，它们实际上都是在描述同一个算法，同一个模型。如果我们理解了算法和模型本身，就可以在学习时主动去找一个算法对应哪一段代码，而不是被动地去理解每一行代码在干什么。</p>
<h3 id="LDM-采样算法"><a href="#LDM-采样算法" class="headerlink" title="LDM 采样算法"></a>LDM 采样算法</h3><p>让我们从最早的DDPM开始，一步一步还原Latent Diffusion Model (LDM)的采样算法。DDPM的采样算法如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddpm_sample</span>(<span class="params">image_shape</span>):</span></span><br><span class="line">  ddpm_scheduler = DDPMScheduler()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  xt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> T ... <span class="number">1</span>:</span><br><span class="line">    eps = unet(xt, t)</span><br><span class="line">    std = ddpm_scheduler.get_std(t)</span><br><span class="line">    xt = ddpm_scheduler.get_xt_prev(xt, t, eps, std)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>在DDPM的实现中，一般会有一个类专门维护扩散模型的$\alpha, \beta$等变量。我们这里把这个类称为<code>DDPMScheduler</code>。此外，DDPM会用到一个U-Net神经网络<code>unet</code>，用于计算去噪过程中图像应该去除的噪声<code>eps</code>。准备好这两个变量后，就可以用<code>randn()</code>从标准正态分布中采样一个纯噪声图像<code>xt</code>。它会被逐渐去噪，最终变成一幅图片。去噪过程中，时刻<code>t</code>会从总时刻<code>T</code>遍历至<code>1</code>(总时刻<code>T</code>一般取<code>1000</code>)。在每一轮去噪步骤中，U-Net会根据这一时刻的图像<code>xt</code>和当前时间戳<code>t</code>估计出此刻应去除的噪声<code>eps</code>，根据<code>xt</code>和<code>eps</code>就能知道下一步图像的均值。除了均值，我们还要获取下一步图像的方差，这一般可以从DDPM调度类中直接获取。有了下一步图像的均值和方差，我们根据DDPM的公式，就能采样出下一步的图像。反复执行去噪循环，<code>xt</code>会从纯噪声图像变成一幅有意义的图像。</p>
<p>DDIM对DDPM的采样过程做了两点改进：1) 去噪的有效步数可以少于<code>T</code>步，由另一个变量<code>ddim_steps</code>决定；2) 采样的方差大小可以由<code>eta</code>决定。因此，改进后的DDIM算法可以写成这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddim_sample</span>(<span class="params">image_shape, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  xt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> timesteps:</span><br><span class="line">    eps = unet(xt, t)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    xt = ddim_scheduler.get_xt_prev(xt, t, eps, std)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>其中，<code>ddim_steps</code>是去噪循环的执行次数。根据<code>ddim_steps</code>，DDIM调度器可以生成所有被使用到的<code>t</code>。比如对于<code>T=1000, ddim_steps=20</code>，被使用到的就只有<code>[1000, 950, 900, ..., 50]</code>这20个时间戳，其他时间戳就可以跳过不算了。<code>eta</code>会被用来计算方差，一般这个值都会设成<code>0</code>。</p>
<blockquote>
<p>DDIM是早期的加速扩散模型采样的算法。如今有许多比DDIM更好的采样方法，但它们多数都保留了<code>steps</code>和<code>eta</code>这两个参数。因此，在使用所有采样方法时，我们可以不用关心实现细节，只关注多出来的这两个参数。</p>
</blockquote>
<p>在DDIM的基础上，LDM从生成像素空间上的图像变为生成隐空间上的图像。隐空间图像需要再做一次解码才能变回真实图像。从代码上来看，使用LDM后，只需要多准备一个VAE，并对最后的隐空间图像<code>zt</code>解码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_ddim_sample</span>(<span class="params">image_shape, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  vae = VAE()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  zt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> timesteps:</span><br><span class="line">    eps = unet(zt, t)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure></p>
<p>而想用LDM实现文生图，则需要给一个额外的文本输入<code>text</code>。文本编码器会把文本编码成张量<code>c</code>，输入进<code>unet</code>。其他地方的实现都和之前的LDM一样。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  vae = VAE()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  zt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line"></span><br><span class="line">  text_encoder = CLIP()</span><br><span class="line">  c = text_encoder.encode(text)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> t = timesteps:</span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>最后这个能实现文生图的LDM就是我们熟悉的Stable Diffusion。Stable Diffusion的采样算法看上去比较复杂，但如果能够从DDPM开始把各个功能都拆开来看，理解起来就不是那么困难了。</p>
<h3 id="U-Net-结构组成"><a href="#U-Net-结构组成" class="headerlink" title="U-Net 结构组成"></a>U-Net 结构组成</h3><p>Stable Diffusion代码实现中的另一个重点是去噪网络U-Net的实现。仿照上一节的学习方法，我们来逐步学习Stable Diffusion中的U-Net是怎么从最经典的纯卷积U-Net逐渐发展而来的。</p>
<p>最早的U-Net的结构如下图所示：</p>
<p><img src="/2024/01/23/20230713-SD3/0-1.jpg" alt></p>
<p>可以看出，U-Net的结构有以下特点：</p>
<ul>
<li>整体上看，U-Net由若干个大层组成。特征在每一大层会被下采样成尺寸更小的特征，再被上采样回原尺寸的特征。整个网络构成一个U形结构。</li>
<li>下采样后，特征的通道数会变多。一般情况下，每次下采样后图像尺寸减半，通道数翻倍。上采样过程则反之。</li>
<li>为了防止信息在下采样的过程中丢失，U-Net每一大层在下采样前的输出会作为额外输入拼接到每一大层上采样前的输入上。这种数据连接方式类似于ResNet中的「短路连接」。</li>
</ul>
<p>DDPM则使用了一种改进版的U-Net。改进主要有两点：</p>
<ul>
<li>原来的卷积层被替换成了ResNet中的残差卷积模块。每一大层有若干个这样的子模块。对于较深的大层，残差卷积模块后面还会接一个自注意力模块。</li>
<li>原来模型每一大层只有一个短路连接。现在每个大层下采样部分的每个子模块的输出都会额外输入到其对称的上采样部分的子模块上。直观上来看，就是短路连接更多了一点，输入信息更不容易在下采样过程中丢失。</li>
</ul>
<p><img src="/2024/01/23/20230713-SD3/0-2.jpg" alt></p>
<p>最后，LDM提出了一种给U-Net添加额外约束信息的方法：把U-Net中的自注意力模块换成交叉注意力模块。具体来说，DDPM的U-Net的自注意力模块被换成了标准的Transformer模块。约束信息$C$可以作为Cross Attention的K, V输入进模块中。</p>
<p>Stable Diffusion的U-Net还在结构上有少许修改，该U-Net的每一大层都有Transformer块，而不是只有较深的大层有。</p>
<p><img src="/2024/01/23/20230713-SD3/0-3.jpg" alt></p>
<p>至此，我们已经学完了Stable Diffusion的采样原理和U-Net结构。接下来我们来看一看它们在不同框架下的代码实现。</p>
<h2 id="Stable-Diffusion-官方-GitHub-仓库"><a href="#Stable-Diffusion-官方-GitHub-仓库" class="headerlink" title="Stable Diffusion 官方 GitHub 仓库"></a>Stable Diffusion 官方 GitHub 仓库</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>克隆仓库后，照着官方Markdown文档安装即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:CompVis/stable-diffusion.git</span><br></pre></td></tr></table></figure>
<p>先用下面的命令创建conda环境，此后<code>ldm</code>环境就是运行Stable Diffusiion的conda环境。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yaml</span><br><span class="line">conda activate ldm</span><br></pre></td></tr></table></figure>
<p>之后去网上下一个Stable Diffusion的模型文件。比较常见一个版本是v1.5，该模型在Hugging Face上：<a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5">https://huggingface.co/runwayml/stable-diffusion-v1-5</a> （推荐下载<code>v1-5-pruned.ckpt</code>）。下载完毕后，把模型软链接到指定位置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p models/ldm/stable-diffusion-v1/</span><br><span class="line">ln -s &lt;path/to/model.ckpt&gt; models/ldm/stable-diffusion-v1/model.ckpt </span><br></pre></td></tr></table></figure>
<p>准备完毕后，只要输入下面的命令，就可以生成实现文生图了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/txt2img.py --prompt &quot;a photograph of an astronaut riding a horse&quot; </span><br></pre></td></tr></table></figure>
<p>在默认的参数下，“一幅骑着马的飞行员的照片”的绘制结果会被保存在<code>outputs/txt2img-samples</code>中。你也可以通过<code>--outdir &lt;dir&gt;</code>参数来指定输出到的文件夹。我得到的一些绘制结果为：</p>
<p><img src="/2024/01/23/20230713-SD3/1.jpg" alt></p>
<blockquote>
<p>如果你在安装时碰到了错误，可以在搜索引擎上或者GitHub的issue里搜索，一般都能搜到其他人遇到的相同错误。</p>
</blockquote>
<h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><p>接下来，我们来探究一下<code>scripts/txt2img.py</code>的执行过程。为了方便阅读，我们可以简化代码中的命令行处理，得到下面这份精简代码。（你可以把这份代码复制到仓库根目录下的一个新Python脚本里并直接运行。别忘了修改代码中的模型路径）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> omegaconf <span class="keyword">import</span> OmegaConf</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm, trange</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> seed_everything</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> autocast</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ldm.util <span class="keyword">import</span> instantiate_from_config</span><br><span class="line"><span class="keyword">from</span> ldm.models.diffusion.ddim <span class="keyword">import</span> DDIMSampler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model_from_config</span>(<span class="params">config, ckpt, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loading model from <span class="subst">&#123;ckpt&#125;</span>&quot;</span>)</span><br><span class="line">    pl_sd = torch.load(ckpt, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;global_step&quot;</span> <span class="keyword">in</span> pl_sd:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Global Step: <span class="subst">&#123;pl_sd[<span class="string">&#x27;global_step&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    sd = pl_sd[<span class="string">&quot;state_dict&quot;</span>]</span><br><span class="line">    model = instantiate_from_config(config.model)</span><br><span class="line">    m, u = model.load_state_dict(sd, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(m) &gt; <span class="number">0</span> <span class="keyword">and</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;missing keys:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(m)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(u) &gt; <span class="number">0</span> <span class="keyword">and</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;unexpected keys:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(u)</span><br><span class="line"></span><br><span class="line">    model.cuda()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    seed = <span class="number">42</span></span><br><span class="line">    config = <span class="string">&#x27;configs/stable-diffusion/v1-inference.yaml&#x27;</span></span><br><span class="line">    ckpt = <span class="string">&#x27;ckpt/v1-5-pruned.ckpt&#x27;</span></span><br><span class="line">    outdir = <span class="string">&#x27;tmp&#x27;</span></span><br><span class="line">    n_samples = batch_size = <span class="number">3</span></span><br><span class="line">    n_rows = batch_size</span><br><span class="line">    n_iter = <span class="number">2</span></span><br><span class="line">    prompt = <span class="string">&#x27;a photograph of an astronaut riding a horse&#x27;</span></span><br><span class="line">    data = [batch_size * [prompt]]</span><br><span class="line">    scale = <span class="number">7.5</span></span><br><span class="line">    C = <span class="number">4</span></span><br><span class="line">    f = <span class="number">8</span></span><br><span class="line">    H = W = <span class="number">512</span></span><br><span class="line">    ddim_steps = <span class="number">50</span></span><br><span class="line">    ddim_eta = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    seed_everything(seed)</span><br><span class="line"></span><br><span class="line">    config = OmegaConf.load(config)</span><br><span class="line">    model = load_model_from_config(config, ckpt)</span><br><span class="line"></span><br><span class="line">    device = torch.device(</span><br><span class="line">        <span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    sampler = DDIMSampler(model)</span><br><span class="line"></span><br><span class="line">    os.makedirs(outdir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    outpath = outdir</span><br><span class="line"></span><br><span class="line">    sample_path = os.path.join(outpath, <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    os.makedirs(sample_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    grid_count = <span class="built_in">len</span>(os.listdir(outpath)) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    start_code = <span class="literal">None</span></span><br><span class="line">    precision_scope = autocast</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">with</span> precision_scope(<span class="string">&quot;cuda&quot;</span>):</span><br><span class="line">            <span class="keyword">with</span> model.ema_scope():</span><br><span class="line">                all_samples = <span class="built_in">list</span>()</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> trange(n_iter, desc=<span class="string">&quot;Sampling&quot;</span>):</span><br><span class="line">                    <span class="keyword">for</span> prompts <span class="keyword">in</span> tqdm(data, desc=<span class="string">&quot;data&quot;</span>):</span><br><span class="line">                        uc = <span class="literal">None</span></span><br><span class="line">                        <span class="keyword">if</span> scale != <span class="number">1.0</span>:</span><br><span class="line">                            uc = model.get_learned_conditioning(</span><br><span class="line">                                batch_size * [<span class="string">&quot;&quot;</span>])</span><br><span class="line">                        <span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">                            prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">                        c = model.get_learned_conditioning(prompts)</span><br><span class="line">                        shape = [C, H // f, W // f]</span><br><span class="line">                        samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                                         conditioning=c,</span><br><span class="line">                                                         batch_size=n_samples,</span><br><span class="line">                                                         shape=shape,</span><br><span class="line">                                                         verbose=<span class="literal">False</span>,</span><br><span class="line">                                                         unconditional_guidance_scale=scale,</span><br><span class="line">                                                         unconditional_conditioning=uc,</span><br><span class="line">                                                         eta=ddim_eta,</span><br><span class="line">                                                         x_T=start_code)</span><br><span class="line"></span><br><span class="line">                        x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br><span class="line">                        x_samples_ddim = torch.clamp(</span><br><span class="line">                            (x_samples_ddim + <span class="number">1.0</span>) / <span class="number">2.0</span>, <span class="built_in">min</span>=<span class="number">0.0</span>, <span class="built_in">max</span>=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">                        all_samples.append(x_samples_ddim)</span><br><span class="line">                grid = torch.stack(all_samples, <span class="number">0</span>)</span><br><span class="line">                grid = rearrange(grid, <span class="string">&#x27;n b c h w -&gt; (n b) c h w&#x27;</span>)</span><br><span class="line">                grid = make_grid(grid, nrow=n_rows)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># to image</span></span><br><span class="line">                grid = <span class="number">255.</span> * rearrange(grid, <span class="string">&#x27;c h w -&gt; h w c&#x27;</span>).cpu().numpy()</span><br><span class="line">                img = Image.fromarray(grid.astype(np.uint8))</span><br><span class="line">                img.save(os.path.join(outpath, <span class="string">f&#x27;grid-<span class="subst">&#123;grid_count:04&#125;</span>.png&#x27;</span>))</span><br><span class="line">                grid_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Your samples are ready and waiting for you here: \n<span class="subst">&#123;outpath&#125;</span> \n&quot;</span></span><br><span class="line">          <span class="string">f&quot; \nEnjoy.&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>抛开前面一大堆初始化操作，代码的核心部分只有下面几行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">uc = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> scale != <span class="number">1.0</span>:</span><br><span class="line">    uc = model.get_learned_conditioning(</span><br><span class="line">        batch_size * [<span class="string">&quot;&quot;</span>])</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">    prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">c = model.get_learned_conditioning(prompts)</span><br><span class="line">shape = [C, H // f, W // f]</span><br><span class="line">samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                  conditioning=c,</span><br><span class="line">                                  batch_size=n_samples,</span><br><span class="line">                                  shape=shape,</span><br><span class="line">                                  verbose=<span class="literal">False</span>,</span><br><span class="line">                                  unconditional_guidance_scale=scale,</span><br><span class="line">                                  unconditional_conditioning=uc,</span><br><span class="line">                                  eta=ddim_eta,</span><br><span class="line">                                  x_T=start_code)</span><br><span class="line"></span><br><span class="line">x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br></pre></td></tr></table></figure>
<p>我们来逐行分析一下这段代码。一开始的几行是执行Classifier-Free Guidance (CFG)。<code>uc</code>表示的是CFG中的无约束下的约束张量。<code>scale</code>表示的是执行CFG的程度，<code>scale</code>不等于<code>1.0</code>即表示启用CFG。<code>model.get_learned_conditioning</code>表示用CLIP把文本编码成张量。对于文本约束的模型，无约束其实就是输入文本为空字符串(<code>&quot;&quot;</code>)。因此，在代码中，若启用了CFG，则会用CLIP编码空字符串，编码结果为<code>uc</code>。</p>
<blockquote>
<p>如果你没学过CFG，也不用担心。你可以暂时不要去理解上面这段话。等读完了后文中有关CFG的代码后，你差不多就能理解CFG的用法了。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uc = None</span><br><span class="line">if scale != 1.0:</span><br><span class="line">    uc = model.get_learned_conditioning(</span><br><span class="line">        batch_size * [&quot;&quot;])</span><br></pre></td></tr></table></figure>
<p>之后的几行是在把用户输入的文本编码成张量。同样，<code>model.get_learned_conditioning</code>表示用CLIP把输入文本编码成张量<code>c</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">    prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">c = model.get_learned_conditioning(prompts)</span><br></pre></td></tr></table></figure></p>
<p>接着是用扩散模型的采样器生成图片。在这份代码中，<code>sampler</code>是DDIM采样器，<code>sampler.sample</code>函数直接完成了图像生成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">shape = [C, H // f, W // f]</span><br><span class="line">samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                  conditioning=c,</span><br><span class="line">                                  batch_size=n_samples,</span><br><span class="line">                                  shape=shape,</span><br><span class="line">                                  verbose=<span class="literal">False</span>,</span><br><span class="line">                                  unconditional_guidance_scale=scale,</span><br><span class="line">                                  unconditional_conditioning=uc,</span><br><span class="line">                                  eta=ddim_eta,</span><br><span class="line">                                  x_T=start_code)</span><br></pre></td></tr></table></figure>
<p>最后，LDM生成的隐空间图片被VAE解码成真实图片。函数<code>model.decode_first_stage</code>负责图片解码。<code>x_samples_ddim</code>在后续的代码中会被后处理成正确格式的RGB图片，并输出至文件里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br></pre></td></tr></table></figure>
<p>Stable Diffusion 官方实现的主函数主要就做了这些事情。这份实现还是有一些凌乱的。采样算法的一部分内容被扔到了主函数里，另一部分放到了DDIM采样器里。在阅读官方实现的源码时，既要去读主函数里的内容，也要去读采样器里的内容。</p>
<p>接下来，我们来看一看DDIM采样器的部分代码，学完采样算法的剩余部分的实现。</p>
<h3 id="DDIM-采样器"><a href="#DDIM-采样器" class="headerlink" title="DDIM 采样器"></a>DDIM 采样器</h3><p>回头看主函数的前半部分，DDIM采样器是在下面的代码里导入的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ldm.models.diffusion.ddim <span class="keyword">import</span> DDIMSampler</span><br></pre></td></tr></table></figure><br>跳转到<code>ldm/models/diffusion/ddim.py</code>文件，我们可以找到<code>DDIMSampler</code>类的实现。</p>
<p>先看一下这个类的构造函数。构造函数主要是把U-Net <code>model</code>给存了下来。后文中的<code>self.model</code>都指的是U-Net。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model, schedule=<span class="string">&quot;linear&quot;</span>, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.model = model</span><br><span class="line">    self.ddpm_num_timesteps = model.num_timesteps</span><br><span class="line">    self.schedule = schedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># in main</span></span><br><span class="line"></span><br><span class="line">config = OmegaConf.load(config)</span><br><span class="line">model = load_model_from_config(config, ckpt)</span><br><span class="line">model = model.to(device)</span><br><span class="line">sampler = DDIMSampler(model)</span><br></pre></td></tr></table></figure></p>
<p>再沿着类的<code>self.sample</code>方法，看一下DDIM采样的实现代码。以下是<code>self.sample</code>方法的主要内容。这个方法其实就执行了一个<code>self.make_schedule</code>，之后把所有参数原封不动地传到了<code>self.ddim_sampling</code>里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">            S,</span></span></span><br><span class="line"><span class="params"><span class="function">            batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">            shape,</span></span></span><br><span class="line"><span class="params"><span class="function">            conditioning=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            ...</span></span></span><br><span class="line"><span class="params"><span class="function">            </span>):</span></span><br><span class="line">    <span class="keyword">if</span> conditioning <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)</span><br><span class="line">    <span class="comment"># sampling</span></span><br><span class="line">    C, H, W = shape</span><br><span class="line">    size = (batch_size, C, H, W)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Data shape for DDIM sampling is <span class="subst">&#123;size&#125;</span>, eta <span class="subst">&#123;eta&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    samples, intermediates = self.ddim_sampling(...)</span><br></pre></td></tr></table></figure>
<p><code>self.make_schedule</code>用于预处理扩散模型的中间计算参数。它的大部分实现细节可以略过。DDIM用到的有效时间戳列表就是在这个函数里设置的，该列表通过<code>make_ddim_timesteps</code>获取，并保存在<code>self.ddim_timesteps</code>中。此外，由<code>ddim_eta</code>决定的扩散模型的方差也是在这个方法里设置的。大致扫完这个方法后，我们可以直接跳到<code>self.ddim_sampling</code>的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span></span><br><span class="line">    self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,</span><br><span class="line">                                              num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>穿越重重的嵌套，我们总算能看到DDIM采样的实现方法<code>self.ddim_sampling</code>了。它的主要内容如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddim_sampling</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">    device = self.model.betas.device</span><br><span class="line">    b = shape[<span class="number">0</span>]</span><br><span class="line">    img = torch.randn(shape, device=device)</span><br><span class="line">    timesteps = self.ddim_timesteps</span><br><span class="line">    intermediates = ...</span><br><span class="line">    time_range = np.flip(timesteps)</span><br><span class="line">    total_steps = timesteps.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    iterator = tqdm(time_range, desc=<span class="string">&#x27;DDIM Sampler&#x27;</span>, total=total_steps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, step <span class="keyword">in</span> <span class="built_in">enumerate</span>(iterator):</span><br><span class="line">        index = total_steps - i - <span class="number">1</span></span><br><span class="line">        ts = torch.full((b,), step, device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        outs = self.p_sample_ddim(img, cond, ts, ...)</span><br><span class="line">        img, pred_x0 = outs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img, intermediates</span><br></pre></td></tr></table></figure>
<p>这段代码和我们之前自己写的伪代码非常相似。一开始，方法获取了在<code>make_schedule</code>里初始化的DDIM有效时间戳列表<code>self.ddim_timesteps</code>，并预处理成一个<code>iterator</code>。该迭代器用于控制DDIM去噪循环。每一轮循环会根据当前时刻的图像<code>img</code>和时间戳<code>ts</code>计算下一步的图像<code>img</code>。具体来说，代码每次用当前的时间戳<code>step</code>创建一个内容全部为<code>step</code>，形状为<code>(b,)</code>的张量<code>ts</code>。该张量会和当前的隐空间图像<code>img</code>，约束信息张量<code>cond</code>一起传给执行一轮DDIM去噪的<code>p_sample_ddim</code>方法。<code>p_sample_ddim</code>方法会返回下一步的图像<code>img</code>。最后，经过多次去噪后，<code>ddim_sampling</code>方法将去噪后的隐空间图像<code>img</code>返回。</p>
<blockquote>
<p><code>p_sample_ddim</code>里的<code>p_sample</code>看上去似乎意义不明，实际上这个叫法来自于DDPM论文。在DDPM论文中，扩散模型的前向过程用字母$q$表示，反向过程用字母$p$表示。因此，反向过程的一轮去噪在代码里被叫做<code>p_sample</code>。</p>
</blockquote>
<p>最后来看一下<code>p_sample_ddim</code>这个方法，它的主体部分如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">p_sample_ddim</span>(<span class="params">self, x, c, t, ...</span>):</span></span><br><span class="line">    b, *_, device = *x.shape, x.device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">        e_t = self.model.apply_model(x, t, c)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">        t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">        c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">        e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">        e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare variables</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># current prediction for x_0</span></span><br><span class="line">    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()</span><br><span class="line">    <span class="keyword">if</span> quantize_denoised:</span><br><span class="line">        pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)</span><br><span class="line">    <span class="comment"># direction pointing to x_t</span></span><br><span class="line">    dir_xt = (<span class="number">1.</span> - a_prev - sigma_t**<span class="number">2</span>).sqrt() * e_t</span><br><span class="line">    noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature</span><br><span class="line">    <span class="keyword">if</span> noise_dropout &gt; <span class="number">0.</span>:</span><br><span class="line">        noise = torch.nn.functional.dropout(noise, p=noise_dropout)</span><br><span class="line">    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise</span><br><span class="line">    <span class="keyword">return</span> x_prev, pred_x0</span><br></pre></td></tr></table></figure>
<p>方法的内容大致可以拆成三段：首先，方法调用U-Net <code>self.model</code>，使用CFG来计算除这一轮该去掉的噪声<code>e_t</code>。然后，方法预处理出DDIM的中间变量。最后，方法根据DDIM的公式，计算出这一轮去噪后的图片<code>x_prev</code>。我们着重看第一部分的代码。</p>
<p>不启用CFG时，方法直接通过<code>self.model.apply_model(x, t, c)</code>调用U-Net，算出这一轮的噪声<code>e_t</code>。而想启用CFG，需要输入空字符串的约束张量<code>unconditional_conditioning</code>，且CFG的强度<code>unconditional_guidance_scale</code>不为1。CFG的执行过程是：对U-Net输入不同的约束<code>c</code>，先用空字符串约束得到一个预测噪声<code>e_t_uncond</code>，再用输入的文本约束得到一个预测噪声<code>e_t</code>。之后令<code>e_t = et_uncond + scale * (e_t - e_t_uncond)</code>。<code>scale</code>大于1，即表明我们希望预测噪声更加靠近有输入文本的那一个。直观上来看，<code>scale</code>越大，最后生成的图片越符合输入文本，越偏离空文本。下面这段代码正是实现了上述这段逻辑，只不过代码使用了一些数据拼接技巧，让空字符串约束下和输入文本约束下的结果在一次U-Net推理中获得。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">    e_t = self.model.apply_model(x, t, c)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">    t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">    c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">    e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">    e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br></pre></td></tr></table></figure></p>
<p><code>p_sample_ddim</code> 方法的后续代码都是在实现下面这个DDIM采样公式。代码工工整整地计算了公式中的<code>predicted_x0</code>, <code>dir_xt</code>, <code>noise</code>，非常易懂，没有需要特别注意的地方。</p>
<p><img src="/2024/01/23/20230713-SD3/1-2.jpg" alt></p>
<p>我们已经看完了<code>p_sample_ddim</code>的代码。该方法可以实现一步去噪操作。多次调用该方法去噪后，我们就能得到生成的隐空间图片。该图片会被返回到main函数里，被VAE的解码器解码成普通图片。至此，我们就学完了Stable Diffusion官方仓库的采样代码。</p>
<p>对照下面这份我们之前写的伪代码，我们再来梳理一下Stable Diffusion官方仓库的代码逻辑。官方仓库的采样代码一部分在main函数里，另一部分在<code>ldm/models/diffusion/ddim.py</code>里。main函数主要完成了编码约束文字、解码隐空间图像这两件事。剩下的DDIM采样以及各种Diffusion图像编辑功能都是在<code>ldm/models/diffusion/ddim.py</code>文件中实现的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>)</span></span><br><span class="line"><span class="function">  <span class="title">ddim_scheduler</span> = <span class="title">DDIMScheduler</span>()</span></span><br><span class="line"><span class="function">  <span class="title">vae</span> = <span class="title">VAE</span>()</span></span><br><span class="line"><span class="function">  <span class="title">unet</span> = <span class="title">UNet</span>()</span></span><br><span class="line"><span class="function">  <span class="title">zt</span> = <span class="title">randn</span>(<span class="params">image_shape</span>)</span></span><br><span class="line"><span class="function">  <span class="title">eta</span> = <span class="title">input</span>()</span></span><br><span class="line"><span class="function">  <span class="title">T</span> = 1000</span></span><br><span class="line"><span class="function">  <span class="title">timesteps</span> = <span class="title">ddim_scheduler</span>.<span class="title">get_timesteps</span>(<span class="params">T, ddim_steps</span>) # [1000, 950, 900, ...]</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">text_encoder</span> = <span class="title">CLIP</span>()</span></span><br><span class="line"><span class="function">  <span class="title">c</span> = <span class="title">text_encoder</span>.<span class="title">encode</span>(<span class="params">text</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">for</span> <span class="title">t</span> = <span class="title">timesteps</span>:</span></span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure>
<p>在学习代码时，要着重学习DDIM采样器部分的代码。大部分基于Diffusion的图像编辑技术都是在DDIM采样的中间步骤中做文章，只要学懂了DDIM采样的代码，学相关图像编辑技术就会非常轻松。除此之外，和LDM相关的文字约束编码、隐空间图像编码解码的接口函数也需要熟悉，不少技术会调用到这几项功能。</p>
<p>还有一些Diffusion相关工作会涉及U-Net的修改。接下来，我们就来看Stable Diffusion官方仓库中U-Net的实现。</p>
<h3 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h3><p>我们来回头看一下main函数和DDIM采样中U-Net的调用逻辑。和U-Net有关的代码如下所示。LDM模型类 <code>model</code>在主函数中通过<code>load_model_from_config</code>从配置文件里创建，随后成为了<code>sampler</code>的成员变量。在DDIM去噪循环中，LDM模型里的U-Net会在<code>self.model.apply_model</code>方法里被调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># main.py</span></span><br><span class="line">config = <span class="string">&#x27;configs/stable-diffusion/v1-inference.yaml&#x27;</span></span><br><span class="line">config = OmegaConf.load(config)</span><br><span class="line">model = load_model_from_config(config, ckpt)</span><br><span class="line">sampler = DDIMSampler(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ldm/models/diffusion/ddim.py</span></span><br><span class="line">e_t = self.model.apply_model(x, t, c)</span><br></pre></td></tr></table></figure>
<p>为了知道U-Net是在哪个类里定义的，我们需要打开配置文件 <code>configs/stable-diffusion/v1-inference.yaml</code>。该配置文件有这样一段话：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">target:</span> <span class="string">ldm.models.diffusion.ddpm.LatentDiffusion</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">conditioning_key:</span> <span class="string">crossattn</span></span><br><span class="line">    <span class="attr">unet_config:</span></span><br><span class="line">        <span class="attr">target:</span> <span class="string">ldm.modules.diffusionmodules.openaimodel.UNetModel</span></span><br></pre></td></tr></table></figure></p>
<p>根据这段话，我们知道LDM类定义在<code>ldm/models/diffusion/ddpm.py</code>的<code>LatentDiffusion</code>里，U-Net类定义在<code>ldm/modules/diffusionmodules/openaimodel.py</code>的<code>UNetModel</code>里。一个LDM类有一个U-Net类的实例。我们先简单看一看<code>LatentDiffusion</code>类的实现。</p>
<p><code>ldm/models/diffusion/ddpm.py</code>原本来自DDPM论文的官方仓库，内含<code>DDPM</code>类的实现。<code>DDPM</code>类维护了扩散模型公式里的一些变量，同时维护了U-Net类的实例。LDM的作者基于之前DDPM的代码进行开发，定义了一个继承自<code>DDPM</code>的<code>LatentDiffusion</code>类。除了DDPM本身的功能外，<code>LatentDiffusion</code>还维护了VAE(<code>self.first_stage_model</code>)，CLIP（<code>self.cond_stage_model</code>）。也就是说，<code>LatentDiffusion</code>主要维护了扩散模型中间变量、U-Net、VAE、CLIP这四类信息。这样，所有带参数的模型都在<code>LatentDiffusion</code>里，我们可以从一个checkpoint文件中读取所有的模型的参数。相关代码定义代码如下：</p>
<blockquote>
<p>把所有模型定义在一起有好处也有坏处。好处在于，用户想使用Stable Diffusion时，只需要下载一个checkpoint文件就行了。坏处在于，哪怕用户只改了某个子模型（如U-Net），为了保存整个模型，他还是得把其他子模型一起存下来。这其中存在着信息冗余，十分不灵活。Diffusers框架没有把模型全存在一个文件里，而是放到了一个文件夹里。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPM</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line">    <span class="comment"># classic DDPM with Gaussian diffusion, in image space</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 unet_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ...</span>):</span></span><br><span class="line">        self.model = DiffusionWrapper(unet_config, conditioning_key)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LatentDiffusion</span>(<span class="params">DDPM</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;main class&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cond_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.instantiate_first_stage(first_stage_config)</span><br><span class="line">        self.instantiate_cond_stage(cond_stage_config)</span><br></pre></td></tr></table></figure>
<p>我们主要关注<code>LatentDiffusion</code>类的<code>apply_model</code>方法，它用于调用U-Net <code>self.model</code>。<code>apply_model</code>看上去有很长，但略过了我们用不到的一些代码后，整个方法其实非常短。一开始，方法对输入的约束信息编码<code>cond</code>做了一个前处理，判断约束是哪种类型。如论文里所描述的，LDM支持两种约束：将约束与输入拼接、将约束注入到交叉注意力层中。方法会根据<code>self.model.conditioning_key</code>是<code>concat</code>还是<code>crossattn</code>，使用不同的约束方式。Stable Diffusion使用的是后者，即<code>self.model.conditioning_key == crossattn</code>。做完前处理后，方法执行了<code>x_recon = self.model(x_noisy, t, **cond)</code>。接下来的处理交给U-Net <code>self.model</code>来完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_model</span>(<span class="params">self, x_noisy, t, cond, return_ids=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(cond, <span class="built_in">dict</span>):</span><br><span class="line">        <span class="comment"># hybrid case, cond is exptected to be a dict</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(cond, <span class="built_in">list</span>):</span><br><span class="line">            cond = [cond]</span><br><span class="line">        key = <span class="string">&#x27;c_concat&#x27;</span> <span class="keyword">if</span> self.model.conditioning_key == <span class="string">&#x27;concat&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;c_crossattn&#x27;</span></span><br><span class="line">        cond = &#123;key: cond&#125;</span><br><span class="line"></span><br><span class="line">    x_recon = self.model(x_noisy, t, **cond)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(x_recon, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="keyword">not</span> return_ids:</span><br><span class="line">        <span class="keyword">return</span> x_recon[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x_recon</span><br></pre></td></tr></table></figure>
<p>现在，我们跳转到<code>ldm/modules/diffusionmodules/openaimodel.py</code>的<code>UNetModel</code>类里。<code>UNetModel</code>只定义了神经网络层的运算，没有多余的功能。我们只需要看它的<code>__init__</code>方法和<code>forward</code>方法。我们先来看较为简短的<code>forward</code>方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = self.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">    h = x.<span class="built_in">type</span>(self.dtype)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> self.input_blocks:</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">        hs.append(h)</span><br><span class="line">    h = self.middle_block(h, emb, context)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> self.output_blocks:</span><br><span class="line">        h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">    h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">    <span class="keyword">return</span> self.out(h)</span><br></pre></td></tr></table></figure>
<p><code>forward</code>方法的输入是<code>x, timesteps, context</code>，分别表示当前去噪时刻的图片、当前时间戳、文本约束编码。根据这些输入，<code>forward</code>会输出当前时刻应去除的噪声<code>eps</code>。一开始，方法会先对<code>timesteps</code>使用Transformer论文中介绍的位置编码<code>timestep_embedding</code>，得到时间戳的编码<code>t_emb</code>。<code>t_emb</code>再经过几个线性层，得到最终的时间戳编码<code>emb</code>。而<code>context</code>已经是CLIP处理过的编码，它不需要做额外的预处理。时间戳编码<code>emb</code>和文本约束编码<code>context</code>随后会注入到U-Net的所有中间模块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = self.time_embed(t_emb)</span><br></pre></td></tr></table></figure>
<p>经过预处理后，方法开始处理U-Net的计算。中间结果<code>h</code>会经过U-Net的下采样模块<code>input_blocks</code>，每一个子模块的临时输出都会被保存进一个栈<code>hs</code>里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> h = x.<span class="built_in">type</span>(self.dtype)</span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> self.input_blocks:</span><br><span class="line">    h = module(h, emb, context)</span><br><span class="line">    hs.append(h)</span><br></pre></td></tr></table></figure>
<p>接着，<code>h</code>会经过U-Net的中间模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h = self.middle_block(h, emb, context)</span><br></pre></td></tr></table></figure>
<p>随后，<code>h</code>开始经过U-Net的上采样模块<code>output_blocks</code>。此时每一个编码器子模块的临时输出会从栈<code>hs</code>里弹出，作为对应解码器子模块的额外输入。额外输入<code>hs.pop()</code>会与中间结果<code>h</code>拼接到一起输入进子模块里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> self.output_blocks:</span><br><span class="line">    h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">    h = module(h, emb, context)</span><br><span class="line">h = h.<span class="built_in">type</span>(x.dtype)</span><br></pre></td></tr></table></figure>
<p>最后，<code>h</code>会被输出层转换成一个通道数正确的<code>eps</code>张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> self.out(h)</span><br></pre></td></tr></table></figure>
<p>这段代码的数据连接图如下所示：</p>
<p><img src="/2024/01/23/20230713-SD3/1-3.jpg" alt></p>
<p>在阅读<code>__init__</code>前，我们先看一下待会会用到的另一个模块类<code>TimestepEmbedSequential</code>的定义。在PyTorch中，一系列输入和输出都只有一个变量的模块在串行连接时，可以用串行模块类<code>nn.Sequential</code>来把多个模块合并简化成一个模块。而在扩散模型中，多数模块的输入是<code>x, t, c</code>三个变量，输出是一个变量。为了也能用类似的串行模块类把扩散模型的模块合并在一起，代码中包含了一个<code>TimestepEmbedSequential</code>类。它的行为类似于<code>nn.Sequential</code>，只不过它支持<code>x, t, c</code>的输入。<code>forward</code>中用到的多数模块都是通过<code>TimestepEmbedSequential</code>创建的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimestepEmbedSequential</span>(<span class="params">nn.Sequential, TimestepBlock</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, TimestepBlock):</span><br><span class="line">                x = layer(x, emb)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(layer, SpatialTransformer):</span><br><span class="line">                x = layer(x, context)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>看完了数据的计算过程，我们回头来看各个子模块在<code>__init__</code>方法中是怎么被详细定义的。<code>__init__</code>的主要内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNetModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.time_embed = nn.Sequential(</span><br><span class="line">            linear(model_channels, time_embed_dim),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(time_embed_dim, time_embed_dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.input_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                TimestepEmbedSequential(</span><br><span class="line">                    conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                )</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">                layers = [</span><br><span class="line">                    ResBlock(...)]</span><br><span class="line">                ch = mult * model_channels</span><br><span class="line">                <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                     layers.append(</span><br><span class="line">                        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...))</span><br><span class="line"></span><br><span class="line">                self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">            <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">                out_ch = ch</span><br><span class="line">                self.input_blocks.append(</span><br><span class="line">                    TimestepEmbedSequential(</span><br><span class="line">                        ResBlock(...)</span><br><span class="line">                        <span class="keyword">if</span> resblock_updown</span><br><span class="line">                        <span class="keyword">else</span> Downsample(...)</span><br><span class="line">                    )</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">        self.middle_block = TimestepEmbedSequential(</span><br><span class="line">            ResBlock(...),</span><br><span class="line">            AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...),</span><br><span class="line">            ResBlock(...),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.output_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">enumerate</span>(channel_mult))[::-<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks + <span class="number">1</span>):</span><br><span class="line">                ich = input_block_chans.pop()</span><br><span class="line">                layers = [</span><br><span class="line">                    ResBlock(...)</span><br><span class="line">                ]</span><br><span class="line">                ch = model_channels * mult</span><br><span class="line">                <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                    layers.append(</span><br><span class="line">                        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...)</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">if</span> level <span class="keyword">and</span> i == num_res_blocks:</span><br><span class="line">                    out_ch = ch</span><br><span class="line">                    layers.append(</span><br><span class="line">                        ResBlock(...)</span><br><span class="line">                        <span class="keyword">if</span> resblock_updown</span><br><span class="line">                        <span class="keyword">else</span> Upsample(...)</span><br><span class="line">                    )</span><br><span class="line">                    ds //= <span class="number">2</span></span><br><span class="line">                self.output_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">    self.out = nn.Sequential(</span><br><span class="line">            normalization(ch),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            zero_module(conv_nd(dims, model_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p><code>__init__</code>方法的代码很长。在阅读这样的代码时，我们不需要每一行都去细读，只需要理解代码能拆成几块，每一块在做什么即可。<code>__init__</code>方法其实就是定义了<code>forward</code>中用到的5个模块，我们一个一个看过去即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNetModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.time_embed = ...</span><br><span class="line"></span><br><span class="line">        self.input_blocks = nn.ModuleList(...)</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">        self.middle_block = ...</span><br><span class="line"></span><br><span class="line">        self.output_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">enumerate</span>(channel_mult))[::-<span class="number">1</span>]:</span><br><span class="line">            ...</span><br><span class="line">    self.out = ...</span><br></pre></td></tr></table></figure>
<p>先来看<code>time_embed</code>。回忆一下，在<code>forward</code>里，输入的整数时间戳会被正弦编码<code>timestep_embedding</code>（即Transformer中的位置编码）编码成一个张量。之后，时间戳编码处理模块<code>time_embed</code>用于进一步提取时间戳编码的特征。从下面的代码中可知，它本质上就是一个由两个普通线性层构成的模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.time_embed = nn.Sequential(</span><br><span class="line">            linear(model_channels, time_embed_dim),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(time_embed_dim, time_embed_dim),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>再来看U-Net最后面的输出模块<code>out</code>。输出模块的结构也很简单，它主要包含了一个卷积层，用于把中间变量的通道数从<code>dims</code>变成<code>model_channels</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.out = nn.Sequential(</span><br><span class="line">            normalization(ch),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            zero_module(conv_nd(dims, model_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>接下来，我们把目光聚焦在U-Net的三个核心模块上：<code>input_blocks</code>, <code>middle_block</code>, <code>output_blocks</code>。这三个模块的组成都很类似，都用到了残差块<code>ResBlock</code>和注意力块。稍有不同的是，<code>input_blocks</code>的每一大层后面都有一个下采样模块，<code>output_blocks</code>的每一大层后面都有一个上采样模块。上下采样模块的结构都很常规，与经典的U-Net无异。我们把学习的重点放在残差块和注意力块上。我们先看这两个模块的内部实现细节，再来看它们是怎么拼接起来的。</p>
<p>Stable Diffusion的U-Net中的<code>ResBlock</code>和原DDPM的U-Net的<code>ResBlock</code>功能完全一样，都是在普通残差块的基础上，支持时间戳编码的额外输入。具体来说，普通的残差块是由两个卷积模块和一条短路连接构成的，即<code>y = x + conv(conv(x))</code>。如果经过两个卷积块后数据的通道数发生了变化，则要在短路连接上加一个转换通道数的卷积，即<code>y = conv(x) + conv(conv(x))</code>。</p>
<p>在这种普通残差块的基础上，扩散模型中的残差块还支持时间戳编码<code>t</code>的输入。为了把<code>t</code>和输入<code>x</code>的信息融合在一起，<code>t</code>会和经过第一个卷积后的中间结果<code>conv(x)</code>加在一起。可是，<code>t</code>的通道数和<code>conv(x)</code>的通道数很可能会不一样。通道数不一样的数据是不能直接加起来的。为此，每一个残差块中都有一个用于转换<code>t</code>通道数的线性层。这样，<code>t</code>和<code>conv(x)</code>就能相加了。整个模块的计算可以表示成<code>y=conv(x) + conv(conv(x) + linear(t))</code>。残差块的示意图和源代码如下：</p>
<p><img src="/2024/01/23/20230713-SD3/1-4.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span>(<span class="params">TimestepBlock</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        self.in_layers = nn.Sequential(</span><br><span class="line">            normalization(channels),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            conv_nd(dims, channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.emb_layers = nn.Sequential(</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(</span><br><span class="line">                emb_channels,</span><br><span class="line">                <span class="number">2</span> * self.out_channels <span class="keyword">if</span> use_scale_shift_norm <span class="keyword">else</span> self.out_channels,</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        self.out_layers = nn.Sequential(</span><br><span class="line">            normalization(self.out_channels),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            zero_module(</span><br><span class="line">                conv_nd(dims, self.out_channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.out_channels == channels:</span><br><span class="line">            self.skip_connection = nn.Identity()</span><br><span class="line">        <span class="keyword">elif</span> use_conv:</span><br><span class="line">            self.skip_connection = conv_nd(</span><br><span class="line">                dims, channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.skip_connection = conv_nd(dims, channels, self.out_channels, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb</span>):</span></span><br><span class="line">        h = self.in_layers(x)</span><br><span class="line">        emb_out = self.emb_layers(emb).<span class="built_in">type</span>(h.dtype)</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(emb_out.shape) &lt; <span class="built_in">len</span>(h.shape):</span><br><span class="line">            emb_out = emb_out[..., <span class="literal">None</span>]</span><br><span class="line">        h = h + emb_out</span><br><span class="line">        h = self.out_layers(h)</span><br><span class="line">        <span class="keyword">return</span> self.skip_connection(x) + h</span><br></pre></td></tr></table></figure>
<p>代码中的<code>in_layers</code>是第一个卷积模块，<code>out_layers</code>是第二个卷积模块。<code>skip_connection</code>是用于调整短路连接通道数的模块。若输入输出的通道数相同，则该模块是一个恒等函数，不对数据做任何修改。<code>emb_layers</code>是调整时间戳编码通道数的线性层模块。这些模块的定义都在<code>ResBlock</code>的<code>__init__</code>里。它们的结构都很常规，没有值得注意的地方。我们可以着重阅读模型的<code>forward</code>方法。</p>
<p>如前文所述，在<code>forward</code>中，输入<code>x</code>会先经过第一个卷积模块<code>in_layers</code>，再与经过了<code>emb_layers</code>调整的时间戳编码<code>emb</code>相加后，输入进第二个卷积模块<code>out_layers</code>。最后，做完计算的数据会和经过了短路连接的原输入<code>skip_connection(x)</code>加在一起，作为整个残差块的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb</span>):</span></span><br><span class="line">    h = self.in_layers(x)</span><br><span class="line">    emb_out = self.emb_layers(emb).<span class="built_in">type</span>(h.dtype)</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(emb_out.shape) &lt; <span class="built_in">len</span>(h.shape):</span><br><span class="line">        emb_out = emb_out[..., <span class="literal">None</span>]</span><br><span class="line">    h = h + emb_out</span><br><span class="line">    h = self.out_layers(h)</span><br><span class="line">    <span class="keyword">return</span> self.skip_connection(x) + h</span><br></pre></td></tr></table></figure>
<p>这里有一点实现细节需要注意。时间戳编码<code>emb_out</code>的形状是<code>[n, c]</code>。为了把它和形状为<code>[n, c, h, w]</code>的图片加在一起，需要把它的形状变成<code>[n, c, 1, 1]</code>后再相加（形状为<code>[n, c, 1, 1]</code>的数据在与形状为<code>[n, c, h, w]</code>的数据做加法时形状会被自动广播成<code>[n, c, h, w]</code>）。在PyTorch中，<code>x=x[..., None]</code>可以在一个数据最后加一个长度为1的维度。比如对于形状为<code>[n, c]</code>的<code>t</code>，<code>t[..., None]</code>的形状就会是<code>[n, c, 1]</code>。</p>
<p>残差块的内容到此结束。我们接着来看注意力模块。在看模块的具体实现之前，我们先看一下源代码中有哪几种注意力模块。在U-Net的代码中，注意力模型是用以下代码创建的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">    layers.append(</span><br><span class="line">        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>第一行<code>if ds in attention_resolutions:</code>用于控制在U-Net的哪几个大层。Stable Diffusion每一大层都用了注意力模块，可以忽略这一行。随后，代码根据是否设置<code>use_spatial_transformer</code>来创建<code>AttentionBlock</code>或是<code>SpatialTransformer</code>。<code>AttentionBlock</code>是DDPM中采样的普通自注意力模块，而<code>SpatialTransformer</code>是LDM中提出的支持额外约束的标准Transfomer块。Stable Diffusion使用的是<code>SpatialTransformer</code>。我们就来看一看这个模块的实现细节。</p>
<p>如前所述，<code>SpatialTransformer</code>使用的是<strong>标准</strong>的Transformer块，它和Transformer中的Transformer块完全一致。输入<code>x</code>先经过一个自注意力层，再过一个交叉注意力层。在此期间，约束编码<code>c</code>会作为交叉注意力层的<code>K, V</code>输入进模块。最后，数据经过一个全连接层。每一层的输入都会和输出做一个残差连接。</p>
<p><img src="/2024/01/23/20230713-SD3/1-5.jpg" alt></p>
<p>当然，标准Transformer是针对一维序列数据的。要把Transformer用到图像上，则需要把图像的宽高拼接到同一维，即对张量做形状变换<code>n c h w -&gt; n c (h * w)</code>。做完这个变换后，就可以把数据直接输入进Transformer模块了。<br>这些图像数据与序列数据的适配都是在<code>SpatialTransformer</code>类里完成的。<code>SpatialTransformer</code>类并没有直接实现Transformer块的细节，仅仅是U-Net和Transformer块之间的一个过渡。Transformer块的实现在它的一个子模块里。我们来看它的实现代码。</p>
<p><code>SpatialTransformer</code>有两个卷积层<code>proj_in</code>, <code>proj_out</code>，负责图像通道数与Transformer模块通道数之间的转换。<code>SpatialTransformer</code>的<code>transformer_blocks</code>才是真正的Transformer模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpatialTransformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, n_heads, d_head,</span></span></span><br><span class="line"><span class="params"><span class="function">                 depth=<span class="number">1</span>, dropout=<span class="number">0.</span>, context_dim=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        inner_dim = n_heads * d_head</span><br><span class="line">        self.norm = Normalize(in_channels)</span><br><span class="line"></span><br><span class="line">        self.proj_in = nn.Conv2d(in_channels,</span><br><span class="line">                                 inner_dim,</span><br><span class="line">                                 kernel_size=<span class="number">1</span>,</span><br><span class="line">                                 stride=<span class="number">1</span>,</span><br><span class="line">                                 padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim)</span><br><span class="line">                <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(depth)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.proj_out = zero_module(nn.Conv2d(inner_dim,</span><br><span class="line">                                              in_channels,</span><br><span class="line">                                              kernel_size=<span class="number">1</span>,</span><br><span class="line">                                              stride=<span class="number">1</span>,</span><br><span class="line">                                              padding=<span class="number">0</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在<code>forward</code>中，图像数据在进出Transformer模块前后都会做形状和通道数上的适配。运算结束后，结果和输入之间还会做一个残差连接。<code>context</code>就是约束信息编码，它会接入到交叉注意力层上。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">    b, c, h, w = x.shape</span><br><span class="line">    x_in = x</span><br><span class="line">    x = self.norm(x)</span><br><span class="line">    x = self.proj_in(x)</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b c h w -&gt; b (h w) c&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> self.transformer_blocks:</span><br><span class="line">        x = block(x, context=context)</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b (h w) c -&gt; b c h w&#x27;</span>, h=h, w=w)</span><br><span class="line">    x = self.proj_out(x)</span><br><span class="line">    <span class="keyword">return</span> x + x_in</span><br></pre></td></tr></table></figure></p>
<p>每一个Transformer模块的结构完全符合上文的示意图。如果你之前学过Transformer，那这些代码你会十分熟悉。我们快速把这部分代码浏览一遍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, n_heads, d_head, dropout=<span class="number">0.</span>, context_dim=<span class="literal">None</span>, gated_ff=<span class="literal">True</span>, checkpoint=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)  <span class="comment"># is a self-attention</span></span><br><span class="line">        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)</span><br><span class="line">        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,</span><br><span class="line">                                    heads=n_heads, dim_head=d_head, dropout=dropout)  <span class="comment"># is self-attn if context is none</span></span><br><span class="line">        self.norm1 = nn.LayerNorm(dim)</span><br><span class="line">        self.norm2 = nn.LayerNorm(dim)</span><br><span class="line">        self.norm3 = nn.LayerNorm(dim)</span><br><span class="line">        self.checkpoint = checkpoint</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.attn1(self.norm1(x)) + x</span><br><span class="line">        x = self.attn2(self.norm2(x), context=context) + x</span><br><span class="line">        x = self.ff(self.norm3(x)) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>自注意力层和交叉注意力层都是用<code>CrossAttention</code>类实现的。该模块与Transformer论文中的多头注意力机制完全相同。当<code>forward</code>的参数<code>context=None</code>时，模块其实只是一个提取特征的自注意力模块；而当<code>context</code>为约束文本的编码时，模块就是一个根据文本约束进行运算的交叉注意力模块。该模块用不到<code>mask</code>，相关的代码可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, query_dim, context_dim=<span class="literal">None</span>, heads=<span class="number">8</span>, dim_head=<span class="number">64</span>, dropout=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head * heads</span><br><span class="line">        context_dim = default(context_dim, query_dim)</span><br><span class="line"></span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span></span><br><span class="line">        self.heads = heads</span><br><span class="line"></span><br><span class="line">        self.to_q = nn.Linear(query_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_k = nn.Linear(context_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_v = nn.Linear(context_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, query_dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        h = self.heads</span><br><span class="line"></span><br><span class="line">        q = self.to_q(x)</span><br><span class="line">        context = default(context, x)</span><br><span class="line">        k = self.to_k(context)</span><br><span class="line">        v = self.to_v(context)</span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; (b h) n d&#x27;</span>, h=h), (q, k, v))</span><br><span class="line"></span><br><span class="line">        sim = einsum(<span class="string">&#x27;b i d, b j d -&gt; b i j&#x27;</span>, q, k) * self.scale</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> exists(mask):</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention, what we cannot get enough of</span></span><br><span class="line">        attn = sim.softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        out = einsum(<span class="string">&#x27;b i j, b j d -&gt; b i d&#x27;</span>, attn, v)</span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;(b h) n d -&gt; b n (h d)&#x27;</span>, h=h)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br></pre></td></tr></table></figure>
<p>Transformer块的内容到此结束。看完了<code>SpatialTransformer</code>和<code>ResBlock</code>，我们可以回头去看模块之间是怎么拼接的了。先来看U-Net的中间块。它其实就是一个<code>ResBlock</code>接一个<code>SpatialTransformer</code>再接一个<code>ResBlock</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.middle_block = TimestepEmbedSequential(</span><br><span class="line">    ResBlock(...),</span><br><span class="line">    SpatialTransformer(...),</span><br><span class="line">    ResBlock(...),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下采样块<code>input_blocks</code>和上采样块<code>output_blocks</code>的结构几乎一模一样，区别只在于每一大层最后是做下采样还是上采样。这里我们以下采样块为例来学习一下这两个块的结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">self.input_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        TimestepEmbedSequential(</span><br><span class="line">            conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        layers = [</span><br><span class="line">            ResBlock(...)]</span><br><span class="line">        ch = mult * model_channels</span><br><span class="line">        <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                layers.append(</span><br><span class="line">                AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...))</span><br><span class="line"></span><br><span class="line">        self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">    <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">        out_ch = ch</span><br><span class="line">        self.input_blocks.append(</span><br><span class="line">            TimestepEmbedSequential(</span><br><span class="line">                ResBlock(...)</span><br><span class="line">                <span class="keyword">if</span> resblock_updown</span><br><span class="line">                <span class="keyword">else</span> Downsample(...)</span><br><span class="line">            )</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>上采样块一开始是一个调整输入图片通道数的卷积层，它的作用和<code>self.out</code>输出层一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.input_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        TimestepEmbedSequential(</span><br><span class="line">            conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>之后正式进行上采样块的构造。此处代码有两层循环，外层循环表示正在构造哪一个大层，内层循环表示正在构造该大层的哪一组模块。也就是说，共有<code>len(channel_mult)</code>个大层，每一大层都有<code>num_res_blocks</code>组相同的模块。在Stable Diffusion中，<code>channel_mult=[1, 2, 4, 4]</code>, <code>num_res_blocks=2</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>每一组模块由一个<code>ResBlock</code>和一个<code>SpatialTransformer</code>构成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">layers = [</span><br><span class="line">    ResBlock(...)</span><br><span class="line">]</span><br><span class="line">ch = mult * model_channels</span><br><span class="line"><span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">    ...</span><br><span class="line">    layers.append(</span><br><span class="line">        SpatialTransformer(...)</span><br><span class="line">    )</span><br><span class="line">self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">...</span><br></pre></td></tr></table></figure><br>构造完每一组模块后，若现在还没到最后一个大层，则添加一个下采样模块。Stable Diffusion有4个大层，只有运行到前3个大层时才会添加下采样模块。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">        out_ch = ch</span><br><span class="line">        self.input_blocks.append(</span><br><span class="line">            TimestepEmbedSequential(</span><br><span class="line">                ResBlock(...)</span><br><span class="line">                <span class="keyword">if</span> resblock_updown</span><br><span class="line">                <span class="keyword">else</span> Downsample(...)</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        ch = out_ch</span><br><span class="line">        input_block_chans.append(ch)</span><br><span class="line">        ds *= <span class="number">2</span></span><br></pre></td></tr></table></figure><br>至此，我们已经学完了Stable Diffusion的U-Net的主要实现代码。让我们来总结一下。U-Net是一种先对数据做下采样，再做上采样的网络结构。为了防止信息丢失，下采样模块和对应的上采样模块之间有残差连接。下采样块、中间块、上采样块都包含了<code>ResBlock</code>和<code>SpatialTransformer</code>两种模块。<code>ResBlock</code>是图像网络中常使用的残差块，而<code>SpatialTransformer</code>是能够融合图像全局信息并融合不同模态信息的Transformer块。Stable Diffusion的U-Net的输入除了有图像外，还有时间戳<code>t</code>和约束编码<code>c</code>。<code>t</code>会先过几个嵌入层和线性层，再输入进每一个<code>ResBlock</code>中。<code>c</code>会直接输入到所有Transformer块的交叉注意力块中。</p>
<p><img src="/2024/01/23/20230713-SD3/0-3.jpg" alt></p>
<h2 id="Diffusers"><a href="#Diffusers" class="headerlink" title="Diffusers"></a>Diffusers</h2><p>Diffusers是由Hugging Face维护的一套Diffusion框架。这个库的代码被封装进了一个Python模块里，我们可以在安装了Diffusers的Python环境中用<code>import diffusers</code>随时调用该库。相比之下，Diffusers的代码架构更加清楚，且各类Stable Diffusion的新技术都会及时集成进Diffusers库中。</p>
<p>由于我们已经在上文中学过了Stable Diffusion官方源码，在学习Diffusers代码时，我们只会大致过一过每一段代码是在做什么，而不会赘述Stable Diffusion的原理。</p>
<h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>安装该库时，不需要克隆仓库，只需要直接用pip即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade diffusers[torch]</span><br></pre></td></tr></table></figure>
<p>之后，随便在某个地方创建一个Python脚本文件，输入官方的示例项目代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DiffusionPipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">pipeline = DiffusionPipeline.from_pretrained(<span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>, torch_dtype=torch.float16)</span><br><span class="line">pipeline.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline(<span class="string">&quot;An image of a squirrel in Picasso style&quot;</span>).images[<span class="number">0</span>].save(<span class="string">&#x27;output.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>运行代码后，”一幅毕加索风格的松鼠图片”的绘制结果会保存在<code>output.jpg</code>中。我得到的结果如下：</p>
<p><img src="/2024/01/23/20230713-SD3/2-1.jpg" alt></p>
<p>在Diffusers中，<code>from_pretrained</code>函数可以直接从Hugging Face的模型仓库中下载预训练模型。比如，示例代码中<code>from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, ...)</code>指的就是从模型仓库<code>https://huggingface.co/runwayml/stable-diffusion-v1-5</code>中获取模型。</p>
<p>如果在当前网络下无法从命令行中访问Hugging Face，可以先想办法在网页上访问上面的模型仓库，手动下载<code>v1-5-pruned.ckpt</code>。之后，克隆Diffusers的GitHub仓库，再用Diffusers的工具把Stable Diffusion模型文件转换成Diffusers支持的模型格式。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:huggingface/diffusers.git</span><br><span class="line">cd diffusers</span><br><span class="line">python scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path &lt;src&gt; --dump_path &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<p>比如，假设你的模型文件存在<code>ckpt/v1-5-pruned.ckpt</code>，你想把输出的Diffusers的模型文件存在<code>ckpt/sd15</code>，则应该输入：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path ckpt/v1-5-pruned.ckpt --dump_path ckpt/sd15 </span><br></pre></td></tr></table></figure><br>之后修改示例脚本中的路径，就可以成功运行了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DiffusionPipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">pipeline = DiffusionPipeline.from_pretrained(<span class="string">&quot;ckpt/sd15&quot;</span>, torch_dtype=torch.float16)</span><br><span class="line">pipeline.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline(<span class="string">&quot;An image of a squirrel in Picasso style&quot;</span>).images[<span class="number">0</span>].save(<span class="string">&#x27;output.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure><br>对于其他的原版SD checkpoint（比如在civitai上下载的），也可以用同样的方式把它们转换成Diffusers兼容的版本。</p>
<h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>Diffusers使用<code>Pipeline</code>来管理一类图像生成算法。和图像生成相关的模块（如U-Net，DDIM采样器）都是<code>Pipeline</code>的成员变量。打开Diffusers版Stable Diffusion模型的配置文件<code>model_index.json</code>（在 <a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json">https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json</a> 网页上直接访问或者在本地的模型文件夹中找到），我们能看到该模型使用的<code>Pipeline</code>:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;StableDiffusionPipeline&quot;</span>,</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py</code>中，我们能找到<code>StableDiffusionPipeline</code>类的定义。所有<code>Pipeline</code>类的代码都非常长，一般我们可以忽略其他部分，只看运行方法<code>__call__</code>里的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">50</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">List</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    guidance_scale: <span class="built_in">float</span> = <span class="number">7.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_images_per_prompt: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    eta: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 0. Default height and width to unet</span></span><br><span class="line">    height = height <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">    width = width <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">    <span class="comment"># to deal with lora scaling and other possible forward hooks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Check inputs. Raise error if not correct</span></span><br><span class="line">    self.check_inputs(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Define call parameters</span></span><br><span class="line">    batch_size = ...</span><br><span class="line"></span><br><span class="line">    device = self._execution_device</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Encode input prompt</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    prompt_embeds, negative_prompt_embeds = self.encode_prompt(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For classifier free guidance, we need to do two forward passes.</span></span><br><span class="line">    <span class="comment"># Here we concatenate the unconditional and text embeddings into a single batch</span></span><br><span class="line">    <span class="comment"># to avoid doing two forward passes</span></span><br><span class="line">    <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">        prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Prepare timesteps</span></span><br><span class="line">    timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Prepare latent variables</span></span><br><span class="line">    num_channels_latents = self.unet.config.in_channels</span><br><span class="line">    latents = self.prepare_latents(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. Prepare extra step kwargs. <span class="doctag">TODO:</span> Logic should ideally just be moved out of the pipeline</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 7. Denoising loop</span></span><br><span class="line">    num_warmup_steps = <span class="built_in">len</span>(timesteps) - num_inference_steps * self.scheduler.order</span><br><span class="line">    self._num_timesteps = <span class="built_in">len</span>(timesteps)</span><br><span class="line">    <span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">        <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br><span class="line">            <span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">            latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># predict the noise residual</span></span><br><span class="line">            noise_pred = self.unet(</span><br><span class="line">                latent_model_input,</span><br><span class="line">                t,</span><br><span class="line">                encoder_hidden_states=prompt_embeds,</span><br><span class="line">                ...</span><br><span class="line">            )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform guidance</span></span><br><span class="line">            <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">                noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">                noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">and</span> self.guidance_rescale &gt; <span class="number">0.0</span>:</span><br><span class="line">                <span class="comment"># Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf</span></span><br><span class="line">                noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># call the callback, if provided</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(timesteps) - <span class="number">1</span> <span class="keyword">or</span> ((i + <span class="number">1</span>) &gt; num_warmup_steps <span class="keyword">and</span> (i + <span class="number">1</span>) % self.scheduler.order == <span class="number">0</span>):</span><br><span class="line">                progress_bar.update()</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> output_type == <span class="string">&quot;latent&quot;</span>:</span><br><span class="line">        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>, generator=generator)[</span><br><span class="line">            <span class="number">0</span></span><br><span class="line">        ]</span><br><span class="line">        image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        image = latents</span><br><span class="line">        has_nsfw_concept = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)</span><br></pre></td></tr></table></figure>
<p>虽然这段代码很长，但代码中的关键内容和我们在本文开头写的伪代码完全一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>)</span></span><br><span class="line"><span class="function">  <span class="title">ddim_scheduler</span> = <span class="title">DDIMScheduler</span>()</span></span><br><span class="line"><span class="function">  <span class="title">vae</span> = <span class="title">VAE</span>()</span></span><br><span class="line"><span class="function">  <span class="title">unet</span> = <span class="title">UNet</span>()</span></span><br><span class="line"><span class="function">  <span class="title">zt</span> = <span class="title">randn</span>(<span class="params">image_shape</span>)</span></span><br><span class="line"><span class="function">  <span class="title">eta</span> = <span class="title">input</span>()</span></span><br><span class="line"><span class="function">  <span class="title">T</span> = 1000</span></span><br><span class="line"><span class="function">  <span class="title">timesteps</span> = <span class="title">ddim_scheduler</span>.<span class="title">get_timesteps</span>(<span class="params">T, ddim_steps</span>) # [1000, 950, 900, ...]</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">text_encoder</span> = <span class="title">CLIP</span>()</span></span><br><span class="line"><span class="function">  <span class="title">c</span> = <span class="title">text_encoder</span>.<span class="title">encode</span>(<span class="params">text</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">for</span> <span class="title">t</span> = <span class="title">timesteps</span>:</span></span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure>
<p>我们可以对照着上面的伪代码来阅读这个方法。经过Diffusers框架本身的一些前处理后，方法先获取了约束文本的编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Encode input prompt</span></span><br><span class="line"><span class="comment"># c = text_encoder.encode(text)</span></span><br><span class="line">prompt_embeds, negative_prompt_embeds = self.encode_prompt(...)</span><br></pre></td></tr></table></figure>
<p>方法再从采样器里获取了要用到的时间戳，并随机生成了一个初始噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Prepare timesteps</span></span><br><span class="line"><span class="comment"># timesteps = ddim_scheduler.get_timesteps(T, ddim_steps)</span></span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Prepare latent variables</span></span><br><span class="line"><span class="comment"># zt = randn(image_shape)</span></span><br><span class="line">num_channels_latents = self.unet.config.in_channels</span><br><span class="line">latents = self.prepare_latents(</span><br><span class="line">    ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>做完准备后，方法进入去噪循环。循环一开始是用U-Net算出当前应去除的噪声<code>noise_pred</code>。由于加入了CFG，U-Net计算的前后有一些对数据形状处理的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">    <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br><span class="line">        <span class="comment"># eps = unet(zt, t, c)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">        latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># predict the noise residual</span></span><br><span class="line">        noise_pred = self.unet(</span><br><span class="line">            latent_model_input,</span><br><span class="line">            t,</span><br><span class="line">            encoder_hidden_states=prompt_embeds,</span><br><span class="line">            ...</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># perform guidance</span></span><br><span class="line">        <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">            noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">            noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">and</span> self.guidance_rescale &gt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="comment"># Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf</span></span><br><span class="line">            noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)</span><br></pre></td></tr></table></figure>
<p>有了应去除的噪声，方法会调用扩散模型采样器对当前的噪声图片进行更新。Diffusers把采样的逻辑全部封装进了采样器的<code>step</code>方法里。对于包括DDIM在内的所有采样器，都可以调用这个通用的接口，完成一步采样。<code>eta</code>等采样器参数会通过<code>**extra_step_kwargs</code>传入采样器的<code>step</code>方法里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># std = ddim_scheduler.get_std(t, eta)</span></span><br><span class="line"><span class="comment"># zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>经过若干次循环后，我们得到了隐空间下的生成图片。我们还需要调用VAE把隐空间图片解码成普通图片。代码中的<code>self.vae.decode(latents / self.vae.config.scaling_factor, ...)</code>用于解码图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> output_type == <span class="string">&quot;latent&quot;</span>:</span><br><span class="line">    image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>, generator=generator)[</span><br><span class="line">        <span class="number">0</span></span><br><span class="line">    ]</span><br><span class="line">    image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    image = latents</span><br><span class="line">    has_nsfw_concept = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)</span><br></pre></td></tr></table></figure>
<p>就这样，我们很快就看完了Diffusers的采样代码。相比之下，Diffusers的封装确实更合理，主要的图像生成逻辑都写在<code>Pipeline</code>类的<code>__call__</code>里，剩余逻辑都封装在VAE、U-Net、采样器等各自的类里。</p>
<h3 id="U-Net-1"><a href="#U-Net-1" class="headerlink" title="U-Net"></a>U-Net</h3><p>接下来我们来看Diffusers中的U-Net实现。还是打开模型配置文件<code>model_index.json</code>，我们可以找到U-Net的类名。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;unet&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;diffusers&quot;</span>,</span><br><span class="line">    <span class="string">&quot;UNet2DConditionModel&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/models/unet_2d_condition.py</code>文件中，我们可以找到类<code>UNet2DConditionModel</code>。由于Diffusers集成了非常多新特性，整个文件就像一锅大杂烩一样，掺杂着各种功能的实现代码。不过，这份U-Net的实现还是基于原版Stable Diffusion的U-Net进行开发的，原版代码的每一部分都能在这份代码里找到对应。在阅读代码时，我们可以跳过无关的功能，只看我们在Stable Diffusion官方仓库中见过的部分。</p>
<p>先看初始化函数的主要内容。初始化函数依然主要包括<code>time_proj, time_embedding</code>, <code>down_blocks</code>, <code>mid_block</code>, <code>up_blocks</code>, <code>conv_in</code>, <code>conv_out</code>这几个模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet2DConditionModel</span>(<span class="params">ModelMixin, ConfigMixin, UNet2DConditionLoadersMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.conv_in = nn.Conv2d(</span><br><span class="line">            in_channels, block_out_channels[<span class="number">0</span>], kernel_size=conv_in_kernel, padding=conv_in_padding</span><br><span class="line">        )</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">elif</span> time_embedding_type == <span class="string">&quot;positional&quot;</span>:</span><br><span class="line">            self.time_proj = Timesteps(block_out_channels[<span class="number">0</span>], flip_sin_to_cos, freq_shift)</span><br><span class="line">        ...</span><br><span class="line">        self.time_embedding = TimestepEmbedding(...)</span><br><span class="line">        self.down_blocks = nn.ModuleList([])</span><br><span class="line">        self.up_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> i, down_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(down_block_types):</span><br><span class="line">            ...</span><br><span class="line">            down_block = get_down_block(...)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> mid_block_type == ...</span><br><span class="line">            self.mid_block = ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, up_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(up_block_types):</span><br><span class="line">            up_block = get_up_block(...)</span><br><span class="line"></span><br><span class="line">        self.conv_out = nn.Conv2d(...)</span><br></pre></td></tr></table></figure>
<p>其中，较为重要的<code>down_blocks</code>, <code>mid_block</code>, <code>up_blocks</code>都是根据模块类名称来创建的。我们可以在Diffusers的Stable Diffusion模型文件夹的U-Net的配置文件<code>unet/config.json</code>中找到对应的模块类名称。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">&quot;down_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;DownBlock2D&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;mid_block_type&quot;</span>: <span class="string">&quot;UNetMidBlock2DCrossAttn&quot;</span>,</span><br><span class="line">  <span class="string">&quot;up_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;UpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/models/unet_2d_blocks.py</code>中，我们可以找到这几个模块类的定义。和原版代码一样，这几个模块的核心组件都是残差块和Transformer块。在Diffusers中，残差块叫做<code>ResnetBlock2D</code>，Transformer块叫做<code>Transformer2DModel</code>。这几个类的执行逻辑和原版仓库的也几乎一样。比如<code>CrossAttnDownBlock2D</code>的定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttnDownBlock2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            resnets.append(ResnetBlock2D(...))</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> dual_cross_attention:</span><br><span class="line">                attentions.append(Transformer2DModel(...))</span><br></pre></td></tr></table></figure>
<p>接着我们来看U-Net的<code>forward</code>方法。忽略掉其他功能的实现，该方法的主要内容如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        sample: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        timestep: <span class="type">Union</span>[torch.Tensor, <span class="built_in">float</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 0. center input if necessary</span></span><br><span class="line">    <span class="keyword">if</span> self.config.center_input_sample:</span><br><span class="line">        sample = <span class="number">2</span> * sample - <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. time</span></span><br><span class="line">    timesteps = timestep</span><br><span class="line">    t_emb = self.time_proj(timesteps)</span><br><span class="line">    emb = self.time_embedding(t_emb, timestep_cond)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. pre-process</span></span><br><span class="line">    sample = self.conv_in(sample)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. down</span></span><br><span class="line">    down_block_res_samples = (sample,)</span><br><span class="line">    <span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">        sample, res_samples = downsample_block(</span><br><span class="line">            hidden_states=sample,</span><br><span class="line">            temb=emb,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line">        down_block_res_samples += res_samples</span><br><span class="line">    <span class="comment"># 4. mid</span></span><br><span class="line">    sample = self.mid_block(</span><br><span class="line">            sample,</span><br><span class="line">            emb,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. up</span></span><br><span class="line">    <span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">        res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">        down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">        sample = upsample_block(</span><br><span class="line">            hidden_states=sample,</span><br><span class="line">            temb=emb,</span><br><span class="line">            res_hidden_states_tuple=res_samples,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 6. post-process</span></span><br><span class="line">    sample = self.conv_out(sample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> UNet2DConditionOutput(sample=sample)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>该方法和原版仓库的实现差不多，唯一要注意的是栈相关的实现。在方法的下采样计算中，每个<code>downsample_block</code>会返回多个残差输出的元组<code>res_samples</code>，该元组会拼接到栈<code>down_block_res_samples</code>的栈顶。在上采样计算中，代码会根据当前的模块个数，从栈顶一次取出<code>len(upsample_block.resnets)</code>个残差输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">down_block_res_samples = (sample,)</span><br><span class="line"><span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">    sample, res_samples = downsample_block(...)</span><br><span class="line">    down_block_res_samples += res_samples</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">    res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">    down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">    sample = upsample_block(...)</span><br></pre></td></tr></table></figure>
<p>现在，我们已经看完了Diffusers中U-Net的主要内容。可以看出，Diffusers的U-Net包含了很多功能，一般情况下是难以自己更改这些代码的。有没有什么办法能方便地修改U-Net的实现呢？由于很多工作都需要修改U-Net的Attention，Diffusers给U-Net添加了几个方法，用于精确地修改每一个Attention模块的实现。我们来学习一个修改Attention模块的示例。</p>
<p>U-Net类的<code>attn_processors</code>属性会返回一个词典，它的key是每个Attention运算类所在位置，比如<code>down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor</code>，它的value是每个Attention运算类的实例。默认情况下，每个Attention运算类都是<code>AttnProcessor</code>，它的实现在<code>diffusers/models/attention_processor.py</code>文件中。</p>
<p>为了修改Attention运算的实现，我们需要构建一个格式一样的词典<code>attn_processor_dict</code>，再调用<code>unet.set_attn_processor(attn_processor_dict)</code>，取代原来的<code>attn_processors</code>。假如我们自己实现了另一个Attention运算类<code>MyAttnProcessor</code>，我们可以编写下面的代码来修改Attention的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">attn_processor_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">    <span class="keyword">if</span> we_want_to_modify(k):</span><br><span class="line">        attn_processor_dict[k] = MyAttnProcessor()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">unet.set_attn_processor(attn_processor_dict)</span><br></pre></td></tr></table></figure>
<p><code>MyAttnProcessor</code>的唯一要求是，它需要实现一个<code>__call__</code>方法，且方法参数与<code>AttnProcessor</code>的一致。除此之外，我们可以自由地实现Attention处理的细节。一般来说，我们可以先把原来<code>AttnProcessor</code>的实现代码复制过去，再对某些细节做修改。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们学习了Stable Diffusion的原版实现和Diffusers实现的主要内容：采样算法和U-Net。具体来说，在原版仓库中，采样的实现一部分在主函数中，一部分在DDIM采样器类中。U-Net由一个简明的PyTorch模块类实现，其中比较重要的子模块是残差块和Transformer块。相比之下，Diffusers实现的封装更好，功能更多。Diffusers用一个Pipeline类来维护采样过程。Diffusers的U-Net实现与原版完全相同，且支持更复杂的功能。此外，Diffusers还给U-Net提供了精确修改Attention计算的接口。</p>
<p>不管是哪个Stable Diffusion的框架，都会提供一些相同的原子操作。各种基于Stable Diffusion的应用都应该基于这些原子操作开发，而无需修改这些操作的细节。在学习时，我们应该注意这些操作在不同的框架下的写法是怎么样的。常用的原子操作包括：</p>
<ul>
<li>VAE的解码和编码</li>
<li>文本编码器（CLIP）的编码</li>
<li>用U-Net预测当前图像应去除的噪声</li>
<li>用采样器计算下一去噪迭代的图像</li>
</ul>
<p>在原版仓库中，相关的实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VAE的解码和编码</span></span><br><span class="line">model.decode_first_stage(...)</span><br><span class="line">model.encode_first_stage(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本编码器（CLIP）的编码</span></span><br><span class="line">model.get_learned_conditioning(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用U-Net预测当前图像应去除的噪声</span></span><br><span class="line">model.apply_model(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用采样器计算下一去噪迭代的图像</span></span><br><span class="line">p_sample_ddim(...)</span><br></pre></td></tr></table></figure>
<p>在Diffusers中，相关的实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VAE的解码和编码</span></span><br><span class="line">image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">latents = self.vae.encode(image).latent_dist.sample(generator) * self.vae.config.scaling_factor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本编码器（CLIP）的编码</span></span><br><span class="line">self.encode_prompt(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用U-Net预测当前图像应去除的噪声</span></span><br><span class="line">self.unet(..., return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用采样器计算下一去噪迭代的图像</span></span><br><span class="line">self.scheduler.step(..., return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<p>如今zero-shot（无需训练）的Stable Diffusion编辑技术一般只会修改采样算法和Attention计算，需训练的编辑技术有时会在U-Net里加几个模块。只要我们熟悉了普通的Stable Diffusion是怎么样生成图像的，知道原来U-Net的结构是怎么样的，我们在阅读新论文的源码时就可以把这份代码与原来的代码进行对比，只看那些有修改的部分。相信读完了本文后，我们不仅加深了对Stable Diffusion本身的理解，以后学习各种新出的Stable Diffusion编辑技术时也会更加轻松。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/23/20230709-SD2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/23/20230709-SD2/" class="post-title-link" itemprop="url">Stable Diffusion 解读（二）：论文精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-23 19:41:52" itemprop="dateCreated datePublished" datetime="2024-01-23T19:41:52+08:00">2024-01-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上一篇文章中，我们梳理了基于自编码器（AE）的图像生成模型的发展脉络，并引出了Stable Diffusion的核心思想。简单来说，Stable Diffusion是一个两阶段的图像生成模型，它先用一个AE压缩图像，再在压缩图像所在的隐空间上用DDPM生成图像。在这篇文章中，我们来精读Stable Diffusion的论文：<em>High-Resolution Image Synthesis with Latent Diffusion Models</em>。</p>
<blockquote>
<p>注意：如果你从未学习过扩散模型，Stable Diffusion并不是你应该的读的第一篇论文。请参照我[上篇文章]的早期工作总结，至少在学会了DDPM后再来学习Stable Diffusion。</p>
</blockquote>
<h2 id="摘要与引言"><a href="#摘要与引言" class="headerlink" title="摘要与引言"></a>摘要与引言</h2><p>论文摘要的大意如下：扩散模型的生成效果很好，但是，在像素空间上训练和推理扩散模型的计算开销都很大。为了在不降低质量与易用性的前提下用较少的计算资源训练扩散模型，我们在一个预训练过的自编码器的隐空间上使用扩散模型。相较以往的工作，在这种表示下训练扩散模型首次在减少计算复杂度和维持图像细节间达到几近最优的平衡点，极大地提升了视觉保真度。通过向模型架构中引入交叉注意力层，我们把扩散模型变成了强大而灵活的带约束图像生成器，它支持常见的约束，如文字、边界框，且能够以纯卷积方式实现高分辨率的图像合成。我们的<strong>隐扩散模型（latent diffusion model, LDM）</strong> 在使用比像素扩散模型少得多的计算资源的前提下，在各项图像合成任务上取得最优成果或顶尖成果。</p>
<p>整理一下。论文提出了一种叫LDM的图像生成模型。论文想解决的问题是减少像素空间扩散模型的运算开销。为此，LDM借助了VQVAE「先压缩、再生成」的想法，把扩散模型用在AE的隐空间上，在几乎不降低生成质量的前提下减少了计算量。另外，LDM还支持带约束图像合成及纯卷积图像超分辨率。</p>
<p>在上一篇回顾LDM早期工作的文章中，我们已经理解了LDM想解决的问题及解决问题的思路。因此，在读完摘要后，我们接下来读文章时只需要关注LDM的两个创新点：</p>
<ol>
<li>LDM的AE是怎么设计以达到压缩比例与质量的平衡的。</li>
<li>LDM怎么实现带约束的图像合成。</li>
</ol>
<p>引言基本是摘要的扩写。首先，引言大致介绍了图像合成任务的背景，提及了扩散模型近期的突出表现。随后，引言介绍了本文想解决的主要问题：扩散模型的训练和推理太耗时了，需要在不降低效果的前提下减少扩散模型的运算量。最后，引言揭示了本工作的解决方法：使用类似VQGAN的两阶段图像生成方法。</p>
<p>引言的前两部分没有什么关键信息，而最后一部分介绍了本工作改进扩散模型的动机，值得一读。如下图所示，DDPM的论文展示了从不同去噪时刻的同一个噪声图像开始的不同生成结果，比如$\mathbf{x}_{750}$指从时刻$t=750$的去噪图像开始，多次以不同随机数执行DDPM的反向过程，生成的多幅图像。LDM作者认为，DDPM的这一实验表明，扩散模型的图像生成分两个阶段：先是对语义进行压缩，再是对图像的感知细节压缩。正因此，随机对早期的噪声图像去噪，生成图像的内容会更多样；而随机对后期的噪声图像去噪，生成图像只是在细节上有所不同。LDM的作者认为，扩散模型的大量计算都浪费在了生成整幅图像的细节上，不如只让扩散模型描述比较关键的语义压缩部分，而让自编码器（AE）负责感知细节压缩部分。</p>
<p><img src="/2024/01/23/20230709-SD2/1.jpg" alt></p>
<p>引言在结尾总结了本工作的贡献：</p>
<ol>
<li>相比之前按序列处理图像的纯Transformer的方法，扩散模型能更好地处理二维数据。因此，LDM生成隐空间图像时不需要那么重的压缩比例（比如DIV2K数据集上，LDM只需要将图像下采样4倍，而之前的纯Transformer方法要下采样8倍或16倍），图像在压缩时能有更高的保真度，整套方法能更高效地生成高分辨率图像。</li>
<li>在大幅降低计算开销的前提下在多项图像生成任务上取得了顶尖成果。</li>
<li>相比于之前同时训练图像压缩模型和图像生成模型的方法，该方法分步训练两个模型，训练起来更加简单。</li>
<li>对于有着稠密约束的任务（如超分辨率、补全、语义生成），该方法的模型能换成一个纯卷积版本的，且能生成边长为1024的图像。</li>
<li>该工作设计了一种通用的约束机制，该机制基于交叉注意力，支持多模态训练。作者训练了多种带约束的模型。</li>
<li>作者把工作开源了，并提供了预训练模型。</li>
</ol>
<p>我们来整理一下这些贡献。读论文时，可以忽略第6条。第2条是成果，与方法设计无关。第1、3条主要描述了提出两阶段图像生成建模方法的贡献。第4条是把方法拓展到稠密约束任务的贡献。第5条是提出了新约束机制的贡献。所以，在学习论文的方法时，我们还是主要关注摘要里就提过的那两个创新点。在读完引言后，我们可以把阅读目标再细化一下：</p>
<ol>
<li>LDM的AE是怎么设计以达到压缩比例与质量的平衡的。与纯基于Transformer的VQGAN相比，它有什么不同。</li>
<li>LDM怎么用交叉注意力机制实现带约束的图像生成。</li>
</ol>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>作者主要从两个角度回顾了早期工作：不同架构的图像生成模型与两阶段的图像合成方法。其回顾逻辑与本系列的第一篇文章类似，在此就不过多介绍了。除了介绍早期工作外，作者重申了引言中的对比结果，强调了LDM相对于扩散模型的创新和相对于两阶段图像生成模型的创新。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在方法章节中，作者先是大致介绍了使用LDM这种两阶段图像生成架构的优点，再分三部分详细介绍了论文的实现细节：图像压缩AE的实现、LDM的实现、约束的实现。开头的介绍和AE的实现相对比较重要，我们放在一起详细阅读；相对于DDPM，LDM几乎没有做任何修改，只是把要拟合的图片从真实图片换成了压缩图片，这一部分我们会快速浏览一遍；而添加约束的方法有所创新，我们会详细阅读一遍。</p>
<h3 id="AE与两阶段图像生成模型"><a href="#AE与两阶段图像生成模型" class="headerlink" title="AE与两阶段图像生成模型"></a>AE与两阶段图像生成模型</h3><p>我们来先读3.1节，看一看AE的具体实现方法，再回头读第3节开头介绍的两阶段图像生成模型的优点。</p>
<p>LDM配套的图像压缩模型（论文中称之为”感知压缩模型”）和VQGAN几乎完全一样。该压缩模型的原型是一个AE。普通的AE会用原图像和重建图像的重建误差（L1误差或者L2误差）来训练。在普通的AE的基础上，该压缩模型参考了GAN的误差设置方法，使用感知误差代替重建误差，并添加了基于patch的对抗误差。</p>
<p><img src="/2024/01/23/20230709-SD2/2.jpg" alt></p>
<p>但该图像压缩模型的输出与VQGAN有所不同。我们先回忆一下VQGAN的原理。VQGAN的输出会接到Transformer里，Transformer的输入必须是离散的。因此，VQGAN必须要额外完成两件事：1）让连续输出变成离散输出；2）用正则化方法防止过拟合。为此，VQGAN使用了VQVAE里的向量离散化操作，该操作能同时完成这两件事。</p>
<p><img src="/2024/01/23/20230709-SD2/3.jpg" alt></p>
<p>而LDM的压缩模型的输出会接入一个扩散模型里，扩散模型的输入是连续的。因此，LDM的压缩模型只需要额外完成使用正则化方法这一件事。该压缩模型不必像VQGAN一样非得用向量离散化来完成正则化。如我们在第一篇文章中讨论的，作者在LDM的压缩模型中使用了两种正则化方法：VQ正则化与KL正则化。前者来自于VQVAE，后者来自于VAE。</p>
<p><img src="/2024/01/23/20230709-SD2/4.jpg" alt></p>
<p>该压缩模型相较VQGAN有一项明显的优势。VQGAN的Transformer只能按一维序列来处理图像（通过把二维图像reshape成一维），且只能处理较小的压缩图像($16\times16$)。而本身用于二维图像生成的LDM能更好地利用二维信息，因此可以处理更大的压缩图像($64\times 64$)。这样，LDM的压缩模型的压缩程度不必那么重，其保真度会比VQGAN高。</p>
<p>看完了3.1节，我们来回头看第3节开头介绍了LDM的三项优点：1）通过规避在高维图像空间上训练扩散模型，作者开发出了一个因在低维空间上采样而计算效率大幅提升的扩散模型；2）作者发掘了扩散模型中来自U-Net架构的归纳偏置（inductive bias），使得它们能高效地处理有空间结构的数据（比如二维图像），避免像之前基于Transformer的方法一样使用激进、有损质量的压缩比例；3）本工作的压缩模型是通用的，它的隐空间能用来训练多种图像生成模型。第一个优点是相对于DDPM。第二个是优点是相对于使用Transformer的VQGAN，我们在上一段已经分析过了。第三个优点是相对于之前那些换一个任务就需要换一个压缩模型的两阶段图像生成模型。</p>
<blockquote>
<p>归纳偏置可以简单理解为某个学习算法对一类数据的优势。比如CNN结构适合处理图像数据。</p>
</blockquote>
<h3 id="隐扩散模型（LDM）"><a href="#隐扩散模型（LDM）" class="headerlink" title="隐扩散模型（LDM）"></a>隐扩散模型（LDM）</h3><p>在DDPM中，一个参数为$\theta$的神经网络$\epsilon_\theta$会根据当前时刻$t$的带噪图片$x_t$预测本时刻的噪声$\epsilon_\theta(x_t, t)$。网络的学习目标是让预测的噪声和真实的噪声$\epsilon$一致。</p>
<script type="math/tex; mode=display">
L_{DM} = \mathbb{E}_{x, \epsilon \sim \mathcal{N}(0, 1), t}[||\epsilon -\epsilon_\theta(x_t, t)||_2^2]</script><p>LDM的原理和DDPM完全一样，只不过训练图片从像素空间上的真实图片$x_0$变成了隐空间上的压缩图片$z_0$，每一轮的带噪图片由$x_t$变成了隐空间上的带噪图片$z_t$。在训练时，相比DDPM，只需要多对$x_0$用一次编码器变成$z_0$即可。</p>
<script type="math/tex; mode=display">
L_{LDM} = \mathbb{E}_{encode(x), \epsilon \sim \mathcal{N}(0, 1), t}[||\epsilon -\epsilon_\theta(z_t, t)||_2^2]</script><p>如果你在理解这部分内容时有疑问，请去阅读DDPM的相关文章。LDM的具体结构我们会在第三篇代码阅读文章中讨论。</p>
<h3 id="约束机制"><a href="#约束机制" class="headerlink" title="约束机制"></a>约束机制</h3><p>让模型支持带约束图像生成，其实就是想办法把额外的约束信息输入进扩散模型中。显然，最简单的添加约束的方法就是把额外的信息和扩散模型原本的输入$z_t$拼接起来。如果约束是一个值，就把相同的值拼接到$z_t$的每一个像素上；如果约束本身具有空间结构（如语音分割图片），就可以把约束重采样至和$z_t$一样的分辨率，再逐像素拼接。除了直接的拼接外，作者在LDM中还使用了另一种融合约束信息的方法。</p>
<p>DDPM中含有自注意力层。自注意力操作其实基于注意力操作$Attention(Q, K, V)$，它可以解释成一个数据库中存储了许多数据$V$，数据的索引（键）是$K$，现在要用查询$Q$查询数据库里的数据并返回查询结果。注意力操作有几种用法，第一种用法是交叉注意力$CrossAttn(A, B)=Attention(W_Q \cdot A, W_K \cdot B, W_V \cdot B)$，可以理解成数据$A$, $B$做了一次信息融合；第二种用法是自注意力$SelfAttn(A)=Attention(W_Q \cdot A, W_K \cdot A, W_V \cdot A)$，可以理解成数据$A$自己做了一次特征提取。</p>
<p>既然交叉注意力操作可以融合两类信息，何不把DDPM的自注意力层换成交叉注意力层，把$K$, $V$换成来自约束的信息，以实现带约束图像生成呢？如下图所示，通过把用编码器$\tau_\theta$编码过的约束信息输入进扩散模型交叉注意力层的$K$, $V$，LDM实现了带约束图像生成。这里的实现细节我们会在第三篇代码阅读文章中讨论。</p>
<blockquote>
<p>根据论文中实验的设计，对于作用于全局的约束，如文本描述，使用交叉注意力较好；对于有空间信息的约束，如语义分割图片，则用拼接的方式较好。</p>
</blockquote>
<p><img src="/2024/01/23/20230709-SD2/5.jpg" alt></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在这一章里，作者按照介绍方法的顺序，依次探究了图像压缩模型、无约束图像生成、带约束图像合成的实验结果。我们主要关心前两部分的实验结果。</p>
<h3 id="感知压缩程度的折衷"><a href="#感知压缩程度的折衷" class="headerlink" title="感知压缩程度的折衷"></a>感知压缩程度的折衷</h3><p>论文首先讨论了图像压缩模型在不同的下采样比例$f$下的实验结果，其中$f\in\{1, 2, 4, 8, 16, 32\}$。这些实验分两部分，第一部分是训练速度上的实验，第二部分是采样速度与效果上的实验。</p>
<p>在ImageNet上以不同下采样比例$f$训练一定步数后LDM的采样指标对比结果如下图所示。其中，FID指标越低越好，Inception Score越高越好。结果显示，无论下采样比例$f$是过大还是过小都会降低训练速度。作者分析了$f$较小或较大时训练速度慢的原因：$f$过小时扩散模型把过多的精力放在了本应由压缩模型负责的感知压缩上；$f$过大时图像信息在压缩中损失过多。LDM-$\{4\text{-}16\}$的表现相对好一些。</p>
<p><img src="/2024/01/23/20230709-SD2/6.jpg" alt></p>
<p>在实验的第二部分中，作者比较了不同采样比例$f$的LDM在CelebA-HQ（下图左侧）和ImageNet（下图右侧）上的采样速度和采样效果。下图中，横坐标为吞吐量，越靠右表示采样速度越快。同一个模型的不同实验结果表示使用不同DDIM采样步数时的实验结果，每一条线上的结果从右到左分别是DDIM采样步数取$\{10, 20, 50, 100, 200\}$的采样结果（DDIM步数越少，采样速度越快，生成图片质量越低）。对于CelebA-HQ上的实验，若采样步数较多，则还是LDM-$\{4, 8\}$效果较好，只有在采样步数较少时压缩比更高的LDM才有优势。而对于ImageNet上的实验，$f$太小或太大的结果都很差，整体上还是LDM-$\{4, 8\}$的结果较好。</p>
<p><img src="/2024/01/23/20230709-SD2/7.jpg" alt></p>
<p>综上，根据实验，作者认为$f$取适中的$4$或$8$比较妥当。下采样比例$f=8$也正是Stable Diffusion采用的配置。</p>
<h3 id="图像生成效果"><a href="#图像生成效果" class="headerlink" title="图像生成效果"></a>图像生成效果</h3><p>在这一节中，作者在几个常见的数据集上对比了LDM与其他模型的无约束图像生成效果。作者主要比较了两类指标：表示采样质量的FID和表示数据分布覆盖率的精确率及召回率（Precision-and-Recall）。</p>
<p>在介绍具体结果之前，先对这个不太常见的精确率及召回率指标做一个解释。精确率及召回率常用于分类等有确定答案的任务中，分别表示所有被分类为正的样本中有多少是分对了的、所有真值为正的样本中有多少是被成功分类成正的。而无约束图像生成中的精确率及召回率的解释可以参加论文<em>Improved Precision and Recall Metric for Assessing<br>Generative Models</em>。如下图所示，设真实分布为蓝色，生成模型的分布为红色，则红色样本落在蓝色分布的比例为精确率，蓝色样本落在红色分布的比例为召回率。简单来说，精确率能描述采样质量，召回率能描述生成分布与真实分布的覆盖情况。</p>
<p><img src="/2024/01/23/20230709-SD2/8.jpg" alt></p>
<p>接下来，我们回头来看论文展示的无约束图像生成对比结果，如下图所示。整体上看，LDM的表现还不错。虽然在FID指标上无法超过GAN或其他扩散模型，但是在精确率和召回率上还是颇具优势。唯一没有被LDM战胜的是LSUN-Bedrooms上的ADM模型，但作者提到，相比ADM，LDM只用了一半的参数，且只需四分之一的训练资源。</p>
<p><img src="/2024/01/23/20230709-SD2/9.jpg" alt></p>
<h3 id="带约束图像合成"><a href="#带约束图像合成" class="headerlink" title="带约束图像合成"></a>带约束图像合成</h3><p>这一节里，作者展示了LDM的文生图能力。论文中的LDM用了一个从头训练的基于Transformer的文本编码器，与后续使用CLIP的Stable Diffusion差别较大。这一部分的结果没那么重要，大致看一看就好。</p>
<p>本文的文生图模型是一个在LAION-400M数据集上训练的KL约束LDM。它的文本编码器是一个Transformer，编码后的特征会以交叉注意力的形式传入LDM。采样时，LDM使用了Classifier-Free Guidance。</p>
<blockquote>
<p>Classifier-Free Guidance可以让输出图片更符合文本约束。这是一种适用于所有扩散模型的采样策略，并非要和LDM绑定，感兴趣可以去阅读相关论文。</p>
</blockquote>
<p>LDM与其他模型的文生图效果对比如下图所示。虽然这个版本的LDM并没有显著优于其他模型，但它的参数量是最少的。</p>
<p><img src="/2024/01/23/20230709-SD2/10.jpg" alt></p>
<p>LDM在类别约束的图像合成上表现也很不错，超越了当时的其他模型。其结果在此略过。</p>
<p>剩余的带约束图像合成任务都可以看成是图像转图像任务，比如图像超分辨率是低质量图像到高质量图像的转换、语义生成是把语义分割图像转换成一幅合成图像。要添加这些约束，只需要把这些任务的输入图片和LDM原本的输入$z_t$拼接起来即可。比如对于图像超分辨率，可以把输入图片直接与隐空间图片$z_t$拼接，解码后图片会被自然上采样$f$倍；对于语义生成，可以把下采样$f$倍的语义分割图与$z_t$拼接。论文用这些任务上的实验证明了LDM的泛用性。由于这部分实验与LDM的主要知识无关，具体实验结果就不在此详细介绍了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文末尾探讨了LDM的两大不足。首先，尽管LDM的计算需求比其他像素空间上的扩散模型要少得多，但受制于扩散模型本身的串行采样，它的采样速度还是比GAN慢上许多。其次，LDM使用了一个自编码器来压缩图像，重建图像带来的精度损失会成为某些需要精准像素值的任务的性能瓶颈。</p>
<p>论文最后再次总结了此方法的贡献。LDM的主要贡献其实只有两点：在不损失效果的情况下用两阶段的图像生成方法大幅提升了训练和采样效率、借助交叉注意力实现了各任务通用的约束机制。这两个贡献总结得非常精准。之后的Stable Diffusion之所以大受欢迎，第一就是因为它采样所需的计算资源不多，大众能使用消费级显卡完成图像生成，第二就是因为它强大的文字转图片生成效果。</p>
<p>我们再从知识学习的角度总结一下LDM。LDM的核心知识是DDPM和VQGAN。如果你能看懂之前这两篇论文，那你一下子就能明白LDM是的核心思想是什么，看论文时只需要精读交叉注意力约束机制那一段即可，其他实验内容在现在看来已经价值不大了。由于近两年有大量基于Stable Diffusion开发的工作，相比论文，阅读源代码的重要性会大很多。我们会在下一篇文章里详细学习Stable Diffusion的官方源码和最常用的Stable Diffusion第三方实现——Diffusers框架。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/23/20230707-SD1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/23/20230707-SD1/" class="post-title-link" itemprop="url">Stable Diffusion 解读（一）：回顾早期工作</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-23 19:41:17" itemprop="dateCreated datePublished" datetime="2024-01-23T19:41:17+08:00">2024-01-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在2022年的这波AI绘画浪潮中，Stable Diffusion无疑是最受欢迎的图像生成模型。究其原因，第一，Stable Diffusion通过压缩图像尺寸显著提升了扩散模型的运行效率，使得每个用户能在自己的商业级显卡上运行模型；第二，有许多基于Stable Diffusion的应用，比如Stable Diffusion自带的文生图、图像补全，以及ControlNet、LoRA、DreamBooth等插件式应用；第三，得益于前两点，Stable Diffusion已经形成了一个庞大的用户社群，大家互相分享模型，交流心得。</p>
<p>不仅是大众，Stable Diffusion也吸引了大量科研人员，很多本来研究GAN的人纷纷转来研究扩散模型。然而，许多人在学习Stable Diffusion时却犯了难：又是公式扎堆的扩散模型，又是VAE，又是U-Net，这该怎么学起呀？</p>
<p>其实，一上来就读Stable Diffusion是很难读懂的。而如果你把之前的一些更基础的文章读懂，再回头来读Stable Diffusion，就会畅行无阻了。在这篇及之后的几篇文章中，我将从科研的角度对Stable Diffusion做一个全面的解读。在第一篇文章中，我将面向完全没接触过图像生成的读者，从头介绍Stable Diffusion是怎样从早期工作中一步一步诞生的；在第二篇文章中，我将详细解读Stable Diffusion的论文；在最后的第三篇文章中，我将带领大家阅读Stable Diffusion的官方源码，以及一些流行的开源库的Stable Diffusion实现。后续我还会写其他和Stable Diffusion相关的文章，比如ControlNet的介绍。</p>
<h2 id="从自编码器谈起"><a href="#从自编码器谈起" class="headerlink" title="从自编码器谈起"></a>从自编码器谈起</h2><p>包括Stable Diffusion在内，很多图像生成模型都可以看成是一种非常简单的模型——自编码器——的改进版。要谈Stable Diffusion是怎么逐渐诞生的，其实就是在谈自编码器是一步一步进化的。我们的学习就从自编码器开始。</p>
<p>尽管PNG、JPG等图像压缩方法已经非常成熟，但我们会想，会不会还有更好的图像压缩算法呢？图像压缩，其实就是找两个映射，一个把图片<strong>编码</strong>成压缩数据，另一个把压缩数据<strong>解码</strong>回图片。我们知道，神经网络理论上可以拟合任何映射。那我们干脆用两个神经网络来拟合两种映射，以实现一个图像压缩算法。负责编码的神经网络叫<strong>编码器（Encoder）</strong>，负责解码的神经网络叫做<strong>解码器（Decoder）</strong>。</p>
<p><img src="/2024/01/23/20230707-SD1/1.jpg" alt></p>
<p>光定义了神经网络还不够，我们还需要给两个神经网络设置一个学习目标。在运行过程中，神经网络应该满足一个显然的约束：编码再解码后的<strong>重建图像</strong>应该和原图像尽可能一致，即二者的均方误差应该尽可能小。这样，我们只需要随便找一张图片，通过编码器和解码器得到重建图像，就能训练神经网络了。我们不需要给图片打上标签，整个训练过程是自监督的。所以我们说，整套模型是一个<strong>自编码器（Autoencoder，AE）</strong>。</p>
<p><img src="/2024/01/23/20230707-SD1/2.jpg" alt></p>
<p>图像压缩模型AE为什么会和图像生成扯上关系呢？你可以试着把AE的输入图像和编码器遮住，只看解码部分。把一个压缩数据解码成图像，换个角度看，不就是在根据某一数据生成图像嘛。</p>
<p>很可惜，AE并不是一个合格的图像生成模型。我们常说的图像生成，具体是指让程序生成各种各样的图片。为了让程序生成不同的图片，我们一般是让程序根据随机数（或是随机向量）来生成图片。而普通的AE会有<strong>过拟合</strong>现象，这导致AE的解码器只认得训练集里的图片经编码器解码出来的压缩数据，而不认得随机生成的压缩数据，进而也无法达到图像生成的要求。</p>
<p>所谓过拟合，就是指模型只能处理训练数据，而不能推广到一般的数据上。举一个极端的例子，如下图所示，编码器和解码器直接记忆了整个数据集，把所有图片压缩成了一个数字。也就是模型把编码器当成一个图片到数字的词典，把解码器当成一个数字到图片的词典。这样，不管数据集有多大，所有图片都可以被压缩成一个数字。这样的AE确实压缩能力很强，但它完全没用，因为它过拟合了，处理不了训练集以外的数据。</p>
<p><img src="/2024/01/23/20230707-SD1/3.jpg" alt></p>
<p>过拟合现象在普通版AE中是不可避免的。为了利用AE的解码器来生成图片，许多工作都在试图克服AE的过拟合现象。AE的改进思路很多，在这篇文章中，我们仅把AE的改进路线粗略地分成两种：解决过拟合问题以直接用AE做图像生成、用AE压缩图像间接实现图像生成。</p>
<h2 id="第一条路线：VAE-和-DDPM"><a href="#第一条路线：VAE-和-DDPM" class="headerlink" title="第一条路线：VAE 和 DDPM"></a>第一条路线：VAE 和 DDPM</h2><p>在第一条改进路线中，许多后续工作都试图用更高级的数学模型来解决AE的过拟合问题。<strong>变分自编码器（Variational Autoencoder, VAE）</strong> 就是其中的代表。</p>
<p>VAE对AE做了若干改动。第一，VAE让编码器的输出不再是一个确定的数据，而是一个正态分布中的一个随机数据。更具体一点，训练时，编码器会同时输出一个均值和方差。随后，模型会从这个均值和方差表达的正态分布里随机采样一个数据，作为解码器的输入。直观上看，这一改动就是在AE的基础上，让编码器多输出了一个方差，使得原AE编码器的输出发生了一点随机扰动。</p>
<p><img src="/2024/01/23/20230707-SD1/4.jpg" alt></p>
<p>这一改动可以缓解过拟合现象。这是为什么呢？我们可以这样想：原来的AE之所以会过拟合，是因为它强行记住了训练集里每一个数据的编码输出。现在，我们在VAE里让编码器不再输出一个固定值，而是随机输出一个在均值附近的值。这样的话，VAE就不能死记硬背了，必须要找出数据中的规律。</p>
<p>VAE的第二项改动是多添加一个学习目标，让编码器的输出和标准正态分布尽可能相似。前面我们谈过，图像生成模型一般会根据一个随机向量来生成图像。最常用的产生随机向量的方法是去标准正态分布里采样。也就是说，在用VAE生成图像时，我们会抛掉编码器，用下图所示的流程来生成图像。如果我们不约束编码器的输出分布，不让它输出一个和标准正态分布很相近的分布的话，解码器就不能很好地根据来自标准正态分布的随机向量生成图像了。</p>
<p><img src="/2024/01/23/20230707-SD1/5.jpg" alt></p>
<p>综上，VAE对AE做了两项改进：使编码器输出一个正态分布，且该分布要尽可能和标准正态分布相似。训练时，模型从编码器输出的分布里随机采样一个数据作为解码器的输入；图像采样（图像生成）时，模型从标准正态分布里随机采样一个数据作为解码器的输入。VAE的误差函数由两部分组成：原图像和重建图像的重建误差、编码器输出和标准正态分布之间的误差。VAE要最小化重建误差，最大化编码器输出与标准正态分布的相似度。</p>
<p><img src="/2024/01/23/20230707-SD1/6.jpg" alt></p>
<p>分布与分布之间的误差可以用一个叫KL散度的指标表示。所以，在上面那个误差函数公式中，负的相似度应该被替换成KL散度。VAE的这两项改动本质上都是在解决AE的过拟合问题，所以，VAE的改动可以被看成一种正则化方法。我们可以把VAE的正则化方法简称为<strong>KL正则化</strong>。</p>
<blockquote>
<p>在机器学习中，正则化方法就是「降低模型过拟合的方法」的简称。</p>
</blockquote>
<p>VAE确实能减轻AE的过拟合。然而，由于VAE只是让重建图像和原图像的均方误差（重建误差）尽可能小，而没有对重建图像的质量施加更多的约束，VAE的重建结果和图像生成结果都非常模糊。以下是VAE在CelebA数据集上图像生成结果。</p>
<p><img src="/2024/01/23/20230707-SD1/7.jpg" alt></p>
<p>在众多对VAE的改进方法中，一个叫做<strong>去噪扩散概率模型（Denoising Diffusion Probabilistic Model, DDPM）</strong> 的图像生成模型脱颖而出。DDPM正是当今扩散模型的开山鼻祖。我们来看一下DDPM是怎样基于VAE对图像生成建模的。</p>
<p>VAE之所以效果不好，很可能是因为它的约束太少了。VAE的编码和解码都是用神经网络表示的。神经网络是一个黑盒，我们不好对神经网络的中间步骤施加约束，只好在编码器的输出（某个正态分布）和解码器的输出（重建图像）上施加约束。能不能让VAE的编码和解码过程更可控一点呢？</p>
<p>DDPM的设计灵感来自热力学：一个分布可以通过一系列简单的变化（如添加高斯噪声）逐渐变成另一个分布。恰好，VAE的编码器不正是想让来自训练集的图像（训练集分布）变成标准正态分布吗？既然如此，就不要用一个可学习的神经网络来表示VAE的编码器了，干脆用一些预定义好的加噪声操作来表示解码过程。可以从数学上证明，经过了多次加噪声操作后，最后的图像分布会是一个标准正态分布。</p>
<p><img src="/2024/01/23/20230707-SD1/8.jpg" alt></p>
<p>既然编码是加噪声，那解码时就应该去掉噪声。DDPM的解码器也不再是一个不可解释的神经网络，而是一个能预测若干个去噪结果的神经网络。</p>
<p><img src="/2024/01/23/20230707-SD1/9.jpg" alt></p>
<p>相比只有两个约束条件的VAE，DDPM的约束条件就多得多了。在DDPM中，第t个去噪操作应该尽可能抵消掉第t个加噪操作。</p>
<p><img src="/2024/01/23/20230707-SD1/10.jpg" alt></p>
<p>让我们来更具体地认识一下DDPM的学习目标。所谓添加噪声，就是在一个均值约等于当前图像的正态分布上采样。比如要对图像$\mathbf{x}$添加噪声，我们可以在$\mathcal{N}(0.9\mathbf{x},\mathbf{I})$这个分布里采样一张新图像。新的图像每个像素的均值是原来的0.9倍左右，且新图像会出现很多噪声。我们设$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1})$为第$t$步加噪声的正态分布。经过一些数学推导，我们可以求出这一步操作的逆操作$q(\mathbf{x}_{t-1} | \mathbf{x}_{t})$，这个加噪声逆操作也是一个正态分布。既然如此，我们可以设第$t$步去噪声也为一个正态分布$p(\mathbf{x}_{t-1} | \mathbf{x}_{t})$，让第$t$步去噪声和第$t$步加噪声的逆操作尽可能相似。</p>
<p><img src="/2024/01/23/20230707-SD1/11.jpg" alt></p>
<p>总结一下，DDPM对VAE做了如下改动：</p>
<ol>
<li>编码器是一系列不可学习（固定）的加噪声操作</li>
<li>解码器是一系列可学习的去噪声操作</li>
<li>图像尺寸自始至终不变</li>
</ol>
<p>相比于VAE，DDPM的编码过程和解码过程的定义更加明确，可以施加的约束更多。因此，如下图所示，它的生成效果会比VAE好很多。同时，DDPM和VAE类似，它在编码时会从分布里采样，而不是只输出一个固定值，不会出现AE的过拟合问题。</p>
<p><img src="/2024/01/23/20230707-SD1/11.5.jpg" alt="DDPM的图像生成结果"></p>
<p>DDPM的生成效果确实很好。但是，由于DDPM始终会对同一个尺寸的数据进行操作，图像的尺寸极大地影响了DDPM的运行速度，用DDPM生成高分辨率图像需要耗费大量计算资源。因此，想要用DDPM生成高质量图像，还得经过另一条路线。</p>
<h2 id="第二条路线：VQVAE"><a href="#第二条路线：VQVAE" class="headerlink" title="第二条路线：VQVAE"></a>第二条路线：VQVAE</h2><p>在AE的第二条改进路线中，一些工作干脆放弃使用AE做图像生成，转而利用AE的图像压缩能力，把图像生成拆成两步来做：先用AE的编码器把图像压缩成更小的图像，再用另一个图像生成模型生成小图像，并用AE的解码器把小图像重建回真实图像。</p>
<p>为什么会有这么奇怪的图像生成方法呢？这得从另一类图像生成模型讲起。在机器翻译模型Transformer横空出世后的一段时间里，有很多工作都想把Transformer用在图像生成上。但是，原本用来生成文本的Transformer无法直接应用在图像上。在自然语言处理（NLP）中，一个句子可以用若干个单词表示。而每个单词又是用一个整数表示。所以，Transformer生成句子时，实际上是在生成若干个离散的整数，也就是生成一个离散向量。而在图像生成模型中，每个像素的颜色值是一个连续的浮点数。想把Transformer直接用在图像生成上，就得想办法把图像用离散向量表示。我们知道，AE可以把图像编码成一个连续向量。能不能做一些修改，让AE把图像编码成一个离散向量呢？</p>
<p><img src="/2024/01/23/20230707-SD1/12.jpg" alt></p>
<p><strong>Vector Quantised-Variational AutoEncoder (VQVAE)</strong> 就是一个能把图像编码成离散向量的AE（虽然作者在取名时用了VAE）。我们来简单看一下VQVAE是怎样把图像编码成离散向量的。</p>
<p>假设我们有了一个能编码出离散向量的AE。</p>
<p><img src="/2024/01/23/20230707-SD1/13.jpg" alt></p>
<p>由于神经网络不能很好地处理离散数据，我们要引入NLP里的通常做法，加一个把离散向量映射成连续向量的嵌入层。</p>
<p><img src="/2024/01/23/20230707-SD1/14.jpg" alt></p>
<p>现在我们再回头讨论怎么让编码器输出一个离散向量。我们可以让AE的解码器保持不变，还是输出一个连续向量，再通过一个「向量离散化」操作，把连续向量变成离散向量。这个操作会把编码器的输出对齐到嵌入层的向量上，其原理类似于把0.99和1.01离散化成1，只不过它是对向量整体考虑，而不是对每一个数单独考虑。向量离散化操作的具体原理我们不在此处细究。</p>
<p><img src="/2024/01/23/20230707-SD1/15.jpg" alt></p>
<p>忽略掉实现细节，我们可以认为VQVAE能够把图像压缩成离散向量。更准确地说，VQVAE能把图像等比例压缩成离散的「小图像」。压缩成二维图像而不是一维向量，能够保留原图像的一些空间特性，为之后第二步图像生成铺路。</p>
<p><img src="/2024/01/23/20230707-SD1/16.jpg" alt></p>
<p>整理一下，VQVAE是一个能把图像压缩成离散小图像的AE。为了用VQVAE生成图像，需要执行一个两阶段的图像生成流程：</p>
<ul>
<li>训练时，先训练一个图像压缩模型（VQVAE），再训练一个生成压缩图像的模型（比如Transformer）</li>
<li>生成时，先用第二个模型生成出一个压缩图像，再用第一个模型的解码器把压缩图像复原成真实图像</li>
</ul>
<p>之所以要执行两阶段的图像生成流程，而不是只用第二个模型生成大图像，有两个原因。第一个原因是前面提到的，Transformer等生成模型只支持生成离散图像，需要用另一个模型把连续的颜色值变成离散值以兼容这些模型。第二个原因是为了减少模型的运算量。以Transformer为例，Transformer的运算次数大致与像素数的平方成正比，拿Transformer生成高分辨率图像的运算开销是不可接受的。而如果用一个AE把图像压缩一下的话，用Transformer就可行了。</p>
<p>VQVAE给后续工作带来了三条启发：第一，可以用AE把图像压缩成离散向量；第二，如果一个图像生成模型生成高分辨率的图像的计算代价太高，可以先用AE把图像压缩，再生成压缩图像。这两条启发对应上一段提到的使用VQVAE的两条动机。</p>
<p>而第三条启发就比较有意思了。在讨论VQVAE的过程中，我们完全没有考虑过拟合的事。这是因为经过了向量离散化操作后，解码器的输入已经不再是编码器的输出，而是嵌入层里的向量了。这种做法杜绝了AE的死记硬背，缓解了过拟合现象。这样，我们可以换一个角度看待VQVAE：编码器还是AE的编码器，编码器的输出是连续向量，后续的向量离散化操作和嵌入层全部都是解码器的一部分。从这个角度看，VQVAE其实提出了一个由向量离散化和嵌入层组成的正则化模块。这个模块和VAE的KL散度约束一样，都解决了AE的过拟合问题。我们把VQVAE的正则化方法叫做<strong>VQ正则化</strong>。</p>
<p><img src="/2024/01/23/20230707-SD1/17.jpg" alt></p>
<p>VQVAE论文提出的图像生成方法效果一般。和普通的AE一样，VQVAE在训练时只用了重建误差来约束图像质量，重建图像的细节依然很模糊。且VQVAE配套的第二阶段图像生成模型不是较为强力的Transformer，而是一个基于CNN的图像生成模型。</p>
<p><img src="/2024/01/23/20230707-SD1/18.jpg" alt></p>
<p>后续的<strong>VQGAN</strong>论文对VQVAE进行了改进。对于一阶段的图像压缩模型，VQGAN在VQVAE的基础上引入了生成对抗网络（GAN）中一些监督误差，提高了图像压缩模型的重建质量；对于两阶段的图像生成模型，该方法使用了Transformer。凭借这些改动，VQGAN方法能够生成高质量的高清图片。并且，通过把额外的约束条件（如语义分割图像、文字）输入进Transformer，VQGAN方法能够实现带约束的图像生成。以下是VQGAN方法根据语义分割图像生成的高清图片。  </p>
<blockquote>
<p>图像生成模型可以是无约束或带约束的。无约束图像生成模型只需要输入一个随机向量，训练数据不需要任何标注，可以进行无监督训练。带约束图像生成模型会在无约束图像生成模型的基础上多加一些输入，并给每个训练图像打上描述约束的标签，执行监督训练。比如要训练文生图模型，就要给每个训练图片带上文字描述。</p>
</blockquote>
<p><img src="/2024/01/23/20230707-SD1/19.jpg" alt></p>
<h2 id="路线的交汇点——Stable-Diffusion"><a href="#路线的交汇点——Stable-Diffusion" class="headerlink" title="路线的交汇点——Stable Diffusion"></a>路线的交汇点——Stable Diffusion</h2><p>看完上面这两条AE的改进路线，相信你已经能够猜出Stable Diffusion的核心思想了。让我们看看Stable Diffusion是怎么从这两条路径中汲取灵感的。</p>
<p>在发布了VQGAN后，德国的CompVis实验室开始探索起VQGAN的改进方法。VQGAN能把图像边长压缩16倍，而VQGAN配套的Transformer只能一次生成$16 \times 16$的图片。也就是说，整套方法一次只能生成$256 \times 256$的图片。为了生成分辨率更高的图片，VQGAN方法需要借助滑动窗口。能不能让模型一次性生成分辨率更高的图片呢？制约VQGAN方法生成分辨率的主要因素是Transformer。如果能把Transformer换成一个效率更高，能生成更高分辨率的图像的模型，不就能生成比$256\times256$更大的图片了吗？CompVis实验室开始把目光着眼于DDPM上。</p>
<p>于是，在发布VQGAN的一年后，CompVis实验室又发布了名为<em>High-Resolution Image Synthesis with Latent Diffusion Models</em>的论文，提出了一种叫做<strong>隐扩散模型（latent diffusion model, LDM）</strong> 的图像生成模型。通过与AI公司Stability AI合作，借助他们庞大的算力资源训练LDM，CompVis实验室发布了商业名为<strong>Stable Diffusion</strong>的开源文生图AI绘画模型。</p>
<p>LDM其实就是在VQGAN方法的基础上，把图像生成模型从Transformer换成了DDPM。或者从另一个角度说，为了让DDPM生成高分辨率图像，LDM利用了VQVAE的第二条启发：先用AE把图像压缩，再用DDPM生成压缩图像。LDM的AE一般是把图像边长压缩8倍，DDPM生成$64 \times 64$的压缩图像，整套LDM能生成$512 \times 512$的图像。</p>
<p>和Transformer不同，DDPM处理的图像是用连续向量表示的。因此，在LDM中使用VQGAN做图像压缩时，不一定需要向量离散化操作，只需要在AE的基础上加一点轻微的正则化就行。作者在实现LDM时讨论了两类正则化，一类是VAE的KL正则化，一类是VQ正则化（对应VQVAE的第三条启发），两种正则化都能取得不错的效果。</p>
<p>LDM依然可以实现带约束的图像生成。用DDPM替换掉Transformer后，额外的约束会输入进DDPM中。作者在论文中讨论了几种把约束输入进DDPM的方式。</p>
<p>在搞懂了早期工作后，理解Stable Diffusion的核心思想就是这么简单。让我们把Stable Diffusion的发展过程及主要结构总结一下。Stable Diffusion由两类AE的变种发展而来，一类是有强大生成能力却需要耗费大量运算资源的DDPM，一类是能够以较高保真度压缩图像的VQVAE。Stable Diffusion是一个两阶段的图像生成模型，它先用一个使用KL正则化或VQ正则化的VQGAN来实现图像压缩，再用DDPM生成压缩图像。可以把额外的约束（如文字）输入进DDPM以实现带约束图像生成。</p>
<h2 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h2><p>本文仅仅对Stable Diffusion的早期工作做了一个简单的梳理。要把Stable Diffusion吃透，还需要多读一些早期论文。我来把早期论文按重要性分个类。</p>
<h3 id="图像生成必读文章"><a href="#图像生成必读文章" class="headerlink" title="图像生成必读文章"></a>图像生成必读文章</h3><p><em>Neural Discrete Representation Learning</em> (VQVAE): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a></p>
<p><em>Taming Transformers for High-Resolution Image Synthesis</em> (VQGAN): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09841">https://arxiv.org/abs/2012.09841</a></p>
<p><em>Denoising Diffusion Probabilistic Models</em> (DDPM): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a></p>
<h3 id="图像生成选读文章"><a href="#图像生成选读文章" class="headerlink" title="图像生成选读文章"></a>图像生成选读文章</h3><p><em>Auto-Encoding Variational Bayes</em> (VAE): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a> 提出VAE的文章。数学公式较多，只需要了解VAE的大致结构就好，不需要详细阅读论文。</p>
<p><em>Pixel Recurrent Neural Networks</em> (PixelCNN): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1601.06759">https://arxiv.org/abs/1601.06759</a> 提出了一种拟合离散分布的图像生成模型，自回归图像生成模型的代表。这是VQVAE使用的第二阶段图像生成模型。有兴趣可以了解一下。</p>
<p><em>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</em>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03585">https://arxiv.org/abs/1503.03585</a> DDPM的前作，首个提出扩散模型思想的文章。其核心原理和DDPM几乎完全一致，但是模型结构和优化目标不够先进，生成效果没有改进后的DDPM好。数学公式较多，不必细读，可以在学习DDPM时对比着阅读。</p>
<p><em>Denoising Diffusion Implicit Models</em> (DDIM): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.02502">https://arxiv.org/abs/2010.02502</a> 一种加速DDPM采样的方法，广泛运用在包含Stable Diffusion在内的扩散模型中。推荐阅读。</p>
<p><em>Classifier-Free Diffusion Guidance</em>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.12598">https://arxiv.org/abs/2207.12598</a> 一种让扩散模型的输出更加贴近约束的方法，广泛运用在包含Stable Diffusion在内的扩散模型中，用于生成更符合文字描述的图片。推荐阅读。</p>
<p><em>Generative Adversarial Networks</em> (GAN): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a> 以及 <em>A Style-Based Generator Architecture for Generative Adversarial Networks</em> (StyleGAN): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04948">https://arxiv.org/abs/1812.04948</a> 可以了解一下GAN是怎么确保图像生成质量的，并认识CelebAHQ和FFHQ这两个常用的人脸数据集。 </p>
<h3 id="其他必读文章"><a href="#其他必读文章" class="headerlink" title="其他必读文章"></a>其他必读文章</h3><p><em>Deep Residual Learning for Image Recognition</em> (ResNet): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a> 深度学习的经典文章。其中提出的残差连接被用到了DDPM中。</p>
<p><em>Attention Is All You Need</em> (Transformer): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a> 深度学习的经典文章。其中提出的自注意力模块被用到了DDPM中。</p>
<h3 id="其他选读文章"><a href="#其他选读文章" class="headerlink" title="其他选读文章"></a>其他选读文章</h3><p><em>Learning Transferable Visual Models From Natural Language Supervision</em> (CLIP): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a> 提出了对齐文本和图像的方法。绝大多数文生图模型的核心。</p>
<p><em>U-Net: Convolutional Networks for Biomedical Image Segmentation</em> (U-Net): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a> 一种被广泛运用的神经网络架构。DDPM的神经网络的主架构。U-Net的结构很简单，可以不用去读论文，直接看代码。</p>
<h2 id="我的解读文章"><a href="#我的解读文章" class="headerlink" title="我的解读文章"></a>我的解读文章</h2><p>我对这上面的很多论文都做过解读。如果你在阅读论文的时候碰到了困难，欢迎阅读我的解读。</p>
<p>轻松理解 VQ-VAE：首个提出 codebook 机制的生成模型</p>
<p>VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型</p>
<p>扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现</p>
<p>抛开数学，轻松学懂 VAE（附 PyTorch 实现）</p>
<p>冷门的自回归生成模型 ~ 详解 PixelCNN 大家族</p>
<p>DDIM 简明讲解与 PyTorch 实现：加速扩散模型采样的通用方法</p>
<p>用18支画笔作画的AI ~ StyleGAN特点浅析</p>
<p>ResNet 论文概览与精读</p>
<p>Attention Is All You Need (Transformer) 论文精读</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2023/07/07/20230702-DDIM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/07/20230702-DDIM/" class="post-title-link" itemprop="url">DDIM 简明讲解与 PyTorch 实现：加速扩散模型采样的通用方法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-07 20:57:59" itemprop="dateCreated datePublished" datetime="2023-07-07T20:57:59+08:00">2023-07-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>相比于多数图像生成模型，去噪扩散概率模型（Denoising Diffusion Probabilistic Model, DDPM）的采样速度非常慢。这是因为DDPM在采样时通常要做1000次去噪操作。但如果你玩过基于扩散模型的图像生成应用的话，你会发现，大多数应用只需要20次去噪即可生成图像。这是为什么呢？原来，这些应用都使用了一种更快速的采样方法——去噪扩散隐式模型（Denoising Diffusion Implicit Model, DDIM）。</p>
<p>基于DDPM，DDIM论文主要提出了两项改进。第一，对于一个已经训练好的DDPM，只需要对采样公式做简单的修改，模型就能在去噪时「跳步骤」，在一步去噪迭代中直接预测若干次去噪后的结果。比如说，假设模型从时刻$T=100$开始去噪，新的模型可以在每步去噪迭代中预测10次去噪操作后的结果，也就是逐步预测时刻$t=90, 80, …, 0$的结果。这样，DDPM的采样速度就被加速了10倍。第二，DDIM论文推广了DDPM的数学模型，从更高的视角定义了DDPM的前向过程（加噪过程）和反向过程（去噪过程）。在这个新数学模型下，我们可以自定义模型的噪声强度，让同一个训练好的DDPM有不同的采样效果。</p>
<p>在这篇文章中，我将言简意赅地介绍DDIM的建模方法，并给出我的DDIM PyTorch实现与实验结果。本文不会深究DDIM的数学推导，对这部分感兴趣的读者可以去阅读我在文末给出的参考资料。</p>
<h2 id="回顾-DDPM"><a href="#回顾-DDPM" class="headerlink" title="回顾 DDPM"></a>回顾 DDPM</h2><p>DDIM是建立在DDPM之上的一篇工作。在正式认识DDIM之前，我们先回顾一下DDPM中的一些关键内容，再从中引出DDIM的改进思想。</p>
<p>DDPM是一个特殊的VAE。它的编码器是$T$步固定的加噪操作，解码器是$T$步可学习的去噪操作。模型的学习目标是让每一步去噪操作尽可能抵消掉对应的加噪操作。</p>
<p><img src="/2023/07/07/20230702-DDIM/1.jpg" alt></p>
<p>DDPM的加噪和去噪操作其实都是在某个正态分布中采样。因此，我们可以用概率$q, p$分别表示加噪和去噪的分布。比如 $q(\mathbf{x}_t|\mathbf{x}_{t-1})$ 就是由第 $t-1$ 时刻的图像到第 $t$ 时刻的图像的加噪声分布， $p(\mathbf{x}_{t-1}|\mathbf{x}_{t})$ 就是由第 $t$ 时刻的图像到第 $t-1$ 时刻的图像的去噪声分布。这样，我们可以说网络的学习目标是让 $p(\mathbf{x}_{t-1} | \mathbf{x}_{t})$ 尽可能与 $q(\mathbf{x}_t | \mathbf{x}_{t-1})$ 和互逆。</p>
<p>但是，「互逆」并不是一个严格的数学表述。更具体地，我们应该让分布$p(\mathbf{x}_{t-1} | \mathbf{x}_{t})$和分布$q(\mathbf{x}_{t-1} | \mathbf{x}_{t})$尽可能相似。$q(\mathbf{x}_{t-1} | \mathbf{x}_{t})$和$p(\mathbf{x}_{t-1} | \mathbf{x}_{t})$的关系就和VAE中原图像与重建图像的关系一样。</p>
<p><img src="/2023/07/07/20230702-DDIM/2.jpg" alt></p>
<p>$q(\mathbf{x}_{t-1} | \mathbf{x}_{t})$是不好求得的，但在给定了输入数据$\mathbf{x}_{0}$时，$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})$是可以用贝叶斯公式求出来的：</p>
<script type="math/tex; mode=display">
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) = q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)\frac{q(\mathbf{x}_{t-1} | \mathbf{x}_0)}{q(\mathbf{x}_{t} | \mathbf{x}_0)}</script><p>我们不必关心具体的求解方法，只需要知道从等式右边的三项$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)、q(\mathbf{x}_{t-1} | \mathbf{x}_0)、q(\mathbf{x}_{t} | \mathbf{x}_0)$可以推导出等式左边的那一项。在DDPM中，$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1})$是一个定义好的式子，且$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}) = q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$。根据$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1})$，可以推出$q(\mathbf{x}_{t} | \mathbf{x}_0)$。知道了$q(\mathbf{x}_{t} | \mathbf{x}_0)$，$q(\mathbf{x}_{t-1} | \mathbf{x}_0)$也就知道了（把公式里的$t$换成$t-1$就行了）。这样，在DDPM中，等式右边的式子全部已知，等式左边的$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_{0})$可以直接求出来。</p>
<p>上述推理过程可以简单地表示为：知道$q(\mathbf{x}_{t} | \mathbf{x}_0)$和$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$，就知道了神经网络的学习目标$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$。这几个公式在DDPM中的具体形式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0) &=\mathcal{N}(\mathbf{x}_{t};\sqrt{1 - \beta_t}\mathbf{x}_{t - 1},\beta_t\mathbf{I}) \\
q(\mathbf{x}_{t} | \mathbf{x}_0)&=\mathcal{N}(\mathbf{x}_{t}; \sqrt{\bar{\alpha}_t}\mathbf{x}_{0}, (1-\bar{\alpha}_t)\mathbf{I}) \\
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) &= 
\mathcal{N}(\mathbf{x}_{t-1}; \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_{t}}\mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_{t}}\mathbf{x}_{0}, \frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}} \cdot \beta_t\mathbf{I})
\end{aligned}</script><p>其中，只有参数$\beta_t$是可调的。$\bar{\alpha}_t$是根据$\beta_t$算出的变量，其计算方法为：$\alpha_t=1-\beta_t, \bar{\alpha}_t=\prod_{i=1}^t\alpha_i$。</p>
<p>由于学习目标$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$里只有一个未知变量$\mathbf{x}_0$，DDPM把学习目标简化成了只让神经网络根据$\mathbf{x}_{t}$拟合公式里的$\mathbf{x}_{0}$（更具体一点，是拟合从$\mathbf{x}_{0}$到$\mathbf{x}_{t}$的噪声）。也就是说，在训练时，$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$的公式不会被用到，只有$\mathbf{x}_{t}$和$\mathbf{x}_{0}$两个量之间的公式$q(\mathbf{x}_{t} | \mathbf{x}_0)$会被用到。只有在采样时，$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$的公式才会被用到。训练目标的推理过程可以总结为：</p>
<p><img src="/2023/07/07/20230702-DDIM/3.jpg" alt></p>
<blockquote>
<p>理解「DDPM的训练目标只有$\mathbf{x}_{0}$」对于理解DDIM非常关键。如果你在回顾DDPM时出现了问题，请再次阅读DDPM的相关介绍文章。</p>
</blockquote>
<h2 id="加速-DDPM"><a href="#加速-DDPM" class="headerlink" title="加速 DDPM"></a>加速 DDPM</h2><p>我们再次审视一下DDPM的推理过程：首先有$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}) = q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$。根据$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1})$，可以推出$q(\mathbf{x}_{t} | \mathbf{x}_0)$。知道$q(\mathbf{x}_{t} | \mathbf{x}_0)$和$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$，由贝叶斯公式，就知道了学习目标$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$。</p>
<p>根据这一推理过程，DDIM论文的作者想到，假如我们把贝叶斯公式中的$t$替换成$t_2$, $t-1$替换成$t_1$，其中$t_2$是比$t_1$大的任意某一时刻，那么我们不就可以从$t_2$到$t_1$跳步骤去噪了吗？比如令$t_2 = t_1 + 10$，我们就可以求出去除10次噪声的公式，去噪的过程就快了10倍。</p>
<script type="math/tex; mode=display">
q(\mathbf{x}_{t_1} | \mathbf{x}_{t_2}, \mathbf{x}_0) = q(\mathbf{x}_{t_2} | \mathbf{x}_{t_1}, \mathbf{x}_0)\frac{q(\mathbf{x}_{t_1} | \mathbf{x}_0)}{q(\mathbf{x}_{t_2} | \mathbf{x}_0)}</script><p>修改之后，$q(\mathbf{x}_{t_1} | \mathbf{x}_0)$和$q(\mathbf{x}_{t_2} | \mathbf{x}_0)$依然很好求，只要把$t_1$, $t_2$代入普通的$q(\mathbf{x}_{t} | \mathbf{x}_0)$公式里就行。</p>
<script type="math/tex; mode=display">
q(\mathbf{x}_{t} | \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t}; \sqrt{\bar{\alpha}_t}\mathbf{x}_{0}, (1-\bar{\alpha}_t)\mathbf{I})</script><p>但是，$q(\mathbf{x}_{t_2} | \mathbf{x}_{t_1}, \mathbf{x}_0)$怎么求呢？原来的$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t};\sqrt{1 - \beta_t}\mathbf{x}_{t - 1},\beta_t\mathbf{I})$来自于DDPM的定义，我们能直接把公式拿来用。能不能把$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$的公式稍微修改一下，让它兼容$q(\mathbf{x}_{t_2} | \mathbf{x}_{t_1}, \mathbf{x}_0)$呢？</p>
<p>修改$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$的思路如下：假如我们能把公式中的$\beta_t$换成一个由$t$和$t-1$决定的变量，我们就能把$t$换成$t_2$，$t-1$换成$t_1$，也就得到了$q(\mathbf{x}_{t_2} | \mathbf{x}_{t_1}, \mathbf{x}_0)$。</p>
<p>那怎么修改$\beta_t$的形式呢？很简单。我们知道$\beta_t$决定了$\bar{\alpha}_t$：$\alpha_t=1-\beta_t, \bar{\alpha}_t=\prod_{i=1}^t\alpha_i$。那么我们用$\bar{\alpha}_t$除以$\bar{\alpha}_{t-1}$，不就得到了$1-\beta_t$了吗？也就是说：</p>
<script type="math/tex; mode=display">
\beta_t = 1-\frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}.</script><p>我们把这个用$\bar{\alpha}_t$和$\bar{\alpha}_{t-1}$表示的$\beta_t$套入$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$的公式里，再把$t$换成$t_2$，$t-1$换成$t_1$，就得到了$q(\mathbf{x}_{t_2} | \mathbf{x}_{t_1}, \mathbf{x}_0)$。有了这一项，贝叶斯公式等式右边那三项我们就全部已知，可以求出$q(\mathbf{x}_{t_1} | \mathbf{x}_{t_2}, \mathbf{x}_0)$，也就是可以一次性得到多个时刻后的去噪结果。</p>
<p>在这个过程中，我们只是把DDPM公式里的$\bar{\alpha}_t$换成$\bar{\alpha}_{t2}$，$\bar{\alpha}_{t-1}$换成$\bar{\alpha}_{t1}$，公式推导过程完全不变。网络的训练目标$\mathbf{x}_{0}$也没有发生改变，只是采样时的公式需要修改。这意味着我们可以先照着原DDPM的方法训练，再用这种更快速的方式采样。</p>
<p>我们之前只讨论了$t_1$到$t_2$为固定值的情况。实际上，我们不一定要间隔固定的时刻去噪一次，完全可以用原时刻序列的任意一个子序列来去噪。比如去噪100次的DDPM的去噪时刻序列为<code>[99, 98, ..., 0]</code>，我们可以随便取一个长度为10的子序列:<code>[99, 98, 77, 66, 55, 44, 33, 22, 1, 0]</code>，按这些时刻来去噪也能让采样速度加速10倍。但实践中没人会这样做，一般都是等间距地取时刻。</p>
<p>这样看来，在采样时，只有部分时刻才会被用到。那我们能不能顺着这个思路，干脆训练一个有效时刻更短（总时刻$T$不变）的DDPM，以加速训练呢？又或者保持有效训练时刻数不变，增大总时刻$T$呢？DDIM论文的作者提出了这些想法，认为这可以作为后续工作的研究方向。</p>
<h2 id="从-DDPM-到-DDIM"><a href="#从-DDPM-到-DDIM" class="headerlink" title="从 DDPM 到 DDIM"></a>从 DDPM 到 DDIM</h2><p>除了加速DDPM外，DDIM论文还提出了一种更普遍的DDPM。在这种新的数学模型下，我们可以任意调节采样时的方差大小。让我们来看一下这个数学模型的推导过程。</p>
<p>DDPM的学习目标$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$由$q(\mathbf{x}_{t} | \mathbf{x}_0)$和$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$决定。具体来说，在求解正态分布$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$时，我们会将它的均值$\tilde{\mu}_t$和方差$\tilde{\beta}_t$设为未知量，并将条件$q(\mathbf{x}_{t} | \mathbf{x}_0)$、$q(\mathbf{x}_{t-1} | \mathbf{x}_0)$、$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$代入，求解出确定的$\tilde{\mu}_t$和$\tilde{\beta}_t$。</p>
<p>在上文我们分析过，DDPM训练时只需要拟合$\mathbf{x}_0$，只需要用到$\mathbf{x}_0$和$\mathbf{x}_t$的关系$q(\mathbf{x}_{t} | \mathbf{x}_0)$。在不修改训练过程的前提下，我们能不能把限制$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$去掉（即$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$可以是任意一个正态分布，而不是我们提前定义好的一个正态分布），得到一个更普遍的DDPM呢？</p>
<p>这当然是可以的。根据基础的解方程知识，我们知道，去掉一个方程后，会多出一个自由变量。取消了$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$的限制后，均值$\tilde{\mu}_t$和方差$\tilde{\beta}_t$就不能同时确定下来了。我们可以令方差$\tilde{\beta}_t$为自由变量，并让$\tilde{\mu}_t$用含$\tilde{\beta}_t$的式子表示出来。这样，我们就得到了一个方差可变的更一般的DDPM。</p>
<p>让我们来看一下这个新模型的具体公式。原来的DDPM的加噪声逆操作的分布为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) = &
\mathcal{N}(\mathbf{x}_{t-1}; \\
&\frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_{t}}\mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_{t}}\mathbf{x}_{0}, \\
&\frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}} \cdot \beta_t\mathbf{I})
\end{aligned}</script><p>新的分布公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) &= 
\mathcal{N}(\mathbf{x}_{t-1}; \\
&\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t-1}-\tilde{\beta}_t} \cdot \frac{\mathbf{x}_{t}-\sqrt{\bar{\alpha}_t}\mathbf{x}_{0}}{\sqrt{1 - \bar{\alpha}_{t}}}, \\ 
&\tilde{\beta}_t\mathbf{I})
\end{aligned}</script><p>新公式是旧公式的一个推广版本。如果我们把DDPM的方差$(1-\bar{\alpha}_{t-1})/(1 - \bar{\alpha}_{t}) \cdot \beta_t$代入新公式里的$\tilde{\beta}_t$，就能把新公式还原成DDPM的公式。和DDPM的公式一样，我们也可以把$\mathbf{x}_{0}$拆成$\mathbf{x}_{t}$和噪声$\epsilon$表示的式子。</p>
<p>现在采样时方差可以随意取了，我们来讨论一种特殊的方差取值——$\tilde{\beta}_t=0$。也就是说，扩散模型的反向过程变成了一个没有噪声的确定性过程。给定随机噪声$\mathbf{x}_{T}$，我们只能得到唯一的采样结果$\mathbf{x}_{0}$。这种结果确定的概率模型被称为隐式概率模型（implicit probabilistic model）。所以，论文作者把方差为0的这种扩散模型称为DDIM（Denoising Diffusion Implicit Model）。</p>
<p>为了方便地选取方差值，作者将方差改写为</p>
<script type="math/tex; mode=display">
\tilde{\beta}_t(\eta)=\eta\frac{(1-\bar{\alpha}_{t-1})}{(1 - \bar{\alpha}_{t})} \cdot \beta_t</script><p>其中，$\eta\in[0, 1]$。通过选择不同的$\eta$，我们实际上是在DDPM和DDIM之间插值。$\eta$控制了插值的比例。$\eta=0$，模型是DDIM；$\eta=1$，模型是DDPM。</p>
<p>除此之外，DDPM论文曾在采样时使用了另一种方差取值：$\tilde{\beta}_t=\beta_t$，即去噪方差等于加噪方差。实验显示这个方差的采样结果还不错。我们可以把这个取值也用到DDIM论文提出的方法里，只不过这个方差值不能直接套进上面的公式。在代码实现部分我会介绍该怎么在DDIM方法中使用这个方差。</p>
<p>注意，在这一节的推导过程中，我们依然没有修改DDPM的训练目标。我们可以把这种的新的采样方法用在预训练的DDPM上。当然，我们可以在使用新的采样方法的同时也使用上一节的加速采样方法。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>到这里为止，我们已经学完了DDIM论文的两大内容：加速采样、更换采样方差。加速采样的意义很好理解，它能大幅减少采样时间。可更换采样方差有什么意义呢？我们看完论文中的实验结果就知道了。</p>
<p>论文展示了新采样方法在不同方差、不同采样步数下的FID指标（越小越好）。其中，$\hat{\sigma}$表示使用DDPM中的$\tilde{\beta}_t=\beta_t$方差取值。实验结果非常有趣。在使用采样加速（步数比总时刻1000要小）时，$\eta=0$的DDIM的表现最好，而$\hat{\sigma}$的情况则非常差。而当$\eta$增大，模型越来越靠近DDPM时，用$\hat{\sigma}$的结果会越来越好。而在DDPM中，用$\hat{\sigma}$的结果是最好的。</p>
<p><img src="/2023/07/07/20230702-DDIM/4.jpg" alt></p>
<p>从这个实验结果中，我们可以得到一条很简单的实践指南：如果使用了采样加速，一定要用效果最好的DDIM；而使用原DDPM的话，可以维持原论文提出的$\tilde{\beta}_t=\beta_t$方差取值。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>DDIM论文提出了DDPM的两个拓展方向：加速采样、变更采样方差。通过同时使用这两个方法，我们能够在不重新训练DDPM、尽可能不降低生成质量的前提下，让扩散模型的采样速度大幅提升（一般可以快50倍）。让我们再从头理一理提出DDIM方法的思考过程。</p>
<p>为了能直接使用预训练的DDPM，我们希望在改进DDPM时不更改DDPM的训练过程。而经过简化后，DDPM的训练目标只有拟合$\mathbf{x}_{0}$，训练时只会用到前向过程公式$q(\mathbf{x}_{t} | \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t}; \sqrt{\bar{\alpha}_t}\mathbf{x}_{0}, (1-\bar{\alpha}_t)\mathbf{I})$。所以，我们的改进应该建立在公式$q(\mathbf{x}_{t} | \mathbf{x}_0)$完全不变的前提下。</p>
<p>通过对DDPM反向过程公式的简单修改，也就是把$t$改成$t_2$，$t-1$改成$t_1$，我们可以把去噪一步的公式改成去噪多步的公式，以大幅加速DDPM。可是，这样改完之后，采样的质量会有明显的下降。</p>
<p>我们可以猜测，减少了采样迭代次数后，采样质量之所以下降，是因为每次估计的去噪均值更加不准确。而每次去噪迭代中的噪声（由方差项决定的那一项）放大了均值的不准确性。我们能不能干脆让去噪时的方差为0呢？为了让去噪时的方差可以自由变动，我们可以去掉DDPM的约束条件。由于贝叶斯公式里的$q(\mathbf{x}_{t} | \mathbf{x}_0)$不能修改，我们只能去掉$q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)$的限制。去掉限制后，方差就成了自由变量。我们让去噪方差为0，让采样过程没有噪声。这样，就得到了本文提出的DDIM模型。实验证明，在采样迭代次数减少后，使用DDIM的生成结果是最优的。</p>
<p>在本文中，我较为严格地区分了DDPM和DDIM的叫法：DDPM指DDPM论文中提出的有1000个扩散时刻的模型，它的采样方差只有两种取值（$\tilde{\beta}_t=(1-\bar{\alpha}_{t-1})/(1 - \bar{\alpha}_{t}) \cdot \beta_t$, $\tilde{\beta}_t=\beta_t$）。DDIM指DDIM论文中提出的$\eta=0$的推广版DDPM模型。DDPM和DDIM都可以使用采样加速。但是，从习惯上我们会把没有优化加速的DDPM称为”DDPM”，把$\eta$可以任取，采样迭代次数可以任取的采样方法统称为”DDIM”。一些开源库中会有叫<code>DDIMSampler</code>的类，调节$\eta$的参数大概会命名为<code>eta</code>，调节迭代次数的参数大概会命名为<code>ddim_num_steps</code>。一般我们令<code>eta=0</code>，<code>ddim_num_steps=20</code>即可。</p>
<p>DDIM的代码实现没有太多的学习价值，只要在DDPM代码的基础上把新数学公式翻译成代码即可。其中唯一值得注意的就是如何在DDIM中使用DDPM的方差$\tilde{\beta}_t=\beta_t$。对此感兴趣的话可以阅读我接下来的代码实现介绍。</p>
<p>在这篇解读中，我略过了DDIM论文中的大部分数学推导细节。对DDIM数学模型的推导过程感兴趣的话，可以阅读我在参考文献中推荐的文章，或者看一看原论文。</p>
<h2 id="DDIM-PyTorch-实现"><a href="#DDIM-PyTorch-实现" class="headerlink" title="DDIM PyTorch 实现"></a>DDIM PyTorch 实现</h2><p>在这个项目中，我们将对一个在CelebAHQ上预训练的DDPM执行DDIM采样，尝试复现论文中的那个FID表格，以观察不同<code>eta</code>和<code>ddim_steps</code>对于采样结果的影响。</p>
<p>代码仓库：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/ddim">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/ddim</a></p>
<h3 id="DDPM-基础项目"><a href="#DDPM-基础项目" class="headerlink" title="DDPM 基础项目"></a>DDPM 基础项目</h3><p>DDIM只是DDPM的一种采样改进策略。为了复现DDIM的结果，我们需要一个DDPM基础项目。由于DDPM并不是本文的重点，在这一小节里我将简要介绍我的DDPM实现代码的框架。</p>
<p>我们的实验需要使用CelebAHQ数据集，请在 <a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256">https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256</a> 下载该数据集并解压到项目的<code>data/celebA/celeba_hq_256</code>目录下。另外，我在Hugging Face上分享了一个在64x64 CelebAHQ上训练的DDPM模型：<a target="_blank" rel="noopener" href="https://huggingface.co/SingleZombie/dldemos/tree/main/ckpt/ddim">https://huggingface.co/SingleZombie/dldemos/tree/main/ckpt/ddim</a> ，请把它放到项目的<code>dldemos/ddim</code>目录下。</p>
<p>先运行<code>dldemos/ddim/dataset.py</code>下载MNIST，再直接运行<code>dldemos/ddim/main.py</code>，代码会自动完成MNIST上的训练，并执行步数1000的两种采样和步数20的三种采样，同时将结果保存在目录<code>work_dirs</code>中。以下是我得到的MNIST DDPM采样结果（存储在<code>work_dirs/diffusion_ddpm_sigma_hat.jpg</code>中）。</p>
<p><img src="/2023/07/07/20230702-DDIM/5.jpg" alt></p>
<p>为了查看64x64 CelebAHQ上的采样结果，可以在<code>dldemos/ddim/main.py</code>的main函数里把<code>config_id</code>改成<code>2</code>，再注释掉训练函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0 for MNIST. See configs.py</span></span><br><span class="line">config_id = <span class="number">2</span></span><br><span class="line">cfg = configs[config_id]</span><br><span class="line">n_steps = <span class="number">1000</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line">model_path = cfg[<span class="string">&#x27;model_path&#x27;</span>]</span><br><span class="line">img_shape = cfg[<span class="string">&#x27;img_shape&#x27;</span>]</span><br><span class="line">to_bgr = <span class="literal">False</span> <span class="keyword">if</span> cfg[<span class="string">&#x27;dataset_type&#x27;</span>] == <span class="string">&#x27;MNIST&#x27;</span> <span class="keyword">else</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">net = UNet(n_steps, img_shape, cfg[<span class="string">&#x27;channels&#x27;</span>], cfg[<span class="string">&#x27;pe_dim&#x27;</span>],</span><br><span class="line">           cfg.get(<span class="string">&#x27;with_attn&#x27;</span>, <span class="literal">False</span>), cfg.get(<span class="string">&#x27;norm_type&#x27;</span>, <span class="string">&#x27;ln&#x27;</span>))</span><br><span class="line">ddpm = DDPM(device, n_steps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train(ddpm,</span></span><br><span class="line"><span class="comment">#       net,</span></span><br><span class="line"><span class="comment">#       cfg[&#x27;dataset_type&#x27;],</span></span><br><span class="line"><span class="comment">#       resolution=(img_shape[1], img_shape[2]),</span></span><br><span class="line"><span class="comment">#       batch_size=cfg[&#x27;batch_size&#x27;],</span></span><br><span class="line"><span class="comment">#       n_epochs=cfg[&#x27;n_epochs&#x27;],</span></span><br><span class="line"><span class="comment">#       device=device,</span></span><br><span class="line"><span class="comment">#       ckpt_path=model_path)</span></span><br></pre></td></tr></table></figure>
<p>以下是我得到的CelebAHQ DDPM采样结果（存储在<code>work_dirs/diffusion_ddpm_sigma_hat.jpg</code>中）。</p>
<p><img src="/2023/07/07/20230702-DDIM/6.jpg" alt></p>
<p>项目目录下的<code>configs.py</code>存储了训练配置，<code>dataset.py</code>定义了<code>DataLoader</code>，<code>network.py</code>定义了U-Net的结构，<code>ddpm.py</code>和<code>ddim.py</code>分别定义了普通的DDPM前向过程和采样以及DDIM采样，<code>dist_train.py</code>提供了并行训练脚本，<code>dist_sample.py</code>提供了并行采样脚本，<code>main.py</code>提供了单卡运行的所有任务脚本。</p>
<p>在这个项目中，我们的主要的目标是基于其他文件，编写<code>ddim.py</code>。我们先来看一下原来的<code>DDPM</code>类是怎么实现的，再仿照它的接口写一个<code>DDIM</code>类。</p>
<h3 id="实现-DDIM-采样"><a href="#实现-DDIM-采样" class="headerlink" title="实现 DDIM 采样"></a>实现 DDIM 采样</h3><p>在我的设计中，<code>DDPM</code>类不是一个神经网络（<code>torch.nn.Module</code>），它仅仅维护了扩散模型的<code>alpha</code>等变量，并描述了前向过程和反向过程。</p>
<p>在<code>DDPM</code>类中，我们可以在初始化函数里定义好要用到的<code>self.betas, self.alphas, self.alpha_bars</code>变量。如果在工程项目中，我们可以预定义好更多的常量以节约采样时间。但在学习时，我们可以少写一点代码，让项目更清晰一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPM</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 device,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_steps: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 min_beta: <span class="built_in">float</span> = <span class="number">0.0001</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 max_beta: <span class="built_in">float</span> = <span class="number">0.02</span></span>):</span></span><br><span class="line">        betas = torch.linspace(min_beta, max_beta, n_steps).to(device)</span><br><span class="line">        alphas = <span class="number">1</span> - betas</span><br><span class="line">        alpha_bars = torch.empty_like(alphas)</span><br><span class="line">        product = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i, alpha <span class="keyword">in</span> <span class="built_in">enumerate</span>(alphas):</span><br><span class="line">            product *= alpha</span><br><span class="line">            alpha_bars[i] = product</span><br><span class="line">        self.betas = betas</span><br><span class="line">        self.n_steps = n_steps</span><br><span class="line">        self.alphas = alphas</span><br><span class="line">        self.alpha_bars = alpha_bars</span><br></pre></td></tr></table></figure>
<p>前向过程就是把正态分布的公式$q(\mathbf{x}_{t} | \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t}; \sqrt{\bar{\alpha}_t}\mathbf{x}_{0}, (1-\bar{\alpha}_t)\mathbf{I})$翻译一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_forward</span>(<span class="params">self, x, t, eps=<span class="literal">None</span></span>):</span></span><br><span class="line">    alpha_bar = self.alpha_bars[t].reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> eps <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        eps = torch.randn_like(x)</span><br><span class="line">    res = eps * torch.sqrt(<span class="number">1</span> - alpha_bar) + torch.sqrt(alpha_bar) * x</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>在反向过程中，我们从<code>self.n_steps</code>到<code>1</code>枚举时刻<code>t</code>（代码中时刻和数组下标有1的偏差），按照公式算出每一步的去噪均值和方差，执行去噪。算法流程如下：</p>
<p><img src="/2023/07/07/20230702-DDIM/7.jpg" alt></p>
<p>参数<code>simple_var=True</code>表示令方差$\sigma_t^2=\beta_t$，而不是$(1-\bar{\alpha}_{t-1})/(1 - \bar{\alpha}_{t}) \cdot \beta_t$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_backward</span>(<span class="params">self, img_or_shape, net, device, simple_var=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(img_or_shape, torch.Tensor):</span><br><span class="line">        x = img_or_shape</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = torch.randn(img_or_shape).to(device)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(self.n_steps - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>), <span class="string">&quot;DDPM sampling&quot;</span>):</span><br><span class="line">        x = self.sample_backward_step(x, t, net, simple_var)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_backward_step</span>(<span class="params">self, x_t, t, net, simple_var=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    n = x_t.shape[<span class="number">0</span>]</span><br><span class="line">    t_tensor = torch.tensor([t] * n,</span><br><span class="line">                            dtype=torch.long).to(x_t.device).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    eps = net(x_t, t_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">        noise = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> simple_var:</span><br><span class="line">            var = self.betas[t]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            var = (<span class="number">1</span> - self.alpha_bars[t - <span class="number">1</span>]) / (</span><br><span class="line">                <span class="number">1</span> - self.alpha_bars[t]) * self.betas[t]</span><br><span class="line">        noise = torch.randn_like(x_t)</span><br><span class="line">        noise *= torch.sqrt(var)</span><br><span class="line"></span><br><span class="line">    mean = (x_t -</span><br><span class="line">            (<span class="number">1</span> - self.alphas[t]) / torch.sqrt(<span class="number">1</span> - self.alpha_bars[t]) *</span><br><span class="line">            eps) / torch.sqrt(self.alphas[t])</span><br><span class="line">    x_t = mean + noise</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_t</span><br></pre></td></tr></table></figure>
<p>接下来，我们来实现<code>DDIM</code>类。<code>DDIM</code>是<code>DDPM</code>的推广，我们可以直接用<code>DDIM</code>类继承<code>DDPM</code>类。它们共享初始化函数与前向过程函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDIM</span>(<span class="params">DDPM</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 device,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_steps: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 min_beta: <span class="built_in">float</span> = <span class="number">0.0001</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 max_beta: <span class="built_in">float</span> = <span class="number">0.02</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(device, n_steps, min_beta, max_beta)</span><br></pre></td></tr></table></figure>
<p>我们要修改的只有反向过程的实现函数。整个函数的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_backward</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                    img_or_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">                    net,</span></span></span><br><span class="line"><span class="params"><span class="function">                    device,</span></span></span><br><span class="line"><span class="params"><span class="function">                    simple_var=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                    ddim_step=<span class="number">20</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                    eta=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> simple_var:</span><br><span class="line">        eta = <span class="number">1</span></span><br><span class="line">    ts = torch.linspace(self.n_steps, <span class="number">0</span>,</span><br><span class="line">                        (ddim_step + <span class="number">1</span>)).to(device).to(torch.long)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(img_or_shape, torch.Tensor):</span><br><span class="line">        x = img_or_shape</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = torch.randn(img_or_shape).to(device)</span><br><span class="line">    batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, ddim_step + <span class="number">1</span>),</span><br><span class="line">                  <span class="string">f&#x27;DDIM sampling with eta <span class="subst">&#123;eta&#125;</span> simple_var <span class="subst">&#123;simple_var&#125;</span>&#x27;</span>):</span><br><span class="line">        cur_t = ts[i - <span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        prev_t = ts[i] - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        ab_cur = self.alpha_bars[cur_t]</span><br><span class="line">        ab_prev = self.alpha_bars[prev_t] <span class="keyword">if</span> prev_t &gt;= <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        t_tensor = torch.tensor([cur_t] * batch_size,</span><br><span class="line">                                dtype=torch.long).to(device).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        eps = net(x, t_tensor)</span><br><span class="line">        var = eta * (<span class="number">1</span> - ab_prev) / (<span class="number">1</span> - ab_cur) * (<span class="number">1</span> - ab_cur / ab_prev)</span><br><span class="line">        noise = torch.randn_like(x)</span><br><span class="line"></span><br><span class="line">        first_term = (ab_prev / ab_cur)**<span class="number">0.5</span> * x</span><br><span class="line">        second_term = ((<span class="number">1</span> - ab_prev - var)**<span class="number">0.5</span> -</span><br><span class="line">                        (ab_prev * (<span class="number">1</span> - ab_cur) / ab_cur)**<span class="number">0.5</span>) * eps</span><br><span class="line">        <span class="keyword">if</span> simple_var:</span><br><span class="line">            third_term = (<span class="number">1</span> - ab_cur / ab_prev)**<span class="number">0.5</span> * noise</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            third_term = var**<span class="number">0.5</span> * noise</span><br><span class="line">        x = first_term + second_term + third_term</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>我们来把整个函数过一遍。先看一下函数的参数。相比DDPM，DDIM的采样会多出两个参数：<code>ddim_step, eta</code>。如正文所述，<code>ddim_step</code>表示执行几轮去噪迭代，<code>eta</code>表示DDPM和DDIM的插值系数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_backward</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                    img_or_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">                    net,</span></span></span><br><span class="line"><span class="params"><span class="function">                    device,</span></span></span><br><span class="line"><span class="params"><span class="function">                    simple_var=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                    ddim_step=<span class="number">20</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                    eta=<span class="number">1</span></span>):</span></span><br></pre></td></tr></table></figure>
<p>在开始迭代前，要做一些预处理。根据论文的描述，如果使用了DDPM的那种简单方差，一定要令<code>eta=1</code>。所以，一开始我们根据<code>simple_var</code>对<code>eta</code>做一个处理。之后，我们要准备好迭代时用到的时刻。整个迭代过程中，我们会用到从<code>self.n_steps</code>到<code>0</code>等间距的<code>ddim_step+1</code>个时刻（<code>self.n_steps</code>是初始时刻，不在去噪迭代中）。比如总时刻<code>self.n_steps=100</code>，<code>ddim_step=10</code>，<code>ts</code>数组里的内容就是<code>[100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 0]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> simple_var:</span><br><span class="line">    eta = <span class="number">1</span></span><br><span class="line">ts = torch.linspace(self.n_steps, <span class="number">0</span>,</span><br><span class="line">                    (ddim_step + <span class="number">1</span>)).to(device).to(torch.long)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(img_or_shape, torch.Tensor):</span><br><span class="line">    x = img_or_shape</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    x = torch.randn(img_or_shape).to(device)</span><br><span class="line">batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">net = net.to(device)</span><br></pre></td></tr></table></figure>
<p>做好预处理后，进入去噪循环。在for循环中，我们从<code>1</code>到<code>ddim_step</code>遍历<code>ts</code>的下标，从时刻数组<code>ts</code>里取出较大的时刻<code>cur_t</code>（正文中的$t_2$）和较小的时刻<code>prev_t</code>（正文中的$t_1$）。由于<code>self.alpha_bars</code>存储的是<code>t=1, t=2, ..., t=n_steps</code>时的变量，时刻和数组下标之间有一个1的偏移，我们要把<code>ts</code>里的时刻减去1得到时刻在<code>self.alpha_bars</code>里的下标，再取出对应的变量<code>ab_cur, ab_prev</code>。注意，在当前时刻为0时，<code>self.alpha_bars</code>是没有定义的。但由于<code>self.alpha_bars</code>表示连乘，我们可以特别地令当前时刻为0（<code>prev_t=-1</code>）时的<code>alpha_bar=1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, ddim_step + <span class="number">1</span>),</span><br><span class="line">              <span class="string">f&#x27;DDIM sampling with eta <span class="subst">&#123;eta&#125;</span> simple_var <span class="subst">&#123;simple_var&#125;</span>&#x27;</span>):</span><br><span class="line">    cur_t = ts[i - <span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">    prev_t = ts[i] - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    ab_cur = self.alpha_bars[cur_t]</span><br><span class="line">    ab_prev = self.alpha_bars[prev_t] <span class="keyword">if</span> prev_t &gt;= <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>准备好时刻后，我们使用和DDPM一样的方法，用U-Net估计生成<code>x_t</code>时的噪声<code>eps</code>，并准备好DDPM采样算法里的噪声<code>noise</code>（公式里的$\mathbf{z}$）。<br>与DDPM不同，在计算方差<code>var</code>时（公式里的$\sigma_t^2$），我们要给方差乘一个权重<code>eta</code>。<br><img src="/2023/07/07/20230702-DDIM/7.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">t_tensor = torch.tensor([cur_t] * batch_size,</span><br><span class="line">                        dtype=torch.long).to(device).unsqueeze(<span class="number">1</span>)</span><br><span class="line">eps = net(x, t_tensor)</span><br><span class="line">var = eta * (<span class="number">1</span> - ab_prev) / (<span class="number">1</span> - ab_cur) * (<span class="number">1</span> - ab_cur / ab_prev)</span><br><span class="line">noise = torch.randn_like(x)</span><br></pre></td></tr></table></figure>
<p>接下来，我们要把之前算好的所有变量用起来，套入DDIM的去噪均值计算公式中。</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) &= 
\mathcal{N}(\mathbf{x}_{t-1}; \\
&\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t-1}-\tilde{\beta}_t} \cdot \frac{\mathbf{x}_{t}-\sqrt{\bar{\alpha}_t}\mathbf{x}_{0}}{\sqrt{1 - \bar{\alpha}_{t}}}, \\ 
&\tilde{\beta}_t\mathbf{I})
\end{aligned}</script><p>也就是(设$\sigma_t^2 = \tilde{\beta}_t$, $\mathbf{z}$为来自标准正态分布的噪声)：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{x}_{t-1} =& \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2} \cdot \frac{\mathbf{x}_{t}-\sqrt{\bar{\alpha}_t}\mathbf{x}_{0}}{\sqrt{1 - \bar{\alpha}_{t}}} + \\
&\sigma_t\mathbf{z}
\end{aligned}</script><p>由于我们只有噪声$\epsilon$，要把$\mathbf{x}_0=(\mathbf{x}_t-\sqrt{1-\bar{\alpha}_t}\epsilon)/\sqrt{\bar{\alpha}_t}$代入，得到不含$\mathbf{x}_0$的公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{x}_{t-1} =& \sqrt{\frac{\bar{\alpha}_{t-1}}{\bar{\alpha}_{t}}} \mathbf{x}_{t}+ \\
&(\sqrt{1-{\bar{\alpha}}_{t-1}-\sigma_t^2} - \sqrt{\frac{\bar{\alpha}_{t-1}(1-\bar{\alpha}_t)}{\bar{\alpha}_t}})\epsilon+\\
&\sigma_t\mathbf{z}
\end{aligned}</script><p>我在代码里把公式的三项分别命名为<code>first_term, second_term, third_term</code>，以便查看。</p>
<p>特别地，当使用DDPM的$\hat{\sigma_t}$方差取值（令$\sigma_t^2=\beta_t=\hat{\sigma_t}^2$）时，不能把这个方差套入公式中，不然$\sqrt{1-{\bar{\alpha}}_{t}-\sigma_t^2}$的根号里的数会小于0。DDIM论文提出的做法是，只修改后面和噪声$\mathbf{z}$有关的方差项，前面这个根号里的方差项保持$\sigma_t^2=(1-\bar{\alpha}_{t-1})/(1 - \bar{\alpha}_{t}) \cdot \beta_t$ ($\eta=1$)的取值。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{x}_{t-1} =& \sqrt{\frac{\bar{\alpha}_{t-1}}{\bar{\alpha}_{t}}} \mathbf{x}_{t}+ \\
&(\sqrt{1-{\bar{\alpha}}_{t-1}-\frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}}\beta_t} - \sqrt{\frac{\bar{\alpha}_{t-1}(1-\bar{\alpha}_t)}{\bar{\alpha}_t}})\epsilon+\\
&\hat{\sigma_t}\mathbf{z}
\end{aligned}</script><p>当然，上面这些公式全都是在描述$t$到$t-1$。当描述$t_2$到$t_1$时，只需要把$\beta_t$换成$1-\frac{\bar{\alpha}_t}{\bar{\alpha}_{t-1}}$，再把所有$t$换成$t_2$，$t-1$换成$t_1$即可。</p>
<p>把上面的公式和处理逻辑翻译成代码，就是这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">first_term = (ab_prev / ab_cur)**<span class="number">0.5</span> * x</span><br><span class="line">second_term = ((<span class="number">1</span> - ab_prev - var)**<span class="number">0.5</span> -</span><br><span class="line">                (ab_prev * (<span class="number">1</span> - ab_cur) / ab_cur)**<span class="number">0.5</span>) * eps</span><br><span class="line"><span class="keyword">if</span> simple_var:</span><br><span class="line">    third_term = (<span class="number">1</span> - ab_cur / ab_prev)**<span class="number">0.5</span> * noise</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    third_term = var**<span class="number">0.5</span> * noise</span><br><span class="line">x = first_term + second_term + third_term</span><br></pre></td></tr></table></figure>
<p>这样，下一刻的<code>x</code>就算完了。反复执行循环即可得到最终的结果。</p>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>写完了DDIM采样后，我们可以编写一个随机生成图片的函数。由于<code>DDPM</code>和<code>DDIM</code>的接口非常相似，我们可以用同一套代码实现DDPM或DDIM的采样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_imgs</span>(<span class="params">ddpm,</span></span></span><br><span class="line"><span class="params"><span class="function">                net,</span></span></span><br><span class="line"><span class="params"><span class="function">                output_path,</span></span></span><br><span class="line"><span class="params"><span class="function">                img_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_sample=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                simple_var=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                to_bgr=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">if</span> img_shape[<span class="number">1</span>] &gt;= <span class="number">256</span>:</span><br><span class="line">        max_batch_size = <span class="number">16</span></span><br><span class="line">    <span class="keyword">elif</span> img_shape[<span class="number">1</span>] &gt;= <span class="number">128</span>:</span><br><span class="line">        max_batch_size = <span class="number">64</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        max_batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    net = net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">while</span> n_sample &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> n_sample &gt;= max_batch_size:</span><br><span class="line">                batch_size = max_batch_size</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                batch_size = n_sample</span><br><span class="line">            n_sample -= batch_size</span><br><span class="line">            shape = (batch_size, *img_shape)</span><br><span class="line">            imgs = ddpm.sample_backward(shape,</span><br><span class="line">                                        net,</span><br><span class="line">                                        device=device,</span><br><span class="line">                                        simple_var=simple_var,</span><br><span class="line">                                        **kwargs).detach().cpu()</span><br><span class="line">            imgs = (imgs + <span class="number">1</span>) / <span class="number">2</span> * <span class="number">255</span></span><br><span class="line">            imgs = imgs.clamp(<span class="number">0</span>, <span class="number">255</span>).to(torch.uint8)</span><br><span class="line"></span><br><span class="line">            img_list = einops.rearrange(imgs, <span class="string">&#x27;n c h w -&gt; n h w c&#x27;</span>).numpy()</span><br><span class="line">            output_dir = os.path.splitext(output_path)[<span class="number">0</span>]</span><br><span class="line">            os.makedirs(output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">for</span> i, img <span class="keyword">in</span> <span class="built_in">enumerate</span>(img_list):</span><br><span class="line">                <span class="keyword">if</span> to_bgr:</span><br><span class="line">                    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)</span><br><span class="line">                cv2.imwrite(<span class="string">f&#x27;<span class="subst">&#123;output_dir&#125;</span>/<span class="subst">&#123;i+index&#125;</span>.jpg&#x27;</span>, img)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># First iteration</span></span><br><span class="line">            <span class="keyword">if</span> index == <span class="number">0</span>:</span><br><span class="line">                imgs = einops.rearrange(imgs,</span><br><span class="line">                                        <span class="string">&#x27;(b1 b2) c h w -&gt; (b1 h) (b2 w) c&#x27;</span>,</span><br><span class="line">                                        b1=<span class="built_in">int</span>(batch_size**<span class="number">0.5</span>))</span><br><span class="line">                imgs = imgs.numpy()</span><br><span class="line">                <span class="keyword">if</span> to_bgr:</span><br><span class="line">                    imgs = cv2.cvtColor(imgs, cv2.COLOR_RGB2BGR)</span><br><span class="line">                cv2.imwrite(output_path, imgs)</span><br><span class="line"></span><br><span class="line">            index += batch_size</span><br></pre></td></tr></table></figure>
<p>为了生成大量图片以计算FID，在这个函数中我加入了很多和batch有关的处理。剔除这些处理代码以及图像存储后处理代码，和采样有关的核心代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_imgs</span>(<span class="params">ddpm,</span></span></span><br><span class="line"><span class="params"><span class="function">                net,</span></span></span><br><span class="line"><span class="params"><span class="function">                output_path,</span></span></span><br><span class="line"><span class="params"><span class="function">                img_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_sample=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                simple_var=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                to_bgr=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    net = net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        shape = (n_sample, *img_shape)</span><br><span class="line">        imgs = ddpm.sample_backward(shape,</span><br><span class="line">                                    net,</span><br><span class="line">                                    device=device,</span><br><span class="line">                                    simple_var=simple_var,</span><br><span class="line">                                    **kwargs).detach().cpu()</span><br></pre></td></tr></table></figure>
<p>如果是用DDPM采样，把参数表里的那些参数填完就行了；如果是DDIM采样，则需要在<code>kwargs</code>里指定<code>ddim_step</code>和<code>eta</code>。</p>
<p>使用这个函数，我们可以进行不同<code>ddim_step</code>和不同<code>eta</code>下的64x64 CelebAHQ采样实验，以尝试复现DDIM论文的实验结果。</p>
<p><img src="/2023/07/07/20230702-DDIM/4.jpg" alt></p>
<p>我们先准备好变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = UNet(n_steps, img_shape, cfg[<span class="string">&#x27;channels&#x27;</span>], cfg[<span class="string">&#x27;pe_dim&#x27;</span>],</span><br><span class="line">            cfg.get(<span class="string">&#x27;with_attn&#x27;</span>, <span class="literal">False</span>), cfg.get(<span class="string">&#x27;norm_type&#x27;</span>, <span class="string">&#x27;ln&#x27;</span>))</span><br><span class="line">ddpm = DDPM(device, n_steps)</span><br><span class="line">ddim = DDIM(device, n_steps)</span><br><span class="line">net.load_state_dict(torch.load(model_path))</span><br></pre></td></tr></table></figure>
<p>第一组实验是总时刻保持1000，使用$\hat{\sigma}_t$（标准DDPM）和$\eta=0$（标准DDIM）的实验。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sample_imgs(ddpm,</span><br><span class="line">            net,</span><br><span class="line">            <span class="string">&#x27;work_dirs/diffusion_ddpm_sigma_hat.jpg&#x27;</span>,</span><br><span class="line">            img_shape,</span><br><span class="line">            device=device,</span><br><span class="line">            to_bgr=to_bgr)</span><br><span class="line">sample_imgs(ddim,</span><br><span class="line">            net,</span><br><span class="line">            <span class="string">&#x27;work_dirs/diffusion_ddpm_eta_0.jpg&#x27;</span>,</span><br><span class="line">            img_shape,</span><br><span class="line">            device=device,</span><br><span class="line">            to_bgr=to_bgr,</span><br><span class="line">            ddim_step=<span class="number">1000</span>,</span><br><span class="line">            simple_var=<span class="literal">False</span>,</span><br><span class="line">            eta=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>把参数<code>n_samples</code>改成<code>30000</code>，就可以生成30000张图像，以和30000张图像的CelebAHQ之间算FID指标。由于总时刻1000的采样速度非常非常慢，建议使用<code>dist_sample.py</code>并行采样。</p>
<p>算FID指标时，可以使用torch fidelity库。使用pip即可安装此库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch-fidelity</span><br></pre></td></tr></table></figure>
<p>之后就可以使用命令<code>fidelity</code>来算指标了。假设我们把降采样过的CelebAHQ存储在<code>data/celebA/celeba_hq_64</code>，把我们生成的30000张图片存在<code>work_dirs/diffusion_ddpm_sigma_hat</code>，就可以用下面的命令算FID指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fidelity --gpu <span class="number">0</span> --fid --input1 work_dirs/diffusion_ddpm_sigma_hat --input2 data/celebA/celeba_hq_64</span><br></pre></td></tr></table></figure>
<p>整体来看，我的模型比论文差一点，总的FID会高一点。各个配置下的对比结果也稍有出入。在第一组实验中，使用$\hat{\sigma}_t$时，我的FID是13.68；使用$\eta=0$时，我的FID是13.09。而论文中用$\hat{\sigma}_t$时的FID比$\eta=0$时更低。</p>
<p>我们还可以做第二组实验，测试<code>ddim_step=20</code>（我设置的默认步数）时使用$\eta=0$, $\eta=1$, $\hat{\sigma}_t$的生成效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">sample_imgs(ddim,</span><br><span class="line">            net,</span><br><span class="line">            <span class="string">&#x27;work_dirs/diffusion_ddim_sigma_hat.jpg&#x27;</span>,</span><br><span class="line">            img_shape,</span><br><span class="line">            device=device,</span><br><span class="line">            simple_var=<span class="literal">True</span>,</span><br><span class="line">            to_bgr=to_bgr)</span><br><span class="line">sample_imgs(ddim,</span><br><span class="line">            net,</span><br><span class="line">            <span class="string">&#x27;work_dirs/diffusion_ddim_eta_1.jpg&#x27;</span>,</span><br><span class="line">            img_shape,</span><br><span class="line">            device=device,</span><br><span class="line">            simple_var=<span class="literal">False</span>,</span><br><span class="line">            eta=<span class="number">1</span>,</span><br><span class="line">            to_bgr=to_bgr)</span><br><span class="line">sample_imgs(ddim,</span><br><span class="line">            net,</span><br><span class="line">            <span class="string">&#x27;work_dirs/diffusion_ddim_eta_0.jpg&#x27;</span>,</span><br><span class="line">            img_shape,</span><br><span class="line">            device=device,</span><br><span class="line">            simple_var=<span class="literal">False</span>,</span><br><span class="line">            eta=<span class="number">0</span>,</span><br><span class="line">            to_bgr=to_bgr)</span><br></pre></td></tr></table></figure>
<p>我的FID结果是：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eta=0: 17.80</span><br><span class="line">eta=1: 24.00</span><br><span class="line">sigma hat: 213.16</span><br></pre></td></tr></table></figure><br>这里得到的实验结果和论文一致。减少采样迭代次数后，生成质量略有降低。同采样步数下，<code>eta=0</code>最优。使用<code>sigma hat</code>的结果会有非常多的噪声，差得完全不能看。</p>
<p>综合上面两个实验来看，不管什么情况下，使用<code>eta=0</code>，得到的结果都不会太差。</p>
<p>从生成速度上来看，在64x64 CelebAHQ上生成256张图片，<code>ddim_step=20</code>时只要3秒不到，而<code>ddim_step=1000</code>时要200秒。基本上是步数减少到几分之一就提速几倍。可见，DDIM加速采样对于扩散模型来说是必要的。</p>
<h2 id="参考文献及学习提示"><a href="#参考文献及学习提示" class="headerlink" title="参考文献及学习提示"></a>参考文献及学习提示</h2><p>如果对DDIM公式推导及其他数学知识感兴趣，欢迎阅读苏剑林的文章：<br><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/9181。">https://spaces.ac.cn/archives/9181。</a></p>
<p>DDIM的论文为Denoising diffusion implicit models(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.02502)。">https://arxiv.org/abs/2010.02502)。</a></p>
<p>我在本文使用的公式符号都基于DDPM论文，与上面两篇文章使用的符号不一样。比如DDIM论文里的$\alpha$在本文中是用$\bar{\alpha}$表示。</p>
<p>DDIM论文在介绍新均值公式时很不友好地在3.1节直接不加解释地给出了公式的形式，并在附录B中以先给结论再证明这种和逻辑思维完全反过来的方法介绍了公式的由来。建议去阅读苏剑林的文章，看看是怎么按正常的思考方式正向推导出DDIM公式。</p>
<p>除了在3.1节直接甩给你一个公式外，DDIM论文后面的地方都很好读懂。DDIM后面还介绍了一些比较有趣的内容，比如4.3节介绍了扩散模型和常微分方程的关系，它可以帮助我们理解为什么DDPM会设置$T=1000$这么长的加噪步数。5.3节中作者介绍了如何用DDIM在两幅图像间插值。</p>
<p>要回顾DDPM的知识，欢迎阅读我之前的文章：DDPM详解。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2023/07/07/20230330-diffusion-model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/07/20230330-diffusion-model/" class="post-title-link" itemprop="url">扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-07 20:56:55" itemprop="dateCreated datePublished" datetime="2023-07-07T20:56:55+08:00">2023-07-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在过去的大半年里，以Stable Diffusion为代表的AI绘画是世界上最为火热的AI方向之一。或许大家会有疑问，Stable Diffusion里的这个”Diffusion”是什么意思？其实，扩散模型(Diffusion Model)正是Stable Diffusion中负责生成图像的模型。想要理解Stable Diffusion的原理，就一定绕不过扩散模型的学习。</p>
<p><img src="/2023/07/07/20230330-diffusion-model/1.jpg" alt="Stable Diffusion以「毕加索笔下的《最后的晚餐》」为题的绘画结果"></p>
<p>在这篇文章里，我会由浅入深地对最基础的去噪扩散概率模型（Denoising Diffusion Probabilistic Models, DDPM）进行讲解。我会先介绍扩散模型生成图像的基本原理，再用简单的数学语言对扩散模型建模，最后给出扩散模型的一份PyTorch实现。本文不会堆砌过于复杂的数学公式，哪怕你没有相关的数学背景，也能够轻松理解扩散模型的原理。</p>
<h2 id="扩散模型与图像生成"><a href="#扩散模型与图像生成" class="headerlink" title="扩散模型与图像生成"></a>扩散模型与图像生成</h2><p>在认识扩散模型之前，我们先退一步，看看一般的神经网络模型是怎么生成图像的。显然，为了生成丰富的图像，一个图像生成程序要根据随机数来生成图像。通常，这种随机数是一个满足标准正态分布的随机向量。这样，每次要生成新图像时，只需要从标准正态分布里随机生成一个向量并输入给程序就行了。</p>
<p>而在AI绘画程序中，负责生成图像的是一个神经网络模型。神经网络需要从数据中学习。对于图像生成任务，神经网络的训练数据一般是一些同类型的图片。比如一个绘制人脸的神经网络会用人脸照片来训练。也就是说，神经网络会学习如何把一个向量映射成一张图片，并确保这个图片和训练集的图片是一类图片。</p>
<p>可是，相比其他AI任务，图像生成任务对神经网络来说更加困难一点——图像生成任务缺乏有效的指导。在其他AI任务中，训练集本身会给出一个「标准答案」，指导AI的输出向标准答案靠拢。比如对于图像分类任务，训练集会给出每一幅图像的类别；对于人脸验证任务，训练集会给出两张人脸照片是不是同一个人；对于目标检测任务，训练集会给出目标的具体位置。然而，图像生成任务是没有标准答案的。图像生成数据集里只有一些同类型图片，却没有指导AI如何画得更好的信息。</p>
<p><img src="/2023/07/07/20230330-diffusion-model/2.jpg" alt></p>
<p>为了解决这一问题，人们专门设计了一些用于生成图像的神经网络架构。这些架构中比较出名的有生成对抗模型（GAN）和变分自编码器（VAE）。</p>
<p>GAN的想法是，既然不知道一幅图片好不好，就干脆再训练一个神经网络，用于辨别某图片是不是和训练集里的图片长得一样。生成图像的神经网络叫做生成器，鉴定图像的神经网络叫做判别器。两个网络互相对抗，共同进步。</p>
<p><img src="/2023/07/07/20230330-diffusion-model/3.jpg" alt></p>
<p>VAE则使用了逆向思维：学习向量生成图像很困难，那就再同时学习怎么用图像生成向量。这样，把某图像变成向量，再用该向量生成图像，就应该得到一幅和原图像一模一样的图像。每一个向量的绘画结果有了一个标准答案，可以用一般的优化方法来指导网络的训练了。VAE中，把图像变成向量的网络叫做编码器，把向量转换回图像的网络叫做解码器。其中，解码器就是负责生成图像的模型。</p>
<p><img src="/2023/07/07/20230330-diffusion-model/4.jpg" alt></p>
<p>一直以来，GAN的生成效果较好，但训练起来比VAE麻烦很多。有没有和GAN一样强大，训练起来又方便的生成网络架构呢？扩散模型正是满足这些要求的生成网络架构。</p>
<p>扩散模型是一种特殊的VAE，其灵感来自于热力学：一个分布可以通过不断地添加噪声变成另一个分布。放到图像生成任务里，就是来自训练集的图像可以通过不断添加噪声变成符合标准正态分布的图像。从这个角度出发，我们可以对VAE做以下修改：1）不再训练一个可学习的编码器，而是把编码过程固定成不断添加噪声的过程；2）不再把图像压缩成更短的向量，而是自始至终都对一个等大的图像做操作。解码器依然是一个可学习的神经网络，它的目的也同样是实现编码的逆操作。不过，既然现在编码过程变成了加噪，那么解码器就应该负责去噪。而对于神经网络来说，去噪任务学习起来会更加有效。因此，扩散模型既不会涉及GAN中复杂的对抗训练，又比VAE更强大一点。</p>
<p>具体来说，扩散模型由<strong>正向过程</strong>和<strong>反向过程</strong>这两部分组成，对应VAE中的编码和解码。在正向过程中，输入$\mathbf{x}_0$会不断混入高斯噪声。经过$T$次加噪声操作后，图像$\mathbf{x}_T$会变成一幅符合标准正态分布的纯噪声图像。而在反向过程中，我们希望训练出一个神经网络，该网络能够学会$T$个去噪声操作，把$\mathbf{x}_T$还原回$\mathbf{x}_0$。网络的学习目标是让$T$个去噪声操作正好能抵消掉对应的加噪声操作。训练完毕后，只需要从标准正态分布里随机采样出一个噪声，再利用反向过程里的神经网络把该噪声恢复成一幅图像，就能够生成一幅图片了。</p>
<blockquote>
<p>高斯噪声，就是一幅各处颜色值都满足高斯分布（正态分布）的噪声图像。</p>
</blockquote>
<p><img src="/2023/07/07/20230330-diffusion-model/5.jpg" alt></p>
<p>总结一下，图像生成网络会学习如何把一个向量映射成一幅图像。设计网络架构时，最重要的是设计学习目标，让网络生成的图像和给定数据集里的图像相似。VAE的做法是使用两个网络，一个学习把图像编码成向量，另一个学习把向量解码回图像，它们的目标是让复原图像和原图像尽可能相似。学习完毕后，解码器就是图像生成网络。扩散模型是一种更具体的VAE。它把编码过程固定为加噪声，并让解码器学习怎么样消除之前添加的每一步噪声。</p>
<h2 id="扩散模型的具体算法"><a href="#扩散模型的具体算法" class="headerlink" title="扩散模型的具体算法"></a>扩散模型的具体算法</h2><p>上一节中，我们只是大概了解扩散模型的整体思想。这一节，我们来引入一些数学表示，来看一看扩散模型的训练算法和采样算法具体是什么。为了便于理解，这一节会出现一些不是那么严谨的数学描述。更加详细的一些数学推导会放到下一节里介绍。</p>
<h3 id="前向过程"><a href="#前向过程" class="headerlink" title="前向过程"></a>前向过程</h3><p>在前向过程中，来自训练集的图像$\mathbf{x}_0$会被添加$T$次噪声，使得$x_T$为符合标准正态分布。准确来说，「加噪声」并不是给上一时刻的图像加上噪声值，而是从一个均值与上一时刻图像相关的正态分布里采样出一幅新图像。如下面的公式所示，$\mathbf{x}_{t - 1}$是上一时刻的图像，$\mathbf{x}_{t}$是这一时刻生成的图像，该图像是从一个均值与$\mathbf{x}_{t - 1}$有关的正态分布里采样出来的。</p>
<script type="math/tex; mode=display">
\mathbf{x}_t \sim \mathcal{N}(\mu_t(\mathbf{x}_{t - 1}),\sigma_t^2\mathbf{I})</script><blockquote>
<p>多数文章会说前向过程是一个<strong>马尔可夫过程</strong>。其实，马尔可夫过程的意思就是当前时刻的状态只由上一时刻的状态决定，而不由更早的状态决定。上面的公式表明，计算$\mathbf{x}_t$，只需要用到$\mathbf{x}_{t - 1}$，而不需要用到$\mathbf{x}_{t - 2}, \mathbf{x}_{t - 3}…$，这符合马尔可夫过程的定义。</p>
</blockquote>
<p>绝大多数扩散模型会把这个正态分布设置成这个形式：</p>
<script type="math/tex; mode=display">
\mathbf{x}_t \sim \mathcal{N}(\sqrt{1 - \beta_t}\mathbf{x}_{t - 1},\beta_t\mathbf{I})</script><p>这个正态分布公式乍看起来很奇怪：$\sqrt{1 - \beta_t}$是哪里冒出来的？为什么会有这种奇怪的系数？别急，我们先来看另一个问题：假如给定$\mathbf{x}_{0}$，也就是从训练集里采样出一幅图片，该怎么计算任意一个时刻$t$的噪声图像$\mathbf{x}_{t}$呢？</p>
<p>我们不妨按照公式，从$\mathbf{x}_{t}$开始倒推。$\mathbf{x}_{t}$其实可以通过一个标准正态分布的样本$\epsilon_{t-1}$算出来：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{x}_t &\sim \mathcal{N}(\sqrt{1 - \beta_t}\mathbf{x}_{t - 1},\beta_t\mathbf{I}) \\
\Rightarrow \mathbf{x}_t &= \sqrt{1 - \beta_t}\mathbf{x}_{t - 1} + \sqrt{\beta_t}\epsilon_{t-1}; \epsilon_{t-1} \sim \mathcal{N}(0, \mathbf{I})
\end{aligned}</script><p>再往前推几步：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{x}_t &= \sqrt{1 - \beta_t}\mathbf{x}_{t - 1} + \sqrt{\beta_t}\epsilon_{t-1}\\
 &= \sqrt{1 - \beta_t}(\sqrt{1 - \beta_{t-1}}\mathbf{x}_{t - 2} + \sqrt{\beta_{t-1}}\epsilon_{t-2}) + \sqrt{\beta_t}\epsilon_{t-1}
 \\
&= \sqrt{(1 - \beta_t)(1 - \beta_{t-1})}\mathbf{x}_{t - 2} + \sqrt{(1 - \beta_t)\beta_{t-1}}\epsilon_{t-2} + \sqrt{\beta_t}\epsilon_{t-1}
\end{aligned}</script><p>由正态分布的性质可知，均值相同的正态分布「加」在一起后，方差也会加到一起。也就是$\mathcal{N}(0, \sigma_1^2 I)$与$\mathcal{N}(0, \sigma_2^2 I)$合起来会得到$\mathcal{N}(0, (\sigma_1^2+\sigma_2^2) I)$。根据这一性质，上面的公式可以化简为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\sqrt{(1 - \beta_t)(1 - \beta_{t-1})}\mathbf{x}_{t - 2} + \sqrt{(1 - \beta_t)\beta_{t-1}}\epsilon_{t-2} + \sqrt{\beta_t}\epsilon_{t-1} \\
= & \sqrt{(1 - \beta_t)(1 - \beta_{t-1})}\mathbf{x}_{t - 2} + \sqrt{(1 - \beta_t)\beta_{t-1} + \beta_t}\epsilon \\
= & \sqrt{(1 - \beta_t)(1 - \beta_{t-1})}\mathbf{x}_{t - 2} + \sqrt{1-(1-\beta_t)(1-\beta_{t-1})}\epsilon
\end{aligned}</script><p>再往前推一步的话，结果是：</p>
<script type="math/tex; mode=display">
\sqrt{(1 - \beta_t)(1 - \beta_{t-1})(1 - \beta_{t-2})}\mathbf{x}_{t - 3} + \sqrt{1-(1-\beta_t)(1-\beta_{t-1})(1 - \beta_{t-2})}\epsilon</script><p>我们已经能够猜出规律来了，可以一直把公式推到$\mathbf{x}_{0}$。令$\alpha_t=1-\beta_t, \bar{\alpha}_t=\prod_{i=1}^t\alpha_i$，则：</p>
<script type="math/tex; mode=display">
\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_t}\epsilon</script><p>有了这个公式，我们就可以讨论加噪声公式为什么是$\mathbf{x}_t \sim \mathcal{N}(\sqrt{1 - \beta_t}\mathbf{x}_{t - 1},\beta_t\mathbf{I})$了。这个公式里的$\beta_t$是一个小于1的常数。在DDPM论文中，$\beta_t$从$\beta_1=10^{-4}$到$\beta_T=0.02$线性增长。这样，$\beta_t$变大，$\alpha_t$也越小，$\bar{\alpha}_t$趋于0的速度越来越快。最后，$\bar{\alpha}_T$几乎为0，代入$\mathbf{x}_T = \sqrt{\bar{\alpha}_T}\mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_T}\epsilon$, $\mathbf{x}_T$就满足标准正态分布了，符合我们对扩散模型的要求。上述推断可以简单描述为：加噪声公式能够从慢到快地改变原图像，让图像最终均值为0，方差为$\mathbf{I}$。</p>
<blockquote>
<p>大家不妨尝试一下，设加噪声公式中均值和方差前的系数分别为$a, b$，按照上述过程计算最终分布的方差。只有$a^2 + b^2 = 1$才能保证最后$\mathbf{x}_T$的方差系数为1。</p>
</blockquote>
<h3 id="反向过程"><a href="#反向过程" class="headerlink" title="反向过程"></a>反向过程</h3><p>在正向过程中，我们人为设置了$T$步加噪声过程。而在反向过程中，我们希望能够倒过来取消每一步加噪声操作，让一幅纯噪声图像变回数据集里的图像。这样，利用这个去噪声过程，我们就可以把任意一个从标准正态分布里采样出来的噪声图像变成一幅和训练数据长得差不多的图像，从而起到图像生成的目的。</p>
<p>现在问题来了：去噪声操作的数学形式是怎么样的？怎么让神经网络来学习它呢？数学原理表明，当$\beta_t$足够小时，每一步加噪声的逆操作也满足正态分布。</p>
<script type="math/tex; mode=display">
\mathbf{x}_{t-1} \sim \mathcal{N}(\tilde{\mu}_t, \tilde{\beta}_t\mathbf{I})</script><p>其中，当前时刻加噪声逆操作的均值$\tilde{\mu}_t$和方差$\tilde{\beta}_t$由当前的时刻$t$、当前的图像$\mathbf{x}_{t}$决定。因此，为了描述所有去噪声操作，神经网络应该输入$t$、$\mathbf{x}_{t}$，拟合当前的均值$\tilde{\mu}_t$和方差$\tilde{\beta}_t$。</p>
<blockquote>
<p>不要被上文的「去噪声」、「加噪声逆操作」绕晕了哦。由于加噪声是固定的，加噪声的逆操作也是固定的。理想情况下，我们希望去噪操作就等于加噪声逆操作。然而，加噪声的逆操作不太可能从理论上求得，我们只能用一个神经网络去拟合它。去噪声操作和加噪声逆操作的关系，就是神经网络的预测值和真值的关系。</p>
</blockquote>
<p>现在问题来了：加噪声逆操作的均值和方差是什么？</p>
<p>直接计算所有数据的加噪声逆操作的分布是不太现实的。但是，如果给定了某个训练集输入$\mathbf{x}_0$，多了一个限定条件后，该分布是可以用贝叶斯公式计算的（其中$q$表示概率分布）：</p>
<script type="math/tex; mode=display">
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) = q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)\frac{q(\mathbf{x}_{t-1} | \mathbf{x}_0)}{q(\mathbf{x}_{t} | \mathbf{x}_0)}</script><p>等式左边的$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t-1};\tilde{\mu}_t, \tilde{\beta}_t\mathbf{I})$表示加噪声操作的逆操作，它的均值和方差都是待求的。右边的$q(\mathbf{x}_{t} | \mathbf{x}_{t-1}, \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t};\sqrt{1 - \beta_t}\mathbf{x}_{t - 1},\beta_t\mathbf{I})$是加噪声的分布。而由于$\mathbf{x}_0$已知，$q(\mathbf{x}_{t-1} | \mathbf{x}_0)$和$q(\mathbf{x}_{t} | \mathbf{x}_0)$两项可以根据前面的公式$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_t}\epsilon_t$得来：</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(\mathbf{x}_{t} | \mathbf{x}_0)&=\mathcal{N}(\mathbf{x}_{t}; \sqrt{\bar{\alpha}_t}\mathbf{x}_{0}, (1-\bar{\alpha}_t)\mathbf{I}) \\
q(\mathbf{x}_{t-1} | \mathbf{x}_0)&=\mathcal{N}(\mathbf{x}_{t-1}; \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_{0}, (1-\bar{\alpha}_{t-1})\mathbf{I})
\end{aligned}</script><p>这样，等式右边的式子全部已知。我们可以把公式套入，算出给定$\mathbf{x}_0$时的去噪声分布。经计算化简，分布的均值为:</p>
<script type="math/tex; mode=display">
\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t -\frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t)</script><p>其中，$\epsilon_t$是用公式算$\mathbf{x}_t$时从标准正态分布采样出的样本，它来自公式</p>
<script type="math/tex; mode=display">
\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_t}\epsilon_t</script><p>分布的方差为：</p>
<script type="math/tex; mode=display">
\tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}} \cdot \beta_t</script><p>注意，$\beta_t$是加噪声的方差，是一个常量。那么，加噪声逆操作的方差$\tilde{\beta}_t$也是一个常量，不与输入$\mathbf{x}_0$相关。这下就省事了，训练去噪网络时，神经网络只用拟合$T$个均值就行，不用再拟合方差了。</p>
<p>知道了均值和方差的真值，训练神经网络只差最后的问题了：该怎么设置训练的损失函数？加噪声逆操作和去噪声操作都是正态分布，网络的训练目标应该是让每对正态分布更加接近。那怎么用损失函数描述两个分布尽可能接近呢？最直观的想法，肯定是让两个正态分布的均值尽可能接近，方差尽可能接近。根据上文的分析，方差是常量，只用让均值尽可能接近就可以了。</p>
<p>那怎么用数学公式表达让均值更接近呢？再观察一下目标均值的公式：</p>
<script type="math/tex; mode=display">
\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t -\frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t)</script><p>神经网络拟合均值时，$\mathbf{x}_{t}$是已知的（别忘了，图像是一步一步倒着去噪的）。式子里唯一不确定的只有$\epsilon_t$。既然如此，神经网络干脆也别预测均值了，直接预测一个噪声$\epsilon_\theta(\mathbf{x}_{t}, t)$（其中$\theta$为可学习参数），让它和生成$\mathbf{x}_{t}$的噪声$\epsilon_t$的均方误差最小就行了。对于一轮训练，最终的误差函数可以写成</p>
<script type="math/tex; mode=display">
L=||\epsilon_t - \epsilon_\theta(\mathbf{x}_{t}, t)||^2</script><p>这样，我们就认识了反向过程的所有内容。总结一下，反向过程中，神经网络应该让$T$个去噪声操作拟合对应的$T$个加噪声逆操作。每步加噪声逆操作符合正态分布，且在给定某个输入时，该正态分布的均值和方差是可以用解析式表达出来的。因此，神经网络的学习目标就是让其输出的去噪声分布和理论计算的加噪声逆操作分布一致。经过数学计算上的一些化简，问题被转换成了拟合生成$\mathbf{x}_{t}$时用到的随机噪声$\epsilon_t$。</p>
<h3 id="训练算法与采样算法"><a href="#训练算法与采样算法" class="headerlink" title="训练算法与采样算法"></a>训练算法与采样算法</h3><p>理解了前向过程和反向过程后，训练神经网络的算法和采样图片（生成图片）的算法就呼之欲出了。</p>
<p>以下是DDPM论文中的训练算法：</p>
<p><img src="/2023/07/07/20230330-diffusion-model/6.jpg" alt></p>
<p>让我们来逐行理解一下这个算法。第二行是指从训练集里取一个数据$\mathbf{x}_{0}$。第三行是指随机从$1, …, T$里取一个时刻用来训练。我们虽然要求神经网络拟合$T$个正态分布，但实际训练时，不用一轮预测$T$个结果，只需要随机预测$T$个时刻中某一个时刻的结果就行。第四行指随机生成一个噪声$\epsilon$，该噪声是用于执行前向过程生成$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_t}\epsilon$的。之后，我们把$\mathbf{x}_t$和$t$传给神经网络$\epsilon_\theta(\mathbf{x}_{t}, t)$，让神经网络预测随机噪声。训练的损失函数是预测噪声和实际噪声之间的均方误差，对此损失函数采用梯度下降即可优化网络。</p>
<p>DDPM并没有规定神经网络的结构。根据任务的难易程度，我们可以自己定义简单或复杂的网络结构。这里只需要把$\epsilon_\theta(\mathbf{x}_{t}, t)$当成一个普通的映射即可。</p>
<p>训练好了网络后，我们可以执行反向过程，对任意一幅噪声图像去噪，以实现图像生成。这个算法如下：</p>
<p><img src="/2023/07/07/20230330-diffusion-model/7.jpg" alt></p>
<p>第一行的$\mathbf{x}_{T}$就是从标准正态分布里随机采样的输入噪声。要生成不同的图像，只需要更换这个噪声。后面的过程就是扩散模型的反向过程。令时刻从$T$到$1$，计算这一时刻去噪声操作的均值和方差，并采样出$\mathbf{x}_{t-1}$。均值是用之前提到的公式计算的：</p>
<script type="math/tex; mode=display">
\mu_{\theta}(\mathbf{x}_{t}, t) = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t -\frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(\mathbf{x}_{t}, t))
\\</script><p>而方差$\sigma_t^2$的公式有两种选择，两个公式都能产生差不多的结果。实验表明，当$\mathbf{x}_{0}$是特定的某个数据时，用上一节推导出来的方差最好。</p>
<script type="math/tex; mode=display">
\sigma_t^2=\frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}} \cdot \beta_t</script><p>而当$\mathbf{x}_{0} \sim \mathcal{N}(0, \mathbf{I})$时，只需要令方差和加噪声时的方差一样即可。</p>
<script type="math/tex; mode=display">
\sigma_t^2= \beta_t</script><p>循环执行去噪声操作。最后生成的$\mathbf{x}_{0}$就是生成出来的图像。</p>
<p>特别地，最后一步去噪声是不用加方差项的。为什么呢，观察公式$\sigma_t^2=\frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}} \cdot \beta_t$。当$t=1$时，分子会出现$\bar{\alpha}_{t-1}=\bar{\alpha}_0$这一项。$\bar{\alpha}_t$是一个连乘，理论上$t$是从$1$开始的，在$t=0$时没有定义。但我们可以特别地令连乘的第0项$\bar{\alpha}_0=1$。这样，$t=1$时方差项的分子$1-\bar{\alpha}_{t-1}$为$0$，不用算这一项了。</p>
<blockquote>
<p>当然，这一解释从数学上来说是不严谨的。据论文说，这部分的解释可以参见朗之万动力学。</p>
</blockquote>
<h2 id="数学推导的补充-（选读）"><a href="#数学推导的补充-（选读）" class="headerlink" title="数学推导的补充 （选读）"></a>数学推导的补充 （选读）</h2><p>理解了训练算法和采样算法，我们就算是搞懂了扩散模型，可以去编写代码了。不过，上文的描述省略了一些数学推导的细节。如果对扩散模型更深的原理感兴趣，可以阅读一下本节。</p>
<h3 id="加噪声逆操作均值和方差的推导"><a href="#加噪声逆操作均值和方差的推导" class="headerlink" title="加噪声逆操作均值和方差的推导"></a>加噪声逆操作均值和方差的推导</h3><p>上一节，我们根据下面几个式子</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) = q(\mathbf{x}_{t} | \mathbf{x}_{t - 1}, \mathbf{x}_0)\frac{q(\mathbf{x}_{t-1} | \mathbf{x}_0)}{q(\mathbf{x}_{t} | \mathbf{x}_0)} \\
q(\mathbf{x}_{t} | \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t}; \sqrt{\bar{\alpha}_t}\mathbf{x}_{0}, (1-\bar{\alpha}_t)\mathbf{I})\\
q(\mathbf{x}_{t} | \mathbf{x}_{t-1}, \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t};\sqrt{1 - \beta_t}\mathbf{x}_{t - 1},\beta_t\mathbf{I})
\end{aligned}</script><p>一步就给出了$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)=\mathcal{N}(\mathbf{x}_{t-1}; \tilde{\mu}_t, \tilde{\beta}_t\mathbf{I})$的均值和方差。</p>
<script type="math/tex; mode=display">
\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t -\frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t)</script><script type="math/tex; mode=display">
\tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}} \cdot \beta_t</script><p>现在我们来看一下推导均值和方差的思路。</p>
<p>首先，把其他几个式子带入贝叶斯公式的等式右边。</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) = &
\frac{1}{\beta_t\sqrt{2\pi}}exp(-\frac{(\mathbf{x}_{t}-\sqrt{1 - \beta_t}\mathbf{x}_{t - 1})^2}{2\beta_t}) \cdot \\

&\frac{1}{(1-\bar{\alpha}_{t-1})\sqrt{2\pi}} exp(-\frac{(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0})^2}{2(1-\bar{\alpha}_{t-1})})\cdot \\

&(\frac{1}{(1-\bar{\alpha}_t)\sqrt{2\pi}} exp(-\frac{(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0})^2}{2(1-\bar{\alpha}_{t})}))^{-1}



\end{aligned}</script><p>由于多个正态分布的乘积还是一个正态分布，我们知道$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$也可以用一个正态分布公式$\mathcal{N}(\mathbf{x}_{t-1}; \tilde{\mu}_t, \tilde{\beta}_t\mathbf{I})$表达，它最后一定能写成这种形式：</p>
<script type="math/tex; mode=display">
q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) = \frac{1}{\tilde{\beta}_t\sqrt{2\pi}}exp(-\frac{(\mathbf{x}_{t-1}-\tilde{\mu}_t)^2}{2\tilde{\beta}_t})</script><p>问题就变成了怎么把开始那个很长的式子化简，算出$\tilde{\mu}_t$和$\tilde{\beta}_t$。</p>
<p>方差$\tilde{\beta}_t$可以从指数函数的系数得来，比较好求。系数为</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\frac{1}{\beta_t\sqrt{2\pi}} \cdot \frac{1}{(1-\bar{\alpha}_{t-1})\sqrt{2\pi}} \cdot (\frac{1}{(1-\bar{\alpha}_t)\sqrt{2\pi}})^{-1} \\
=&\frac{(1-\bar{\alpha}_t)}{\beta_t(1-\bar{\alpha}_{t-1})\sqrt{2\pi}}
\end{aligned}</script><p>所以，方差为：</p>
<script type="math/tex; mode=display">
\tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}} \cdot \beta_t</script><p>接下来只要关注指数函数的指数部分。指数部分一定是一个关于的$\mathbf{x}_{t-1}$的二次函数，只要化简成$(\mathbf{x}_{t-1}-C)^2$的形式，再除以一下$-2$倍方差，就可以得到均值了。</p>
<p>指数部分为：</p>
<script type="math/tex; mode=display">
-\frac{1}{2}(\frac{(\mathbf{x}_{t}-\sqrt{1 - \beta_t}\mathbf{x}_{t - 1})^2}{\beta_t}+\frac{(\mathbf{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_{0})^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_{t}-\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0})^2}{1-\bar{\alpha}_{t}})</script><p>$\mathbf{x}_{t-1}$只在前两项里有。把和$\mathbf{x}_{t-1}$有关的项计算化简，可以计算出均值：</p>
<script type="math/tex; mode=display">
\tilde{\mu}_t = \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_{t}}\mathbf{x}_{t}+\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_{t}}\mathbf{x}_{0}</script><p>回想一下，在去噪声中，神经网络的输入是$\mathbf{x}_{t}$和$t$。也就是说，上式中$\mathbf{x}_{t}$已知，只有$\mathbf{x}_{0}$一个未知量。要算均值，还需要算出$\mathbf{x}_{0}$。$\mathbf{x}_{0}$和$\mathbf{x}_{t}$之间是有一定联系的。$\mathbf{x}_{t}$是$\mathbf{x}_{0}$在正向过程中第$t$步加噪声的结果。而根据正向过程的公式倒推：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{x}_t &= \sqrt{\bar{\alpha}_t}\mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_t}\epsilon_t \\
\mathbf{x}_0 &= \frac{\mathbf{x}_t - \sqrt{1-\bar{\alpha}_t}\epsilon_t}{\sqrt{\bar{\alpha}_t}}
\end{aligned}</script><p>把这个$\mathbf{x}_{0}$带入均值公式，均值最后会化简成我们熟悉的形式。</p>
<script type="math/tex; mode=display">
\tilde{\mu}_t = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t -\frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t)</script><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>上一节，我们只是简单地说神经网络的优化目标是让加噪声和去噪声的均值接近。而让均值接近，就是让生成$\mathbf{x}_t$的噪声$\epsilon_t$更接近。实际上，这个优化目标是经过简化得来的。扩散模型最早的优化目标是有一定的数学意义的。</p>
<p>扩散模型，全称为扩散概率模型（Diffusion Probabilistic Model）。最简单的一类扩散模型，是去噪扩散概率模型（Denoising Diffusion Probabilistic Model），也就是常说的DDPM。DDPM的框架主要是由两篇论文建立起来的。第一篇论文是首次提出扩散模型思想的<em>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</em>。在此基础上，<em>Denoising Diffusion Probabilistic Models</em>对最早的扩散模型做出了一定的简化，让图像生成效果大幅提升，促成了扩散模型的广泛使用。我们上一节看到的公式，全部是简化后的结果。</p>
<p>扩散概率模型的名字之所以有「概率」二字，是因为这个模型是在描述一个系统的概率。准确来说，扩散模型是在描述经反向过程生成出某一项数据的概率。也就是说，扩散模型$p_{\theta}(\mathbf{x}_0)$是一个有着可训练参数$\theta$的模型，它描述了反向过程生成出数据$\mathbf{x}_0$的概率。$p_{\theta}(\mathbf{x}_0)$满足$p_{\theta}(\mathbf{x}_0)=\int p_{\theta}(\mathbf{x}_{0:T})d\mathbf{x}_{1:T}$，其中$p_{\theta}(\mathbf{x}_{0:T})$就是我们熟悉的反向过程，只不过它是以概率计算的形式表达：</p>
<script type="math/tex; mode=display">
p_{\theta}(\mathbf{x}_{0:T})=p(\mathbf{x}_T)\prod_{t-1}^Tp_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t})</script><script type="math/tex; mode=display">
p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t}) = \mathcal{N}(\mathbf{x}_{t-1};\mu_{\theta}(\mathbf{x}_{t}, t), \Sigma_\theta(\mathbf{x}_{t}, t))</script><p>我们上一节里见到的优化目标，是让去噪声操作$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t})$和加噪声操作的逆操作$q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0)$尽可能相似。然而，这个描述并不确切。扩散模型原本的目标，是最大化$p_{\theta}(\mathbf{x}_0)$这个概率，其中$\mathbf{x}_0$是来自训练集的数据。换个角度说，给定一个训练集的数据$\mathbf{x}_0$，经过前向过程和反向过程，扩散模型要让复原出$\mathbf{x}_0$的概率尽可能大。这也是我们在本文开头认识VAE时见到的优化目标。</p>
<p>最大化$p_{\theta}(\mathbf{x}_0)$，一般会写成最小化其负对数值，即最小化$-log  p_{\theta}(\mathbf{x}_0)$。使用和VAE类似的变分推理，可以把优化目标转换成优化一个叫做变分下界(variational lower bound, VLB)的量。它最终可以写成：</p>
<script type="math/tex; mode=display">
L_{VLB}=\mathbb{E}[D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p_\theta(\mathbf{x}_T))+\sum_{t=2}^{T}D_{KL}(q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t})) - logp_\theta(\mathbf{x}_{0}|\mathbf{x}_{1})]</script><blockquote>
<p>这里的$D_{KL}(P||Q)$表示分布P和Q之间的KL散度。KL散度是衡量两个分布相似度的指标。如果$P, Q$都是正态分布，则它们的KL散度可以由一个简单的公式给出。关于KL散度的知识可以参见我之前的文章：从零理解熵、交叉熵、KL散度。</p>
</blockquote>
<p>其中，第一项$D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p_\theta(\mathbf{x}_T))$和可学习参数$\theta$无关（因为可学习参数只描述了每一步去噪声操作，也就是只描述了$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t})$），可以不去管它。那么这个优化目标就由两部分组成：</p>
<ol>
<li>最小化$D_{KL}(q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t}))$表示的是最大化每一个去噪声操作和加噪声逆操作的相似度。</li>
<li>最小化$- logp_\theta(\mathbf{x}_{0}|\mathbf{x}_{1})$就是已知$\mathbf{x}_{1}$时，让最后复原原图$\mathbf{x}_{0}$概率更高。</li>
</ol>
<p>我们分别看这两部分是怎么计算的。</p>
<p>对于第一部分，我们先回顾一下正态分布之间的KL散度公式。设一维正态分布$P, Q$的公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(x) = \frac{1}{\sqrt{2\pi}\sigma_1}exp(-\frac{(x - \mu_1)^2}{2\sigma_1^2}) \\
Q(x) = \frac{1}{\sqrt{2\pi}\sigma_2}exp(-\frac{(x - \mu_2)^2}{2\sigma_2^2})
\end{aligned}</script><p>则</p>
<script type="math/tex; mode=display">
D_{KL}(P||Q) = log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}</script><p>而对于$D_{KL}(q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t}))$，根据前文的分析，我们知道，待求方差$\Sigma_\theta(\mathbf{x}_{t}, t)$可以直接由计算得到。</p>
<script type="math/tex; mode=display">
\Sigma_\theta(\mathbf{x}_{t}, t) = \tilde{\beta}_t\mathbf{I}=\frac{1-\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t}} \cdot \beta_t\mathbf{I}</script><p>两个正态分布方差的比值是常量。所以，在计算KL散度时，不用管方差那一项了，只需要管均值那一项。</p>
<script type="math/tex; mode=display">
D_{KL}(q(\mathbf{x}_{t-1} | \mathbf{x}_{t}, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t}))\to\frac{1}{2\tilde{\beta}_t^2}||\mu_{\theta}(\mathbf{x}_{t}, t)-\tilde{\mu}_{t}(\mathbf{x}_{t}, t)||^2</script><p>由根据之前的均值公式</p>
<script type="math/tex; mode=display">
\tilde{\mu}_t(\mathbf{x}_{t}, t) = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t -\frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t)</script><script type="math/tex; mode=display">
\mu_{\theta}(\mathbf{x}_{t}, t) = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t -\frac{1 - \alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(\mathbf{x}_{t}, t))
\\</script><p>这一部分的优化目标可以化简成</p>
<script type="math/tex; mode=display">
\frac{(1 - \alpha_t)^2}{2\alpha_t(1-\bar{\alpha}_t)\tilde{\beta}_t^2}||\epsilon_t-\epsilon_{\theta}(\mathbf{x}_{t}, t)||^2</script><p>DDPM论文指出，如果把前面的系数全部丢掉的话，模型的效果更好。最终，我们就能得到一个非常简单的优化目标：</p>
<script type="math/tex; mode=display">
||\epsilon_t-\epsilon_{\theta}(\mathbf{x}_{t}, t)||^2</script><p>这就是我们上一节见到的优化目标。</p>
<p>当然，还没完，别忘了优化目标里还有$- logp_\theta(\mathbf{x}_{0}|\mathbf{x}_{1})$这一项。它的形式为：</p>
<script type="math/tex; mode=display">
- logp_\theta(\mathbf{x}_{0}|\mathbf{x}_{1})=-log\frac{1}{\sqrt{2\pi}\tilde{\beta}_1^2}+\frac{||\mathbf{x}_{0} - \mu_{\theta}(\mathbf{x}_{1}, 1)||^2}{2\tilde{\beta}_1^2}</script><p>只管后面有$\theta$的那一项（注意，$\alpha_1=\bar{\alpha}_1=1-\beta_1$）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{(\mathbf{x}_{0} - \mu_{\theta}(\mathbf{x}_{1}, 1))^2}{2\tilde{\beta}_1^2} &= \frac{1}{2\tilde{\beta}_1^2}||\mathbf{x}_{0}-\frac{1}{\sqrt{\alpha_1}}(\mathbf{x}_1 -\frac{1 - \alpha_1}{\sqrt{1-\bar{\alpha}_1}}\epsilon_\theta(\mathbf{x}_{1}, 1))||^2 \\
&=\frac{1}{2\tilde{\beta}_1^2}||\mathbf{x}_{0}-\frac{1}{\sqrt{\alpha_1}}(\sqrt{\bar{\alpha}_1}\mathbf{x}_{0}+ \sqrt{1-\bar{\alpha}_1}\epsilon_1-\frac{1 - \alpha_1}{\sqrt{1-\bar{\alpha}_1}}\epsilon_\theta(\mathbf{x}_{1}, 1))||^2 \\
&=\frac{1}{2\tilde{\beta}_1^2\alpha_1}|| \sqrt{1-\bar{\alpha}_1}\epsilon_1-\frac{1 - \alpha_1}{\sqrt{1-\bar{\alpha}_1}}\epsilon_\theta(\mathbf{x}_{1}, 1)||^2 \\
&=\frac{1-\bar{\alpha}_1}{2\tilde{\beta}_1^2\alpha_1}|| \epsilon_1-\epsilon_\theta(\mathbf{x}_{1}, 1)||^2 \\
\end{aligned}</script><p>这和那些KL散度项$t=1$时的形式相同，我们可以用相同的方式简化优化目标，只保留$|| \epsilon_1-\epsilon_\theta(\mathbf{x}_{1}, 1)||^2$。这样，损失函数的形式全都是$||\epsilon_t-\epsilon_{\theta}(\mathbf{x}_{t}, t)||^2$了。</p>
<blockquote>
<p>DDPM论文里写$- logp_\theta(\mathbf{x}_{0}|\mathbf{x}_{1})$这一项可以直接满足简化后的公式$t=1$时的情况，而没有去掉系数的过程。我在网上没找到文章解释这一点，只好按自己的理解来推导这个误差项了。不论如何，推导的过程不是那么重要，重要的是最后的简化形式。</p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>图像生成任务就是把随机生成的向量（噪声）映射成和训练图像类似的图像。为此，扩散模型把这个过程看成是对纯噪声图像的去噪过程。通过学习把图像逐步变成纯噪声的逆操作，扩散模型可以把任何一个纯噪声图像变成有意义的图像，也就是完成图像生成。</p>
<p>对于不同程度的读者，应该对本文有不同的认识。</p>
<p>对于只想了解扩散模型大概原理的读者，只需要阅读第一节，并大概了解：</p>
<ul>
<li>图像生成任务的通常做法</li>
<li>图像生成任务需要监督</li>
<li>VAE通过把图像编码再解码来训练一个解码器</li>
<li>扩散模型是一类特殊的VAE，它的编码固定为加噪声，解码固定为去噪声</li>
</ul>
<p>对于想认真学习扩散模型的读者，只需读懂第二节的主要内容：</p>
<ul>
<li>扩散模型的优化目标：让反向过程尽可能成为正向过程的逆操作</li>
<li>正向过程的公式</li>
<li>反向过程的做法（采样算法）</li>
<li>加噪声逆操作的均值和方差在给定$\mathbf{x}_{0}$时可以求出来的，加噪声逆操作的均值就是去噪声的学习目标</li>
<li>简化后的损失函数与训练算法</li>
</ul>
<p>对有学有余力对数学感兴趣的读者，可以看一看第三节的内容：</p>
<ul>
<li>加噪声逆操作均值和方差的推导</li>
<li>扩散模型最早的优化目标与DDPM论文是如何简化优化目标的</li>
</ul>
<p>我个人认为，由于扩散模型的优化目标已经被大幅度简化，除非你的研究目标是改进扩散模型本身，否则没必要花过多的时间钻研数学原理。在学习时，建议快点看懂扩散模型的整体思想，搞懂最核心的训练算法和采样算法，跑通代码。之后就可以去看较新的论文了。</p>
<p>在附录中，我给出了一份DDPM的简单实现。欢迎大家参考，并自己动手复现一遍DDPM。</p>
<h2 id="参考资料与学习建议"><a href="#参考资料与学习建议" class="headerlink" title="参考资料与学习建议"></a>参考资料与学习建议</h2><p>网上绝大多数的中英文教程都是照搬 <a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a> 这篇文章的。这篇文章像教科书一样严谨，适合有一定数学基础的人阅读，但不适合给初学者学习。建议在弄懂扩散模型的大概原理后再来阅读这篇文章补充细节。</p>
<p>多数介绍扩散模型的文章对没学过相关数学知识的人来说很不友好，我在阅读此类文章时碰到了大量的问题：为什么前向公式里有个$\sqrt{1-\beta}$？为什么突然冒出一个快速算$\mathbf{x}_{t}$的公式？为什么反向过程里来了个贝叶斯公式？优化目标是什么？$-log  p_{\theta}(\mathbf{x}_0)$是什么？为什么优化目标里一大堆项，每一项的意义又是什么？为什么最后莫名其妙算一个$\epsilon$？为什么采样时$t=0$就不用加方差项了？好不容易，我才把这些问题慢慢搞懂，并在本文做出了解释。希望我的解答能够帮助到同样有这些困惑的读者。想逐步学习扩散模型，可以先看懂我这篇文章的大概讲解，再去其他文章里学懂一些细节。无论是教，还是学，最重要的都是搞懂整体思路，知道动机，最后再去强调细节。</p>
<p>再强烈推荐一位作者写的DDPM系列介绍：<a target="_blank" rel="noopener" href="https://kexue.fm/archives/9119">https://kexue.fm/archives/9119</a> 。这位作者是全网为数不多的能令我敬佩的作者。早知道有这些文章，我也没必要自己写一遍了。</p>
<p>这里还有篇文章给出了扩散模型中数学公式的详细推导，并补充了变分推理的背景介绍，适合从头学起：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.11970">https://arxiv.org/abs/2208.11970</a></p>
<p>想深入学习DDPM，可以看一看最重要的两篇论文：<em>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</em>、<em>Denoising Diffusion Probabilistic Models</em>。当然，后者更重要一些，里面的一些实验结果仍有阅读价值。</p>
<p>我在代码复现时参考了<a target="_blank" rel="noopener" href="https://medium.com/mlearning-ai/enerating-images-with-ddpms-a-pytorch-implementation-cef5a2ba8cb1">这篇文章</a>。相对于网上的其他开源DDPM实现，这份代码比较简短易懂，更适合学习。不过，这份代码有一点问题。它的神经网络不够强大，采样结果会有一点问题。</p>
<h2 id="附录：代码复现"><a href="#附录：代码复现" class="headerlink" title="附录：代码复现"></a>附录：代码复现</h2><p>在这个项目中，我们要用PyTorch实现一个基于U-Net的DDPM，并在MNIST数据集（经典的手写数字数据集）上训练它。模型几分钟就能训练完，我们可以方便地做各种各样的实验。</p>
<p>后续讲解只会给出代码片段，完整的代码请参见 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/ddpm">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/ddpm</a> 。git clone 仓库并安装后，可以直接运行目录里的<code>main.py</code>训练模型并采样。</p>
<h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><p>PyTorch的<code>torchvision</code>提供了获取了MNIST的接口，我们只需要用下面的函数就可以生成MNIST的<code>Dataset</code>实例。参数中，<code>root</code>为数据集的下载路径，<code>download</code>为是否自动下载数据集。令<code>download=True</code>的话，第一次调用该函数时会自动下载数据集，而第二次之后就不用下载了，函数会读取存储在<code>root</code>里的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist = torchvision.datasets.MNIST(root=<span class="string">&#x27;data/mnist&#x27;</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>我们可以用下面的代码来下载MNIST并输出该数据集的一些信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_dataset</span>():</span></span><br><span class="line">    mnist = torchvision.datasets.MNIST(root=<span class="string">&#x27;data/mnist&#x27;</span>, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;length of MNIST&#x27;</span>, <span class="built_in">len</span>(mnist))</span><br><span class="line">    <span class="built_in">id</span> = <span class="number">4</span></span><br><span class="line">    img, label = mnist[<span class="built_in">id</span>]</span><br><span class="line">    <span class="built_in">print</span>(img)</span><br><span class="line">    <span class="built_in">print</span>(label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># On computer with monitor</span></span><br><span class="line">    <span class="comment"># img.show()</span></span><br><span class="line"></span><br><span class="line">    img.save(<span class="string">&#x27;work_dirs/tmp.jpg&#x27;</span>)</span><br><span class="line">    tensor = ToTensor()(img)</span><br><span class="line">    <span class="built_in">print</span>(tensor.shape)</span><br><span class="line">    <span class="built_in">print</span>(tensor.<span class="built_in">max</span>())</span><br><span class="line">    <span class="built_in">print</span>(tensor.<span class="built_in">min</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    download_dataset()</span><br></pre></td></tr></table></figure><br>执行这段代码，输出大致为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">length of MNIST 60000</span><br><span class="line">&lt;PIL.Image.Image image mode=L size=28x28 at 0x7FB3F09CCE50&gt;</span><br><span class="line">9</span><br><span class="line">torch.Size([1, 28, 28])</span><br><span class="line">tensor(1.)</span><br><span class="line">tensor(0.)</span><br></pre></td></tr></table></figure>
<p>第一行输出表明，MNIST数据集里有60000张图片。而从第二行和第三行输出中，我们发现每一项数据由图片和标签组成，图片是大小为<code>28x28</code>的PIL格式的图片，标签表明该图片是哪个数字。我们可以用<code>torchvision</code>里的<code>ToTensor()</code>把PIL图片转成PyTorch张量，进一步查看图片的信息。最后三行输出表明，每一张图片都是单通道图片（灰度图），颜色值的取值范围是0~1。</p>
<p>我们可以查看一下每张图片的样子。如果你是在用带显示器的电脑，可以去掉<code>img.show</code>那一行的注释，直接查看图片；如果你是在用服务器，可以去<code>img.save</code>的路径里查看图片。该图片的应该长这个样子：</p>
<p><img src="/2023/07/07/20230330-diffusion-model/c1.jpg" alt></p>
<p>我们可以用下面的代码预处理数据并创建<code>DataLoader</code>。由于DDPM会把图像和正态分布关联起来，我们更希望图像颜色值的取值范围是<code>[-1, 1]</code>。为此，我们可以对图像做一个线性变换，减0.5再乘2。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader</span>(<span class="params">batch_size: <span class="built_in">int</span></span>):</span></span><br><span class="line">    transform = Compose([ToTensor(), Lambda(<span class="keyword">lambda</span> x: (x - <span class="number">0.5</span>) * <span class="number">2</span>)])</span><br><span class="line">    dataset = torchvision.datasets.MNIST(root=<span class="string">&#x27;data/mnist&#x27;</span>,</span><br><span class="line">                                         transform=transform)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="DDPM-类"><a href="#DDPM-类" class="headerlink" title="DDPM 类"></a>DDPM 类</h3><p>在代码中，我们要实现一个<code>DDPM</code>类。它维护了扩散过程中的一些常量(比如$\alpha$），并且可以计算正向过程和反向过程的结果。</p>
<p>先来实现一下<code>DDPM</code>类的初始化函数。一开始，我们遵从论文的配置，用<code>torch.linspace(min_beta, max_beta, n_steps)</code>从<code>min_beta</code>到<code>max_beta</code>线性地生成<code>n_steps</code>个时刻的$\beta$。接着，我们根据公式$\alpha_t=1-\beta_t, \bar{\alpha}_t=\prod_{i=1}^t\alpha_i$，计算每个时刻的<code>alpha</code>和<code>alpha_bar</code>。注意，为了方便实现，我们让<code>t</code>的取值从0开始，要比论文里的$t$少1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPM</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># n_steps 就是论文里的 T</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 device,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_steps: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 min_beta: <span class="built_in">float</span> = <span class="number">0.0001</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 max_beta: <span class="built_in">float</span> = <span class="number">0.02</span></span>):</span></span><br><span class="line">        betas = torch.linspace(min_beta, max_beta, n_steps).to(device)</span><br><span class="line">        alphas = <span class="number">1</span> - betas</span><br><span class="line">        alpha_bars = torch.empty_like(alphas)</span><br><span class="line">        product = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i, alpha <span class="keyword">in</span> <span class="built_in">enumerate</span>(alphas):</span><br><span class="line">            product *= alpha</span><br><span class="line">            alpha_bars[i] = product</span><br><span class="line">        self.betas = betas</span><br><span class="line">        self.n_steps = n_steps</span><br><span class="line">        self.alphas = alphas</span><br><span class="line">        self.alpha_bars = alpha_bars</span><br></pre></td></tr></table></figure>
<blockquote>
<p>部分实现会让 DDPM 继承<code>torch.nn.Module</code>，但我认为这样不好。DDPM本身不是一个神经网络，它只是描述了前向过程和后向过程的一些计算。只有涉及可学习参数的神经网络类才应该继承 <code>torch.nn.Module</code>。</p>
</blockquote>
<p>准备好了变量后，我们可以来实现<code>DDPM</code>类的其他方法。先实现正向过程方法，该方法会根据公式$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_{0} + \sqrt{1-\bar{\alpha}_t}\epsilon_t$计算正向过程中的$\mathbf{x}_t$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_forward</span>(<span class="params">self, x, t, eps=<span class="literal">None</span></span>):</span></span><br><span class="line">    alpha_bar = self.alpha_bars[t].reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> eps <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        eps = torch.randn_like(x)</span><br><span class="line">    res = eps * torch.sqrt(<span class="number">1</span> - alpha_bar) + torch.sqrt(alpha_bar) * x</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这里要解释一些PyTorch编程上的细节。这份代码中，<code>self.alpha_bars</code>是一个一维<code>Tensor</code>。而在并行训练中，我们一般会令<code>t</code>为一个形状为<code>(batch_size, )</code>的<code>Tensor</code>。PyTorch允许我们直接用<code>self.alpha_bars[t]</code>从<code>self.alpha_bars</code>里取出<code>batch_size</code>个数，就像用一个普通的整型索引来从数组中取出一个数一样。有些实现会用<code>torch.gather</code>从<code>self.alpha_bars</code>里取数，其作用是一样的。</p>
<p>我们可以随机从训练集取图片做测试，看看它们在前向过程中是怎么逐步变成噪声的。</p>
<p><img src="/2023/07/07/20230330-diffusion-model/c2.jpg" alt></p>
<p>接下来实现反向过程。在反向过程中，DDPM会用神经网络预测每一轮去噪的均值，把$\mathbf{x}_t$复原回$\mathbf{x}_0$，以完成图像生成。反向过程即对应论文中的采样算法。</p>
<p><img src="/2023/07/07/20230330-diffusion-model/7.jpg" alt></p>
<p>其实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_backward</span>(<span class="params">self, img_shape, net, device, simple_var=<span class="literal">True</span></span>):</span></span><br><span class="line">    x = torch.randn(img_shape).to(device)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(self.n_steps - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        x = self.sample_backward_step(x, t, net, simple_var)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_backward_step</span>(<span class="params">self, x_t, t, net, simple_var=<span class="literal">True</span></span>):</span></span><br><span class="line">    n = x_t.shape[<span class="number">0</span>]</span><br><span class="line">    t_tensor = torch.tensor([t] * n,</span><br><span class="line">                            dtype=torch.long).to(x_t.device).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    eps = net(x_t, t_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">        noise = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> simple_var:</span><br><span class="line">            var = self.betas[t]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            var = (<span class="number">1</span> - self.alpha_bars[t - <span class="number">1</span>]) / (</span><br><span class="line">                <span class="number">1</span> - self.alpha_bars[t]) * self.betas[t]</span><br><span class="line">        noise = torch.randn_like(x_t)</span><br><span class="line">        noise *= torch.sqrt(var)</span><br><span class="line"></span><br><span class="line">    mean = (x_t -</span><br><span class="line">            (<span class="number">1</span> - self.alphas[t]) / torch.sqrt(<span class="number">1</span> - self.alpha_bars[t]) *</span><br><span class="line">            eps) / torch.sqrt(self.alphas[t])</span><br><span class="line">    x_t = mean + noise</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_t</span><br></pre></td></tr></table></figure>
<p>其中，<code>sample_backward</code>是用来给外部调用的方法，而<code>sample_backward_step</code>是执行一步反向过程的方法。</p>
<p><code>sample_backward</code>会随机生成纯噪声<code>x</code>（对应$\mathbf{x}_T$），再令<code>t</code>从<code>n_steps - 1</code>到<code>0</code>，调用<code>sample_backward_step</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_backward</span>(<span class="params">self, img_shape, net, device, simple_var=<span class="literal">True</span></span>):</span></span><br><span class="line">    x = torch.randn(img_shape).to(device)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(self.n_steps - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        x = self.sample_backward_step(x, t, net, simple_var)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>在<code>sample_backward_step</code>中，我们先准备好这一步的神经网络输出<code>eps</code>。为此，我们要把整型的<code>t</code>转换成一个格式正确的<code>Tensor</code>。考虑到输入里可能有多个batch，我们先获取batch size <code>n</code>，再根据它来生成<code>t_tensor</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_backward_step</span>(<span class="params">self, x_t, t, net, simple_var=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    n = x_t.shape[<span class="number">0</span>]</span><br><span class="line">    t_tensor = torch.tensor([t] * n,</span><br><span class="line">                            dtype=torch.long).to(x_t.device).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    eps = net(x_t, t_tensor)</span><br></pre></td></tr></table></figure>
<p>之后，我们来处理反向过程公式中的方差项。根据伪代码，我们仅在<code>t</code>非零的时候算方差项。方差项用到的方差有两种取值，效果差不多，我们用<code>simple_var</code>来控制选哪种取值方式。获取方差后，我们再随机采样一个噪声，根据公式，得到方差项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">    noise = <span class="number">0</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> simple_var:</span><br><span class="line">        var = self.betas[t]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        var = (<span class="number">1</span> - self.alpha_bars[t - <span class="number">1</span>]) / (</span><br><span class="line">            <span class="number">1</span> - self.alpha_bars[t]) * self.betas[t]</span><br><span class="line">    noise = torch.randn_like(x_t)</span><br><span class="line">    noise *= torch.sqrt(var)</span><br></pre></td></tr></table></figure>
<p>最后，我们把<code>eps</code>和方差项套入公式，得到这一步更新过后的图像<code>x_t</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mean = (x_t -</span><br><span class="line">        (<span class="number">1</span> - self.alphas[t]) / torch.sqrt(<span class="number">1</span> - self.alpha_bars[t]) *</span><br><span class="line">        eps) / torch.sqrt(self.alphas[t])</span><br><span class="line">x_t = mean + noise</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> x_t</span><br></pre></td></tr></table></figure>
<p>稍后完成了训练后，我们再来看反向过程的输出结果。</p>
<h3 id="训练算法"><a href="#训练算法" class="headerlink" title="训练算法"></a>训练算法</h3><p>接下来，我们先跳过神经网络的实现，直接完成论文里的训练算法。</p>
<p>再回顾一遍伪代码。首先，我们要随机选取训练图片$\mathbf{x}_{0}$，随机生成当前要训练的时刻$t$，以及随机生成一个生成$\mathbf{x}_{t}$的高斯噪声。之后，我们把$\mathbf{x}_{t}$和$t$输入进神经网络，尝试预测噪声。最后，我们以预测噪声和实际噪声的均方误差为损失函数做梯度下降。</p>
<p><img src="/2023/07/07/20230330-diffusion-model/6.jpg" alt></p>
<p>为此，我们可以用下面的代码实现训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> dldemos.ddpm.dataset <span class="keyword">import</span> get_dataloader, get_img_shape</span><br><span class="line"><span class="keyword">from</span> dldemos.ddpm.ddpm <span class="keyword">import</span> DDPM</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> einops</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">512</span></span><br><span class="line">n_epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">ddpm: DDPM, net, device, ckpt_path</span>):</span></span><br><span class="line">    <span class="comment"># n_steps 就是公式里的 T</span></span><br><span class="line">    <span class="comment"># net 是某个继承自 torch.nn.Module 的神经网络</span></span><br><span class="line">    n_steps = ddpm.n_steps</span><br><span class="line">    dataloader = get_dataloader(batch_size)</span><br><span class="line">    net = net.to(device)</span><br><span class="line">    loss_fn = nn.MSELoss()</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), <span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> x, _ <span class="keyword">in</span> dataloader:</span><br><span class="line">            current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">            x = x.to(device)</span><br><span class="line">            t = torch.randint(<span class="number">0</span>, n_steps, (current_batch_size, )).to(device)</span><br><span class="line">            eps = torch.randn_like(x).to(device)</span><br><span class="line">            x_t = ddpm.sample_forward(x, t, eps)</span><br><span class="line">            eps_theta = net(x_t, t.reshape(current_batch_size, <span class="number">1</span>))</span><br><span class="line">            loss = loss_fn(eps_theta, eps)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">    torch.save(net.state_dict(), ckpt_path)</span><br></pre></td></tr></table></figure>
<p>代码的主要逻辑都在循环里。首先是完成训练数据$\mathbf{x}_{0}$、$t$、噪声的采样。采样$\mathbf{x}_{0}$的工作可以交给PyTorch的DataLoader完成，每轮遍历得到的<code>x</code>就是训练数据。$t$的采样可以用<code>torch.randint</code>函数随机从<code>[0, n_steps - 1]</code>取数。采样高斯噪声可以直接用<code>torch.randn_like(x)</code>生成一个和训练图片<code>x</code>形状一样的符合标准正态分布的图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x, _ <span class="keyword">in</span> dataloader:</span><br><span class="line">    current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    t = torch.randint(<span class="number">0</span>, n_steps, (current_batch_size, )).to(device)</span><br><span class="line">    eps = torch.randn_like(x).to(device)</span><br></pre></td></tr></table></figure>
<p>之后计算$\mathbf{x}_{t}$并将其和$t$输入进神经网络<code>net</code>。计算$\mathbf{x}_{t}$的任务会由<code>DDPM</code>类的<code>sample_forward</code>方法完成，我们在上文已经实现了它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_t = ddpm.sample_forward(x, t, eps)</span><br><span class="line">eps_theta = net(x_t, t.reshape(current_batch_size, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>得到了预测的噪声<code>eps_theta</code>，我们调用PyTorch的API，算均方误差并调用优化器即可。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), <span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">        loss = loss_fn(eps_theta, eps)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure></p>
<h3 id="去噪神经网络"><a href="#去噪神经网络" class="headerlink" title="去噪神经网络"></a>去噪神经网络</h3><p>在DDPM中，理论上我们可以用任意一种神经网络架构。但由于DDPM任务十分接近图像去噪任务，而U-Net又是去噪任务中最常见的网络架构，因此绝大多数DDPM都会使用基于U-Net的神经网络。</p>
<p>我一直想训练一个尽可能简单的模型。经过多次实验，我发现DDPM的神经网络很难训练。哪怕是对于比较简单的MNIST数据集，结构差一点的网络（比如纯ResNet）都不太行，只有带了残差块和时序编码的U-Net才能较好地完成去噪。注意力模块倒是可以不用加上。</p>
<p>由于神经网络结构并不是DDPM学习的重点，我这里就不对U-Net的写法做解说，而是直接贴上代码了。代码中大部分内容都和普通的U-Net无异。唯一要注意的地方就是时序编码。去噪网络的输入除了图像外，还有一个时间戳<code>t</code>。我们要考虑怎么把<code>t</code>的信息和输入图像信息融合起来。大部分人的做法是对<code>t</code>进行Transformer中的位置编码，把该编码加到图像的每一处上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> dldemos.ddpm.dataset <span class="keyword">import</span> get_img_shape</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, max_seq_len: <span class="built_in">int</span>, d_model: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assume d_model is an even number for convenience</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_seq_len, d_model)</span><br><span class="line">        i_seq = torch.linspace(<span class="number">0</span>, max_seq_len - <span class="number">1</span>, max_seq_len)</span><br><span class="line">        j_seq = torch.linspace(<span class="number">0</span>, d_model - <span class="number">2</span>, d_model // <span class="number">2</span>)</span><br><span class="line">        pos, two_i = torch.meshgrid(i_seq, j_seq)</span><br><span class="line">        pe_2i = torch.sin(pos / <span class="number">10000</span>**(two_i / d_model))</span><br><span class="line">        pe_2i_1 = torch.cos(pos / <span class="number">10000</span>**(two_i / d_model))</span><br><span class="line">        pe = torch.stack((pe_2i, pe_2i_1), <span class="number">2</span>).reshape(max_seq_len, d_model)</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(max_seq_len, d_model)</span><br><span class="line">        self.embedding.weight.data = pe</span><br><span class="line">        self.embedding.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, t</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.embedding(t)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_c: <span class="built_in">int</span>, out_c: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_c, out_c, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_c)</span><br><span class="line">        self.actvation1 = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(out_c, out_c, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_c)</span><br><span class="line">        self.actvation2 = nn.ReLU()</span><br><span class="line">        <span class="keyword">if</span> in_c != out_c:</span><br><span class="line">            self.shortcut = nn.Sequential(nn.Conv2d(in_c, out_c, <span class="number">1</span>),</span><br><span class="line">                                          nn.BatchNorm2d(out_c))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.shortcut = nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        x = self.conv1(<span class="built_in">input</span>)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.actvation1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        x += self.shortcut(<span class="built_in">input</span>)</span><br><span class="line">        x = self.actvation2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_steps,</span></span></span><br><span class="line"><span class="params"><span class="function">                 intermediate_channels=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">                 pe_dim=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 insert_t_to_all_layers=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        C, H, W = get_img_shape()  <span class="comment"># 1, 28, 28</span></span><br><span class="line">        self.pe = PositionalEncoding(n_steps, pe_dim)</span><br><span class="line"></span><br><span class="line">        self.pe_linears = nn.ModuleList()</span><br><span class="line">        self.all_t = insert_t_to_all_layers</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> insert_t_to_all_layers:</span><br><span class="line">            self.pe_linears.append(nn.Linear(pe_dim, C))</span><br><span class="line"></span><br><span class="line">        self.residual_blocks = nn.ModuleList()</span><br><span class="line">        prev_channel = C</span><br><span class="line">        <span class="keyword">for</span> channel <span class="keyword">in</span> intermediate_channels:</span><br><span class="line">            self.residual_blocks.append(ResidualBlock(prev_channel, channel))</span><br><span class="line">            <span class="keyword">if</span> insert_t_to_all_layers:</span><br><span class="line">                self.pe_linears.append(nn.Linear(pe_dim, prev_channel))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.pe_linears.append(<span class="literal">None</span>)</span><br><span class="line">            prev_channel = channel</span><br><span class="line">        self.output_layer = nn.Conv2d(prev_channel, C, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, t</span>):</span></span><br><span class="line">        n = t.shape[<span class="number">0</span>]</span><br><span class="line">        t = self.pe(t)</span><br><span class="line">        <span class="keyword">for</span> m_x, m_t <span class="keyword">in</span> <span class="built_in">zip</span>(self.residual_blocks, self.pe_linears):</span><br><span class="line">            <span class="keyword">if</span> m_t <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                pe = m_t(t).reshape(n, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">                x = x + pe</span><br><span class="line">            x = m_x(x)</span><br><span class="line">        x = self.output_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnetBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, shape, in_c, out_c, residual=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ln = nn.LayerNorm(shape)</span><br><span class="line">        self.conv1 = nn.Conv2d(in_c, out_c, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_c, out_c, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.activation = nn.ReLU()</span><br><span class="line">        self.residual = residual</span><br><span class="line">        <span class="keyword">if</span> residual:</span><br><span class="line">            <span class="keyword">if</span> in_c == out_c:</span><br><span class="line">                self.residual_conv = nn.Identity()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.residual_conv = nn.Conv2d(in_c, out_c, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.ln(x)</span><br><span class="line">        out = self.conv1(out)</span><br><span class="line">        out = self.activation(out)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        <span class="keyword">if</span> self.residual:</span><br><span class="line">            out += self.residual_conv(x)</span><br><span class="line">        out = self.activation(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_steps,</span></span></span><br><span class="line"><span class="params"><span class="function">                 channels=[<span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">80</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">                 pe_dim=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 residual=<span class="literal">False</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        C, H, W = get_img_shape()</span><br><span class="line">        layers = <span class="built_in">len</span>(channels)</span><br><span class="line">        Hs = [H]</span><br><span class="line">        Ws = [W]</span><br><span class="line">        cH = H</span><br><span class="line">        cW = W</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(layers - <span class="number">1</span>):</span><br><span class="line">            cH //= <span class="number">2</span></span><br><span class="line">            cW //= <span class="number">2</span></span><br><span class="line">            Hs.append(cH)</span><br><span class="line">            Ws.append(cW)</span><br><span class="line"></span><br><span class="line">        self.pe = PositionalEncoding(n_steps, pe_dim)</span><br><span class="line"></span><br><span class="line">        self.encoders = nn.ModuleList()</span><br><span class="line">        self.decoders = nn.ModuleList()</span><br><span class="line">        self.pe_linears_en = nn.ModuleList()</span><br><span class="line">        self.pe_linears_de = nn.ModuleList()</span><br><span class="line">        self.downs = nn.ModuleList()</span><br><span class="line">        self.ups = nn.ModuleList()</span><br><span class="line">        prev_channel = C</span><br><span class="line">        <span class="keyword">for</span> channel, cH, cW <span class="keyword">in</span> <span class="built_in">zip</span>(channels[<span class="number">0</span>:-<span class="number">1</span>], Hs[<span class="number">0</span>:-<span class="number">1</span>], Ws[<span class="number">0</span>:-<span class="number">1</span>]):</span><br><span class="line">            self.pe_linears_en.append(</span><br><span class="line">                nn.Sequential(nn.Linear(pe_dim, prev_channel), nn.ReLU(),</span><br><span class="line">                              nn.Linear(prev_channel, prev_channel)))</span><br><span class="line">            self.encoders.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    UnetBlock((prev_channel, cH, cW),</span><br><span class="line">                              prev_channel,</span><br><span class="line">                              channel,</span><br><span class="line">                              residual=residual),</span><br><span class="line">                    UnetBlock((channel, cH, cW),</span><br><span class="line">                              channel,</span><br><span class="line">                              channel,</span><br><span class="line">                              residual=residual)))</span><br><span class="line">            self.downs.append(nn.Conv2d(channel, channel, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            prev_channel = channel</span><br><span class="line"></span><br><span class="line">        self.pe_mid = nn.Linear(pe_dim, prev_channel)</span><br><span class="line">        channel = channels[-<span class="number">1</span>]</span><br><span class="line">        self.mid = nn.Sequential(</span><br><span class="line">            UnetBlock((prev_channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                      prev_channel,</span><br><span class="line">                      channel,</span><br><span class="line">                      residual=residual),</span><br><span class="line">            UnetBlock((channel, Hs[-<span class="number">1</span>], Ws[-<span class="number">1</span>]),</span><br><span class="line">                      channel,</span><br><span class="line">                      channel,</span><br><span class="line">                      residual=residual),</span><br><span class="line">        )</span><br><span class="line">        prev_channel = channel</span><br><span class="line">        <span class="keyword">for</span> channel, cH, cW <span class="keyword">in</span> <span class="built_in">zip</span>(channels[-<span class="number">2</span>::-<span class="number">1</span>], Hs[-<span class="number">2</span>::-<span class="number">1</span>], Ws[-<span class="number">2</span>::-<span class="number">1</span>]):</span><br><span class="line">            self.pe_linears_de.append(nn.Linear(pe_dim, prev_channel))</span><br><span class="line">            self.ups.append(nn.ConvTranspose2d(prev_channel, channel, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">            self.decoders.append(</span><br><span class="line">                nn.Sequential(</span><br><span class="line">                    UnetBlock((channel * <span class="number">2</span>, cH, cW),</span><br><span class="line">                              channel * <span class="number">2</span>,</span><br><span class="line">                              channel,</span><br><span class="line">                              residual=residual),</span><br><span class="line">                    UnetBlock((channel, cH, cW),</span><br><span class="line">                              channel,</span><br><span class="line">                              channel,</span><br><span class="line">                              residual=residual)))</span><br><span class="line"></span><br><span class="line">            prev_channel = channel</span><br><span class="line"></span><br><span class="line">        self.conv_out = nn.Conv2d(prev_channel, C, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, t</span>):</span></span><br><span class="line">        n = t.shape[<span class="number">0</span>]</span><br><span class="line">        t = self.pe(t)</span><br><span class="line">        encoder_outs = []</span><br><span class="line">        <span class="keyword">for</span> pe_linear, encoder, down <span class="keyword">in</span> <span class="built_in">zip</span>(self.pe_linears_en, self.encoders,</span><br><span class="line">                                            self.downs):</span><br><span class="line">            pe = pe_linear(t).reshape(n, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            x = encoder(x + pe)</span><br><span class="line">            encoder_outs.append(x)</span><br><span class="line">            x = down(x)</span><br><span class="line">        pe = self.pe_mid(t).reshape(n, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        x = self.mid(x + pe)</span><br><span class="line">        <span class="keyword">for</span> pe_linear, decoder, up, encoder_out <span class="keyword">in</span> <span class="built_in">zip</span>(self.pe_linears_de,</span><br><span class="line">                                                       self.decoders, self.ups,</span><br><span class="line">                                                       encoder_outs[::-<span class="number">1</span>]):</span><br><span class="line">            pe = pe_linear(t).reshape(n, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            x = up(x)</span><br><span class="line"></span><br><span class="line">            pad_x = encoder_out.shape[<span class="number">2</span>] - x.shape[<span class="number">2</span>]</span><br><span class="line">            pad_y = encoder_out.shape[<span class="number">3</span>] - x.shape[<span class="number">3</span>]</span><br><span class="line">            x = F.pad(x, (pad_x // <span class="number">2</span>, pad_x - pad_x // <span class="number">2</span>, pad_y // <span class="number">2</span>,</span><br><span class="line">                          pad_y - pad_y // <span class="number">2</span>))</span><br><span class="line">            x = torch.cat((encoder_out, x), dim=<span class="number">1</span>)</span><br><span class="line">            x = decoder(x + pe)</span><br><span class="line">        x = self.conv_out(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">convnet_small_cfg = &#123;</span><br><span class="line">    <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;ConvNet&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;intermediate_channels&#x27;</span>: [<span class="number">10</span>, <span class="number">20</span>],</span><br><span class="line">    <span class="string">&#x27;pe_dim&#x27;</span>: <span class="number">128</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">convnet_medium_cfg = &#123;</span><br><span class="line">    <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;ConvNet&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;intermediate_channels&#x27;</span>: [<span class="number">10</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">40</span>, <span class="number">80</span>, <span class="number">80</span>],</span><br><span class="line">    <span class="string">&#x27;pe_dim&#x27;</span>: <span class="number">256</span>,</span><br><span class="line">    <span class="string">&#x27;insert_t_to_all_layers&#x27;</span>: <span class="literal">True</span></span><br><span class="line">&#125;</span><br><span class="line">convnet_big_cfg = &#123;</span><br><span class="line">    <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;ConvNet&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;intermediate_channels&#x27;</span>: [<span class="number">20</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">40</span>, <span class="number">80</span>, <span class="number">80</span>, <span class="number">160</span>, <span class="number">160</span>],</span><br><span class="line">    <span class="string">&#x27;pe_dim&#x27;</span>: <span class="number">256</span>,</span><br><span class="line">    <span class="string">&#x27;insert_t_to_all_layers&#x27;</span>: <span class="literal">True</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">unet_1_cfg = &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;UNet&#x27;</span>, <span class="string">&#x27;channels&#x27;</span>: [<span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">80</span>], <span class="string">&#x27;pe_dim&#x27;</span>: <span class="number">128</span>&#125;</span><br><span class="line">unet_res_cfg = &#123;</span><br><span class="line">    <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;UNet&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;channels&#x27;</span>: [<span class="number">10</span>, <span class="number">20</span>, <span class="number">40</span>, <span class="number">80</span>],</span><br><span class="line">    <span class="string">&#x27;pe_dim&#x27;</span>: <span class="number">128</span>,</span><br><span class="line">    <span class="string">&#x27;residual&#x27;</span>: <span class="literal">True</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_network</span>(<span class="params">config: <span class="built_in">dict</span>, n_steps</span>):</span></span><br><span class="line">    network_type = config.pop(<span class="string">&#x27;type&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> network_type == <span class="string">&#x27;ConvNet&#x27;</span>:</span><br><span class="line">        network_cls = ConvNet</span><br><span class="line">    <span class="keyword">elif</span> network_type == <span class="string">&#x27;UNet&#x27;</span>:</span><br><span class="line">        network_cls = UNet</span><br><span class="line"></span><br><span class="line">    network = network_cls(n_steps, **config)</span><br><span class="line">    <span class="keyword">return</span> network</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="实验结果与采样"><a href="#实验结果与采样" class="headerlink" title="实验结果与采样"></a>实验结果与采样</h3><p>把之前的所有代码综合一下，我们以带残差块的U-Net为去噪网络，执行训练。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n_steps = <span class="number">1000</span></span><br><span class="line">    config_id = <span class="number">4</span></span><br><span class="line">    device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line">    model_path = <span class="string">&#x27;dldemos/ddpm/model_unet_res.pth&#x27;</span></span><br><span class="line"></span><br><span class="line">    config = unet_res_cfg</span><br><span class="line">    net = build_network(config, n_steps)</span><br><span class="line">    ddpm = DDPM(device, n_steps)</span><br><span class="line"></span><br><span class="line">    train(ddpm, net, device=device, ckpt_path=model_path)</span><br></pre></td></tr></table></figure></p>
<p>按照默认训练配置，在3090上花5分钟不到，训练30~40个epoch即可让网络基本收敛。最终收敛时loss在0.023~0.024左右。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">batch size: 512</span><br><span class="line">epoch 0 loss: 0.23103461712201437 elapsed 7.01s</span><br><span class="line">epoch 1 loss: 0.0627968365987142 elapsed 13.66s</span><br><span class="line">epoch 2 loss: 0.04828845852613449 elapsed 20.25s</span><br><span class="line">epoch 3 loss: 0.04148937337398529 elapsed 26.80s</span><br><span class="line">epoch 4 loss: 0.03801360730528831 elapsed 33.37s</span><br><span class="line">epoch 5 loss: 0.03604260584712028 elapsed 39.96s</span><br><span class="line">epoch 6 loss: 0.03357676289876302 elapsed 46.57s</span><br><span class="line">epoch 7 loss: 0.0335664684087038 elapsed 53.15s</span><br><span class="line">...</span><br><span class="line">epoch 30 loss: 0.026149748386939366 elapsed 204.64s</span><br><span class="line">epoch 31 loss: 0.025854381563266117 elapsed 211.24s</span><br><span class="line">epoch 32 loss: 0.02589433005253474 elapsed 217.84s</span><br><span class="line">epoch 33 loss: 0.026276464049021404 elapsed 224.41s</span><br><span class="line">...</span><br><span class="line">epoch 96 loss: 0.023299352884292603 elapsed 640.25s</span><br><span class="line">epoch 97 loss: 0.023460942271351815 elapsed 646.90s</span><br><span class="line">epoch 98 loss: 0.023584651704629263 elapsed 653.54s</span><br><span class="line">epoch 99 loss: 0.02364126600921154 elapsed 660.22s</span><br></pre></td></tr></table></figure>
<p>训练这个网络时，并没有特别好的测试指标，我们只能通过观察采样图像来评价网络的表现。我们可以用下面的代码调用DDPM的反向传播方法，生成多幅图像并保存下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_imgs</span>(<span class="params">ddpm,</span></span></span><br><span class="line"><span class="params"><span class="function">                net,</span></span></span><br><span class="line"><span class="params"><span class="function">                output_path,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_sample=<span class="number">81</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                simple_var=<span class="literal">True</span></span>):</span></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    net = net.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        shape = (n_sample, *get_img_shape())  <span class="comment"># 1, 3, 28, 28</span></span><br><span class="line">        imgs = ddpm.sample_backward(shape,</span><br><span class="line">                                    net,</span><br><span class="line">                                    device=device,</span><br><span class="line">                                    simple_var=simple_var).detach().cpu()</span><br><span class="line">        imgs = (imgs + <span class="number">1</span>) / <span class="number">2</span> * <span class="number">255</span></span><br><span class="line">        imgs = imgs.clamp(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">        imgs = einops.rearrange(imgs,</span><br><span class="line">                                <span class="string">&#x27;(b1 b2) c h w -&gt; (b1 h) (b2 w) c&#x27;</span>,</span><br><span class="line">                                b1=<span class="built_in">int</span>(n_sample**<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">        imgs = imgs.numpy().astype(np.uint8)</span><br><span class="line"></span><br><span class="line">        cv2.imwrite(output_path, imgs)</span><br></pre></td></tr></table></figure>
<p>一切顺利的话，我们可以得到一些不错的生成结果。下图是我得到的一些生成图片：</p>
<p><img src="/2023/07/07/20230330-diffusion-model/c3.jpg" alt></p>
<p>大部分生成的图片都对应一个阿拉伯数字，它们和训练集MNIST里的图片非常接近。这算是一个不错的生成结果。</p>
<p>如果神经网络的拟合能力较弱，生成结果就会差很多。下图是我训练一个简单的ResNet后得到的采样结果：<br><img src="/2023/07/07/20230330-diffusion-model/c4.jpg" alt></p>
<p>可以看出，每幅图片都很乱，基本对应不上一个数字。这就是一个较差的训练结果。</p>
<p>如果网络再差一点，可能会生成纯黑或者纯白的图片。这是因为网络的预测结果不准，在反向过程中，图像的均值不断偏移，偏移到远大于1或者远小于-1的值了。</p>
<p>总结一下，在复现DDPM时，最主要是要学习DDPM论文的两个算法，即训练算法和采样算法。两个算法很简单，可以轻松地把它们翻译成代码。而为了成功完成复现，还需要花一点心思在编写U-Net上，尤其是注意处理时间戳的部分。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/07/01/20230622-VQVAE-2/" class="post-title-link" itemprop="url">VQVAE PyTorch 实现教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-01 12:35:10" itemprop="dateCreated datePublished" datetime="2023-07-01T12:35:10+08:00">2023-07-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>前段时间我写了一篇VQVAE的解读，现在再补充一篇VQVAE的PyTorch实现教程。在这个项目中，我们会实现VQVAE论文，在MNIST和CelebAHQ两个数据集上完成图像生成。具体来说，我们会先实现并训练一个图像压缩网络VQVAE，它能把真实图像编码成压缩图像，或者把压缩图像解码回真实图像。之后，我们会训练一个生成压缩图像的生成网络PixelCNN。</p>
<p>代码仓库：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/VQVAE">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/VQVAE</a></p>
<h2 id="项目运行示例"><a href="#项目运行示例" class="headerlink" title="项目运行示例"></a>项目运行示例</h2><p>如果你只是想快速地把项目运行起来，可以只阅读本节。</p>
<p>在本地安装好项目后，运行<code>python dldemos/VQVAE/dataset.py</code>来下载MNIST数据集。之后运行<code>python dldemos/VQVAE/main.py</code>，这个脚本会完成以下四个任务：</p>
<ol>
<li>训练VQVAE</li>
<li>用VQVAE重建数据集里的随机数据</li>
<li>训练PixelCNN</li>
<li>用PixelCNN+VQVAE随机生成图片</li>
</ol>
<p>第二步得到的重建结果大致如下（每对图片中左图是原图，右图是重建结果）：</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/1.jpg" alt></p>
<p>第四步得到的随机生成结果大致如下：</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/2.jpg" alt></p>
<p>如果你要使用CelebAHQ数据集，请照着下一节的指示把CelebAHQ下载到指定目录，再执行<code>python dldemos/VQVAE/main.py -c 4</code>。</p>
<h2 id="数据集准备"><a href="#数据集准备" class="headerlink" title="数据集准备"></a>数据集准备</h2><p>MNIST数据集可以用PyTorch的API自动下载。我们可以用下面的代码下载MNIST数据集并查看数据的格式。从输出中可知，MNIST的图片形状为<code>[1, 28, 28]</code>，颜色取值范围为<code>[0, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_mnist</span>():</span></span><br><span class="line">    mnist = torchvision.datasets.MNIST(root=<span class="string">&#x27;data/mnist&#x27;</span>, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;length of MNIST&#x27;</span>, <span class="built_in">len</span>(mnist))</span><br><span class="line">    <span class="built_in">id</span> = <span class="number">4</span></span><br><span class="line">    img, label = mnist[<span class="built_in">id</span>]</span><br><span class="line">    <span class="built_in">print</span>(img)</span><br><span class="line">    <span class="built_in">print</span>(label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># On computer with monitor</span></span><br><span class="line">    <span class="comment"># img.show()</span></span><br><span class="line"></span><br><span class="line">    img.save(<span class="string">&#x27;work_dirs/tmp_mnist.jpg&#x27;</span>)</span><br><span class="line">    tensor = transforms.ToTensor()(img)</span><br><span class="line">    <span class="built_in">print</span>(tensor.shape)</span><br><span class="line">    <span class="built_in">print</span>(tensor.<span class="built_in">max</span>())</span><br><span class="line">    <span class="built_in">print</span>(tensor.<span class="built_in">min</span>())</span><br></pre></td></tr></table></figure>
<p>我们可以用下面的代码把它封成简单的<code>Dataset</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNISTImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, img_shape=(<span class="params"><span class="number">28</span>, <span class="number">28</span></span>)</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.img_shape = img_shape</span><br><span class="line">        self.mnist = torchvision.datasets.MNIST(root=<span class="string">&#x27;data/mnist&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.mnist)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span></span><br><span class="line">        img = self.mnist[index][<span class="number">0</span>]</span><br><span class="line">        pipeline = transforms.Compose(</span><br><span class="line">            [transforms.Resize(self.img_shape),</span><br><span class="line">             transforms.ToTensor()])</span><br><span class="line">        <span class="keyword">return</span> pipeline(img)</span><br></pre></td></tr></table></figure>
<p>接下来准备CelebAHQ。CelebAHQ数据集原本的图像大小是1024x1024，但我们这个项目用不到这么大的图片。我在kaggle上找到了一个256x256的CelebAHQ (<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256)，所有文件加起来只有300MB左右，很适合我们项目。请在该页面下载压缩包，并把压缩包解压到项目的`data/celebA/celeba_hq_256`目录下。">https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256)，所有文件加起来只有300MB左右，很适合我们项目。请在该页面下载压缩包，并把压缩包解压到项目的`data/celebA/celeba_hq_256`目录下。</a></p>
<p>下载完数据后，我们可以写一个简单的从目录中读取图片的<code>Dataset</code>类。和MNIST的预处理流程不同，我这里给CelebAHQ的图片加了一个中心裁剪的操作，一来可以让人脸占比更大，便于模型学习，二来可以让该类兼容CelebA数据集（CelebA数据集的图片不是正方形，需要裁剪）。这个操作是可选的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CelebADataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root, img_shape=(<span class="params"><span class="number">64</span>, <span class="number">64</span></span>)</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.root = root</span><br><span class="line">        self.img_shape = img_shape</span><br><span class="line">        self.filenames = <span class="built_in">sorted</span>(os.listdir(root))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.filenames)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span></span><br><span class="line">        path = os.path.join(self.root, self.filenames[index])</span><br><span class="line">        img = Image.<span class="built_in">open</span>(path)</span><br><span class="line">        pipeline = transforms.Compose([</span><br><span class="line">            transforms.CenterCrop(<span class="number">168</span>),</span><br><span class="line">            transforms.Resize(self.img_shape),</span><br><span class="line">            transforms.ToTensor()</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">return</span> pipeline(img)</span><br></pre></td></tr></table></figure>
<p>有了数据集类后，我们可以用它们生成<code>Dataloader</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">CELEBA_DIR = <span class="string">&#x27;data/celebA/img_align_celeba&#x27;</span></span><br><span class="line">CELEBA_HQ_DIR = <span class="string">&#x27;data/celebA/celeba_hq_256&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader</span>(<span class="params"><span class="built_in">type</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                   img_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   dist_train=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   num_workers=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span> == <span class="string">&#x27;CelebA&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> img_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            kwargs[<span class="string">&#x27;img_shape&#x27;</span>] = img_shape</span><br><span class="line">        dataset = CelebADataset(CELEBA_DIR, **kwargs)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;CelebAHQ&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> img_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            kwargs[<span class="string">&#x27;img_shape&#x27;</span>] = img_shape</span><br><span class="line">        dataset = CelebADataset(CELEBA_HQ_DIR, **kwargs)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;MNIST&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> img_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            dataset = MNISTImageDataset(img_shape)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dataset = MNISTImageDataset()</span><br><span class="line">    <span class="keyword">if</span> dist_train:</span><br><span class="line">        sampler = DistributedSampler(dataset)</span><br><span class="line">        dataloader = DataLoader(dataset,</span><br><span class="line">                                batch_size=batch_size,</span><br><span class="line">                                sampler=sampler,</span><br><span class="line">                                num_workers=num_workers)</span><br><span class="line">        <span class="keyword">return</span> dataloader, sampler</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dataloader = DataLoader(dataset,</span><br><span class="line">                                batch_size=batch_size,</span><br><span class="line">                                shuffle=<span class="literal">True</span>,</span><br><span class="line">                                num_workers=num_workers)</span><br><span class="line">        <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>
<p>我们可以利用<code>Dataloader</code>来查看CelebAHQ数据集的内容及数据格式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> os.path.exists(CELEBA_HQ_DIR):</span><br><span class="line">    dataloader = get_dataloader(<span class="string">&#x27;CelebAHQ&#x27;</span>, <span class="number">16</span>)</span><br><span class="line">    img = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloader))</span><br><span class="line">    <span class="built_in">print</span>(img.shape)</span><br><span class="line">    N = img.shape[<span class="number">0</span>]</span><br><span class="line">    img = einops.rearrange(img,</span><br><span class="line">                            <span class="string">&#x27;(n1 n2) c h w -&gt; c (n1 h) (n2 w)&#x27;</span>,</span><br><span class="line">                            n1=<span class="built_in">int</span>(N**<span class="number">0.5</span>))</span><br><span class="line">    <span class="built_in">print</span>(img.shape)</span><br><span class="line">    <span class="built_in">print</span>(img.<span class="built_in">max</span>())</span><br><span class="line">    <span class="built_in">print</span>(img.<span class="built_in">min</span>())</span><br><span class="line">    img = transforms.ToPILImage()(img)</span><br><span class="line">    img.save(<span class="string">&#x27;work_dirs/tmp_celebahq.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure><br>从输出中可知，CelebAHQ的颜色取值范围同样是<code>[0, 1]</code>。经我们的预处理流水线得到的图片如下。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/3.jpg" alt></p>
<h2 id="实现并训练-VQVAE"><a href="#实现并训练-VQVAE" class="headerlink" title="实现并训练 VQVAE"></a>实现并训练 VQVAE</h2><p>要用VQVAE做图像生成，其实要训练两个模型：一个是用于压缩图像的VQVAE，另一个是生成压缩图像的PixelCNN。这两个模型是可以分开训练的。我们先来实现并训练VQVAE。</p>
<p>VQVAE的架构非常简单：一个编码器，一个解码器，外加中间一个嵌入层。损失函数为图像的重建误差与编码器输出与其对应嵌入之间的误差。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/4.jpg" alt></p>
<p>VQVAE的编码器和解码器的结构也很简单，仅由普通的上/下采样层和残差块组成。具体来说，编码器先是有两个3x3卷积+2倍下采样卷积的模块，再有两个残差块(ReLU, 3x3卷积, ReLU, 1x1卷积)；解码器则反过来，先有两个残差块，再有两个3x3卷积+2倍上采样反卷积的模块。为了让代码看起来更清楚一点，我们不用过度封装，仅实现一个残差块模块，再用残差块和PyTorch自带模块拼成VQVAE。</p>
<p>先实现残差块。注意，由于模型比较简单，残差块内部和VQVAE其他地方都可以不使用BatchNorm。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.conv1 = nn.Conv2d(dim, dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(dim, dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        tmp = self.relu(x)</span><br><span class="line">        tmp = self.conv1(tmp)</span><br><span class="line">        tmp = self.relu(tmp)</span><br><span class="line">        tmp = self.conv2(tmp)</span><br><span class="line">        <span class="keyword">return</span> x + tmp</span><br></pre></td></tr></table></figure>
<p>有了残差块类后，我们可以直接实现VQVAE类。我们先在初始化函数里把模块按顺序搭好。编码器和解码器的结构按前文的描述搭起来即可。嵌入空间(codebook)其实就是个普通的嵌入层。此处我仿照他人代码给嵌入层显式初始化参数，但实测下来和默认的初始化参数方式差别不大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VQVAE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_dim, dim, n_embedding</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.encoder = nn.Sequential(nn.Conv2d(input_dim, dim, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.ReLU(), nn.Conv2d(dim, dim, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.ReLU(), nn.Conv2d(dim, dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                     ResidualBlock(dim), ResidualBlock(dim))</span><br><span class="line">        self.vq_embedding = nn.Embedding(n_embedding, dim)</span><br><span class="line">        self.vq_embedding.weight.data.uniform_(-<span class="number">1.0</span> / n_embedding,</span><br><span class="line">                                               <span class="number">1.0</span> / n_embedding)</span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Conv2d(dim, dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            ResidualBlock(dim), ResidualBlock(dim),</span><br><span class="line">            nn.ConvTranspose2d(dim, dim, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>), nn.ReLU(),</span><br><span class="line">            nn.ConvTranspose2d(dim, input_dim, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        self.n_downsample = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>之后，我们来实现模型的前向传播。这里的逻辑就略显复杂了。整体来看，这个函数完成了编码、取最近邻、解码这三步。其中，取最近邻的部分最为复杂。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="comment"># encode</span></span><br><span class="line">    ze = self.encoder(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ze: [N, C, H, W]</span></span><br><span class="line">    <span class="comment"># embedding [K, C]</span></span><br><span class="line">    embedding = self.vq_embedding.weight.data</span><br><span class="line">    N, C, H, W = ze.shape</span><br><span class="line">    K, _ = embedding.shape</span><br><span class="line">    embedding_broadcast = embedding.reshape(<span class="number">1</span>, K, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    ze_broadcast = ze.reshape(N, <span class="number">1</span>, C, H, W)</span><br><span class="line">    distance = torch.<span class="built_in">sum</span>((embedding_broadcast - ze_broadcast)**<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    nearest_neighbor = torch.argmin(distance, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># make C to the second dim</span></span><br><span class="line">    zq = self.vq_embedding(nearest_neighbor).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># stop gradient</span></span><br><span class="line">    decoder_input = ze + (zq - ze).detach()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># decode</span></span><br><span class="line">    x_hat = self.decoder(decoder_input)</span><br><span class="line">    <span class="keyword">return</span> x_hat, ze, zq</span><br></pre></td></tr></table></figure><br>我们来详细看一看取最近邻的实现。取最近邻时，我们要用到两块数据：编码器输出<code>ze</code>与嵌入矩阵<code>embedding</code>。<code>ze</code>可以看成一个形状为<code>[N, H, W]</code>的数组，数组存储了长度为<code>C</code>的向量。而嵌入矩阵里有<code>K</code>个长度为<code>C</code>的向量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ze: [N, C, H, W]</span></span><br><span class="line"><span class="comment"># embedding [K, C]</span></span><br><span class="line">embedding = self.vq_embedding.weight.data</span><br><span class="line">N, C, H, W = ze.shape</span><br><span class="line">K, _ = embedding.shape</span><br></pre></td></tr></table></figure><br>为了求<code>N*H*W</code>个向量在嵌入矩阵里的最近邻，我们要先算这每个向量与嵌入矩阵里<code>K</code>个向量的距离。在算距离前，我们要把<code>embedding</code>和<code>ze</code>的形状变换一下，保证<code>(embedding_broadcast - ze_broadcast)**2</code>的形状为<code>[N, K, C, H, W]</code>。我们对这个临时结果的第2号维度（<code>C</code>所在维度）求和，得到形状为<code>[N, K, H, W]</code>的<code>distance</code>。它的含义是，对于<code>N*H*W</code>个向量，每个向量到嵌入空间里<code>K</code>个向量的距离分别是多少。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embedding_broadcast = embedding.reshape(<span class="number">1</span>, K, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">ze_broadcast = ze.reshape(N, <span class="number">1</span>, C, H, W)</span><br><span class="line">distance = torch.<span class="built_in">sum</span>((embedding_broadcast - ze_broadcast)**<span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>有了距离张量后，我们再对其1号维度（<code>K</code>所在维度）求最近邻所在下标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nearest_neighbor = torch.argmin(distance, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>有了下标后，我们可以用<code>self.vq_embedding(nearest_neighbor)</code>从嵌入空间取出最近邻了。别忘了，<code>nearest_neighbor</code>的形状是<code>[N, H, W]</code>，<code>self.vq_embedding(nearest_neighbor)</code>的形状会是<code>[N, H, W, C]</code>。我们还要把<code>C</code>维度转置一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make C to the second dim</span></span><br><span class="line">zq = self.vq_embedding(nearest_neighbor).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>最后，我们用论文里提到的停止梯度算子，把<code>zq</code>变形一下。这样，算误差的时候用的是<code>zq</code>，算梯度时<code>ze</code>会接收解码器传来的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># stop gradient</span></span><br><span class="line">decoder_input = ze + (zq - ze).detach()</span><br></pre></td></tr></table></figure>
<p>求最近邻的部分就到此结束了。最后再补充一句，前向传播函数不仅返回了重建结果<code>x_hat</code>，还返回了<code>ze</code>, <code>zq</code>。这是因为我们待会要在训练时根据<code>ze</code>, <code>zq</code>求损失函数。</p>
<p>准备好了模型类后，假设我们已经用某些超参数初始化好了模型<code>model</code>，我们可以用下面的代码训练VQVAE。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_vqvae</span>(<span class="params">model: VQVAE,</span></span></span><br><span class="line"><span class="params"><span class="function">                img_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                ckpt_path=<span class="string">&#x27;dldemos/VQVAE/model.pth&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dataset_type=<span class="string">&#x27;MNIST&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                lr=<span class="number">1e-3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_epochs=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                l_w_embedding=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                l_w_commitment=<span class="number">0.25</span></span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;batch size:&#x27;</span>, batch_size)</span><br><span class="line">    dataloader = get_dataloader(dataset_type,</span><br><span class="line">                                batch_size,</span><br><span class="line">                                img_shape=img_shape,</span><br><span class="line">                                use_lmdb=USE_LMDB)</span><br><span class="line">    model.to(device)</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr)</span><br><span class="line">    mse_loss = nn.MSELoss()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> dataloader:</span><br><span class="line">            current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">            x = x.to(device)</span><br><span class="line"></span><br><span class="line">            x_hat, ze, zq = model(x)</span><br><span class="line">            l_reconstruct = mse_loss(x, x_hat)</span><br><span class="line">            l_embedding = mse_loss(ze.detach(), zq)</span><br><span class="line">            l_commitment = mse_loss(ze, zq.detach())</span><br><span class="line">            loss = l_reconstruct + \</span><br><span class="line">                l_w_embedding * l_embedding + l_w_commitment * l_commitment</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            total_loss += loss.item() * current_batch_size</span><br><span class="line">        total_loss /= <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">        toc = time.time()</span><br><span class="line">        torch.save(model.state_dict(), ckpt_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;e&#125;</span> loss: <span class="subst">&#123;total_loss&#125;</span> elapsed <span class="subst">&#123;(toc - tic):<span class="number">.2</span>f&#125;</span>s&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Done&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>先看一下训练函数的参数。其他参数都没什么特别的，只有误差权重<code>l_w_embedding=1,l_w_commitment=0.25</code>需要讨论一下。误差函数有三项，但论文只给了第三项的权重（0.25），默认第二项的权重为1。我在实现时把第二项的权重<code>l_w_embedding</code>也加上了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_vqvae</span>(<span class="params">model: VQVAE,</span></span></span><br><span class="line"><span class="params"><span class="function">                img_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                ckpt_path=<span class="string">&#x27;dldemos/VQVAE/model.pth&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dataset_type=<span class="string">&#x27;MNIST&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                lr=<span class="number">1e-3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_epochs=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                l_w_embedding=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                l_w_commitment=<span class="number">0.25</span></span>):</span></span><br></pre></td></tr></table></figure><br>再来把函数体过一遍。一开始，我们可以用传来的参数把<code>dataloader</code>初始化一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;batch size:&#x27;</span>, batch_size)</span><br><span class="line">dataloader = get_dataloader(dataset_type,</span><br><span class="line">                            batch_size,</span><br><span class="line">                            img_shape=img_shape,</span><br><span class="line">                            use_lmdb=USE_LMDB)</span><br></pre></td></tr></table></figure>
<p>再把模型的状态调好，并准备好优化器和算均方误差的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)</span><br><span class="line">model.train()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr)</span><br><span class="line">mse_loss = nn.MSELoss()</span><br></pre></td></tr></table></figure>
<p>准备好变量后，进入训练循环。训练的过程比较常规，唯一要注意的就是误差计算部分。由于我们把复杂的逻辑都放在了模型类中，这里我们可以直接先用<code>model(x)</code>得到重建图像<code>x_hat</code>和算误差的<code>ze, zq</code>，再根据论文里的公式算3个均方误差，最后求一个加权和，代码比较简明。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> dataloader:</span><br><span class="line">        current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = x.to(device)</span><br><span class="line"></span><br><span class="line">        x_hat, ze, zq = model(x)</span><br><span class="line">        l_reconstruct = mse_loss(x, x_hat)</span><br><span class="line">        l_embedding = mse_loss(ze.detach(), zq)</span><br><span class="line">        l_commitment = mse_loss(ze, zq.detach())</span><br><span class="line">        loss = l_reconstruct + \</span><br><span class="line">            l_w_embedding * l_embedding + l_w_commitment * l_commitment</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure></p>
<p>训练完毕后，我们可以用下面的代码来测试VQVAE的重建效果。所谓重建，就是模拟训练的过程，随机取一些图片，先编码后解码，看解码出来的图片和原图片是否一致。为了获取重建后的图片，我们只需要直接执行前向传播函数<code>model(x)</code>即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span>(<span class="params">model, x, device, dataset_type=<span class="string">&#x27;MNIST&#x27;</span></span>):</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        x_hat, _, _ = model(x)</span><br><span class="line">    n = x.shape[<span class="number">0</span>]</span><br><span class="line">    n1 = <span class="built_in">int</span>(n**<span class="number">0.5</span>)</span><br><span class="line">    x_cat = torch.concat((x, x_hat), <span class="number">3</span>)</span><br><span class="line">    x_cat = einops.rearrange(x_cat, <span class="string">&#x27;(n1 n2) c h w -&gt; (n1 h) (n2 w) c&#x27;</span>, n1=n1)</span><br><span class="line">    x_cat = (x_cat.clip(<span class="number">0</span>, <span class="number">1</span>) * <span class="number">255</span>).cpu().numpy().astype(np.uint8)</span><br><span class="line">    <span class="keyword">if</span> dataset_type == <span class="string">&#x27;CelebA&#x27;</span> <span class="keyword">or</span> dataset_type == <span class="string">&#x27;CelebAHQ&#x27;</span>:</span><br><span class="line">        x_cat = cv2.cvtColor(x_cat, cv2.COLOR_RGB2BGR)</span><br><span class="line">    cv2.imwrite(<span class="string">f&#x27;work_dirs/vqvae_reconstruct_<span class="subst">&#123;dataset_type&#125;</span>.jpg&#x27;</span>, x_cat)</span><br><span class="line"></span><br><span class="line">vqvae = ...</span><br><span class="line">dataloader = get_dataloader(...)</span><br><span class="line">img = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloader)).to(device)</span><br><span class="line">reconstruct(vqvae, img, device, cfg[<span class="string">&#x27;dataset_type&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h2 id="训练压缩图像生成模型-PixelCNN"><a href="#训练压缩图像生成模型-PixelCNN" class="headerlink" title="训练压缩图像生成模型 PixelCNN"></a>训练压缩图像生成模型 PixelCNN</h2><p>有了一个VQVAE后，我们要用另一个模型对VQVAE的离散空间采样，也就是训练一个能生成压缩图片的模型。我们可以按照VQVAE论文的方法，使用PixelCNN来生成压缩图片。</p>
<p>PixelCNN 的原理及实现方法就不在这里过多介绍了。详情可以参见我之前的PixelCNN解读文章。简单来说，PixelCNN给每个像素从左到右，从上到下地编了一个序号，让每个像素仅由之前所有像素决定。采样时，PixelCNN按序号从左上到右下逐个生成图像的每一个像素；训练时，PixelCNN使用了某种掩码机制，使得每个像素只能看到编号更小的像素，并行地输出每一个像素的生成结果。</p>
<p>PixelCNN具体的训练示意图如下。模型的输入是一幅图片，每个像素的取值是0~255；模型给图片的每个像素输出了一个概率分布，即表示此处颜色取0，取1，……，取255的概率。由于神经网络假设数据的输入符合标准正态分布，我们要在数据输入前把整型的颜色转换成0~1之间的浮点数。最简单的转换方法是除以255。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/5.jpg" alt></p>
<p>以上是训练PixelCNN生成普通图片的过程。而在训练PixelCNN生成压缩图片时，上述过程需要修改。压缩图片的取值是离散编码。离散编码和颜色值不同，它不是连续的。你可以说颜色1和颜色0、2相近，但不能说离散编码1和离散编码0、2相近。因此，为了让PixelCNN建模离散编码，需要把原来的除以255操作换成一个嵌入层，使得网络能够读取离散编码。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/6.jpg" alt></p>
<p>反映在代码中，假设我们已经有了一个普通的PixelCNN模型<code>GatedPixelCNN</code>，我们需要在整个模型的最前面套一个嵌入层，嵌入层的嵌入个数等于离散编码的个数(<code>color_level</code>)，嵌入长度等于模型的特征长度(<code>p</code>)。由于嵌入层会直接输出一个长度为<code>p</code>的向量，我们还需要把第一个模块的输入通道数改成<code>p</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dldemos.pixelcnn.model <span class="keyword">import</span> GatedPixelCNN, GatedBlock</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PixelCNNWithEmbedding</span>(<span class="params">GatedPixelCNN</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_blocks, p, linear_dim, bn=<span class="literal">True</span>, color_level=<span class="number">256</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(n_blocks, p, linear_dim, bn, color_level)</span><br><span class="line">        self.embedding = nn.Embedding(color_level, p)</span><br><span class="line">        self.block1 = GatedBlock(<span class="string">&#x27;A&#x27;</span>, p, p, bn)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().forward(x)</span><br></pre></td></tr></table></figure>
<p>有了一个能处理离散编码的PixelCNN后，我们可以用下面的代码来训练PixelCNN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_generative_model</span>(<span class="params">vqvae: VQVAE,</span></span></span><br><span class="line"><span class="params"><span class="function">                           model,</span></span></span><br><span class="line"><span class="params"><span class="function">                           img_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           ckpt_path=<span class="string">&#x27;dldemos/VQVAE/gen_model.pth&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           dataset_type=<span class="string">&#x27;MNIST&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           n_epochs=<span class="number">50</span></span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;batch size:&#x27;</span>, batch_size)</span><br><span class="line">    dataloader = get_dataloader(dataset_type,</span><br><span class="line">                                batch_size,</span><br><span class="line">                                img_shape=img_shape,</span><br><span class="line">                                use_lmdb=USE_LMDB)</span><br><span class="line">    vqvae.to(device)</span><br><span class="line">    vqvae.<span class="built_in">eval</span>()</span><br><span class="line">    model.to(device)</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), <span class="number">1e-3</span>)</span><br><span class="line">    loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> dataloader:</span><br><span class="line">            current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                x = x.to(device)</span><br><span class="line">                x = vqvae.encode(x)</span><br><span class="line"></span><br><span class="line">            predict_x = model(x)</span><br><span class="line">            loss = loss_fn(predict_x, x)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            total_loss += loss.item() * current_batch_size</span><br><span class="line">        total_loss /= <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">        toc = time.time()</span><br><span class="line">        torch.save(model.state_dict(), ckpt_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;e&#125;</span> loss: <span class="subst">&#123;total_loss&#125;</span> elapsed <span class="subst">&#123;(toc - tic):<span class="number">.2</span>f&#125;</span>s&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Done&#x27;</span>)</span><br><span class="line">gen_model = PixelCNNWithEmbedding(cfg[<span class="string">&#x27;pixelcnn_n_blocks&#x27;</span>],</span><br><span class="line">                                      cfg[<span class="string">&#x27;pixelcnn_dim&#x27;</span>],</span><br><span class="line">                                      cfg[<span class="string">&#x27;pixelcnn_linear_dim&#x27;</span>], <span class="literal">True</span>,</span><br><span class="line">                                      cfg[<span class="string">&#x27;n_embedding&#x27;</span>])</span><br><span class="line">vqvae.load_state_dict(torch.load(cfg[<span class="string">&#x27;vqvae_path&#x27;</span>]))</span><br><span class="line">train_generative_model(vqvae,</span><br><span class="line">                        gen_model,</span><br><span class="line">                        img_shape=(img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]),</span><br><span class="line">                        device=device,</span><br><span class="line">                        ckpt_path=cfg[<span class="string">&#x27;gen_model_path&#x27;</span>],</span><br><span class="line">                        dataset_type=cfg[<span class="string">&#x27;dataset_type&#x27;</span>],</span><br><span class="line">                        batch_size=cfg[<span class="string">&#x27;batch_size_2&#x27;</span>],</span><br><span class="line">                        n_epochs=cfg[<span class="string">&#x27;n_epochs_2&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>训练部分的核心代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> dataloader:</span><br><span class="line">    current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        x = vqvae.encode(x)</span><br><span class="line"></span><br><span class="line">    predict_x = model(x)</span><br><span class="line">    loss = loss_fn(predict_x, x)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>这段代码的意思是说，从训练集里随机取图片<code>x</code>，再将图片压缩成离散编码<code>x = vqvae.encode(x)</code>。这时，<code>x</code>既是PixelCNN的输入，也是PixelCNN的拟合目标。把它输入进PixelCNN，PixelCNN会输出每个像素的概率分布。用交叉熵损失函数约束输出结果即可。</p>
<p>训练完毕后，我们可以用下面的函数来完成整套图像生成流水线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_imgs</span>(<span class="params">vqvae: VQVAE,</span></span></span><br><span class="line"><span class="params"><span class="function">                gen_model,</span></span></span><br><span class="line"><span class="params"><span class="function">                img_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_sample=<span class="number">81</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dataset_type=<span class="string">&#x27;MNIST&#x27;</span></span>):</span></span><br><span class="line">    vqvae = vqvae.to(device)</span><br><span class="line">    vqvae.<span class="built_in">eval</span>()</span><br><span class="line">    gen_model = gen_model.to(device)</span><br><span class="line">    gen_model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    C, H, W = img_shape</span><br><span class="line">    H, W = vqvae.get_latent_HW((C, H, W))</span><br><span class="line">    input_shape = (n_sample, H, W)</span><br><span class="line">    x = torch.zeros(input_shape).to(device).to(torch.long)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">                output = gen_model(x)</span><br><span class="line">                prob_dist = F.softmax(output[:, :, i, j], -<span class="number">1</span>)</span><br><span class="line">                pixel = torch.multinomial(prob_dist, <span class="number">1</span>)</span><br><span class="line">                x[:, i, j] = pixel[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    imgs = vqvae.decode(x)</span><br><span class="line"></span><br><span class="line">    imgs = imgs * <span class="number">255</span></span><br><span class="line">    imgs = imgs.clip(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    imgs = einops.rearrange(imgs,</span><br><span class="line">                            <span class="string">&#x27;(n1 n2) c h w -&gt; (n1 h) (n2 w) c&#x27;</span>,</span><br><span class="line">                            n1=<span class="built_in">int</span>(n_sample**<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">    imgs = imgs.detach().cpu().numpy().astype(np.uint8)</span><br><span class="line">    <span class="keyword">if</span> dataset_type == <span class="string">&#x27;CelebA&#x27;</span> <span class="keyword">or</span> dataset_type == <span class="string">&#x27;CelebAHQ&#x27;</span>:</span><br><span class="line">        imgs = cv2.cvtColor(imgs, cv2.COLOR_RGB2BGR)</span><br><span class="line"></span><br><span class="line">    cv2.imwrite(<span class="string">f&#x27;work_dirs/vqvae_sample_<span class="subst">&#123;dataset_type&#125;</span>.jpg&#x27;</span>, imgs)</span><br></pre></td></tr></table></figure>
<p>抛掉前后处理，和图像生成有关的代码如下。一开始，我们要随便创建一个空图片<code>x</code>，用于储存PixelCNN生成的压缩图片。之后，我们按顺序遍历每个像素，把当前图片输入进PixelCNN，让PixelCNN预测下一个像素的概率分布<code>prob_dist</code>。我们再用<code>torch.multinomial</code>从概率分布中采样，把采样的结果填回图片。遍历结束后，我们用VQVAE的解码器把压缩图片变成真实图片。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">C, H, W = img_shape</span><br><span class="line">H, W = vqvae.get_latent_HW((C, H, W))</span><br><span class="line">input_shape = (n_sample, H, W)</span><br><span class="line">x = torch.zeros(input_shape).to(device).to(torch.long)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">            output = gen_model(x)</span><br><span class="line">            prob_dist = F.softmax(output[:, :, i, j], -<span class="number">1</span>)</span><br><span class="line">            pixel = torch.multinomial(prob_dist, <span class="number">1</span>)</span><br><span class="line">            x[:, i, j] = pixel[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">imgs = vqvae.decode(x)</span><br></pre></td></tr></table></figure><br>至此，我们已经实现了用VQVAE做图像生成的四个任务：训练VQVAE、重建图像、训练PixelCNN、随机生成图像。完整的<code>main</code>函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    os.makedirs(<span class="string">&#x27;work_dirs&#x27;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;-c&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;-d&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    cfg = get_cfg(args.c)</span><br><span class="line"></span><br><span class="line">    device = <span class="string">f&#x27;cuda:<span class="subst">&#123;args.d&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line">    img_shape = cfg[<span class="string">&#x27;img_shape&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    vqvae = VQVAE(img_shape[<span class="number">0</span>], cfg[<span class="string">&#x27;dim&#x27;</span>], cfg[<span class="string">&#x27;n_embedding&#x27;</span>])</span><br><span class="line">    gen_model = PixelCNNWithEmbedding(cfg[<span class="string">&#x27;pixelcnn_n_blocks&#x27;</span>],</span><br><span class="line">                                      cfg[<span class="string">&#x27;pixelcnn_dim&#x27;</span>],</span><br><span class="line">                                      cfg[<span class="string">&#x27;pixelcnn_linear_dim&#x27;</span>], <span class="literal">True</span>,</span><br><span class="line">                                      cfg[<span class="string">&#x27;n_embedding&#x27;</span>])</span><br><span class="line">    <span class="comment"># 1. Train VQVAE</span></span><br><span class="line">    train_vqvae(vqvae,</span><br><span class="line">                img_shape=(img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]),</span><br><span class="line">                device=device,</span><br><span class="line">                ckpt_path=cfg[<span class="string">&#x27;vqvae_path&#x27;</span>],</span><br><span class="line">                batch_size=cfg[<span class="string">&#x27;batch_size&#x27;</span>],</span><br><span class="line">                dataset_type=cfg[<span class="string">&#x27;dataset_type&#x27;</span>],</span><br><span class="line">                lr=cfg[<span class="string">&#x27;lr&#x27;</span>],</span><br><span class="line">                n_epochs=cfg[<span class="string">&#x27;n_epochs&#x27;</span>],</span><br><span class="line">                l_w_embedding=cfg[<span class="string">&#x27;l_w_embedding&#x27;</span>],</span><br><span class="line">                l_w_commitment=cfg[<span class="string">&#x27;l_w_commitment&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Test VQVAE by visualizaing reconstruction result</span></span><br><span class="line">    vqvae.load_state_dict(torch.load(cfg[<span class="string">&#x27;vqvae_path&#x27;</span>]))</span><br><span class="line">    dataloader = get_dataloader(cfg[<span class="string">&#x27;dataset_type&#x27;</span>],</span><br><span class="line">                                <span class="number">16</span>,</span><br><span class="line">                                img_shape=(img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]))</span><br><span class="line">    img = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloader)).to(device)</span><br><span class="line">    reconstruct(vqvae, img, device, cfg[<span class="string">&#x27;dataset_type&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Train Generative model (Gated PixelCNN in our project)</span></span><br><span class="line">    vqvae.load_state_dict(torch.load(cfg[<span class="string">&#x27;vqvae_path&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    train_generative_model(vqvae,</span><br><span class="line">                           gen_model,</span><br><span class="line">                           img_shape=(img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]),</span><br><span class="line">                           device=device,</span><br><span class="line">                           ckpt_path=cfg[<span class="string">&#x27;gen_model_path&#x27;</span>],</span><br><span class="line">                           dataset_type=cfg[<span class="string">&#x27;dataset_type&#x27;</span>],</span><br><span class="line">                           batch_size=cfg[<span class="string">&#x27;batch_size_2&#x27;</span>],</span><br><span class="line">                           n_epochs=cfg[<span class="string">&#x27;n_epochs_2&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Sample VQVAE</span></span><br><span class="line">    vqvae.load_state_dict(torch.load(cfg[<span class="string">&#x27;vqvae_path&#x27;</span>]))</span><br><span class="line">    gen_model.load_state_dict(torch.load(cfg[<span class="string">&#x27;gen_model_path&#x27;</span>]))</span><br><span class="line">    sample_imgs(vqvae,</span><br><span class="line">                gen_model,</span><br><span class="line">                cfg[<span class="string">&#x27;img_shape&#x27;</span>],</span><br><span class="line">                device=device,</span><br><span class="line">                dataset_type=cfg[<span class="string">&#x27;dataset_type&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>VQVAE有两个超参数：嵌入个数<code>n_embedding</code>、特征向量长度<code>dim</code>。论文中<code>n_embedding=512</code>，<code>dim=256</code>。而经我实现发现，用更小的参数量也能达到不错的效果。</p>
<p>所有实验的配置文件我都放在了该项目目录下<code>config.py</code>文件中。对于MNIST数据集，我使用的模型超参数为：<code>dim=32, n_embedding=32</code>。VQVAE重建结果如下所示。可以说重建得几乎完美（每对图片左图为原图，右图为重建结果）。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/1.jpg" alt></p>
<p>而对于CelebAHQ数据集，我测试了不同输入尺寸下的不同VQVAE，共有4组配置。</p>
<ol>
<li><code>shape=(3, 128, 128) dim=128 n_embedding=64</code></li>
<li><code>shape=(3, 128, 128) dim=128 n_embedding=128</code></li>
<li><code>shape=(3, 64, 64) dim=128 n_embedding=64</code></li>
<li><code>shape=(3, 64, 64) dim=128 n_embedding=32</code></li>
</ol>
<p>实验的结果很好预测。对于同尺寸的图片，嵌入数越多重建效果越好。这里我只展示下第一组和第二组的重建结果。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/7.jpg" alt></p>
<p>可以看出，VQVAE的重建效果还不错。但由于只使用了均方误差，重建图片在细节上还是比较模糊。重建效果还是很重要的，它决定了该方法做图像生成的质量上限。后续有很多工作都试图提升VQVAE的重建效果。</p>
<p>接下来来看一下随机图像生成的实验。PixelCNN主要有模块数<code>n_blocks</code>、特征长度<code>dim</code>，输出线性层特征长度<code>linear_dim</code>这三个超参数。其中模块数一般是固定的，而输出线性层就被用了一次，其特征长度的影响不大。最需要调节的是特征长度<code>dim</code>。对于MNIST，我的超参数设置为</p>
<ul>
<li><code>n_blocks=15 dim=128 linear_dim=32</code>.</li>
</ul>
<p>对于CelebAHQ，我的超参数设置为</p>
<ul>
<li><code>n_blocks=15 dim=384 linear_dim=256</code>.</li>
</ul>
<p>PixelCNN的训练时间主要由输入图片尺寸和<code>dim</code>决定，训练难度主要由VQVAE的嵌入个数（即多分类的类别数）决定。PixelCNN训起来很花时间。如果时间有限，在CelebAHQ上建议只训练最小最简单的第4组配置。我在项目中提供了PixelCNN的并行训练脚本，比如用下面的命令可以用4张卡在1号配置下并行训练。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=4 dldemos/VQVAE/dist_train_pixelcnn.py -c 1</span><br></pre></td></tr></table></figure>
<p>来看一下实验结果。MNIST上的采样结果还是非常不错的。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/2.jpg" alt></p>
<p>CelebAHQ上的结果会差一点。以下是第4组配置(图像边长<code>64</code>，嵌入数<code>32</code>)的采样结果。大部分图片都还行，起码看得出是一张人脸。但<code>64x64</code>的图片本来就分辨率不高，加上VQVAE解码的损耗，放大来看人脸还是比较模糊的。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/8.jpg" alt></p>
<p>第1组配置（图像边长<code>128</code>，嵌入数<code>64</code>）的PixelCNN实在训练得太慢了，我只训了一个半成品模型。由于部分生成结果比较吓人，我只挑了几个还能看得过去的生成结果。可以看出，如果把模型训完的话，边长128的模型肯定比边长64的模型效果更好。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/9.jpg" alt></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>网上几乎找不到在CelebAHQ上训练的VQVAE PyTorch项目。我在实现这份代码时，参考了以下项目：</p>
<ul>
<li>官方TensorFlow实现 <a target="_blank" rel="noopener" href="https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb">https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb</a> 。主要代码都写在一个notebook里。</li>
<li>官方实现的PyTorch复现 <a target="_blank" rel="noopener" href="https://github.com/MishaLaskin/vqvae。">https://github.com/MishaLaskin/vqvae。</a></li>
<li>苏剑林的TensorFlow实现。用的生成模型不是PixelCNN而是Transformer。<a target="_blank" rel="noopener" href="https://github.com/bojone/vae/blob/master/vq_vae_keras.py">https://github.com/bojone/vae/blob/master/vq_vae_keras.py</a></li>
</ul>
<h2 id="实验经历分享"><a href="#实验经历分享" class="headerlink" title="实验经历分享"></a>实验经历分享</h2><p>别看VQVAE的代码不难，我做这些实验时还是经历了不少波折的。</p>
<p>一开始，我花一天就把代码写完了，并完成了MNIST上的实验。我觉得在MNIST上做实验的难度太低，不过瘾，就准备把数据集换成CelebA再做一些实验。结果这一做就是两个星期。</p>
<p>换成CelebA后，我碰到的第一个问题是VQVAE训练速度太慢。我尝试减半模型参数，训练时间却减小得不明显。我大致猜出是数据读取占用了大量时间，用性能分析工具一查，果然如此。原来我在<code>DataLoader</code>中一直只用了一个线程，加上<code>num_workers=4</code>就好了。我还把数据集打包成LMDB格式进一步加快数据读取速度。</p>
<p>之后，我又发现VQVAE在CelebA上的重建效果很差。我尝试增加模型参数，没起作用。我又怀疑是64x64的图片质量太低，模型学不到东西，就尝试把输入尺寸改成128x128，并把数据集从CelebA换成CelebAHQ，重建效果依然不行。我调了很多参数，发现了一些奇怪的现象：在嵌入层前使用和不使用BatchNorm对结果的影响很大，且显式初始化嵌入层会让模型的误差一直居高不下。我实在是找不到问题，就拿代码对着别人的PyTorch实现一行一行比较过去。总算，我发现我在使用嵌入层时是用<code>vq_embedding.weight.data[x]</code>（因为前面已经获取了这个矩阵，这样写比较自然），别人是用<code>vq_embedding(x)</code>。我的写法会把嵌入层排除在梯度计算外，嵌入层根本得不到优化。我说怎么换了一个嵌入层的初始化方法模型就根本训不动了。改完bug之后，只训了5个epoch，新模型的误差比原来训练数小时的模型要低了。新模型的重建效果非常好。</p>
<p>总算，任务完成了一半，现在只剩PixelCNN要训练了。我先尝试训练输入为128x128，嵌入数64的模型，采样结果很差。为了加快实验速度，我把输入尺寸减小到64x64，再次训练，采样结果还是不行。根据我之前的经验，PixelCNN的训练难度主要取决于类别数。于是，我把嵌入的数量从64改成了32，并大幅增加PixelCNN的参数量，再次训练。过了很久，训练误差终于降到0.08左右。我一测，这次的采样结果还不错。</p>
<p>这样看来，之前的采样效果不好，是输入128x128，嵌入数64的实验太难了。我毕竟只是想做一个demo，在一个小型实验上成功就行了，没必要花时间去做更耗时的实验。按理说，我应该就此收手。但是，我就是咽不下这一口气，就是想在128x128的实验上成功。我再次加大了PixelCNN的参数量，用128x128的配置，大火慢炖，训练了一天一夜。第二天一早起来，我看到这回的误差也降到了0.08。上次的实验误差降到这个程度时实验已经成功了。我迫不及待地去测试采样效果，却发现采样效果还是稀烂。没办法，我选择投降，开始写这篇文章，准备收工。</p>
<p>写到PixelCNN介绍的那一章节时，我正准备讲解代码。看到PixelCNN训练之前预处理除以<code>color_level</code>那一行时，我楞了一下：这行代码是用来做什么的来着？这段代码全是从PixelCNN项目里复制过来的。当时是做普通图片的图像生成，所以要对输入颜色做一个预处理，把整数颜色变成0~1之间的浮点数。但现在是在生成压缩图片，不能这样处理啊！我恍然大悟，知道是在处理离散输入时做错了。应该多加一个嵌入层，把离散值转换成向量。由于VQVAE的重点不在生成模型上，原论文根本没有强调PixelCNN在离散编码上的实现细节。网上几乎所有文章也都没谈这一点。因此，我在实现PixelCNN时，直接不假思索地把原来的代码搬了过来，根本没想过这种地方会出现bug。</p>
<p>把这处bug改完后，我再次开启训练。这下所有模型的采样结果都正常了。误差降到0.5左右就已经有不错的采样结果了，原来我之前把误差降到0.08完全是无用功。太气人了。</p>
<p>这次的实验让我学到了很多东西。首先是PyTorch编程上的一些注意事项：</p>
<ul>
<li>调用<code>embedding.weight.data[x]</code>是传不了梯度的。</li>
<li>如果读数据时有费时的处理操作（读写硬盘、解码），要在<code>Dataloader</code>里设置<code>num_workers</code>。</li>
</ul>
<p>另外，在测试一个模型是否实现成功时有一个重要的准则：</p>
<ul>
<li>不要仅在简单的数据集（如MNIST）上测试。测试成功可能只是暴力拟合的结果。只有在一个难度较大的数据集上测试成功才能说模型没有问题。</li>
</ul>
<p>在观察模型是否训成功时，还需要注意：</p>
<ul>
<li>训练误差降低不代表模型更优。训练误差的评价方法和模型实际使用方法可能完全不同。不能像我这样偷懒不加测试指标。</li>
</ul>
<p>除了学到的东西外，我还有一些感想。在别人的项目的基础上修改、照着他人代码复现、完全自己动手从零开始写，对于深度学习项目来说，这三种实现方式的难度是依次递增的。改别人的项目，你可能去配置文件里改一两个数字就行了。而照着他人代码复现，最起码你能把代码改成和他人的代码一模一样，然后再去比较哪一块错了。自己动手写，则是有bug都找不到可以参考的地方了。说深度学习的算法难以调试，难就难在这里。效果不好，你很难说清是训练代码错了、超参数没设置好、训练流程错了，或是测试代码错了。可以出错的地方太多了，通常的代码调试手段难以用在深度学习项目上。</p>
<p>对于想要在深度学习上有所建树的初学者，我建议一定要从零动手复现项目。很多工程经验是难以总结的，只有踩了一遍坑才能知道。除了凭借经验外，还可以掌握一些特定的工程方法来减少bug的出现。比如运行训练之前先拿性能工具分析一遍，看看代码是否有误，是否可以提速；又比如可以训练几步后看所有可学习参数是否被正确修改。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2023/06/19/20230605-VQGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/19/20230605-VQGAN/" class="post-title-link" itemprop="url">VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-19 17:53:59" itemprop="dateCreated datePublished" datetime="2023-06-19T17:53:59+08:00">2023-06-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>2022年中旬，以扩散模型为核心的图像生成模型将AI绘画带入了大众的视野。实际上，在更早的一年之前，就有了一个能根据文字生成高清图片的模型——VQGAN。VQGAN不仅本身具有强大的图像生成能力，更是传承了前作VQVAE把图像压缩成离散编码的思想，推广了「先压缩，再生成」的两阶段图像生成思路，启发了无数后续工作。</p>
<p><img src="/2023/06/19/20230605-VQGAN/0.jpg" alt="VQGAN生成出的高清图片"></p>
<p>在这篇文章中，我将对VQGAN的论文和源码中的关键部分做出解读，提炼出VQGAN中的关键知识点。由于VQGAN的核心思想和VQVAE如出一辙，我不会过多地介绍VQGAN的核心思想，强烈建议读者先去学懂VQVAE，再来看VQGAN。</p>
<h2 id="VQGAN-核心思想"><a href="#VQGAN-核心思想" class="headerlink" title="VQGAN 核心思想"></a>VQGAN 核心思想</h2><p>VQGAN的论文名为<em>Taming Transformers for High-Resolution Image Synthesis</em>，直译过来是「驯服Transformer模型以实现高清图像合成」。可以看出，该方法是在用Transformer生成图像。可是，为什么这个模型叫做VQGAN，是一个GAN呢？这是因为，VQGAN使用了两阶段的图像生成方法：</p>
<ul>
<li>训练时，先训练一个图像压缩模型（包括编码器和解码器两个子模型），再训练一个生成压缩图像的模型。</li>
<li>生成时，先用第二个模型生成出一个压缩图像，再用第一个模型复原成真实图像。</li>
</ul>
<p>其中，第一个图像压缩模型叫做VQGAN，第二个压缩图像生成模型是一个基于Transformer的模型。</p>
<p>为什么会有这种乍看起来非常麻烦的图像生成方法呢？要理解VQGAN的这种设计动机，有两条路线可以走。两条路线看待问题的角度不同，但实际上是在讲同一件事。</p>
<p>第一条路线是从Transformer入手。Transformer已经在文本生成领域大展身手。同时，Transformer也在视觉任务中开始崭露头角。相比擅长捕捉局部特征的CNN，Transformer的优势在于它能更好地融合图像的全局信息。可是，Transformer的自注意力操作开销太大，只能生成一些分辨率较低的图像。因此，作者认为，可以综合CNN和Transformer的优势，先用基于CNN的VQGAN把图像压缩成一个尺寸更小、信息更丰富的小图像，再用Transformer来生成小图像。</p>
<p>第二条路线是从VQVAE入手。VQVAE是VQGAN的前作，它有着和VQGAN一模一样两阶段图像生成方法。不同的是，VQVAE没有使用GAN结构，且其配套的压缩图像生成模型是基于CNN的。为提升VQVAE的生成效果，作者提出了两项改进策略：1) 图像压缩模型VQVAE仅使用了均方误差，压缩图像的复原结果较为模糊，可以把图像压缩模型换成GAN；2) 在生成压缩图片这个任务上，基于CNN的图像生成模型比不过Transformer，可以用Transformer代替原来的CNN。</p>
<p>第一条思路是作者在论文的引言中描述的，听起来比较高大上；而第二条思路是读者读过文章后能够自然总结出来的，相对来说比较清晰易懂。如果你已经理解了VQVAE，你能通过第二条思路瞬间弄懂VQGAN的原理。说难听点，VQGAN就是一个改进版的VQVAE。然而，VQGAN的改进非常有效，且使用了若干技巧来实现带约束（比如根据文字描述）的高清图像生成，有非常多地方值得学习。</p>
<p>在下文中，我将先补充VQVAE的背景以方便讨论，再介绍VQGAN论文的四大知识点：VQGAN的设计细节、生成压缩图像的Transformer的设计细节、带约束图像生成的实现方法、高清图像生成的实现方法。</p>
<h2 id="VQVAE-背景知识补充"><a href="#VQVAE-背景知识补充" class="headerlink" title="VQVAE 背景知识补充"></a>VQVAE 背景知识补充</h2><p>VQVAE的学习目标是用一个编码器把图像压缩成离散编码，再用一个解码器把图像尽可能地还原回原图像。</p>
<p><img src="/2023/06/19/20230605-VQGAN/1.jpg" alt></p>
<p>通俗来说，VQVAE就是把一幅真实图像压缩成一个小图像。这个小图像和真实图像有着一些相同的性质：小图像的取值和像素值（0-255的整数）一样，都是离散的；小图像依然是二维的，保留了某些空间信息。因此，VQVAE的示意图画成这样会更形象一些：</p>
<p><img src="/2023/06/19/20230605-VQGAN/2.jpg" alt></p>
<p>但小图像和真实图像有一个关键性的区别：与像素值不同，小图像的离散取值之间没有关联。真实图像的像素值其实是一个连续颜色的离散采样，相邻的颜色值也更加相似。比如颜色254和颜色253和颜色255比较相似。而小图像的取值之间是没有关联的，你不能说编码为1与编码为0和编码为2比较相似。由于神经网络不能很好地处理这种离散量，在实际实现中，编码并不是以整数表示的，而是以类似于NLP中的嵌入向量的形式表示的。VAE使用了嵌入空间（又称codebook）来完成整数序号到向量的转换。</p>
<p><img src="/2023/06/19/20230605-VQGAN/3.jpg" alt></p>
<p>为了让任意一个编码器输出向量都变成一个固定的嵌入向量，VQVAE采取了一种离散化策略：把每个输出向量$z_e(x)$替换成嵌入空间中最近的那个向量$z_q(x)$。$z_e(x)$的离散编码就是$z_q(x)$在嵌入空间的下标。这个过程和把254.9的输出颜色值离散化成255的整数颜色值的原理类似。</p>
<p><img src="/2023/06/19/20230605-VQGAN/4.jpg" alt></p>
<p>VQVAE的损失函数由两部分组成：重建误差和嵌入空间误差。</p>
<script type="math/tex; mode=display">
L_{VQ} = L_{reconstruct} + L_{embedding}</script><p>其中，重建误差就是输入和输出之间的均方误差。</p>
<script type="math/tex; mode=display">
L_{reconstruct} = ||x - \hat{x}||_2^2</script><p>嵌入空间误差为解码器输出向量$z_e(x)$和它在嵌入空间对应向量$z_q(x)$的均方误差。</p>
<script type="math/tex; mode=display">
L_{embedding} = ||z_e(x) - z_q(x)||_2^2</script><p>作者在误差中还使用了一种「停止梯度」的技巧。这个技巧在VQGAN中被完全保留，此处就不过多介绍了。</p>
<h2 id="图像压缩模型-VQGAN"><a href="#图像压缩模型-VQGAN" class="headerlink" title="图像压缩模型 VQGAN"></a>图像压缩模型 VQGAN</h2><p>回顾了VQVAE的背景知识后，我们来正式认识VQGAN的几个创新点。第一点，图像压缩模型VQVAE被改进成了VQGAN。</p>
<p>一般VAE重建出来出来的图像都会比较模糊。这是因为VAE只使用了均方误差，而均方误差只能保证像素值尽可能接近，却不能保证图像的感知效果更加接近。为此，作者把GAN的一些方法引入VQVAE，改造出了VQGAN。</p>
<p>具体来说，VQGAN有两项改进。第一，作者用感知误差(perceptual loss)代替原来的均方误差作为VQGAN的重建误差。第二，作者引入了GAN的对抗训练机制，加入了一个基于图块的判别器，把GAN误差加入了总误差。</p>
<blockquote>
<p>计算感知误差的方法如下：把两幅图像分别输入VGG，取出中间某几层卷积层的特征，计算特征图像之间的均方误差。如果你之前没学过相关知识，请搜索”perceptual loss”。</p>
<p>基于图块的判别器，即判别器不为整幅图输出一个真或假的判断结果，而是把图像拆成若干图块，分别输出每个图块的判断结果，再对所有图块的判断结果取一个均值。这只是GAN的一种改进策略而已，没有对GAN本身做太大的改动。如果你之前没学过相关知识，请搜索”PatchGAN”。</p>
</blockquote>
<p>这样，总的误差可以写成：</p>
<script type="math/tex; mode=display">
L = L_{VQ} + \lambda L_{GAN}</script><p>其中，$\lambda$是控制两种误差比例的权重。作者在论文中使用了一个公式来自适应地设置$\lambda$。和普通的GAN一样，VQGAN的编码器、解码器（即生成器）、codebook会最小化误差，判别器会最大化误差。</p>
<p>用VQGAN代替VQVAE后，重建图片中的模糊纹理清晰了很多。</p>
<p><img src="/2023/06/19/20230605-VQGAN/5.jpg" alt></p>
<p>有了一个保真度高的图像压缩模型，我们可以进入下一步，训练一个生成压缩图像的模型。</p>
<h2 id="基于-Transformer-的压缩图像生成模型"><a href="#基于-Transformer-的压缩图像生成模型" class="headerlink" title="基于 Transformer 的压缩图像生成模型"></a>基于 Transformer 的压缩图像生成模型</h2><p>如前所述，经VQGAN得到的压缩图像与真实图像有一个本质性的不同：真实图像的像素值具有连续性，相邻的颜色更加相似，而压缩图像的像素值则没有这种连续性。压缩图像的这一特性让寻找一个压缩图像生成模型变得异常困难。多数强大的真实图像生成模型（比如GAN）都是输出一个连续的浮点颜色值，再做一个浮点转整数的操作，得到最终的像素值。而对于压缩图像来说，这种输出连续颜色的模型都不适用了。因此，之前的VQVAE使用了一个能建模离散颜色的PixelCNN模型作为压缩图像生成模型。但PixelCNN的表现不够优秀。</p>
<p>恰好，功能强大的Transformer天生就支持建模离散的输出。在NLP中，每个单词都可以用一个离散的数字表示。Transformer会不断生成表示单词的数字，以达到生成句子的效果。</p>
<p><img src="/2023/06/19/20230605-VQGAN/6.jpg" alt="Transformer 随机生成句子的过程"></p>
<p>为了让Transformer生成图像，我们可以把生成句子的一个个单词，变成生成压缩图像的一个个像素。但是，要让Transformer生成二维图像，还需要克服一个问题：在生成句子时，Transformer会先生成第一个单词，再根据第一个单词生成第二个单词，再根据第一、第二个单词生成第三个单词……。也就是说，Transformer每次会根据<strong>之前</strong>所有的单词来生成下一单词。而图像是二维数据，没有先后的概念，怎样让像素和文字一样有先后顺序呢？</p>
<p>VQGAN的作者使用了自回归图像生成模型的常用做法，给图像的每个像素从左到右，从上到下规定一个顺序。有了先后顺序后，图像就可以被视为一个一维句子，可以用Transfomer生成句子的方式来生成图像了。在第$i$步，Transformer会根据前$i - 1$个像素$s_{ &lt; i}$生成第$i$个像素$s_i$，</p>
<p><img src="/2023/06/19/20230605-VQGAN/7.jpg" alt></p>
<h2 id="带约束的图像生成"><a href="#带约束的图像生成" class="headerlink" title="带约束的图像生成"></a>带约束的图像生成</h2><p>在生成新图像时，我们更希望模型能够根据我们的需求生成图像。比如，我们希望模型生成「一幅优美的风景画」，又或者希望模型在一幅草图的基础上作画。这些需求就是模型的约束。为了实现带约束的图像生成，一般的做法是先有一个无约束（输入是随机数）的图像生成模型，再在这个模型的基础上把一个表示约束的向量插入进图像生成的某一步。</p>
<p>把约束向量插入进模型的方法是需要设计的，插入约束向量的方法往往和模型架构有着密切关系。比如假设一个生成模型是U-Net架构，我们可以把约束向量和当前特征图拼接在一起，输入进U-Net的每一大层。</p>
<p>为了实现带约束的图像生成，VQGAN的作者再次借鉴了Transformer实现带约束文字生成的方法。许多自然语言处理任务都可以看成是带约束的文字生成。比如机器翻译，其实可以看成在给定一种语言的句子的前提下，让模型「随机」生成一个另一种语言的句子。比如要把「简要访问非洲」翻译成英语，我们可以对之前无约束文字生成的Transformer做一些修改。</p>
<p><img src="/2023/06/19/20230605-VQGAN/8.jpg" alt></p>
<p>也就是说，给定约束的句子$c$，在第$i$步，Transformer会根据前$i-1$个输出单词$s_{ &lt; i}$以及$c$生成第$i$个单词$s_i$。表示约束的单词被添加到了所有输出之前，作为这次「随机生成」的额外输入。</p>
<blockquote>
<p>上述方法并不是唯一的文字生成方法。这种文字生成方法被称为”decoder-only”。实际上，也有使用一个编码器来额外维护约束信息的文字生成方法。最早的Transformer就用到了带编码器的方法。</p>
</blockquote>
<p>我们同样可以把这种思想搬到压缩图像生成里。比如对于MNIST数据集，我们希望模型只生成0~9这些数字中某一个数字的手写图像。也就是说，约束是类别信息，约束的取值是0~9。我们就可以把这个0~9的约束信息添加到Transformer的输入$s_{ &lt; i}$之前，以实现由类别约束的图像生成。</p>
<p><img src="/2023/06/19/20230605-VQGAN/9.jpg" alt></p>
<p>但这种设计又会产生一个新的问题。假设约束条件不能简单地表示成整数，而是一些其他类型的数据，比如语义分割图像，那该怎么办呢？对于这种以图像形式表示的约束，作者的做法是，再训练另一个VQGAN，把约束图像压缩成另一套压缩图片。这一套压缩图片和生成图像的压缩图片有着不同的codebook，就像两种语言有着不同的单词一样。这样，约束图像也变成了一系列的整数，可以用之前的方法进行带约束图像生成了。</p>
<p><img src="/2023/06/19/20230605-VQGAN/10.jpg" alt></p>
<h2 id="生成高清图像"><a href="#生成高清图像" class="headerlink" title="生成高清图像"></a>生成高清图像</h2><p>由于Transformer注意力计算的开销很大，作者在所有配置中都只使用了$16 \times 16$的压缩图像，再增大压缩图像尺寸的话计算资源就不够了。而另一方面，每张图像在VQGAN中的压缩比例是有限的。如果图像压缩得过多，则VQGAN的重建质量就不够好了。因此，设边长压缩了$f$倍，则该方法一次能生成的图片的最大尺寸是$16f \times 16f$。在多项实验中，$f=16$的表现都较好。这样算下来，该方法一次只能生成$256 \times 256$的图片。这种尺寸的图片还称不上高清图片。</p>
<p>为了生成更大尺寸的图片，作者先训练好了一套能生成$256 \times 256$的图片的VQGAN+Transformer，再用了一种基于滑动窗口的采样机制来生成大图片。具体来说，作者把待生成图片划分成若干个$16\times16$像素的图块，每个图块对应压缩图像的一个像素。之后，在每一轮生成时，只有待生成图块周围的$16\times16$个图块（$256\times256$个像素）会被输入进VQGAN和Transformer，由Transformer生成一个新的压缩图像像素，再把该压缩图像像素解码成图块。(在下面的示意图中，每个方块是一个图块，transformer的输入是$3\times3$个图块)</p>
<p><img src="/2023/06/19/20230605-VQGAN/11.jpg" alt></p>
<p>这个滑动窗口算法不是那么好理解，需要多想一下才能理解它的具体做法。在理解这个算法时，你可能会有这样的问题：上面的示意图中，待生成像素有的时候在最左边，有的时候在中间，有的时候在右边，每次约束它的像素都不一样。这么复杂的约束逻辑怎么编写？其实，Transformer自动保证了每个像素只会由之前的像素约束，而看不到后面的像素。因此，在实现时，只需要把待生成像素框起来，直接用Transformer预测待生成像素即可，不需要编写额外的约束逻辑。</p>
<blockquote>
<p>如果你没有学过Transformer的话，理解这部分会有点困难。Transformer可以根据第1~k-1个像素并行地生成第2~k个像素，且保证生成每个像素时不会偷看到后面像素的信息。因此，假设我们要生成第i个像素，其实是预测了所有第2~k个像素的结果，再取出第i个结果，填回待生成图像。</p>
</blockquote>
<p>由于论文篇幅有限，作者没有对滑动窗口机制做过多的介绍，也没有讲带约束的滑动窗口是怎么实现的。如果你在理解这一部分时碰到了问题，不用担心，这很正常。稍后我们会在代码阅读章节彻底理解滑动窗口的实现方法。我也是看了代码才看懂此处的做法。</p>
<p>作者在论文中解释了为什么用滑动窗口生成高清图像是合理的。作者先是讨论了两种情况，只要满足这两种情况中的任意一种，拿滑动窗口生成图像就是合理的。第一种情况是数据集的统计规律是几乎空间不变，也就是说训练集图片每$256\times256$个像素的统计规律是类似的。这和我们拿$3\times3$卷积卷图像是因为图像每$3\times3$个像素的统计规律类似的原理是一样的。第二种情况是有空间上的约束信息。比如之前提到的用语义分割图来指导图像生成。由于语义分割也是一张图片，它给每个待生成像素都提供了额外信息。这样，哪怕是用滑动窗口，在局部语义的指导下，模型也足以生成图像了。</p>
<p>若是两种情况都不满足呢？比如在对齐的人脸数据集上做无约束生成。在对齐的人脸数据集里，每张图片中人的五官所在的坐标是差不多的，图片的空间不变性不满足；做无约束生成，自然也没有额外的空间信息。在这种情况下，我们可以人为地添加一个坐标约束，即从左到右、从上到下地给每个像素标一个序号，把每个滑动窗口里的坐标序号做为约束。有了坐标约束后，就还原成了上面的第二种情况，每个像素有了额外的空间信息，基于滑动窗口的方法依然可行。</p>
<p><img src="/2023/06/19/20230605-VQGAN/12.jpg" alt></p>
<p>学完了论文的四大知识点，我们知道VQGAN是怎么根据约束生成高清图像的了。接下来，我们来看看论文的实验部分，看看作者是怎么证明方法的有效性的。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在实验部分，作者先是分别验证了基于Transformer的压缩图像生成模型较以往模型的优越性（4.1节）、VQGAN较以往模型的优越性（4.4节末尾）、使用VQGAN做图像压缩的必要性及相关消融实验（4.3节），再把整个生成方法综合起来，在多项图像生成任务上与以往的图像生成模型做定量对比（4.4节），最后展示了该方法惊艳的带约束生成效果（4.2节）。</p>
<p>在论文4.1节中，作者验证了基于Transformer的压缩图像生成模型的有效性。之前，压缩图像都是使用能输出离散分布的PixelCNN系列模型来生成的。PixelCNN系列的最强模型是PixelSNAIL。为确保公平，作者对比了相同训练时间、相同训练步数下两个网络在不同训练集下的负对数似然（NLL）指标。结果表明，基于Transformer的模型确实训练得更快。</p>
<blockquote>
<p>对于直接能建模离散分布的模型来说，NLL就是交叉熵损失函数。</p>
</blockquote>
<p><img src="/2023/06/19/20230605-VQGAN/13.jpg" alt></p>
<p>在论文4.4节末尾，作者将VQGAN和之前的图像压缩模型对比，验证了引入感知误差和GAN结构的有效性。作者汇报了各模型重建图像集与原数据集（ImageNet的训练集和验证集）的FID（指标FID是越低越好）。同时，结果也说明，增大codebook的尺寸或者编码种类都能提升重建效果。</p>
<p><img src="/2023/06/19/20230605-VQGAN/19.jpg" alt></p>
<p>在论文4.3节中，作者验证了使用VQGAN的必要性。作者训了两个模型，一个直接让Transformer做真实图像生成，一个用VQGAN把图像边长压缩2倍，再用Transformer生成压缩图像。经比较，使用了VQGAN后，图像生成速度快了10多倍，且图像生成效果也有所提升。</p>
<p>另外，作者还做了有关图像边长压缩比例$f$的消融实验。作者固定让Transformer生成$16 \times 16$的压缩图片，即每次训练时用到的图像尺寸都是$16f \times 16f$。之后，作者训练训练了不同$f$下的模型，用各个模型来生成图片。结果显示$f=16$时效果最好。这是因为，在固定Transformer的生成分辨率的前提下，$f$越小，Transformer的感受野越小。如果Transformer的感受野过小，就学习不到足够的信息。</p>
<p><img src="/2023/06/19/20230605-VQGAN/16.jpg" alt></p>
<p>在论文4.4节中，作者探究了VQGAN+Transformer在多项基准测试（benchmark）上的结果。</p>
<p>首先是语义图像合成（根据语义分割图像来生成）任务。本文的这套方法还不错。</p>
<p><img src="/2023/06/19/20230605-VQGAN/17.jpg" alt></p>
<p>接着是人脸生成任务。这套方法表现还行，但还是比不过专精于某一任务的GAN。</p>
<p><img src="/2023/06/19/20230605-VQGAN/18.jpg" alt></p>
<p>作者还比较了各模型在ImageNet上的生成结果。这一比较的数据量较多，欢迎大家自行阅读原论文。</p>
<p>在论文4.2节中，作者展示了多才多艺的VQGAN+Transformer在各种约束下的图像生成结果。这些图像都是按照默认配置生成的，大小为$256\times256$。</p>
<p><img src="/2023/06/19/20230605-VQGAN/14.jpg" alt></p>
<p>作者还展示了使用了滑动窗口算法后，模型生成的不同分辨率的图像。</p>
<p><img src="/2023/06/19/20230605-VQGAN/15.jpg" alt></p>
<p>本文开头的那张高清图片也来自论文。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>VQGAN是一个改进版的VQVAE，它将感知误差和GAN引入了图像压缩模型，把压缩图像生成模型替换成了更强大的Transformer。相比纯种的GAN（如StyleGAN），VQGAN的强大之处在于它支持带约束的高清图像生成。VQGAN借助NLP中”decoder-only”策略实现了带约束图像生成，并使用滑动窗口机制实现了高清图像生成。虽然在某些特定任务上VQGAN还是落后于其他GAN，但VQGAN的泛化性和灵活性都要比纯种GAN要强。它的这些潜力直接促成了Stable Diffusion的诞生。</p>
<p>如果你是读完了VQVAE再来读的VQGAN，为了完全理解VQGAN，你只需要掌握本文提到的4个知识点：VQVAE到VQGAN的改进方法、使用Transformer做图像生成的方法、使用”decoder-only”策略做带约束图像生成的方法、用滑动滑动窗口生成任意尺寸的图片的思想。</p>
<h2 id="代码阅读"><a href="#代码阅读" class="headerlink" title="代码阅读"></a>代码阅读</h2><p>在代码阅读章节中，我将先简略介绍官方源码的项目结构以方便大家学习，再介绍代码中的几处核心代码。具体来说，我会介绍模型是如何组织配置文件的、模型的定义代码在哪、训练代码在哪、采样代码在哪，同时我会主要分析VQGAN的结构、Transformer的结构、损失函数、滑动窗口采样算法这几部分的代码。</p>
<p>官方源码地址：<a target="_blank" rel="noopener" href="https://github.com/CompVis/taming-transformers。">https://github.com/CompVis/taming-transformers。</a></p>
<p>官方的Git仓库里有很多很大的图片，且git记录里还藏了一些很大的数据，整个Git仓库非常大。如果你的网络不好，建议以zip形式下载仓库，或者只把代码部分下载下来。</p>
<h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">├─assets</span><br><span class="line">├─configs</span><br><span class="line">├─scripts</span><br><span class="line">└─taming</span><br><span class="line">    ├─data</span><br><span class="line">    │  └─conditional_builder</span><br><span class="line">    ├─models</span><br><span class="line">    └─modules</span><br><span class="line">        ├─diffusionmodules</span><br><span class="line">        ├─discriminator</span><br><span class="line">        ├─losses</span><br><span class="line">        ├─misc</span><br><span class="line">        ├─transformer</span><br><span class="line">        └─vqvae</span><br></pre></td></tr></table></figure>
<p><code>configs</code>目录下存放的是模型配置文件。VQGAN和Transformer的模型配置是分开来放的。每个模型配置文件都会指向一个Python模型类，比如<code>taming.models.vqgan.VQModel</code>，配置里的参数就是模型类的初始化参数。我们可用通过阅读配置文件找到模型的定义位置。</p>
<p>运行脚本包括根目录下的<code>main.py</code>和<code>scripts</code>文件夹下的脚本。<code>main.py</code>是用于训练的。<code>scripts</code>文件夹下有各种采样脚本和数据集可视化脚本。</p>
<p><code>taming</code>是源代码的主目录。其<code>data</code>子文件夹下放置了各数据集的预处理代码，<code>models</code>放置了VQGAN和Transformer PyTorch模型的定义代码，<code>modules</code>则放置了模型中用到的模块，主要包括VQGAN编码解码模块（<code>diffusionmodules</code>）、判别器模块（<code>discriminator</code>）、误差模块（<code>losses</code>）、Transformer模块（<code>transformer</code>）、codebook模块（<code>vqvae</code>）。</p>
<h2 id="VQGAN-模型结构"><a href="#VQGAN-模型结构" class="headerlink" title="VQGAN 模型结构"></a>VQGAN 模型结构</h2><p>打开<code>configs\faceshq_vqgan.yaml</code>，我们能够找到高清人脸生成任务使用的VQGAN模型配置。我们来学习一下这个模型的定义方法。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">base_learning_rate:</span> <span class="number">4.5e-6</span></span><br><span class="line">  <span class="attr">target:</span> <span class="string">taming.models.vqgan.VQModel</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">embed_dim:</span> <span class="number">256</span></span><br><span class="line">    <span class="attr">n_embed:</span> <span class="number">1024</span></span><br><span class="line">    <span class="attr">ddconfig:</span></span><br><span class="line">      <span class="string">...</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">lossconfig:</span></span><br><span class="line">      <span class="attr">target:</span> <span class="string">taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator</span></span><br><span class="line">      <span class="attr">params:</span></span><br><span class="line">        <span class="string">...</span></span><br></pre></td></tr></table></figure>
<p>从配置文件的<code>target</code>字段中，我们知道VQGAN定义在模块<code>taming.models.vqgan.VQModel</code>中。我们可以打开<code>taming\models\vqgan.py</code>这个文件，查看其中<code>VQModel</code>类的代码。</p>
<p>首先先看一下初始化函数。初始化函数主要是初始化了<code>encoder</code>、<code>decoder</code>、<code>loss</code>、<code>quantize</code>这几个模块，我们可以从文件开头的import语句中找到这几个模块的定义位置。不过，先不急，我们来继续看一下模型的前向传播函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> taming.modules.diffusionmodules.model <span class="keyword">import</span> Encoder, Decoder</span><br><span class="line"><span class="keyword">from</span> taming.modules.vqvae.quantize <span class="keyword">import</span> VectorQuantizer2 <span class="keyword">as</span> VectorQuantizer</span><br><span class="line"><span class="keyword">from</span> taming.modules.vqvae.quantize <span class="keyword">import</span> GumbelQuantize</span><br><span class="line"><span class="keyword">from</span> taming.modules.vqvae.quantize <span class="keyword">import</span> EMAVectorQuantizer</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VQModel</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ddconfig,</span></span></span><br><span class="line"><span class="params"><span class="function">                 lossconfig,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_embed,</span></span></span><br><span class="line"><span class="params"><span class="function">                 embed_dim,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ckpt_path=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ignore_keys=[],</span></span></span><br><span class="line"><span class="params"><span class="function">                 image_key=<span class="string">&quot;image&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 colorize_nlabels=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 monitor=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 remap=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 sane_index_shape=<span class="literal">False</span>,  <span class="comment"># tell vector quantizer to return indices as bhw</span></span></span></span><br><span class="line"><span class="params"><span class="function">                 </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.image_key = image_key</span><br><span class="line">        self.encoder = Encoder(**ddconfig)</span><br><span class="line">        self.decoder = Decoder(**ddconfig)</span><br><span class="line">        self.loss = instantiate_from_config(lossconfig)</span><br><span class="line">        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=<span class="number">0.25</span>,</span><br><span class="line">                                        remap=remap, sane_index_shape=sane_index_shape)</span><br><span class="line">        self.quant_conv = torch.nn.Conv2d(ddconfig[<span class="string">&quot;z_channels&quot;</span>], embed_dim, <span class="number">1</span>)</span><br><span class="line">        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[<span class="string">&quot;z_channels&quot;</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)</span><br><span class="line">        self.image_key = image_key</span><br><span class="line">        <span class="keyword">if</span> colorize_nlabels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">type</span>(colorize_nlabels)==<span class="built_in">int</span></span><br><span class="line">            self.register_buffer(<span class="string">&quot;colorize&quot;</span>, torch.randn(<span class="number">3</span>, colorize_nlabels, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> monitor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.monitor = monitor</span><br></pre></td></tr></table></figure>
<p>模型的前向传播逻辑非常清晰。<code>self.encoder</code>可以把一张图片变为特征，<code>self.decoder</code>可以把特征变回图片。<code>self.quant_conv</code>和<code>post_quant_conv</code>则分别完成了编码器到codebook、codebook到解码器的通道数转换。<code>self.quantize</code>实现了VQVAE和VQGAN中那个找codebook里的最近邻、替换成最近邻的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    h = self.encoder(x)</span><br><span class="line">    h = self.quant_conv(h)</span><br><span class="line">    quant, emb_loss, info = self.quantize(h)</span><br><span class="line">    <span class="keyword">return</span> quant, emb_loss, info</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, quant</span>):</span></span><br><span class="line">    quant = self.post_quant_conv(quant)</span><br><span class="line">    dec = self.decoder(quant)</span><br><span class="line">    <span class="keyword">return</span> dec</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">    quant, diff, _ = self.encode(<span class="built_in">input</span>)</span><br><span class="line">    dec = self.decode(quant)</span><br><span class="line">    <span class="keyword">return</span> dec, diff</span><br></pre></td></tr></table></figure>
<p>接下来，我们再看一看VQGAN的各个模块的定义。编码器和解码器的定义都可以在<code>taming\modules\diffusionmodules\model.py</code>里找到。VQGAN使用的编码器和解码器基于DDPM论文中的U-Net架构（而此架构又可以追溯到PixelCNN++的模型架构）。相比于最经典的U-Net，此U-Net每一层由若干个残差块和若干个自注意力块构成。为了把这个U-Net用到VQGAN里，U-Net的下采样部分和上采样部分被拆开，分别做成了VQGAN的编码器和解码器。</p>
<p>此处代码过长，我就只贴出部分关键代码了。以下是编码器的<code>__init__</code>函数和<code>forward</code>函数的关键代码。<code>self.down</code>存储了U-Net各层的模块。对于第i层，<code>down[i].block</code>是所有残差块，<code>down[i].attn</code>是所有自注意力块，<code>down[i].downsample</code>是下采样操作。它们在<code>forward</code>里会被依次调用。解码器的结构与之类似，只不过下采样变成了上采样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *, ch, out_ch, ch_mult=(<span class="params"><span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span></span>), num_res_blocks,</span></span></span><br><span class="line"><span class="params"><span class="function">                 attn_resolutions, dropout=<span class="number">0.0</span>, resamp_with_conv=<span class="literal">True</span>, in_channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                 resolution, z_channels, double_z=<span class="literal">True</span>, **ignore_kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line">        self.down = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(self.num_resolutions):</span><br><span class="line">            block = nn.ModuleList()</span><br><span class="line">            attn = nn.ModuleList()</span><br><span class="line">            block_in = ch*in_ch_mult[i_level]</span><br><span class="line">            block_out = ch*ch_mult[i_level]</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(self.num_res_blocks):</span><br><span class="line">                block.append(ResnetBlock(in_channels=block_in,</span><br><span class="line">                                         out_channels=block_out,</span><br><span class="line">                                         temb_channels=self.temb_ch,</span><br><span class="line">                                         dropout=dropout))</span><br><span class="line">                block_in = block_out</span><br><span class="line">                <span class="keyword">if</span> curr_res <span class="keyword">in</span> attn_resolutions:</span><br><span class="line">                    attn.append(AttnBlock(block_in))</span><br><span class="line">            down = nn.Module()</span><br><span class="line">            down.block = block</span><br><span class="line">            down.attn = attn</span><br><span class="line">            <span class="keyword">if</span> i_level != self.num_resolutions-<span class="number">1</span>:</span><br><span class="line">                down.downsample = Downsample(block_in, resamp_with_conv)</span><br><span class="line">                curr_res = curr_res // <span class="number">2</span></span><br><span class="line">            self.down.append(down)</span><br><span class="line"></span><br><span class="line">       ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        hs = [self.conv_in(x)]</span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(self.num_resolutions):</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(self.num_res_blocks):</span><br><span class="line">                h = self.down[i_level].block[i_block](hs[-<span class="number">1</span>], temb)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(self.down[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                    h = self.down[i_level].attn[i_block](h)</span><br><span class="line">                hs.append(h)</span><br><span class="line">            <span class="keyword">if</span> i_level != self.num_resolutions-<span class="number">1</span>:</span><br><span class="line">                hs.append(self.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure>
<p>之后，我们再看看离散化层的代码，即把编码器的输出变成codebook里的嵌入的实现代码。作者在<code>taming\modules\vqvae\quantize.py</code>中提供了VQVAE原版的离散化操作以及若干个改进过的离散化操作。我们就来看一下原版的离散化模块<code>VectorQuantizer</code>是怎么实现的。</p>
<p>离散化模块的初始化非常简洁，主要是初始化了一个嵌入层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VectorQuantizer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_e, e_dim, beta</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VectorQuantizer, self).__init__()</span><br><span class="line">        self.n_e = n_e</span><br><span class="line">        self.e_dim = e_dim</span><br><span class="line">        self.beta = beta</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.n_e, self.e_dim)</span><br><span class="line">        self.embedding.weight.data.uniform_(-<span class="number">1.0</span> / self.n_e, <span class="number">1.0</span> / self.n_e)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在前向传播时，作者先是算出了编码器输出<code>z</code>和所有嵌入的距离<code>d</code>，再用<code>argmin</code>算出了最近邻嵌入的下标<code>min_encodings</code>，最后根据下标取出解码器输入<code>z_q</code>。同时，该函数还计算了其他几个可能用到的量，比如和codebook有关的误差 <code>loss</code>。注意，在计算<code>loss</code>和<code>z_q</code>时，作者都使用到了停止梯度算子（<code>.detach()</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, z</span>):</span></span><br><span class="line">    z = z.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">    z_flattened = z.view(-<span class="number">1</span>, self.e_dim)</span><br><span class="line">    <span class="comment"># distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z</span></span><br><span class="line"></span><br><span class="line">    d = torch.<span class="built_in">sum</span>(z_flattened ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + \</span><br><span class="line">        torch.<span class="built_in">sum</span>(self.embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">2</span> * \</span><br><span class="line">        torch.matmul(z_flattened, self.embedding.weight.t())</span><br><span class="line"></span><br><span class="line">    <span class="comment">## could possible replace this here</span></span><br><span class="line">    <span class="comment"># #\start...</span></span><br><span class="line">    <span class="comment"># find closest encodings</span></span><br><span class="line">    min_encoding_indices = torch.argmin(d, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    min_encodings = torch.zeros(</span><br><span class="line">        min_encoding_indices.shape[<span class="number">0</span>], self.n_e).to(z)</span><br><span class="line">    min_encodings.scatter_(<span class="number">1</span>, min_encoding_indices, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)</span><br><span class="line">    <span class="comment">#.........\end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss for embedding</span></span><br><span class="line">    loss = torch.mean((z_q.detach()-z)**<span class="number">2</span>) + self.beta * \</span><br><span class="line">        torch.mean((z_q - z.detach()) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># preserve gradients</span></span><br><span class="line">    z_q = z + (z_q - z).detach()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perplexity</span></span><br><span class="line">    e_mean = torch.mean(min_encodings, dim=<span class="number">0</span>)</span><br><span class="line">    perplexity = torch.exp(-torch.<span class="built_in">sum</span>(e_mean * torch.log(e_mean + <span class="number">1e-10</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape back to match original input shape</span></span><br><span class="line">    z_q = z_q.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> z_q, loss, (perplexity, min_encodings, min_encoding_indices)</span><br></pre></td></tr></table></figure>
<p>VQGAN的三个主要模块已经看完了。最后，我们来看一下误差的定义。误差的定义在<code>taming\modules\losses\vqperceptual.py</code>的<code>VQLPIPSWithDiscriminator</code>类里。误差类名里的LPIPS（Learned Perceptual Image Patch Similarity，学习感知图像块相似度）就是感知误差的全称，”WithDiscriminator”表示误差是带了判定器误差的。我们来把这两类误差分别看一下。</p>
<p>说实话，这个误差模块乱得一塌糊涂，一边自己在算误差，一边又维护了codebook误差和重建误差的权重，最后会把自己维护的两个误差和其他误差合在一起输出。功能全部耦合在一起。我们就跳过这个类的实现细节，主要关注<code>self.perceptual_loss</code>和<code>self.discriminator</code>是怎么调用其他模块的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> taming.modules.losses.lpips <span class="keyword">import</span> LPIPS</span><br><span class="line"><span class="keyword">from</span> taming.modules.discriminator.model <span class="keyword">import</span> NLayerDiscriminator, weights_init</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VQLPIPSWithDiscriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.perceptual_loss = LPIPS().<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">        self.discriminator = NLayerDiscriminator...</span><br></pre></td></tr></table></figure>
<p>感知误差模块在<code>taming\modules\losses\vqperceptual.py</code>文件里。这个文件来自GitHub项目 PerceptualSimilarity。</p>
<p>感知误差可以简单地理解为两张图片在VGG中几个卷积层输出的误差的加权和。加权的权重是可以学习的。作者使用的是已经学习好的感知误差。感知误差的初始化函数如下。其中，<code>self.lin0</code>等模块就是算权重的模块，<code>self.net</code>是VGG。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LPIPS</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># Learned perceptual metric</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, use_dropout=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.scaling_layer = ScalingLayer()</span><br><span class="line">        self.chns = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">512</span>]  <span class="comment"># vg16 features</span></span><br><span class="line">        self.net = vgg16(pretrained=<span class="literal">True</span>, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.lin0 = NetLinLayer(self.chns[<span class="number">0</span>], use_dropout=use_dropout)</span><br><span class="line">        self.lin1 = NetLinLayer(self.chns[<span class="number">1</span>], use_dropout=use_dropout)</span><br><span class="line">        self.lin2 = NetLinLayer(self.chns[<span class="number">2</span>], use_dropout=use_dropout)</span><br><span class="line">        self.lin3 = NetLinLayer(self.chns[<span class="number">3</span>], use_dropout=use_dropout)</span><br><span class="line">        self.lin4 = NetLinLayer(self.chns[<span class="number">4</span>], use_dropout=use_dropout)</span><br><span class="line">        self.load_from_pretrained()</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>在算误差时，先是把图像<code>input</code>和<code>target</code>都输入进VGG，获取各层输出<code>outs0, outs1</code>，再求出两个图像的输出的均方误差<code>diffs</code>，最后用<code>lins</code>给各层误差加权，求和。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, target</span>):</span></span><br><span class="line">    in0_input, in1_input = (self.scaling_layer(<span class="built_in">input</span>), self.scaling_layer(target))</span><br><span class="line">    outs0, outs1 = self.net(in0_input), self.net(in1_input)</span><br><span class="line">    feats0, feats1, diffs = &#123;&#125;, &#123;&#125;, &#123;&#125;</span><br><span class="line">    lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]</span><br><span class="line">    <span class="keyword">for</span> kk <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.chns)):</span><br><span class="line">        feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])</span><br><span class="line">        diffs[kk] = (feats0[kk] - feats1[kk]) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    res = [spatial_average(lins[kk].model(diffs[kk]), keepdim=<span class="literal">True</span>) <span class="keyword">for</span> kk <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.chns))]</span><br><span class="line">    val = res[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.chns)):</span><br><span class="line">        val += res[l]</span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure>
<p>GAN的判别器写在<code>taming\modules\discriminator\model.py</code>文件里。这个文件来自GitHub上的 pytorch-CycleGAN-and-pix2pix 项目。这个判别器非常简单，就是一个全卷积网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLayerDiscriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Defines a PatchGAN discriminator as in Pix2Pix</span></span><br><span class="line"><span class="string">        --&gt; see https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_nc=<span class="number">3</span>, ndf=<span class="number">64</span>, n_layers=<span class="number">3</span>, use_actnorm=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Construct a PatchGAN discriminator</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_nc (int)  -- the number of channels in input images</span></span><br><span class="line"><span class="string">            ndf (int)       -- the number of filters in the last conv layer</span></span><br><span class="line"><span class="string">            n_layers (int)  -- the number of conv layers in the discriminator</span></span><br><span class="line"><span class="string">            norm_layer      -- normalization layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(NLayerDiscriminator, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> use_actnorm:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            norm_layer = ActNorm</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(norm_layer) == functools.partial:  <span class="comment"># no need to use bias as BatchNorm2d has affine parameters</span></span><br><span class="line">            use_bias = norm_layer.func != nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer != nn.BatchNorm2d</span><br><span class="line"></span><br><span class="line">        kw = <span class="number">4</span></span><br><span class="line">        padw = <span class="number">1</span></span><br><span class="line">        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=<span class="number">2</span>, padding=padw), nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>)]</span><br><span class="line">        nf_mult = <span class="number">1</span></span><br><span class="line">        nf_mult_prev = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_layers):  <span class="comment"># gradually increase the number of filters</span></span><br><span class="line">            nf_mult_prev = nf_mult</span><br><span class="line">            nf_mult = <span class="built_in">min</span>(<span class="number">2</span> ** n, <span class="number">8</span>)</span><br><span class="line">            sequence += [</span><br><span class="line">                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=<span class="number">2</span>, padding=padw, bias=use_bias),</span><br><span class="line">                norm_layer(ndf * nf_mult),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>)</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line">        nf_mult_prev = nf_mult</span><br><span class="line">        nf_mult = <span class="built_in">min</span>(<span class="number">2</span> ** n_layers, <span class="number">8</span>)</span><br><span class="line">        sequence += [</span><br><span class="line">            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=<span class="number">1</span>, padding=padw, bias=use_bias),</span><br><span class="line">            norm_layer(ndf * nf_mult),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        sequence += [</span><br><span class="line">            nn.Conv2d(ndf * nf_mult, <span class="number">1</span>, kernel_size=kw, stride=<span class="number">1</span>, padding=padw)]  <span class="comment"># output 1 channel prediction map</span></span><br><span class="line">        self.main = nn.Sequential(*sequence)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Standard forward.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.main(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Transformer-模型结构"><a href="#Transformer-模型结构" class="headerlink" title="Transformer 模型结构"></a>Transformer 模型结构</h2><p>此方法使用的Transformer是GPT2。我们先看一下该项目封装Transformer的模型类<code>taming.models.cond_transformer.Net2NetTransformer</code>，再稍微看一下GPT类<code>taming.modules.transformer.mingpt.GPT</code>的具体实现。</p>
<p><code>Net2NetTransformer</code>主要是实现了论文中提到的带约束生成。它会把输入<code>x</code>和约束<code>c</code>分别用一个VQGAN转成压缩图像，把图像压扁成一维，再调用GPT。我们来看一下这个类的主要内容。</p>
<p>初始化函数主要是初始化了输入图像的VQGAN <code>self.first_stage_model</code>、约束图像的VQGAN <code>self.cond_stage_model</code>、Transformer <code>self.transformer</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net2NetTransformer</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 transformer_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cond_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 permuter_config=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ckpt_path=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ignore_keys=[],</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_stage_key=<span class="string">&quot;image&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cond_stage_key=<span class="string">&quot;depth&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 downsample_cond_size=-<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pkeep=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 sos_token=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 unconditional=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.be_unconditional = unconditional</span><br><span class="line">        self.sos_token = sos_token</span><br><span class="line">        self.first_stage_key = first_stage_key</span><br><span class="line">        self.cond_stage_key = cond_stage_key</span><br><span class="line">        self.init_first_stage_from_ckpt(first_stage_config)</span><br><span class="line">        self.init_cond_stage_from_ckpt(cond_stage_config)</span><br><span class="line">        <span class="keyword">if</span> permuter_config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            permuter_config = &#123;<span class="string">&quot;target&quot;</span>: <span class="string">&quot;taming.modules.transformer.permuter.Identity&quot;</span>&#125;</span><br><span class="line">        self.permuter = instantiate_from_config(config=permuter_config)</span><br><span class="line">        self.transformer = instantiate_from_config(config=transformer_config)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)</span><br><span class="line">        self.downsample_cond_size = downsample_cond_size</span><br><span class="line">        self.pkeep = pkeep</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_first_stage_from_ckpt</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        model = instantiate_from_config(config)</span><br><span class="line">        model = model.<span class="built_in">eval</span>()</span><br><span class="line">        model.train = disabled_train</span><br><span class="line">        self.first_stage_model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_cond_stage_from_ckpt</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.cond_stage_model = ...</span><br></pre></td></tr></table></figure>
<p>模型的前向传播函数如下。一开始，函数调用<code>encode_to_z</code>和<code>encode_to_c</code>，根据<code>self.cond_stage_model</code>和<code>self.first_stage_model</code>把约束图像和输入图像编码成压扁至一维的压缩图像。之后函数做了一个类似Dropout的操作，根据<code>self.pkeep</code>随机替换掉约束编码。最后，函数把约束编码和输入编码拼接起来，使用通常方法调用Transformer。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, c</span>):</span></span><br><span class="line">    <span class="comment"># one step to produce the logits</span></span><br><span class="line">    _, z_indices = self.encode_to_z(x)</span><br><span class="line">    _, c_indices = self.encode_to_c(c)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">and</span> self.pkeep &lt; <span class="number">1.0</span>:</span><br><span class="line">        mask = torch.bernoulli(self.pkeep*torch.ones(z_indices.shape,</span><br><span class="line">                                                      device=z_indices.device))</span><br><span class="line">        mask = mask.<span class="built_in">round</span>().to(dtype=torch.int64)</span><br><span class="line">        r_indices = torch.randint_like(z_indices, self.transformer.config.vocab_size)</span><br><span class="line">        a_indices = mask*z_indices+(<span class="number">1</span>-mask)*r_indices</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        a_indices = z_indices</span><br><span class="line"></span><br><span class="line">    cz_indices = torch.cat((c_indices, a_indices), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># target includes all sequence elements (no need to handle first one</span></span><br><span class="line">    <span class="comment"># differently because we are conditioning)</span></span><br><span class="line">    target = z_indices</span><br><span class="line">    <span class="comment"># make the prediction</span></span><br><span class="line">    logits, _ = self.transformer(cz_indices[:, :-<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># cut off conditioning outputs - output i corresponds to p(z_i | z_&#123;&lt;i&#125;, c)</span></span><br><span class="line">    logits = logits[:, c_indices.shape[<span class="number">1</span>]-<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits, target</span><br></pre></td></tr></table></figure>
<p>GPT2的结构不是本文的重点，我们就快速把模型结构过一遍了。GPT2的模型定义在<code>taming.modules.transformer.mingpt.GPT</code>里。GPT2的结构并不复杂，就是一个只有解码器的Transformer。前向传播时，数据先通过嵌入层<code>self.tok_emb</code>，再经过若干个Transformer模块<code>self.blocks</code>，最后过一个LayerNorm层<code>self.ln_f</code>和线性层<code>self.head</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GPT</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, idx, embeddings=<span class="literal">None</span>, targets=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># forward the GPT model</span></span><br><span class="line">        token_embeddings = self.tok_emb(idx) <span class="comment"># each index maps to a (learnable) vector</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> embeddings <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># prepend explicit embeddings</span></span><br><span class="line">            token_embeddings = torch.cat((embeddings, token_embeddings), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        t = token_embeddings.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">assert</span> t &lt;= self.block_size, <span class="string">&quot;Cannot forward, model block size is exhausted.&quot;</span></span><br><span class="line">        position_embeddings = self.pos_emb[:, :t, :] <span class="comment"># each position maps to a (learnable) vector</span></span><br><span class="line">        x = self.drop(token_embeddings + position_embeddings)</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.ln_f(x)</span><br><span class="line">        logits = self.head(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if we are given some desired targets also calculate the loss</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = F.cross_entropy(logits.view(-<span class="number">1</span>, logits.size(-<span class="number">1</span>)), targets.view(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits, loss</span><br></pre></td></tr></table></figure>
<p>每个Transformer块就是非常经典的自注意力加全连接层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; an unassuming Transformer block &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ln1 = nn.LayerNorm(config.n_embd)</span><br><span class="line">        self.ln2 = nn.LayerNorm(config.n_embd)</span><br><span class="line">        self.attn = CausalSelfAttention(config)</span><br><span class="line">        self.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(config.n_embd, <span class="number">4</span> * config.n_embd),</span><br><span class="line">            nn.GELU(),  <span class="comment"># nice</span></span><br><span class="line">            nn.Linear(<span class="number">4</span> * config.n_embd, config.n_embd),</span><br><span class="line">            nn.Dropout(config.resid_pdrop),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, layer_past=<span class="literal">None</span>, return_present=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> check that training still works</span></span><br><span class="line">        <span class="keyword">if</span> return_present: <span class="keyword">assert</span> <span class="keyword">not</span> self.training</span><br><span class="line">        <span class="comment"># layer past: tuple of length two with B, nh, T, hs</span></span><br><span class="line">        attn, present = self.attn(self.ln1(x), layer_past=layer_past)</span><br><span class="line"></span><br><span class="line">        x = x + attn</span><br><span class="line">        x = x + self.mlp(self.ln2(x))</span><br><span class="line">        <span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">or</span> return_present:</span><br><span class="line">            <span class="keyword">return</span> x, present</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="基于滑动窗口的带约束图像生成"><a href="#基于滑动窗口的带约束图像生成" class="headerlink" title="基于滑动窗口的带约束图像生成"></a>基于滑动窗口的带约束图像生成</h2><p>看完了所有模型的结构，我们最后来学习一下论文中没能详细介绍的滑动窗口算法。在<code>scripts\taming-transformers.ipynb</code>里有一个采样算法的最简实现，我们就来学习一下这份代码。</p>
<p>这份代码可以根据一幅语义分割图像来生成高清图像。一开始，代码会读入模型和语义分割图像。大致的代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> taming.models.cond_transformer <span class="keyword">import</span> Net2NetTransformer</span><br><span class="line">model = Net2NetTransformer(**config.model.params)</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">segmentation_path = <span class="string">&quot;data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png&quot;</span></span><br><span class="line">segmentation = Image.<span class="built_in">open</span>(segmentation_path)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/19/20230605-VQGAN/20.jpg" alt><br>之后，代码把约束图像用对应的VQGAN编码进压缩空间，得到<code>c_indices</code>。由于待生成图像为空，我们可以随便生成一个待生成图像的压缩图像<code>z_indices</code>，代码中使用了<code>randint</code>初始化待生成的压缩图像。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">c_code, c_indices = model.encode_to_c(segmentation)</span><br><span class="line">z_indices = torch.randint(codebook_size, z_indices_shape, device=model.device)</span><br><span class="line"></span><br><span class="line">idx = z_indices</span><br><span class="line">idx = idx.reshape(z_code_shape[<span class="number">0</span>],z_code_shape[<span class="number">2</span>],z_code_shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">cidx = c_indices</span><br><span class="line">cidx = cidx.reshape(c_code.shape[<span class="number">0</span>],c_code.shape[<span class="number">2</span>],c_code.shape[<span class="number">3</span>])</span><br></pre></td></tr></table></figure><br>最后就是最关键的滑动窗口采样部分了。我们先稍微浏览一遍代码，再详细地一行一行看过去。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">temperature = <span class="number">1.0</span></span><br><span class="line">top_k = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, z_code_shape[<span class="number">2</span>]-<span class="number">0</span>):</span><br><span class="line">  <span class="keyword">if</span> i &lt;= <span class="number">8</span>:</span><br><span class="line">    local_i = i</span><br><span class="line">  <span class="keyword">elif</span> z_code_shape[<span class="number">2</span>]-i &lt; <span class="number">8</span>:</span><br><span class="line">    local_i = <span class="number">16</span>-(z_code_shape[<span class="number">2</span>]-i)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    local_i = <span class="number">8</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,z_code_shape[<span class="number">3</span>]-<span class="number">0</span>):</span><br><span class="line">    <span class="keyword">if</span> j &lt;= <span class="number">8</span>:</span><br><span class="line">      local_j = j</span><br><span class="line">    <span class="keyword">elif</span> z_code_shape[<span class="number">3</span>]-j &lt; <span class="number">8</span>:</span><br><span class="line">      local_j = <span class="number">16</span>-(z_code_shape[<span class="number">3</span>]-j)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      local_j = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">    i_start = i-local_i</span><br><span class="line">    i_end = i_start+<span class="number">16</span></span><br><span class="line">    j_start = j-local_j</span><br><span class="line">    j_end = j_start+<span class="number">16</span></span><br><span class="line">    </span><br><span class="line">    patch = idx[:,i_start:i_end,j_start:j_end]</span><br><span class="line">    patch = patch.reshape(patch.shape[<span class="number">0</span>],-<span class="number">1</span>)</span><br><span class="line">    cpatch = cidx[:, i_start:i_end, j_start:j_end]</span><br><span class="line">    cpatch = cpatch.reshape(cpatch.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">    patch = torch.cat((cpatch, patch), dim=<span class="number">1</span>)</span><br><span class="line">    logits,_ = model.transformer(patch[:,:-<span class="number">1</span>])</span><br><span class="line">    logits = logits[:, -<span class="number">256</span>:, :]</span><br><span class="line">    logits = logits.reshape(z_code_shape[<span class="number">0</span>],<span class="number">16</span>,<span class="number">16</span>,-<span class="number">1</span>)</span><br><span class="line">    logits = logits[:,local_i,local_j,:]</span><br><span class="line"></span><br><span class="line">    logits = logits/temperature</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      logits = model.top_k_logits(logits, top_k)</span><br><span class="line"></span><br><span class="line">    probs = torch.nn.functional.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    idx[:,i,j] = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x_sample = model.decode_to_img(idx, z_code_shape)</span><br><span class="line">show_image(x_sample)</span><br></pre></td></tr></table></figure><br>一开始的<code>temperature</code>和<code>top_k</code>是得到logit后的采样参数，和滑动窗口算法无关。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temperature = <span class="number">1.0</span></span><br><span class="line">top_k = <span class="number">100</span></span><br></pre></td></tr></table></figure></p>
<p>进入生成图像循环后，<code>i, j</code>分别表示压缩图像的竖索引和横索引，<code>i_start, i_end, j_start, j_end</code>是滑动窗口上下左右边界。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, z_code_shape[<span class="number">2</span>]-<span class="number">0</span>):</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,z_code_shape[<span class="number">3</span>]-<span class="number">0</span>):</span><br><span class="line">    ...</span><br><span class="line">    i_start = i-local_i</span><br><span class="line">    i_end = i_start+<span class="number">16</span></span><br><span class="line">    j_start = j-local_j</span><br><span class="line">    j_end = j_start+<span class="number">16</span></span><br></pre></td></tr></table></figure>
<p>为了获取这四个滑动窗口的范围，代码用了若干条件语句计算待生成像素在滑动窗口里的相对位置<code>local_i, local_j</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, z_code_shape[<span class="number">2</span>]-<span class="number">0</span>):</span><br><span class="line">  <span class="keyword">if</span> i &lt;= <span class="number">8</span>:</span><br><span class="line">    local_i = i</span><br><span class="line">  <span class="keyword">elif</span> z_code_shape[<span class="number">2</span>]-i &lt; <span class="number">8</span>:</span><br><span class="line">    local_i = <span class="number">16</span>-(z_code_shape[<span class="number">2</span>]-i)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    local_i = <span class="number">8</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,z_code_shape[<span class="number">3</span>]-<span class="number">0</span>):</span><br><span class="line">    <span class="keyword">if</span> j &lt;= <span class="number">8</span>:</span><br><span class="line">      local_j = j</span><br><span class="line">    <span class="keyword">elif</span> z_code_shape[<span class="number">3</span>]-j &lt; <span class="number">8</span>:</span><br><span class="line">      local_j = <span class="number">16</span>-(z_code_shape[<span class="number">3</span>]-j)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      local_j = <span class="number">8</span></span><br></pre></td></tr></table></figure><br>得到了滑动窗口的边界后，代码用滑动窗口从约束图像的压缩图像和待生成图像的压缩图像上各取出一个图块，并拼接起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">patch = idx[:,i_start:i_end,j_start:j_end]</span><br><span class="line">patch = patch.reshape(patch.shape[<span class="number">0</span>],-<span class="number">1</span>)</span><br><span class="line">cpatch = cidx[:, i_start:i_end, j_start:j_end]</span><br><span class="line">cpatch = cpatch.reshape(cpatch.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">patch = torch.cat((cpatch, patch), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>之后，只需要把拼接的图块直接输入进Transformer，得到输出<code>logits</code>，再用<code>local_i,local_j</code>去输出图块的对应位置取出下一个压缩图像像素的概率分布，就可以随机生成下一个压缩图像像素了。如前文所述，Transformer类会把二维的图块压扁到一维，输入进GPT。同时，GPT会自动保证前面的像素看不到后面的像素，我们不需要人为地指定约束像素。这个地方的调用逻辑其实非常简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">logits,_ = model.transformer(patch[:,:-<span class="number">1</span>])</span><br><span class="line">logits = logits[:, -<span class="number">256</span>:, :]</span><br><span class="line">logits = logits.reshape(z_code_shape[<span class="number">0</span>],<span class="number">16</span>,<span class="number">16</span>,-<span class="number">1</span>)</span><br><span class="line">logits = logits[:,local_i,local_j,:]</span><br></pre></td></tr></table></figure>
<p>最后只要从<code>logits</code>里采样，把采样出的压缩图像像素填入<code>idx</code>，就完成了一步生成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logits = logits/temperature</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    logits = model.top_k_logits(logits, top_k)</span><br><span class="line"></span><br><span class="line">probs = torch.nn.functional.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">idx[:,i,j] = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>反复执行循环，就能将压缩图像生成完毕。最后将压缩图像过一遍VQGAN的解码器即可得到最终的生成图像。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_sample = model.decode_to_img(idx, z_code_shape)</span><br><span class="line">show_image(x_sample)</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/19/20230605-VQGAN/21.jpg" alt></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>VQGAN论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09841">https://arxiv.org/abs/2012.09841</a></p>
<p>VQGAN GitHub：<a target="_blank" rel="noopener" href="https://github.com/CompVis/taming-transformers">https://github.com/CompVis/taming-transformers</a></p>
<p>如果你需要补充学习早期工作，欢迎阅读我之前的文章。</p>
<p>Transformer解读</p>
<p>PixelCNN解读</p>
<p>VQVAE解读</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2023/06/11/20221106-transformer-pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/11/20221106-transformer-pytorch/" class="post-title-link" itemprop="url">PyTorch Transformer 英中翻译超详细教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-11 00:09:48" itemprop="dateCreated datePublished" datetime="2023-06-11T00:09:48+08:00">2023-06-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这篇文章中，我将详细地介绍一个英中翻译 Transformer 的 PyTorch 实现。这篇文章会完整地展示一个深度学习项目的搭建过程，从数据集准备，到模型定义、训练。这篇文章不仅会讲解如何把 Transformer 的论文翻译成代码，还会讲清楚代码实现中的诸多细节，并分享我做实验时碰到的种种坑点。相信初学者能够从这篇文章中学到丰富的知识。</p>
<p>项目网址: <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/Transformer">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/Transformer</a></p>
<p>如果你对 Transformer 的论文不熟，欢迎阅读我之前的文章：<a href="https://zhouyifan.net/2022/11/12/20220925-Transformer/">Attention Is All You Need (Transformer) 论文精读</a>。</p>
<h2 id="数据集准备"><a href="#数据集准备" class="headerlink" title="数据集准备"></a>数据集准备</h2><p>我在 <a target="_blank" rel="noopener" href="https://github.com/P3n9W31/transformer-pytorch">https://github.com/P3n9W31/transformer-pytorch</a> 项目中找到了一个较小的中英翻译数据集。数据集只有几KB大小，中英词表只有10000左右，比较适合做Demo。如果要实现更加强大实用的模型，则需要换更大的数据集。但相应地，你要多花费更多的时间来训练。</p>
<p>我在代码仓库中提供了<code>data_load.py</code>文件。执行这个文件后，实验所需要的数据会自动下载到项目目录的<code>data</code>文件夹下。</p>
<p>该数据集由<code>cn.txt</code>, <code>en.txt</code>, <code>cn.txt.vocab.tsv</code>, <code>en.txt.vocab.tsv</code>这四个文件组成。前两个文件包含相互对应的中英文句子，其中中文已做好分词，英文全为小写且标点已被分割好。后两个文件是预处理好的词表。语料来自2000年左右的中国新闻，其第一条的中文及其翻译如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">目前 粮食 出现 阶段性 过剩 , 恰好 可以 以 粮食 换 森林 、 换 草地 , 再造 西部 秀美 山川 。</span><br><span class="line">the present food surplus can specifically serve the purpose of helping western china restore its woodlands , grasslands , and the beauty of its landscapes .</span><br></pre></td></tr></table></figure>
<p>词表则统计了各个单词的出现频率。通过使用词表，我们能实现单词和序号的相互转换（比如中文里的5号对应“的”字，英文里的5号对应”the”）。词表的前四个单词是特殊字符，分别为填充字符、频率太少没有被加入词典的词语、句子开始字符、句子结束字符。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;PAD&gt;	1000000000</span><br><span class="line">&lt;UNK&gt;	1000000000</span><br><span class="line">&lt;S&gt;	1000000000</span><br><span class="line">&lt;/S&gt;	1000000000</span><br><span class="line">的	8461</span><br><span class="line">是	2047</span><br><span class="line">和	1836</span><br><span class="line">在	1784</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;PAD&gt;	1000000000</span><br><span class="line">&lt;UNK&gt;	1000000000</span><br><span class="line">&lt;S&gt;	1000000000</span><br><span class="line">&lt;/S&gt;	1000000000</span><br><span class="line">the	13680</span><br><span class="line">and	6845</span><br><span class="line">of	6259</span><br><span class="line">to	4292</span><br></pre></td></tr></table></figure>
<p>只要运行一遍<code>data_load.py</code>下好数据后，我们待会就能用<code>load_train_data()</code>来获取已经打成batch的训练数据，并用API获取<code>cn2idx, idx2cn, en2idx, idx2en</code>这四个描述中英文序号与单词转换的词典。我们会在之后的训练代码里见到它们的用法。</p>
<h2 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h2><p>准备好数据后，接下来就要进入这个项目最重要的部分——Transformer 模型实现了。我将按照代码的执行顺序，从前往后，自底向上地介绍 Transformer 的各个模块：Positional Encoding, MultiHeadAttention, Encoder &amp; Decoder, 最后介绍如何把各个模块拼到一起。在这个过程中，我还会着重介绍一个论文里没有提及，但是代码实现时非常重要的一个细节——<code>&lt;pad&gt;</code>字符的处理。</p>
<p>说实话，用 PyTorch 实现 Transformer 没有什么有变数的地方，大家的代码写得都差不多，我也是参考着别人的教程写的。但是，Transformer 的代码实现中有很多坑。大部分人只会云淡风轻地介绍一下最终的代码成品，不会去讲他们 debug 耗费了多少时间，哪些地方容易出错。而我会着重讲一下代码中的一些细节，以及我碰到过的问题。</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p><img src="/2023/06/11/20221106-transformer-pytorch/1.jpg" alt></p>
<p>模型一开始是一个 Embedding 层加一个 Positional Encoding。Embedding 在 PyTorch 里已经有实现了，且不是文章的创新点，我们就直接来看 Positional Encoding 的写法。</p>
<p>求 Positional Encoding，其实就是求一个二元函数的许多函数值构成的矩阵。对于二元函数$PE(pos, i)$，我们要求出$pos \in [0, seqlen - 1],  i \in [0, d_{model} - 1]$时所有的函数值，其中，$seqlen$是该序列的长度，$d_{model}$是每一个词向量的长度。</p>
<p>理论上来说，每个句子的序列长度$seqlen$是不固定的。但是，我们可以提前预处理一个$seqlen$很大的 Positional Encoding 矩阵 。每次有句子输入进来，根据这个句子的序列长度，去预处理好的矩阵里取一小段出来即可。</p>
<p>这样，整个类的实现应该如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, max_seq_len: <span class="built_in">int</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Assume d_model is an even number for convenience</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        i_seq = torch.linspace(<span class="number">0</span>, max_seq_len - <span class="number">1</span>, max_seq_len)</span><br><span class="line">        j_seq = torch.linspace(<span class="number">0</span>, d_model - <span class="number">2</span>, d_model // <span class="number">2</span>)</span><br><span class="line">        pos, two_i = torch.meshgrid(i_seq, j_seq)</span><br><span class="line">        pe_2i = torch.sin(pos / <span class="number">10000</span>**(two_i / d_model))</span><br><span class="line">        pe_2i_1 = torch.cos(pos / <span class="number">10000</span>**(two_i / d_model))</span><br><span class="line">        pe = torch.stack((pe_2i, pe_2i_1), <span class="number">2</span>).reshape(<span class="number">1</span>, max_seq_len, d_model)</span><br><span class="line"></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        n, seq_len, d_model = x.shape</span><br><span class="line">        pe: torch.Tensor = self.pe</span><br><span class="line">        <span class="keyword">assert</span> seq_len &lt;= pe.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">assert</span> d_model == pe.shape[<span class="number">2</span>]</span><br><span class="line">        rescaled_x = x * d_model**<span class="number">0.5</span></span><br><span class="line">        <span class="keyword">return</span> rescaled_x + pe[:, <span class="number">0</span>:seq_len, :]</span><br></pre></td></tr></table></figure></p>
<p>代码中有不少需要讲解的部分。首先，先看一下预处理好的矩阵<code>pe</code>是怎么在<code>__init__</code>中算出来的。<code>pe</code>可以很直接地用两层循环算出来。由于这段预处理代码只会执行一次，相对于冗长的训练时间，哪怕生成<code>pe</code>的代码性能差一点也没事。然而，作为一个编程高手，我准备秀一下如何用并行的方法求出<code>pe</code>。</p>
<p>为了并行地求<code>pe</code>，我们要初始化一个二维网格，表示自变量$pos, i$。生成网格可以用下面的代码实现。（由于$i$要分奇偶讨论，$i$的个数是$\frac{d_{model}}{2}$)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i_seq = torch.linspace(<span class="number">0</span>, max_seq_len - <span class="number">1</span>, max_seq_len)</span><br><span class="line">j_seq = torch.linspace(<span class="number">0</span>, d_model - <span class="number">2</span>, d_model // <span class="number">2</span>)</span><br><span class="line">pos, two_i = torch.meshgrid(i_seq, j_seq)</span><br></pre></td></tr></table></figure>
<p><code>torch.meshgrid</code>用于生成网格。比如<code>torch.meshgrid([0, 1], [0, 1])</code>就可以生成<code>[[(0, 0), (0, 1)], [(1, 0), (1, 1)]]</code>这四个坐标构成的网格。不过，这个函数会把坐标的两个分量分别返回。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i, j = torch.meshgrid([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># i: [[0, 0], [1, 1]]</span></span><br><span class="line"><span class="comment"># j: [[0, 1], [0, 1]]</span></span><br></pre></td></tr></table></figure>
<p>利用这个函数的返回结果，我们可以把<code>pos, two_i</code>套入论文的公式，并行地分别算出奇偶位置的 PE 值。</p>
<script type="math/tex; mode=display">
\begin{aligned}
PE(pos, 2i) &= sin(pos/10000^{2i/d_{model}}) \\
PE(pos, 2i+1) &= cos(pos/10000^{2i/d_{model}})
\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pe_2i = torch.sin(pos / <span class="number">10000</span>**(two_i / d_model))</span><br><span class="line">pe_2i_1 = torch.cos(pos / <span class="number">10000</span>**(two_i / d_model))</span><br></pre></td></tr></table></figure>
<p>有了奇偶处的值，现在的问题是怎么把它们优雅地拼到同一个维度上。我这里先把它们堆成了形状为<code>seq_len, d_model/2, 2</code>的一个张量，再把最后一维展平，就得到了最后的<code>pe</code>矩阵。这一操作等于新建一个<code>seq_len, d_model</code>形状的张量，再把奇偶位置处的值分别填入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pe = torch.stack((pe_2i, pe_2i_1), <span class="number">2</span>).reshape(<span class="number">1</span>, max_seq_len, d_model)</span><br></pre></td></tr></table></figure>
<p>最后，要注意一点。只用 <code>self.pe = pe</code> 记录这个量是不够好的。我们最好用 <code>self.register_buffer(&#39;pe&#39;, pe, False)</code> 把这个量登记成 <code>torch.nn.Module</code> 的一个存储区（这一步会自动完成<code>self.pe = pe</code>）。这里涉及到 PyTorch 的一些知识了。</p>
<p>PyTorch 的 <code>Module</code> 会记录两类参数，一类是 <code>parameter</code> 可学习参数，另一类是 <code>buffer</code> 不可学习的参数。把变量登记成 <code>buffer</code> 的最大好处是，在使用 <code>model.to(device)</code> 把一个模型搬到另一个设备上时，所有 <code>parameter</code> 和 <code>buffer</code> 都会自动被搬过去。另外，<code>buffer</code> 和 <code>parameter</code> 一样，也可以被记录到 <code>state_dict</code> 中，并保存到文件里。<code>register_buffer</code> 的第三个参数决定了是否将变量加入 <code>state_dict</code>。由于 pe 可以直接计算，不需要记录，可以把这个参数设成 <code>False</code>。</p>
<p>预处理好 pe 后，用起来就很方便了。每次读取输入的序列长度，从中取一段出来即可。</p>
<p>另外，Transformer 给嵌入层乘了个系数$\sqrt{d_{model}}$。为了方便起见，我把这个系数放到了 <code>PositionalEncoding</code> 类里面。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">    n, seq_len, d_model = x.shape</span><br><span class="line">    pe: torch.Tensor = self.pe</span><br><span class="line">    <span class="keyword">assert</span> seq_len &lt;= pe.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">assert</span> d_model == pe.shape[<span class="number">2</span>]</span><br><span class="line">    rescaled_x = x * d_model**<span class="number">0.5</span></span><br><span class="line">    <span class="keyword">return</span> rescaled_x + pe[:, <span class="number">0</span>:seq_len, :]</span><br></pre></td></tr></table></figure>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p><img src="/2023/06/11/20221106-transformer-pytorch/2.jpg" alt></p>
<p>下一步是多头注意力层。为了实现多头注意力，我们先要实现 Transformer 里经典的注意力计算。而在讲注意力计算之前，我还要补充一下 Transformer 中有关 mask 的一些知识。</p>
<h4 id="Transformer-里的-mask"><a href="#Transformer-里的-mask" class="headerlink" title="Transformer 里的 mask"></a>Transformer 里的 mask</h4><p>Transformer 最大的特点就是能够并行训练。给定翻译好的第1~n个词语，它默认会并行地预测第2~(n+1)个下一个词语。为了模拟串行输出的情况，第$t$个词语不应该看到第$t+1$个词语之后的信息。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>输入信息</th>
<th>输出</th>
</tr>
</thead>
<tbody>
<tr>
<td>(y1, —, —, —)</td>
<td>y2</td>
</tr>
<tr>
<td>(y1, y2, —, —)</td>
<td>y3</td>
</tr>
<tr>
<td>(y1, y2, y3, —)</td>
<td>y4</td>
</tr>
<tr>
<td>(y1, y2, y3, y4)</td>
<td>y5</td>
</tr>
</tbody>
</table>
</div>
<p>为了实现这一功能，Transformer 在Decoder里使用了掩码。掩码取1表示这个地方的数是有效的，取0表示这个地方的数是无效的。Decoder 里的这种掩码应该是一个上三角全1矩阵。</p>
<p>掩码是在注意力计算中生效的。对于掩码取0的区域，其softmax前的$QK^T$值取负无穷。这是因为，对于softmax</p>
<script type="math/tex; mode=display">
softmax = \frac{e^{x_i}}{e^{x_1} + e^{x_2} + ... e^{x_n}},</script><p>令$x_i=-\infty$可以让它在 softmax 的分母里不产生任何贡献。</p>
<p>以上是论文里提到的 mask，它用来模拟 Decoder 的串行推理。而在代码实现中，还有其他地方会产生 mask。在生成一个 batch 的数据时，要给句子填充 <code>&lt;pad&gt;</code>。这个特殊字符也没有实际意义，不应该对计算产生任何贡献。因此，有 <code>&lt;pad&gt;</code> 的地方的 mask 也应该为0。之后讲 Transformer 模型类时，我会介绍所有的 mask 该怎么生成，这里我们仅关注注意力计算是怎么用到 mask 的。</p>
<h4 id="注意力计算"><a href="#注意力计算" class="headerlink" title="注意力计算"></a>注意力计算</h4><p>补充完了背景知识，我们来看注意力计算的实现代码。由于注意力计算没有任何的状态，因此它应该写成一个函数，而不是一个类。我们可以轻松地用 PyTorch 代码翻译注意力计算的公式。（注意，我这里的 mask 表示哪些地方要填负无穷，而不是像之前讲的表示哪些地方有效）</p>
<script type="math/tex; mode=display">
Attention(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">MY_INF = <span class="number">1e12</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">q: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">              k: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">              v: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">              mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Note: The dtype of mask must be bool</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># q shape: [n, heads, q_len, d_k]</span></span><br><span class="line">    <span class="comment"># k shape: [n, heads, k_len, d_k]</span></span><br><span class="line">    <span class="comment"># v shape: [n, heads, k_len, d_v]</span></span><br><span class="line">    <span class="keyword">assert</span> q.shape[-<span class="number">1</span>] == k.shape[-<span class="number">1</span>]</span><br><span class="line">    d_k = k.shape[-<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># tmp shape: [n, heads, q_len, k_len]</span></span><br><span class="line">    tmp = torch.matmul(q, k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / d_k**<span class="number">0.5</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tmp.masked_fill_(mask, -MY_INF)</span><br><span class="line">    tmp = F.softmax(tmp, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># tmp shape: [n, heads, q_len, d_v]</span></span><br><span class="line">    tmp = torch.matmul(tmp, v)</span><br><span class="line">    <span class="keyword">return</span> tmp</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这里有一个很坑的地方。引入了 <code>&lt;pad&gt;</code> 带来的 mask 后，会产生一个新的问题：可能一整行数据都是失效的，softmax 用到的所有 $x_i$ 可能都是负无穷。</p>
<script type="math/tex; mode=display">
softmax = \frac{e^{-\infty}}{e^{-\infty} + e^{-\infty} + ... e^{-\infty}}</script><p>这个数是没有意义的。如果用<code>torch.inf</code>来表示无穷大，就会令<code>exp(torch.inf)=0</code>，最后 softmax 结果会出现 NaN，代码大概率是跑不通的。</p>
<p>但是，大多数 PyTorch Transformer 教程压根就没提这一点，而他们的代码又还是能够跑通。拿放大镜仔细对比了代码后，我发现，他们的无穷大用的不是 <code>torch.inf</code>，而是自己随手设的一个极大值。这样，<code>exp(-MY_INF)</code>得到的不再是0，而是一个极小值。softmax 的结果就会等于分母的项数，而不是 NaN，不会有数值计算上的错误。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p><img src="/2023/06/11/20221106-transformer-pytorch/3.jpg" alt></p>
<p>有了注意力计算，就可以实现多头注意力层了。多头注意力层是有学习参数的，它应该写成一个类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, heads: <span class="built_in">int</span>, d_model: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> d_model % heads == <span class="number">0</span></span><br><span class="line">        <span class="comment"># dk == dv</span></span><br><span class="line">        self.d_k = d_model // heads</span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.v = nn.Linear(d_model, d_model)</span><br><span class="line">        self.out = nn.Linear(d_model, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                q: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                k: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                v: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># batch should be same</span></span><br><span class="line">        <span class="keyword">assert</span> q.shape[<span class="number">0</span>] == k.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">assert</span> q.shape[<span class="number">0</span>] == v.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># the sequence length of k and v should be aligned</span></span><br><span class="line">        <span class="keyword">assert</span> k.shape[<span class="number">1</span>] == v.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        n, q_len = q.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        n, k_len = k.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        q_ = self.q(q).reshape(n, q_len, self.heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        k_ = self.k(k).reshape(n, k_len, self.heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        v_ = self.v(v).reshape(n, k_len, self.heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        attention_res = attention(q_, k_, v_, mask)</span><br><span class="line">        concat_res = attention_res.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(</span><br><span class="line">            n, q_len, self.d_model)</span><br><span class="line">        concat_res = self.dropout(concat_res)</span><br><span class="line"></span><br><span class="line">        output = self.out(concat_res)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这段代码一处很灵性的地方。在 Transformer 的论文中，多头注意力是先把每个词的表示拆成$h$个头，再对每份做投影、注意力，最后拼接起来，再投影一次。其实，拆开与拼接操作是多余的。我们可以通过一些形状上的操作，等价地实现拆开与拼接，以提高运行效率。</p>
<p>具体来说，我们可以一开始就让所有头的数据经过同一个线性层。之后在做注意力之前把头和序列数这两维转置一下。这两步操作和拆开来做投影、注意力是等价的。做完了注意力操作之后，再把两个维度转置回来，这和拼接操作是等价的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">            q: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">            k: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">            v: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">            mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># batch should be same</span></span><br><span class="line">    <span class="keyword">assert</span> q.shape[<span class="number">0</span>] == k.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">assert</span> q.shape[<span class="number">0</span>] == v.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># the sequence length of k and v should be aligned</span></span><br><span class="line">    <span class="keyword">assert</span> k.shape[<span class="number">1</span>] == v.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    n, q_len = q.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    n, k_len = k.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    q_ = self.q(q).reshape(n, q_len, self.heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    k_ = self.k(k).reshape(n, k_len, self.heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    v_ = self.v(v).reshape(n, k_len, self.heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    attention_res = attention(q_, k_, v_, mask)</span><br><span class="line">    concat_res = attention_res.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(</span><br><span class="line">        n, q_len, self.d_model)</span><br><span class="line">    concat_res = self.dropout(concat_res)</span><br><span class="line"></span><br><span class="line">    output = self.out(concat_res)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="前馈网络"><a href="#前馈网络" class="headerlink" title="前馈网络"></a>前馈网络</h3><p><img src="/2023/06/11/20221106-transformer-pytorch/4.jpg" alt></p>
<p>前馈网络太简单了，两个线性层，没什么好说的。注意内部那个隐藏层的维度大小$d_{ff}$会比$d_{model}$更大一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, d_ff: <span class="built_in">int</span>, dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.layer2 = nn.Linear(d_ff, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.dropout(F.relu(x))</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Encoder-amp-Decoder"><a href="#Encoder-amp-Decoder" class="headerlink" title="Encoder &amp; Decoder"></a>Encoder &amp; Decoder</h3><p>准备好一切组件后，就可以把模型一层一层搭起来了。先搭好每个 Encoder 层和 Decoder 层，再拼成 Encoder 和 Decoder。</p>
<p>Encoder 层和 Decoder 层的结构与论文中的描述一致，且每个子层后面都有一个 dropout，和上一层之间使用了残差连接。归一化的方法是 <code>LayerNorm</code>。顺带一提，不仅是这些层，前面很多子层的计算中都加入了 dropout。</p>
<p>再提一句 mask。由于 encoder 和 decoder 的输入不同，它们的填充情况不同，产生的 mask 也不同。后文会展示这些 mask 的生成方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 heads: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_model: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_ff: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.self_attention = MultiHeadAttention(heads, d_model, dropout)</span><br><span class="line">        self.ffn = FeedForward(d_model, d_ff, dropout)</span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.dropout1 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout2 = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, src_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span></span><br><span class="line">        tmp = self.self_attention(x, x, x, src_mask)</span><br><span class="line">        tmp = self.dropout1(tmp)</span><br><span class="line">        x = self.norm1(x + tmp)</span><br><span class="line">        tmp = self.ffn(x)</span><br><span class="line">        tmp = self.dropout2(tmp)</span><br><span class="line">        x = self.norm2(x + tmp)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 heads: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_model: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_ff: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.self_attention = MultiHeadAttention(heads, d_model, dropout)</span><br><span class="line">        self.attention = MultiHeadAttention(heads, d_model, dropout)</span><br><span class="line">        self.ffn = FeedForward(d_model, d_ff, dropout)</span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        self.dropout1 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout2 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout3 = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                x,</span></span></span><br><span class="line"><span class="params"><span class="function">                encoder_kv: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                dst_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                src_dst_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span></span><br><span class="line">        tmp = self.self_attention(x, x, x, dst_mask)</span><br><span class="line">        tmp = self.dropout1(tmp)</span><br><span class="line">        x = self.norm1(x + tmp)</span><br><span class="line">        tmp = self.attention(x, encoder_kv, encoder_kv, src_dst_mask)</span><br><span class="line">        tmp = self.dropout2(tmp)</span><br><span class="line">        x = self.norm2(x + tmp)</span><br><span class="line">        tmp = self.ffn(x)</span><br><span class="line">        tmp = self.dropout3(tmp)</span><br><span class="line">        x = self.norm3(x + tmp)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>Encoder 和 Decoder 就是在所有子层前面加了一个嵌入层、一个位置编码，再把多个子层堆起来了而已，其他输入输出照搬即可。注意，我们可以给嵌入层输入<code>pad_idx</code>参数，让<code>&lt;pad&gt;</code>的计算不对梯度产生贡献。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 vocab_size: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pad_idx: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_model: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_ff: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_layers: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 heads: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 max_seq_len: <span class="built_in">int</span> = <span class="number">120</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)</span><br><span class="line">        self.pe = PositionalEncoding(d_model, max_seq_len)</span><br><span class="line">        self.layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">            self.layers.append(EncoderLayer(heads, d_model, d_ff, dropout))</span><br><span class="line">        self.layers = nn.ModuleList(self.layers)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, src_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.pe(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, src_mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 vocab_size: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pad_idx: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_model: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_ff: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_layers: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 heads: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 max_seq_len: <span class="built_in">int</span> = <span class="number">120</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, d_model, pad_idx)</span><br><span class="line">        self.pe = PositionalEncoding(d_model, max_seq_len)</span><br><span class="line">        self.layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_layers):</span><br><span class="line">            self.layers.append(DecoderLayer(heads, d_model, d_ff, dropout))</span><br><span class="line">        self.layers = nn.Sequential(*self.layers)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                x,</span></span></span><br><span class="line"><span class="params"><span class="function">                encoder_kv,</span></span></span><br><span class="line"><span class="params"><span class="function">                dst_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                src_dst_mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = self.pe(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, encoder_kv, dst_mask, src_dst_mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="Transformer-类"><a href="#Transformer-类" class="headerlink" title="Transformer 类"></a>Transformer 类</h3><p><img src="/2023/06/11/20221106-transformer-pytorch/5.jpg" alt></p>
<p>终于，激动人心的时候到来了。我们要把各个子模块组成变形金刚（Transformer）了。先过一遍所有的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 src_vocab_size: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dst_vocab_size: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pad_idx: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_model: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_ff: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_layers: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 heads: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 max_seq_len: <span class="built_in">int</span> = <span class="number">200</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.encoder = Encoder(src_vocab_size, pad_idx, d_model, d_ff,</span><br><span class="line">                               n_layers, heads, dropout, max_seq_len)</span><br><span class="line">        self.decoder = Decoder(dst_vocab_size, pad_idx, d_model, d_ff,</span><br><span class="line">                               n_layers, heads, dropout, max_seq_len)</span><br><span class="line">        self.pad_idx = pad_idx</span><br><span class="line">        self.output_layer = nn.Linear(d_model, dst_vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_mask</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                      q_pad: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                      k_pad: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                      with_left_mask: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="comment"># q_pad shape: [n, q_len]</span></span><br><span class="line">        <span class="comment"># k_pad shape: [n, k_len]</span></span><br><span class="line">        <span class="comment"># q_pad k_pad dtype: bool</span></span><br><span class="line">        <span class="keyword">assert</span> q_pad.device == k_pad.device</span><br><span class="line">        n, q_len = q_pad.shape</span><br><span class="line">        n, k_len = k_pad.shape</span><br><span class="line"></span><br><span class="line">        mask_shape = (n, <span class="number">1</span>, q_len, k_len)</span><br><span class="line">        <span class="keyword">if</span> with_left_mask:</span><br><span class="line">            mask = <span class="number">1</span> - torch.tril(torch.ones(mask_shape))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mask = torch.zeros(mask_shape)</span><br><span class="line">        mask = mask.to(q_pad.device)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            mask[i, :, q_pad[i], :] = <span class="number">1</span></span><br><span class="line">            mask[i, :, :, k_pad[i]] = <span class="number">1</span></span><br><span class="line">        mask = mask.to(torch.<span class="built_in">bool</span>)</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line"></span><br><span class="line">        src_pad_mask = x == self.pad_idx</span><br><span class="line">        dst_pad_mask = y == self.pad_idx</span><br><span class="line">        src_mask = self.generate_mask(src_pad_mask, src_pad_mask, <span class="literal">False</span>)</span><br><span class="line">        dst_mask = self.generate_mask(dst_pad_mask, dst_pad_mask, <span class="literal">True</span>)</span><br><span class="line">        src_dst_mask = self.generate_mask(dst_pad_mask, src_pad_mask, <span class="literal">False</span>)</span><br><span class="line">        encoder_kv = self.encoder(x, src_mask)</span><br><span class="line">        res = self.decoder(y, encoder_kv, dst_mask, src_dst_mask)</span><br><span class="line">        res = self.output_layer(res)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>我们一点一点来看。先看初始化函数。初始化函数的输入其实就是 Transformer 模型的超参数。总结一下，Transformer 应该有这些超参数：</p>
<ul>
<li><code>d_model</code> 模型中大多数词向量表示的维度大小</li>
<li><code>d_ff</code> 前馈网络隐藏层维度大小</li>
<li><code>n_layers</code> 堆叠的 Encoder &amp; Decoder 层数</li>
<li><code>head</code> 多头注意力的头数</li>
<li><code>dropout</code> Dropout 的几率</li>
</ul>
<p>另外，为了构建嵌入层，要知道源语言、目标语言的词典大小，并且提供<code>pad_idx</code>。为了预处理位置编码，需要提前知道一个最大序列长度。</p>
<p>照着子模块的初始化参数表，把参数归纳到<code>__init__</code>的参数表里即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">            src_vocab_size: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            dst_vocab_size: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            pad_idx: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_model: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            d_ff: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            n_layers: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            heads: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            dropout: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            max_seq_len: <span class="built_in">int</span> = <span class="number">200</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.encoder = Encoder(src_vocab_size, pad_idx, d_model, d_ff,</span><br><span class="line">                            n_layers, heads, dropout, max_seq_len)</span><br><span class="line">    self.decoder = Decoder(dst_vocab_size, pad_idx, d_model, d_ff,</span><br><span class="line">                            n_layers, heads, dropout, max_seq_len)</span><br><span class="line">    self.pad_idx = pad_idx</span><br><span class="line">    self.output_layer = nn.Linear(d_model, dst_vocab_size)</span><br></pre></td></tr></table></figure>
<p>再看一下 <code>forward</code> 函数。<code>forward</code>先预处理好了所有的 mask，再逐步执行 Transformer 的计算：先是通过 Encoder 获得源语言的中间表示<code>encoder_kv</code>，再把它和目标语言<code>y</code>的输入一起传入 Decoder，最后经过线性层输出结果<code>res</code>。由于 PyTorch 的交叉熵损失函数自带了 softmax 操作，这里不需要多此一举。</p>
<blockquote>
<p>Transformer 论文提到，softmax 前的那个线性层可以和嵌入层共享权重。也就是说，嵌入和输出前的线性层分别完成了词序号到词嵌入的正反映射，两个操作应该是互逆的。但是，词嵌入矩阵不是一个方阵，它根本不能求逆矩阵。我想破头也没想清楚是怎么让线性层可以和嵌入层共享权重的。网上的所有实现都没有对这个细节多加介绍，只是新建了一个线性层。我也照做了。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line"></span><br><span class="line">    src_pad_mask = x == self.pad_idx</span><br><span class="line">    dst_pad_mask = y == self.pad_idx</span><br><span class="line">    src_mask = self.generate_mask(src_pad_mask, src_pad_mask, <span class="literal">False</span>)</span><br><span class="line">    dst_mask = self.generate_mask(dst_pad_mask, dst_pad_mask, <span class="literal">True</span>)</span><br><span class="line">    src_dst_mask = self.generate_mask(dst_pad_mask, src_pad_mask, <span class="literal">False</span>)</span><br><span class="line">    encoder_kv = self.encoder(x, src_mask)</span><br><span class="line">    res = self.decoder(y, encoder_kv, dst_mask, src_dst_mask)</span><br><span class="line">    res = self.output_layer(res)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>等了很久，现在可以来仔细看一看 mask 的生成方法了。回忆一下，表示该字符是否有效的 mask 有两个来源。第一个是论文里提到的，用于模拟串行推理的 mask；另一个是填充操作的空白字符引入的 mask。<code>generate_mask</code> 用于生成这些 mask。</p>
<p><code>generate_mask</code> 的输入有 query 句子和 key 句子的 pad mask <code>q_pad, k_pad</code>，它们的形状为<code>[n, seq_len]</code>。若某处为 True，则表示这个地方的字符是<code>&lt;pad&gt;</code>。对于自注意力，query 和 key 都是一样的；而在 Decoder 的第二个多头注意力层中，query 来自目标语言，key 来自源语言。<code>with_left_mask</code> 表示是不是要加入 Decoder 里面的模拟串行推理的 mask，它会在掩码自注意力里用到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mask</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                q_pad: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                k_pad: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                with_left_mask: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br></pre></td></tr></table></figure>
<p>一开始，先取好维度信息，定好张量的形状。在注意力操作中，softmax 前的那个量的形状是 <code>[n, heads, q_len, k_len]</code>，表示每一批每一个头的每一个query对每个key之间的相似度。每一个头的mask是一样的。因此，除<code>heads</code>维可以广播外，mask 的形状应和它一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mask_shape = (n, <span class="number">1</span>, q_len, k_len)</span><br></pre></td></tr></table></figure>
<p>再新建一个表示最终 mask 的张量。如果不用 Decoder 的那种 mask，就生成一个全零的张量；否则，生成一个上三角为0，其余地方为1的张量。注意，在我的代码中，mask 为 True 或1就表示这个地方需要填负无穷。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> with_left_mask:</span><br><span class="line">    mask = <span class="number">1</span> - torch.tril(torch.ones(mask_shape))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    mask = torch.zeros(mask_shape)</span><br></pre></td></tr></table></figure>
<p>最后，把有 <code>&lt;pad&gt;</code> 的地方也标记一下。从<code>mask</code>的形状<code>[n, 1, q_len, k_len]</code>可以知道，<code>q_pad</code> 表示哪些行是无效的，<code>k_pad</code> 表示哪些列是无效的。如果query句子的第<code>i</code>个字符是<code>&lt;pad&gt;</code>，则应该令<code>mask[:, :, i, :] = 1</code>; 如果key句子的第<code>j</code>个字符是<code>&lt;pad&gt;</code>，则应该令<code>mask[:, :, :, j] = 1</code>。</p>
<p>下面的代码利用了PyTorch的取下标机制，直接并行地完成了mask赋值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    mask[i, :, q_pad[i], :] = <span class="number">1</span></span><br><span class="line">    mask[i, :, :, k_pad[i]] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mask</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                    q_pad: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                    k_pad: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                    with_left_mask: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="comment"># q_pad shape: [n, q_len]</span></span><br><span class="line">    <span class="comment"># k_pad shape: [n, k_len]</span></span><br><span class="line">    <span class="comment"># q_pad k_pad dtype: bool</span></span><br><span class="line">    <span class="keyword">assert</span> q_pad.device == k_pad.device</span><br><span class="line">    n, q_len = q_pad.shape</span><br><span class="line">    n, k_len = k_pad.shape</span><br><span class="line"></span><br><span class="line">    mask_shape = (n, <span class="number">1</span>, q_len, k_len)</span><br><span class="line">    <span class="keyword">if</span> with_left_mask:</span><br><span class="line">        mask = <span class="number">1</span> - torch.tril(torch.ones(mask_shape))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mask = torch.zeros(mask_shape)</span><br><span class="line">    mask = mask.to(q_pad.device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        mask[i, :, q_pad[i], :] = <span class="number">1</span></span><br><span class="line">        mask[i, :, :, k_pad[i]] = <span class="number">1</span></span><br><span class="line">    mask = mask.to(torch.<span class="built_in">bool</span>)</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure>
<p>看完了mask的生成方法后，我们回到前一步，看看mask会在哪些地方被调用。</p>
<p>在 Transformer 中，有三类多头注意力层，它们的 mask 也不同。Encoder 的多头注意力层的 query 和 key 都来自源语言；Decoder 的第一个多头注意力层的 query 和 key 都来自目标语言；Decoder 的第二个多头注意力层的 query 来自目标语言， key 来自源语言。另外，Decoder 的第一个多头注意力层要加串行推理的那个 mask。按照上述描述生成mask即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">    src_pad_mask = x == self.pad_idx</span><br><span class="line">    dst_pad_mask = y == self.pad_idx</span><br><span class="line">    src_mask = self.generate_mask(src_pad_mask, src_pad_mask, <span class="literal">False</span>)</span><br><span class="line">    dst_mask = self.generate_mask(dst_pad_mask, dst_pad_mask, <span class="literal">True</span>)</span><br><span class="line">    src_dst_mask = self.generate_mask(dst_pad_mask, src_pad_mask, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    encoder_kv = self.encoder(x, src_mask)</span><br><span class="line">    res = self.decoder(y, encoder_kv, dst_mask, src_dst_mask)</span><br><span class="line">    res = self.output_layer(res)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>到此，Transfomer 模型总算编写完成了。</p>
<p>这里再帮大家排一个坑。PyTorch的官方Transformer中使用了下面的参数初始化方式。但是，实际测试后，不知道为什么，我发现使用这种初始化会让模型训不起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br></pre></td></tr></table></figure>
<p>我去翻了翻PyTorch的Transformer示例，发现官方的示例根本没用到<code>Transformer</code>，而是用子模块<code>nn.TransformerDecoder</code>, <code>nn.TransformerEncoder</code>自己搭了一个新的Transformer。这些子模块其实都有自己的<code>init_weights</code>方法。看来官方都信不过自己的<code>Transformer</code>，这个<code>Transformer</code>类的初始化方法就有问题。</p>
<p>在我们的代码中，我们不必手动对参数初始化。PyTorch对每个线性层默认的参数初始化方式就够好了。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>准备好了模型、数据集后，剩下的工作非常惬意，只要随便调用一下就行了。训练的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.Transformer.data_load <span class="keyword">import</span> (get_batch_indices, load_cn_vocab,</span><br><span class="line">                                           load_en_vocab, load_train_data,</span><br><span class="line">                                           maxlen)</span><br><span class="line"><span class="keyword">from</span> dldemos.Transformer.model <span class="keyword">import</span> Transformer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Config</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">d_ff = <span class="number">2048</span></span><br><span class="line">n_layers = <span class="number">6</span></span><br><span class="line">heads = <span class="number">8</span></span><br><span class="line">dropout_rate = <span class="number">0.2</span></span><br><span class="line">n_epochs = <span class="number">60</span></span><br><span class="line">PAD_ID = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line">    cn2idx, idx2cn = load_cn_vocab()</span><br><span class="line">    en2idx, idx2en = load_en_vocab()</span><br><span class="line">    <span class="comment"># X: en</span></span><br><span class="line">    <span class="comment"># Y: cn</span></span><br><span class="line">    Y, X = load_train_data()</span><br><span class="line"></span><br><span class="line">    print_interval = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    model = Transformer(<span class="built_in">len</span>(en2idx), <span class="built_in">len</span>(cn2idx), PAD_ID, d_model, d_ff,</span><br><span class="line">                        n_layers, heads, dropout_rate, maxlen)</span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr)</span><br><span class="line"></span><br><span class="line">    citerion = nn.CrossEntropyLoss(ignore_index=PAD_ID)</span><br><span class="line">    tic = time.time()</span><br><span class="line">    cnter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> index, _ <span class="keyword">in</span> get_batch_indices(<span class="built_in">len</span>(X), batch_size):</span><br><span class="line">            x_batch = torch.LongTensor(X[index]).to(device)</span><br><span class="line">            y_batch = torch.LongTensor(Y[index]).to(device)</span><br><span class="line">            y_input = y_batch[:, :-<span class="number">1</span>]</span><br><span class="line">            y_label = y_batch[:, <span class="number">1</span>:]</span><br><span class="line">            y_hat = model(x_batch, y_input)</span><br><span class="line"></span><br><span class="line">            y_label_mask = y_label != PAD_ID</span><br><span class="line">            preds = torch.argmax(y_hat, -<span class="number">1</span>)</span><br><span class="line">            correct = preds == y_label</span><br><span class="line">            acc = torch.<span class="built_in">sum</span>(y_label_mask * correct) / torch.<span class="built_in">sum</span>(y_label_mask)</span><br><span class="line"></span><br><span class="line">            n, seq_len = y_label.shape</span><br><span class="line">            y_hat = torch.reshape(y_hat, (n * seq_len, -<span class="number">1</span>))</span><br><span class="line">            y_label = torch.reshape(y_label, (n * seq_len, ))</span><br><span class="line">            loss = citerion(y_hat, y_label)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1</span>)</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cnter % print_interval == <span class="number">0</span>:</span><br><span class="line">                toc = time.time()</span><br><span class="line">                interval = toc - tic</span><br><span class="line">                minutes = <span class="built_in">int</span>(interval // <span class="number">60</span>)</span><br><span class="line">                seconds = <span class="built_in">int</span>(interval % <span class="number">60</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;cnter:08d&#125;</span> <span class="subst">&#123;minutes:02d&#125;</span>:<span class="subst">&#123;seconds:02d&#125;</span>&#x27;</span></span><br><span class="line">                      <span class="string">f&#x27; loss: <span class="subst">&#123;loss.item()&#125;</span> acc: <span class="subst">&#123;acc.item()&#125;</span>&#x27;</span>)</span><br><span class="line">            cnter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    model_path = <span class="string">&#x27;dldemos/Transformer/model.pth&#x27;</span></span><br><span class="line">    torch.save(model.state_dict(), model_path)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Model saved to <span class="subst">&#123;model_path&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>所有的超参数都写在代码开头。在模型结构上，我使用了和原论文一样的超参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Config</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">d_ff = <span class="number">2048</span></span><br><span class="line">n_layers = <span class="number">6</span></span><br><span class="line">heads = <span class="number">8</span></span><br><span class="line">dropout_rate = <span class="number">0.2</span></span><br><span class="line">n_epochs = <span class="number">60</span></span><br><span class="line">PAD_ID = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>之后，进入主函数。一开始，我们调用<code>load_data.py</code>提供的API，获取中英文序号到单词的转换词典，并获取已经打包好的训练数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line">    cn2idx, idx2cn = load_cn_vocab()</span><br><span class="line">    en2idx, idx2en = load_en_vocab()</span><br><span class="line">    <span class="comment"># X: en</span></span><br><span class="line">    <span class="comment"># Y: cn</span></span><br><span class="line">    Y, X = load_train_data()</span><br></pre></td></tr></table></figure>
<p>接着，我们用参数初始化好要用到的对象，比如模型、优化器、损失函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print_interval = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">model = Transformer(<span class="built_in">len</span>(en2idx), <span class="built_in">len</span>(cn2idx), PAD_ID, d_model, d_ff,</span><br><span class="line">                    n_layers, heads, dropout_rate, maxlen)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr)</span><br><span class="line"></span><br><span class="line">citerion = nn.CrossEntropyLoss(ignore_index=PAD_ID)</span><br><span class="line">tic = time.time()</span><br><span class="line">cnter = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>再然后，进入训练循环。我们从<code>X, Y</code>里取出源语言和目标语言的序号数组，输入进模型里。别忘了，Transformer可以并行训练。我们给模型输入目标语言前<code>n-1</code>个单词，用第<code>2</code>到第<code>n</code>个单词作为监督标签。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> index, _ <span class="keyword">in</span> get_batch_indices(<span class="built_in">len</span>(X), batch_size):</span><br><span class="line">        x_batch = torch.LongTensor(X[index]).to(device)</span><br><span class="line">        y_batch = torch.LongTensor(Y[index]).to(device)</span><br><span class="line">        y_input = y_batch[:, :-<span class="number">1</span>]</span><br><span class="line">        y_label = y_batch[:, <span class="number">1</span>:]</span><br><span class="line">        y_hat = model(x_batch, y_input)</span><br></pre></td></tr></table></figure><br>得到模型的预测<code>y_hat</code>后，我们可以把输出概率分布中概率最大的那个单词作为模型给出的预测单词，算一个单词预测准确率。当然，我们要排除掉<code>&lt;pad&gt;</code>的影响。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_label_mask = y_label != PAD_ID</span><br><span class="line">preds = torch.argmax(y_hat, -<span class="number">1</span>)</span><br><span class="line">correct = preds == y_label</span><br><span class="line">acc = torch.<span class="built_in">sum</span>(y_label_mask * correct) / torch.<span class="built_in">sum</span>(y_label_mask)</span><br></pre></td></tr></table></figure>
<p>我们最后算一下<code>loss</code>，并执行梯度下降，训练代码就写完了。为了让训练更稳定，不出现梯度过大的情况，我们可以用<code>torch.nn.utils.clip_grad_norm_(model.parameters(), 1)</code>裁剪梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n, seq_len = y_label.shape</span><br><span class="line">y_hat = torch.reshape(y_hat, (n * seq_len, -<span class="number">1</span>))</span><br><span class="line">y_label = torch.reshape(y_label, (n * seq_len, ))</span><br><span class="line">loss = citerion(y_hat, y_label)</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在本项目的实验中，使用单卡3090，约10分钟就能完成训练。最终的训练准确率可以到达90%以上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">00006300 <span class="number">12</span>:<span class="number">12</span> loss: <span class="number">0.43494755029678345</span> acc: <span class="number">0.9049844145774841</span></span><br></pre></td></tr></table></figure>
<p>该数据集没有提供测试集（原仓库里的测试集来自训练集，这显然不合理）。且由于词表太小，不太好构建测试集。因此，我没有编写从测试集里生成句子并算BLEU score的代码，而是写了一份翻译给定句子的代码。要编写测试BLUE score的代码，只需要把翻译任意句子的代码改个输入，加一个求BLEU score的函数即可。这份翻译任意句子的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.Transformer.data_load <span class="keyword">import</span> (load_cn_vocab, load_en_vocab,</span><br><span class="line">                                           idx_to_sentence, maxlen)</span><br><span class="line"><span class="keyword">from</span> dldemos.Transformer.model <span class="keyword">import</span> Transformer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Config</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">lr = <span class="number">0.0001</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">d_ff = <span class="number">2048</span></span><br><span class="line">n_layers = <span class="number">6</span></span><br><span class="line">heads = <span class="number">8</span></span><br><span class="line">dropout_rate = <span class="number">0.2</span></span><br><span class="line">n_epochs = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">PAD_ID = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line">    cn2idx, idx2cn = load_cn_vocab()</span><br><span class="line">    en2idx, idx2en = load_en_vocab()</span><br><span class="line"></span><br><span class="line">    model = Transformer(<span class="built_in">len</span>(en2idx), <span class="built_in">len</span>(cn2idx), <span class="number">0</span>, d_model, d_ff, n_layers,</span><br><span class="line">                        heads, dropout_rate, maxlen)</span><br><span class="line">    model.to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    model_path = <span class="string">&#x27;dldemos/Transformer/model.pth&#x27;</span></span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br><span class="line"></span><br><span class="line">    my_input = [<span class="string">&#x27;we&#x27;</span>, <span class="string">&quot;should&quot;</span>, <span class="string">&quot;protect&quot;</span>, <span class="string">&quot;environment&quot;</span>]</span><br><span class="line">    x_batch = torch.LongTensor([[en2idx[x] <span class="keyword">for</span> x <span class="keyword">in</span> my_input]]).to(device)</span><br><span class="line"></span><br><span class="line">    cn_sentence = idx_to_sentence(x_batch[<span class="number">0</span>], idx2en, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(cn_sentence)</span><br><span class="line"></span><br><span class="line">    y_input = torch.ones(batch_size, maxlen,</span><br><span class="line">                         dtype=torch.long).to(device) * PAD_ID</span><br><span class="line">    y_input[<span class="number">0</span>] = en2idx[<span class="string">&#x27;&lt;S&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># y_input = y_batch</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, y_input.shape[<span class="number">1</span>]):</span><br><span class="line">            y_hat = model(x_batch, y_input)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">                y_input[j, i] = torch.argmax(y_hat[j, i - <span class="number">1</span>])</span><br><span class="line">    output_sentence = idx_to_sentence(y_input[<span class="number">0</span>], idx2cn, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(output_sentence)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>一开始，还是先获取词表，并初始化模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    device = <span class="string">&#x27;cuda&#x27;</span></span><br><span class="line">    cn2idx, idx2cn = load_cn_vocab()</span><br><span class="line">    en2idx, idx2en = load_en_vocab()</span><br><span class="line"></span><br><span class="line">    model = Transformer(<span class="built_in">len</span>(en2idx), <span class="built_in">len</span>(cn2idx), <span class="number">0</span>, d_model, d_ff, n_layers,</span><br><span class="line">                        heads, dropout_rate, maxlen)</span><br><span class="line">    model.to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    model_path = <span class="string">&#x27;dldemos/Transformer/model.pth&#x27;</span></span><br><span class="line">    model.load_state_dict(torch.load(model_path))</span><br></pre></td></tr></table></figure>
<p>之后，我们用自己定义的句子（要做好分词）代替原来的输入<code>x_batch</code>。如果要测试某个数据集，只要把这里<code>x_batch</code>换成测试集里的数据即可。<br>我们可以顺便把序号数组用<code>idx_to_sentence</code>转回英文，看看序号转换有没有出错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_input = [<span class="string">&#x27;we&#x27;</span>, <span class="string">&quot;should&quot;</span>, <span class="string">&quot;protect&quot;</span>, <span class="string">&quot;environment&quot;</span>]</span><br><span class="line">x_batch = torch.LongTensor([[en2idx[x] <span class="keyword">for</span> x <span class="keyword">in</span> my_input]]).to(device)</span><br><span class="line"></span><br><span class="line">cn_sentence = idx_to_sentence(x_batch[<span class="number">0</span>], idx2en, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(cn_sentence)</span><br></pre></td></tr></table></figure>
<p>这段代码会输出<code>we should protect environment</code>。这说明<code>x_batch</code>是我们想要的序号数组。</p>
<p>最后，我们利用Transformer自回归地生成句子，并输出句子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">y_input = torch.ones(batch_size, maxlen,</span><br><span class="line">                         dtype=torch.long).to(device) * PAD_ID</span><br><span class="line">y_input[<span class="number">0</span>] = en2idx[<span class="string">&#x27;&lt;S&gt;&#x27;</span>]</span><br><span class="line"><span class="comment"># y_input = y_batch</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, y_input.shape[<span class="number">1</span>]):</span><br><span class="line">        y_hat = model(x_batch, y_input)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            y_input[j, i] = torch.argmax(y_hat[j, i - <span class="number">1</span>])</span><br><span class="line">output_sentence = idx_to_sentence(y_input[<span class="number">0</span>], idx2cn, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(output_sentence)</span><br></pre></td></tr></table></figure>
<p>要自回归地生成句子，我们先给句子填入无效字符<code>&lt;pad&gt;</code>，再把第一个字符换成句子开始字符<code>&lt;S&gt;</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_input = torch.ones(batch_size, maxlen,</span><br><span class="line">                         dtype=torch.long).to(device) * PAD_ID</span><br><span class="line">y_input[<span class="number">0</span>] = en2idx[<span class="string">&#x27;&lt;S&gt;&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>之后，我们循环调用<code>Transformer</code>，获取下一个单词的概率分布。我们可以认为，概率最大的那个单词就是模型预测的下一个单词。因此，我们可以用<code>argmax</code>获取预测的下一个单词的序号，填回<code>y_input</code>。这里的<code>y_input</code>和训练时那个<code>y_batch</code>是同一个东西。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y_input = y_batch</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, y_input.shape[<span class="number">1</span>]):</span><br><span class="line">        y_hat = model(x_batch, y_input)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            y_input[j, i] = torch.argmax(y_hat[j, i - <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>最后只要输出生成的句子即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output_sentence = idx_to_sentence(y_input[<span class="number">0</span>], idx2cn, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(output_sentence)</span><br></pre></td></tr></table></figure>
<p>由于训练数据非常少，而且数据都来自新闻，我只好选择了一个比较常见的句子”we should protect environment”作为输入。模型翻译出了一个比较奇怪的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;S&gt; 要 保护 环境 保护 环境 保护 环境 保护 环境 保护 环境 保护 环境 保护 环境 的 生态 环境 落实 好 环境 &lt;/S&gt; 环境 &lt;/S&gt; 有效 保护 环境 &lt;/S&gt;...</span><br></pre></td></tr></table></figure>
<p>可以看出，模型确实学到了东西，能翻译出“要保护环境”。但是，这翻译的结果也太长太奇怪了。感觉是对训练数据过拟合了。当然，还是那句话，训练集里的数据太少。要提升模型性能并缓解过拟合，加数据集是最好的方法。这个结果起码说明我们Tranformer的编写没有问题。</p>
<p>在生成新句子的时候，我直接拿概率最高的单词当做预测的下一个单词。其实，还有一些更加高级的生成算法，比如Beam Search。如果模型训练得比较好，可以用这些高级一点的算法提高生成句子的质量。</p>
<p>我读了网上几份Transformer实现。这些实现在生成句子算BLEU score时，竟然直接输入测试句子的前<code>n-1</code>个单词，把输出的<code>n-1</code>个单词拼起来，作为模型的翻译结果。这个过程等价于告诉你前<code>i</code>个翻译答案，你去输出第<code>i+1</code>个单词，再把每个结果拼起来。这样写肯定是不合理的。正常来说应该是照着我这样自回归地生成翻译句子。大家参考网上的Transformer代码时要多加留心。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>只要读懂了 Transfomer 的论文，用 PyTorch 实现一遍 Transformer 是很轻松的。但是，代码实现中有非常多论文不会提及的细节，你自己实现时很容易踩坑。在这篇文章里，我完整地介绍了一个英中翻译 Transformer 的 PyTorch 实现，相信读者能够跟随这篇文章实现自己的 Transformer，并在代码实现的过程中加深对论文的理解。</p>
<p>再稍微总结一下代码实现中的一些值得注意的地方。代码中最大的难点是 mask 的实现。mask 的处理稍有闪失，就可能会让计算结果中遍布 NaN。一定要想清楚各个模块的 mask 是从哪来的，它们在注意力计算里是怎么被用上的。</p>
<p>另外，有两处地方的实现比较灵活。一处是位置编码的实现，一处是多头注意力中怎么描述“多头”。其他模块的实现都大差不差，千篇一律。</p>
<p>最后再提醒一句，要从头训练一个模型，一定要从小数据集上开始做。不然你训练个半天，结果差了，你不知道是数据有问题，还是代码有问题。我之前一直在使用很大的训练集，每次调试都非常麻烦，浪费了很多时间。希望大家引以为戒。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>感谢 <a target="_blank" rel="noopener" href="https://github.com/P3n9W31/transformer-pytorch">https://github.com/P3n9W31/transformer-pytorch</a> 提供的数据集。</p>
<p>一份简明的Transformer实现代码 <a target="_blank" rel="noopener" href="https://github.com/hyunwoongko/transformer">https://github.com/hyunwoongko/transformer</a></p>
<p>一篇不错的Transformer实现教程 <a target="_blank" rel="noopener" href="https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec">https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec</a></p>
<h2 id="过期内容"><a href="#过期内容" class="headerlink" title="过期内容"></a>过期内容</h2><p>我第一次写这篇文章时过于仓促，文章中有不少错误，实验部分也没写完。我后来把本文又重新修改了一遍，补充了实验部分。</p>
<p>我之前使用了一个较大的数据集，但发现做实验做得很慢，于是换了一个较小的数据集。以前的数据集预处理介绍就挪到这里了。</p>
<h3 id="数据集与评测方法"><a href="#数据集与评测方法" class="headerlink" title="数据集与评测方法"></a>数据集与评测方法</h3><p>在开启一个深度学习项目之初，要把任务定义好。准确来说，我们要明白这个任务是在完成一个怎样的映射，并准备一个用于评测的数据集，定义好评价指标。</p>
<p>英中翻译，这个任务非常明确，就是把英文的句子翻译成中文。英中翻译的数据集应该包含若干个句子对，每个句子对由一句英文和它对应的中文翻译组成。</p>
<p>中英翻译的数据集不是很好找。有几个比较出名的数据集的链接已经失效了，还有些数据集需要注册与申请后才能获取。我在中文NLP语料库仓库(<a target="_blank" rel="noopener" href="https://github.com/brightmart/nlp_chinese_corpus)找到了中英文平行语料">https://github.com/brightmart/nlp_chinese_corpus)找到了中英文平行语料</a> translation2019zh。该语料库由520万对中英文语料构成，训练集516万对，验证集3.9万对。用作训练和验证中英翻译模型是足够了。</p>
<p>机器翻译的评测指标叫做BLEU Score。如果模型输出的翻译和参考译文有越多相同的单词、连续2个相同单词、连续3个相同单词……，则得分越高。</p>
<p>PyTorch 提供了便捷的API，我们可以用一行代码算完BLEU Score。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torchtext.data.metrics <span class="keyword">import</span> bleu_score</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>candidate_corpus = [[<span class="string">&#x27;My&#x27;</span>, <span class="string">&#x27;full&#x27;</span>, <span class="string">&#x27;pytorch&#x27;</span>, <span class="string">&#x27;test&#x27;</span>], [<span class="string">&#x27;Another&#x27;</span>, <span class="string">&#x27;Sentence&#x27;</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>references_corpus = [[[<span class="string">&#x27;My&#x27;</span>, <span class="string">&#x27;full&#x27;</span>, <span class="string">&#x27;pytorch&#x27;</span>, <span class="string">&#x27;test&#x27;</span>], [<span class="string">&#x27;Completely&#x27;</span>, <span class="string">&#x27;Different&#x27;</span>]], [[<span class="string">&#x27;No&#x27;</span>, <span class="string">&#x27;Match&#x27;</span>]]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bleu_score(candidate_corpus, references_corpus)</span><br><span class="line">    <span class="number">0.8408964276313782</span></span><br></pre></td></tr></table></figure>
<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>得到数据集后，下一步要做的是对数据集做处理，把原始数据转化成能够输入神经网络的张量。对于图片，预处理可能是裁剪、缩放，使所有图片都有一样的大小；对于文本，预处理可能是分词、填充。</p>
<p>在<a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1EX8eE5YWBxCaohBO8Fh4e2j3b9C2bTVQ">网盘</a>上下载好 translation2019zh 数据集后，我们来一步一步清洗这个数据集。这个数据集只有两个文件<code>translation2019zh_train.json</code>, <code>translation2019zh_valid.json</code>，它们的结构如下：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;english&quot;: &lt;english&gt;, &quot;chinese&quot;: &lt;chinese&gt;&#125;</span><br><span class="line">&#123;&quot;english&quot;: &lt;english&gt;, &quot;chinese&quot;: &lt;chinese&gt;&#125;</span><br><span class="line">&#123;&quot;english&quot;: &lt;english&gt;, &quot;chinese&quot;: &lt;chinese&gt;&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>这些json文件有点不合标准，每对句子由一行json格式的记录组成。<code>english</code>属性是英文句子，<code>chinese</code>属性是中文句子。比如：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;english&quot;: &quot;In Italy ...&quot;, &quot;chinese&quot;: &quot;在意大利 ...&quot;&#125;</span><br></pre></td></tr></table></figure>
<p>因此，在读取数据时，我们可以用下面的代码提取每对句子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fp:</span><br><span class="line">        line = json.loads(line)</span><br><span class="line">        english, chinese = line[<span class="string">&#x27;english&#x27;</span>], line[<span class="string">&#x27;chinese&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>这个数据集有一点不干净，有一些句子对的中英文句子颠倒过来了。为此，我们要稍微处理一下，把这些句子对翻转过来。如果一个英文句子不全由 ASCII 组成，则它可能是一个被标错的中文句子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Correct mislabeled data</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> english.isascii():</span><br><span class="line">        english, chinese = chinese, english</span><br></pre></td></tr></table></figure>
<p>经过这一步，我们只得到了中英文的字符文本。而在NLP中，大部分处理的最小单位都是符号（token）——对于英文来说，符号是单词、标点；对于中文来说，符号是词语、标点。我们还需要一个符号化的过程。</p>
<p>英文符号化非常方便，torchtext 提供了非常便捷的英文分词 API。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">english = tokenizer(english)</span><br></pre></td></tr></table></figure>
<p>而中文分词方面，我使用了<code>jieba</code>库。该库可以直接 pip 安装。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure>
<p>分词的 API 是 <code>jieba.cut</code>。由于分词的结果中，相邻的词之间有空格，我一股脑地把所有空白符给过滤掉了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">chinese = <span class="built_in">list</span>(jieba.cut(chinese))</span><br><span class="line">chinese = [x <span class="keyword">for</span> x <span class="keyword">in</span> chinese <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> &#123;<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;\t&#x27;</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>经过这些处理后，每句话被转换成了中文词语或英文单词的数组。整个处理代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_file</span>(<span class="params">json_path</span>):</span></span><br><span class="line">    english_sentences = []</span><br><span class="line">    chinese_sentences = []</span><br><span class="line">    tokenizer = get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(json_path, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fp:</span><br><span class="line">            line = json.loads(line)</span><br><span class="line">            english, chinese = line[<span class="string">&#x27;english&#x27;</span>], line[<span class="string">&#x27;chinese&#x27;</span>]</span><br><span class="line">            <span class="comment"># Correct mislabeled data</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> english.isascii():</span><br><span class="line">                english, chinese = chinese, english</span><br><span class="line">            <span class="comment"># Tokenize</span></span><br><span class="line">            english = tokenizer(english)</span><br><span class="line">            chinese = <span class="built_in">list</span>(jieba.cut(chinese))</span><br><span class="line">            chinese = [x <span class="keyword">for</span> x <span class="keyword">in</span> chinese <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> &#123;<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;\t&#x27;</span>&#125;]</span><br><span class="line">            english_sentences.append(english)</span><br><span class="line">            chinese_sentences.append(chinese)</span><br><span class="line">    <span class="keyword">return</span> english_sentences, chinese_sentences</span><br></pre></td></tr></table></figure>
<h3 id="词语转序号"><a href="#词语转序号" class="headerlink" title="词语转序号"></a>词语转序号</h3><p>为了让计算机更方便地处理单词，我们还要把单词转换成序号。比如令<code>apple</code>为0号，<code>banana</code>为1号，则句子<code>apple banana apple</code>就转换成了<code>0 1 0</code>。</p>
<p>给每一个单词选一个标号，其实就是要建立一个词典。一般来说，我们可以利用他人的统计结果，挑选最常用的一些英文单词和中文词语构成词典。不过，现在我们已经有了一个庞大的中英语料库了，我们可以直接从这个语料库中挑选出最常见的词构成词典。</p>
<p>根据上一步处理得到的句子数组<code>sentences</code>，我们可以用下面的 Python 代码统计出最常见的一些词语，把它们和4个特殊字符<code>&lt;sos&gt;, &lt;eos&gt;, &lt;unk&gt;, &lt;pad&gt;</code>（句子开始字符、句子结束字符、频率太少没有被加入词典的词语、填充字符）一起构成词典。统计字符出现次数是通过 Python 的 <code>Counter</code> 类实现的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_vocab</span>(<span class="params">sentences, max_element=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Note that max_element includes special characters&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    default_list = [<span class="string">&#x27;&lt;sos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="string">&#x27;&lt;pad&gt;&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    char_set = Counter()</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        c_set = Counter(sentence)</span><br><span class="line">        char_set.update(c_set)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> max_element <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> default_list + <span class="built_in">list</span>(char_set.keys())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        max_element -= <span class="number">4</span></span><br><span class="line">        words_freq = char_set.most_common(max_element)</span><br><span class="line">        <span class="comment"># pair array to double array</span></span><br><span class="line">        words, freq = <span class="built_in">zip</span>(*words_freq)</span><br><span class="line">        <span class="keyword">return</span> default_list + <span class="built_in">list</span>(words)</span><br></pre></td></tr></table></figure>
<p>准备好了词典后，我还编写了两个工具函数<code>sentence_to_tensor</code>，<code>tensor_to_sentence</code>，它们可以用于字符串数组与序号数组的互相转换。测试这些代码的脚本及其输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dataset.py</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    en_sens, zh_sens = read_file(</span><br><span class="line">        <span class="string">&#x27;data/translation2019zh/translation2019zh_valid.json&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(*en_sens[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>(*zh_sens[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">    en_vocab = create_vocab(en_sens, <span class="number">10000</span>)</span><br><span class="line">    zh_vocab = create_vocab(zh_sens, <span class="number">30000</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(en_vocab)[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">list</span>(zh_vocab)[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    en_tensors = sentence_to_tensor(en_sens, en_vocab)</span><br><span class="line">    zh_tensors = sentence_to_tensor(zh_sens, zh_vocab)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(tensor_to_sentence(en_tensors[<span class="number">0</span>], en_vocab, <span class="literal">True</span>))</span><br><span class="line">    <span class="built_in">print</span>(tensor_to_sentence(zh_tensors[<span class="number">0</span>], zh_vocab))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;slowly&#x27;, &#x27;and&#x27;, &#x27;not&#x27;, &#x27;without&#x27;, &#x27;struggle&#x27;, &#x27;,&#x27;, &#x27;america&#x27;, &#x27;began&#x27;, &#x27;to&#x27;, &#x27;listen&#x27;, &#x27;.&#x27;] ...]</span><br><span class="line">[&#x27;美国&#x27;, &#x27;缓慢&#x27;, &#x27;地&#x27;, &#x27;开始&#x27;, &#x27;倾听&#x27;, &#x27;，&#x27;, &#x27;但&#x27;, &#x27;并非&#x27;, &#x27;没有&#x27;, &#x27;艰难曲折&#x27;, &#x27;。&#x27;] ...]</span><br><span class="line">[&#x27;&lt;sos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;the&#x27;, &#x27;.&#x27;, &#x27;,&#x27;, &#x27;of&#x27;, &#x27;and&#x27;, &#x27;to&#x27;]</span><br><span class="line">[&#x27;&lt;sos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;, &#x27;&lt;unk&gt;&#x27;, &#x27;&lt;pad&gt;&#x27;, &#x27;的&#x27;, &#x27;，&#x27;, &#x27;。&#x27;, &#x27;在&#x27;, &#x27;了&#x27;, &#x27;和&#x27;]</span><br><span class="line">slowly and not without struggle , america began to listen .</span><br><span class="line">美国缓慢地开始倾听，但并非没有&lt;unk&gt;。</span><br></pre></td></tr></table></figure>
<p>在这一步中，有一个重要的参数：词典的大小。显然，词典越大，能处理的词语越多，但训练速度也会越慢。由于这个项目只是一个用于学习的demo，我设置了比较小的词典大小。想提升整个模型的性能的话，调大词典大小是一个最快的方法。</p>
<h3 id="生成-Dataloader"><a href="#生成-Dataloader" class="headerlink" title="生成 Dataloader"></a>生成 Dataloader</h3><p>都说程序员是新时代的农民工，这非常有道理。因为，作为程序员，你免不了要写一些繁重、无聊的数据处理脚本。还好，写完这些无聊的预处理代码后，总算可以使用 PyTorch 的 API 写一些有趣的代码了。</p>
<p>把词语数组转换成序号句子数组后，我们要考虑怎么把序号句子数组输入给模型了。文本数据通常长短不一，为了一次性处理一个 batch 的数据，要把短的句子填充，使得一批句子长度相等。写 Dataloader 时最主要的工作就是填充并对齐句子。</p>
<p>先看一下<code>Dataset</code>的写法。上一步得到的序号句子数组可以塞进<code>Dataset</code>里。注意，每个句子的前后要加上表示句子开始和结束的特殊符号。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">SOS_ID = <span class="number">0</span></span><br><span class="line">EOS_ID = <span class="number">1</span></span><br><span class="line">UNK_ID = <span class="number">2</span></span><br><span class="line">PAD_ID = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TranslationDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, en_tensor: np.ndarray, zh_tensor: np.ndarray</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(en_tensor) == <span class="built_in">len</span>(zh_tensor)</span><br><span class="line">        self.length = <span class="built_in">len</span>(en_tensor)</span><br><span class="line">        self.en_tensor = en_tensor</span><br><span class="line">        self.zh_tensor = zh_tensor</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.length</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        x = np.concatenate(([SOS_ID], self.en_tensor[index], [EOS_ID]))</span><br><span class="line">        x = torch.from_numpy(x)</span><br><span class="line">        y = np.concatenate(([SOS_ID], self.zh_tensor[index], [EOS_ID]))</span><br><span class="line">        y = torch.from_numpy(y)</span><br><span class="line">        <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure>
<p>接下来看一下 DataLoader 的写法。在创建 Dataloader 时，最重要的是 <code>collate_fn</code> 的编写，这个函数决定了怎么把多条数据合成一个等长的 batch。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader</span>(<span class="params">en_tensor: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">                   zh_tensor: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">                   batch_size=<span class="number">16</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">batch</span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    dataset = TranslationDataset(en_tensor, zh_tensor)</span><br><span class="line">    dataloader = DataLoader(dataset,</span><br><span class="line">                            batch_size=batch_size,</span><br><span class="line">                            shuffle=<span class="literal">True</span>,</span><br><span class="line">                            collate_fn=collate_fn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>
<p><code>collate_fn</code> 的输入是多个 <code>dataset</code> <code>__getitem__</code> 的返回结果构成的数组。对于我们的 <code>dataset</code> 来说，<code>collate_fn</code> 的输入是 <code>[(x1, y1), (x2, y2), ...]</code> 。我们可以用 <code>zip(*batch)</code> 把二元组数组拆成两个数组 <code>x, y</code> 。</p>
<p><code>collate_fn</code> 的输出就是将来 <code>dataloader</code> 的输出。PyTorch 提供了 <code>pad_sequence</code> 函数用来把一批数据填充至等长。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">batch</span>):</span></span><br><span class="line">    x, y = <span class="built_in">zip</span>(*batch)</span><br><span class="line">    x_pad = pad_sequence(x, batch_first=<span class="literal">True</span>, padding_value=PAD_ID)</span><br><span class="line">    y_pad = pad_sequence(y, batch_first=<span class="literal">True</span>, padding_value=PAD_ID)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_pad, y_pad</span><br></pre></td></tr></table></figure>
<p>实现完<code>collate_fn</code>后，我们就可以得到了DataLoader。这样，数据集预处理部分大功告成。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2023/06/06/20230527-VQVAE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/06/20230527-VQVAE/" class="post-title-link" itemprop="url">轻松理解 VQ-VAE：首个提出 codebook 机制的生成模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-06 23:15:59" itemprop="dateCreated datePublished" datetime="2023-06-06T23:15:59+08:00">2023-06-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>近两年，有许多图像生成类任务的前沿工作都使用了一种叫做”codebook”的机制。追溯起来，codebook机制最早是在VQ-VAE论文中提出的。相比于普通的VAE，VQ-VAE能利用codebook机制把图像编码成离散向量，为图像生成类任务提供了一种新的思路。VQ-VAE的这种建模方法启发了无数的后续工作，包括声名远扬的Stable Diffusion。</p>
<p>在这篇文章中，我将先以易懂的逻辑带领大家一步一步领悟VQ-VAE的核心思想，再介绍VQ-VAE中关键算法的具体形式，最后把VQ-VAE的贡献及其对其他工作的影响做一个总结。通过阅读这篇文章，你不仅能理解VQ-VAE本身的原理，更能知道如何将VQ-VAE中的核心机制活学活用。</p>
<h2 id="从-AE-到-VQ-VAE"><a href="#从-AE-到-VQ-VAE" class="headerlink" title="从 AE 到 VQ-VAE"></a>从 AE 到 VQ-VAE</h2><p>为什么VQ-VAE想要把图像编码成离散向量？让我们从最早的自编码器（Autoencoder, AE）开始一步一步谈起。AE是一类能够把图片压缩成较短的向量的神经网络模型，其结构如下图所示。AE包含一个编码器$e()$和一个解码器$d()$。在训练时，输入图像$\mathbf{x}$会被编码成一个较短的向量$\mathbf{z}$，再被解码回另一幅长得差不多的图像$\hat{\mathbf{x}}$。网络的学习目标是让重建出来的图像$\hat{\mathbf{x}}$和原图像$\mathbf{x}$尽可能相似。</p>
<p><img src="/2023/06/06/20230527-VQVAE/1.jpg" alt></p>
<p>解码器可以把一个向量解码成图片。换一个角度看，解码器就是一个图像生成模型，因为它可以根据向量来生成图片。那么，AE可不可以用来做图像生成呢？很可惜，AE的编码器编码出来的向量空间是不规整的。也就是说，解码器只认识经编码器编出来的向量，而不认识其他的向量。如果你把自己随机生成出来的向量输入给解码器，解码器是生成不出有意义的图片的。AE不能够随机生成图片，所以它不能很好地完成图像生成任务，只能起到把图像压缩的作用。</p>
<p>AE离图像生成只差一步了。只要AE的编码空间比较规整，符合某个简单的数学分布（比如最常见的标准正态分布），那我们就可以从这个分布里随机采样向量，再让解码器根据这个向量来完成随机图片生成了。VAE就是这样一种改进版的AE。它用一些巧妙的方法约束了编码向量$\mathbf{z}$，使得$\mathbf{z}$满足标准正态分布。这样，解码器不仅认识编码器编出的向量，还认识其他来自标准正态分布的向量。训练完成后，我们就可以扔掉编码器，用来自标准正态分布的随机向量和解码器来实现随机图像生成了。</p>
<p><img src="/2023/06/06/20230527-VQVAE/2.jpg" alt></p>
<p>VAE的实现细节就不在这里赘述了，是否理解它对理解VQ-VAE没有影响。我们只需知道VAE可以把图片编码成符合标准正态分布的向量即可。让向量符合标准正态分布的原因是方便随机采样。同时，需要强调的是，VAE编码出来的向量是<strong>连续向量</strong>，也就是向量的每一维都是浮点数。如果把向量的某一维稍微改动0.0001，解码器还是认得这个向量，并且会生成一张和原向量对应图片差不多的图片。</p>
<p>但是，VAE生成出来的图片都不是很好看。VQ-VAE的作者认为，VAE的生成图片之所以质量不高，是因为图片被编码成了连续向量。而实际上，把图片编码成<strong>离散向量</strong>会更加自然。比如我们想让画家画一个人，我们会说这个是男是女，年龄是偏老还是偏年轻，体型是胖还是壮，而不会说这个人性别是0.5，年龄是0.6，体型是0.7。因此，VQ-VAE会把图片编码成离散向量，如下图所示。</p>
<p><img src="/2023/06/06/20230527-VQVAE/3.jpg" alt></p>
<p>把图像编码成离散向量后，又会带来两个新的问题。第一个问题是，神经网络会默认输入满足一个连续的分布，而不善于处理离散的输入。如果你直接输入0, 1, 2这些数字，神经网络会默认1是一个处于0, 2中间的一种状态。为了解决这一问题，我们可以借鉴NLP中对于离散单词的处理方法。为了处理离散的输入单词，NLP模型的第一层一般都是词嵌入层，它可以把每个输入单词都映射到一个独一无二的连续向量上。这样，每个离散的数字都变成了一个特别的连续向量了。</p>
<p><img src="/2023/06/06/20230527-VQVAE/3.5.jpg" alt></p>
<p>我们可以把类似的嵌入层加到VQ-VAE的解码器前。这个嵌入层在VQ-VAE里叫做”embedding space（嵌入空间）”，在后续文章中则被称作”codebook”。</p>
<p><img src="/2023/06/06/20230527-VQVAE/4.jpg" alt></p>
<p>离散向量的另一个问题是它不好采样。回忆一下，VAE之所以把图片编码成符合正态分布的连续向量，就是为了能在图像生成时把编码器扔掉，让随机采样出的向量也能通过解码器变成图片。现在倒好，VQ-VAE把图片编码了一个离散向量，这个离散向量构成的空间是不好采样的。VQ-VAE不是面临着和AE一样的问题嘛。</p>
<p>这个问题是无解的。没错！VQ-VAE根本不是一个图像生成模型。它和AE一样，只能很好地完成图像压缩，把图像变成一个短得多的向量，而不支持随机图像生成。VQ-VAE和AE的唯一区别，就是VQ-VAE会编码出离散向量，而AE会编码出连续向量。</p>
<p>可为什么VQ-VAE会被归类到图像生成模型中呢？这是因为VQ-VAE的作者利用VQ-VAE能编码离散向量的特性，使用了一种特别的方法对VQ-VAE的离散编码空间采样。VQ-VAE的作者之前设计了一种图像生成网络，叫做PixelCNN。PixelCNN能拟合一个离散的分布。比如对于图像，PixelCNN能输出某个像素的某个颜色通道取0~255中某个值的概率分布。这不刚好嘛，VQ-VAE也是把图像编码成离散向量。换个更好理解的说法，VQ-VAE能把图像映射成一个「小图像」。我们可以把PixelCNN生成图像的方法搬过来，让PixelCNN学习生成「小图像」。这样，我们就可以用PixelCNN生成离散编码，再利用VQ-VAE的解码器把离散编码变成图像。</p>
<p>让我们来整理一下VQ-VAE的工作过程。</p>
<ol>
<li>训练VQ-VAE的编码器和解码器，使得VQ-VAE能把图像变成「小图像」，也能把「小图像」变回图像。</li>
<li>训练PixelCNN，让它学习怎么生成「小图像」。</li>
<li>随机采样时，先用PixelCNN采样出「小图像」，再用VQ-VAE把「小图像」翻译成最终的生成图像。</li>
</ol>
<p>到这里，我们已经学完了VQ-VAE的核心思想。让我们来总结一下。VQ-VAE不是一个VAE，而是一个AE。它的目的是把图像压缩成离散向量。或者换个角度说，它提供了把大图像翻译成「小图像」的方法，也提供了把「小图像」翻译成大图像的方法。这样，一个随机生成大图像的问题，就被转换成了一个等价的随机生成一个较小的「图像」的问题。有一些图像生成模型，比如PixelCNN，更适合拟合离散分布。可以用它们来完成生成「小图像」的问题，填补上VQ-VAE生成图片的最后一片空缺。</p>
<h2 id="VQ-VAE-设计细节"><a href="#VQ-VAE-设计细节" class="headerlink" title="VQ-VAE 设计细节"></a>VQ-VAE 设计细节</h2><p>在上一节中，我们虽然认识了VQ-VAE的核心思想，但略过了不少实现细节，比如：</p>
<ul>
<li>VQ-VAE的编码器怎么输出离散向量。</li>
<li>VQ-VAE怎么优化编码器和解码器。</li>
<li>VQ-VAE怎么优化嵌入空间。</li>
</ul>
<p>在这一节里，我们来详细探究这些细节。</p>
<h3 id="输出离散编码"><a href="#输出离散编码" class="headerlink" title="输出离散编码"></a>输出离散编码</h3><p>想让神经网络输出一个整数，最简单的方法是和多分类模型一样，输出一个Softmax过的概率分布。之后，从概率分布里随机采样一个类别，这个类别的序号就是我们想要的整数。比如在下图中，我们想得到一个由3个整数构成的离散编码，就应该让编码器输出3组logit，再经过Softmax与采样，得到3个整数。</p>
<p><img src="/2023/06/06/20230527-VQVAE/5.jpg" alt></p>
<p>但是，这么做不是最高效的。得到离散编码后，下一步我们又要根据嵌入空间把离散编码转回一个向量。可见，获取离散编码这一步有一点多余。能不能把编码器的输出张量（它之前的名字叫logit）、解码器的输入张量embedding、嵌入空间直接关联起来呢？</p>
<p><img src="/2023/06/06/20230527-VQVAE/6.jpg" alt></p>
<p>VQ-VAE使用了如下方式关联编码器的输出与解码器的输入：假设嵌入空间已经训练完毕，对于编码器的每个输出向量$z_e(x)$，找出它在嵌入空间里的最近邻$z_q(x)$，把$z_e(x)$替换成$z_q(x)$作为解码器的输入。</p>
<blockquote>
<p>求最近邻，即先计算向量与嵌入空间$K$个向量每个向量的距离，再对距离数组取一个<code>argmin</code>，求出最近的下标（比如图中的0, 1, 1），最后用下标去嵌入空间里取向量。下标构成的数组（比如图中的[0, 1, 1]）也正是VQ-VAE的离散编码。</p>
</blockquote>
<p><img src="/2023/06/06/20230527-VQVAE/7.jpg" alt></p>
<p>就这样，我们知道了VQ-VAE是怎么生成离散编码的。VQ-VAE的编码器其实不会显式地输出离散编码，而是输出了多个「假嵌入」$z_e(x)$。之后，VQ-VAE对每个$z_e(x)$在嵌入空间里找最近邻，得到真正的嵌入$z_q(x)$，把$z_q(x)$作为解码器的输入。</p>
<p>虽然我们现在能把编码器和解码器拼接到一起，但现在又多出了一个问题：怎么让梯度从解码器的输入$z_q(x)$传到$z_e(x)$？从$z_e(x)$到$z_q(x)$的变换是一个从数组里取值的操作，这个操作是求不了导的。我们在下一小节里来详细探究一下怎么优化VQ-VAE的编码器和解码器。</p>
<h3 id="优化编码器和解码器"><a href="#优化编码器和解码器" class="headerlink" title="优化编码器和解码器"></a>优化编码器和解码器</h3><p>为了优化编码器和解码器，我们先来制订一下VQ-VAE的整体优化目标。由于VQ-VAE其实是一个AE，误差函数里应该只有原图像和目标图像的重建误差。</p>
<blockquote>
<p>或者非要从VAE的角度说也行。VQ-VAE相当于输出了一个one-hot离散分布。假设输入图像$x$的离散编码$z$是$k$，则分布中仅有$q(z=k|x)=1$，$q(z=others|x)=0$。令离散编码$z$的先验分布是均匀分布（假设不知道输入图像$x$，每个离散编码取到的概率是等同的），则先验分布$q(z)$和后验分布$q(z|x)$的KL散度是常量。因此，KL散度项不用算入损失函数里。理解此处的数学推导意义不大，还不如直接理解成VQ-VAE其实是一个AE。</p>
</blockquote>
<script type="math/tex; mode=display">
L_{reconstruct} = ||x - decoder(z_q(x))||_2^2</script><p>但直接拿这个误差来训练是不行的。误差中，$z_q(x)$是解码器的输入。从编码器输出$z_e(x)$到$z_q(x)$这一步是不可导的，误差无法从解码器传递到编码器上。要是可以把$z_q(x)$的梯度直接原封不动地复制到$z_e(x)$上就好了。</p>
<p>VQ-VAE使用了一种叫做”straight-through estimator”的技术来完成梯度复制。这种技术是说，前向传播和反向传播的计算可以不对应。你可以为一个运算随意设计求梯度的方法。基于这一技术，VQ-VAE使用了一种叫做$sg$(stop gradient，停止梯度)的运算：</p>
<script type="math/tex; mode=display">
sg(x) = \left\{
    \begin{aligned}
        &x  \ (in \ forward \ propagation)\\
        &0  \ (in \ backward \ propagation)
    \end{aligned}
\right.</script><p>也就是说，前向传播时，$sg$里的值不变；反向传播时，$sg$按值为0求导，即此次计算无梯度。（反向传播其实不会用到式子的值，只会用到式子的梯度。反向传播用到的loss值是在前向传播中算的）。</p>
<p>基于这种运算，我们可以设计一个把梯度从$z_e(x)$复制到$z_q(x)$的误差：</p>
<script type="math/tex; mode=display">
L_{reconstruct} = ||x - decoder(z_e(x) + sg(z_q(x) - z_e(x)))||_2^2</script><p>也就是说，前向传播时，就是拿解码器输入$z_q(x)$来算梯度。</p>
<script type="math/tex; mode=display">
L_{reconstruct} = ||x - decoder(z_q(x))||_2^2</script><p>而反向传播时，按下面这个公式求梯度，等价于把解码器的梯度全部传给$z_e(x)$。</p>
<script type="math/tex; mode=display">
L_{reconstruct} = ||x - decoder(z_e(x))||_2^2</script><p>这部分的PyTorch实现如下所示。在PyTorch里，<code>(x).detach()</code>就是$sg(x)$，它的值在前向传播时取<code>x</code>，反向传播时取<code>0</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L = x - decoder(z_e + (z_q - z_e).detach())</span><br></pre></td></tr></table></figure>
<p>通过这一技巧，我们完成了梯度的传递，可以正常地训练编码器和解码器了。</p>
<h3 id="优化嵌入空间"><a href="#优化嵌入空间" class="headerlink" title="优化嵌入空间"></a>优化嵌入空间</h3><p>到目前为止，我们的讨论都是建立在嵌入空间已经训练完毕的前提上的。现在，我们来讨论一下嵌入空间的训练方法。</p>
<p>嵌入空间的优化目标是什么呢？嵌入空间的每一个向量应该能概括一类编码器输出的向量，比如一个表示「青年」的向量应该能概括所有14-35岁的人的照片的编码器输出。因此，嵌入空间的向量应该和其对应编码器输出尽可能接近。如下面的公式所示，$z_e(x)$是编码器的输出向量，$z_q(x)$是其在嵌入空间的最近邻向量。</p>
<script type="math/tex; mode=display">
L_e = ||z_e(x) - z_q(x)||_2^2</script><p>但作者认为，编码器和嵌入向量的学习速度应该不一样快。于是，他们再次使用了停止梯度的技巧，把上面那个误差函数拆成了两部分。其中，$\beta$控制了编码器的相对学习速度。作者发现，算法对$\beta$的变化不敏感，$\beta$取0.1~2.0都差不多。</p>
<script type="math/tex; mode=display">
L_e = ||sg(z_e(x)) - z_q(x)||_2^2 + \beta||z_e(x) - sg(z_q(x))||_2^2</script><blockquote>
<p>其实，在论文中，作者分别讨论了上面公式里的两个误差。第一个误差来自字典学习算法里的经典算法Vector Quantisation(VQ)，也就是VQ-VAE里的那个VQ，它用于优化嵌入空间。第二个误差叫做专注误差，它用于约束编码器的输出，不让它跑到离嵌入空间里的向量太远的地方。</p>
</blockquote>
<p>这样，VQ-VAE总体的损失函数可以写成：（由于算上了重建误差，我们多加一个$\alpha$用于控制不同误差之间的比例）</p>
<script type="math/tex; mode=display">
\begin{aligned}
L = &||x - decoder(z_e(x) + sg(z_q(x) - z_e(x)))||_2^2 \\
&+ \alpha ||sg(z_e(x)) - z_q(x)||_2^2 + \beta||z_e(x) - sg(z_q(x))||_2^2
\end{aligned}</script><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>VQ-VAE是一个把图像编码成离散向量的图像压缩模型。为了让神经网络理解离散编码，VQ-VAE借鉴了NLP的思想，让每个离散编码值对应一个嵌入，所有的嵌入都存储在一个嵌入空间（又称”codebook”）里。这样，VQ-VAE编码器的输出是若干个「假嵌入」，「假嵌入」会被替换成嵌入空间里最近的真嵌入，输入进解码器里。</p>
<p>VQ-VAE的优化目标由两部分组成：重建误差和嵌入空间误差。重建误差为输入图片和重建图片的均方误差。为了让梯度从解码器传到编码器，作者使用了一种巧妙的停止梯度算子，让正向传播和反向传播按照不同的方式计算。嵌入空间误差为嵌入和其对应的编码器输出的均方误差。为了让嵌入和编码器以不同的速度优化，作者再次使用了停止梯度算子，把嵌入的更新和编码器的更新分开计算。</p>
<p>训练完成后，为了实现随机图像生成，需要对VQ-VAE的离散分布采样，再把采样出来的离散向量对应的嵌入输入进解码器。VQ-VAE论文使用了PixelCNN来采样离散分布。实际上，PixelCNN不是唯一一种可用的拟合离散分布的模型。我们可以把它换成Transformer，甚至是diffusion模型。如果你当年看完VQ-VAE后立刻把PixelCNN换成了diffusion模型，那么恭喜你，你差不多提前设计出了Stable Diffusion。</p>
<p>可见，VQ-VAE最大的贡献是提供了一种图像压缩思路，把生成大图像的问题转换成了一个更简单的生成「小图像」的问题。图像压缩成离散向量时主要借助了嵌入空间，或者说”codebook”这一工具。这种解决问题的思路可以应用到所有图像生成类任务上，比如超分辨率、图像修复、图像去模糊等。所以近两年我们能看到很多使用了codebook的图像生成类工作。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>PixelCNN的介绍可以参见我之前的文章：详解PixelCNN大家族。</p>
<p>VQ-VAE的论文为<em>Neural Discrete Representation Learning</em>。这篇文章不是很好读懂，建议直接读我的这篇解读。再推荐另一份还不错的中文解读 <a target="_blank" rel="noopener" href="https://www.spaces.ac.cn/archives/6760。">https://www.spaces.ac.cn/archives/6760。</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">132</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
