<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:type" content="website">
<meta property="og:title" content="周弈帆的博客">
<meta property="og:url" content="https://zhouyifan.net/page/5/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhou Yifan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/09/21/DLS-note-15-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/21/DLS-note-15-2/" class="post-title-link" itemprop="url">在 PyTorch 中借助 GloVe 词嵌入完成情感分析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-21 17:47:35" itemprop="dateCreated datePublished" datetime="2022-09-21T17:47:35+08:00">2022-09-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>词嵌入能够用更加有意义的向量表示单词。在NLP任务中使用预训练的词嵌入，往往能极大地加快训练效率。在这篇文章中，我将面向NLP初学者，分享一下如何在PyTorch中使用预训练的GloVe词嵌入，并借助它完成一个简单的NLP任务——情感分析。</p>
<p>相关的背景知识可以参考我之前有关词嵌入的文章：<a href>词嵌入 (Word2Vec, GloVe)</a></p>
<p>项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/SentimentAnalysis">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/SentimentAnalysis</a></p>
<h2 id="GloVe-词嵌入"><a href="#GloVe-词嵌入" class="headerlink" title="GloVe 词嵌入"></a>GloVe 词嵌入</h2><p>GloVe是一种学习词嵌入的方法，它希望拟合给定上下文单词$i$时单词$j$出现的次数$x_{ij}$。使用的误差函数为：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{N}\sum_{j=1}^{N}f(x_{ij})(\theta^{T}_je_i+b_i+b'_j-logx_{ij})</script><p>其中，$N$是词汇表大小，$\theta, b$是线性层参数，$e_i$是词嵌入。$f(x)$是权重项，用于平衡不同频率的单词对误差的影响，并消除$log0$时式子不成立的情况。</p>
<p>GloVe作者提供了官方的预训练词嵌入（<a target="_blank" rel="noopener" href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a> ）。预训练的GloVe有好几个版本，按数据来源，可以分成：</p>
<ul>
<li>维基百科+gigaword（6B）</li>
<li>爬虫（42B）</li>
<li>爬虫（840B）</li>
<li>推特（27B）</li>
</ul>
<p>其中，括号里的数字表示数据集的token数。</p>
<p>按照词嵌入向量的大小分，又可以分成50维、100维、200维等不同维度。</p>
<p>预训练GloVe的文件格式非常简明。一行表示一个单词向量，每行先是一个单词，再是若干个浮点数，表示该单词向量的每一个元素。</p>
<p>当然，在使用PyTorch时，我们不必自己去下载解析GloVe，而是可以直接调用PyTorch的库自动下载解析GloVe。首先，我们要安装PyTorch的NLP库——torchtext。</p>
<p>conda可以用下面的命令安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c pytorch torchtext</span><br></pre></td></tr></table></figure>
<p>pip可以直接安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchtext</span><br></pre></td></tr></table></figure>
<p>之后，在Python里运行下面的代码，就可以获取GloVe的类了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> GloVe</span><br><span class="line"></span><br><span class="line">glove = GloVe(name=<span class="string">&#x27;6B&#x27;</span>, dim=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p>如前文所述，GloVe的版本可以由其数据来源和向量维数确定。在构建GloVe类时，要提供这两个参数。最好是去GloVe的官网查好一个确定的版本，用该版本的参数构建这个GloVe类。我在这个项目中使用的是6B token，维度数100的GloVe。</p>
<p>调用<code>glove.get_vecs_by_tokens</code>，我们能够把token转换成GloVe里的向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get vectors</span></span><br><span class="line">tensor = glove.get_vecs_by_tokens([<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;1998&#x27;</span>, <span class="string">&#x27;199999998&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>], <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>
<p>PyTorch提供的这个函数非常方便。如果token不在GloVe里的话，该函数会返回一个全0向量。如果你运行上面的代码，可以观察到一些有趣的事：空字符串和199999998这样的不常见数字不在词汇表里，而1998这种常见的数字以及标点符号都在词汇表里。</p>
<p><code>GloVe</code>类内部维护了一个矩阵，即每个单词向量的数组。因此，<code>GloVe</code>需要一个映射表来把单词映射成向量数组的下标。<code>glove.itos</code>和<code>glove.stoi</code>完成了下标与单词字符串的相互映射。比如用下面的代码，我们可以知道词汇表的大小，并访问词汇表的前几个单词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">myvocab = glove.itos</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(myvocab))</span><br><span class="line"><span class="built_in">print</span>(myvocab[<span class="number">0</span>], myvocab[<span class="number">1</span>], myvocab[<span class="number">2</span>], myvocab[<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<p>最后，我们来通过一个实际的例子认识一下词嵌入的意义。词嵌入就是向量，向量的关系常常与语义关系对应。利用词嵌入的相对关系，我们能够回答“x1之于y1，相当于x2之于谁？”这种问题。比如，男人之于女人，相当于国王之于王后。设我们要找的向量为y2，我们想让x1-y1=x2-y2，即找出一个和x2-(x1-y1)最相近的向量y2出来。这一过程可以用如下的代码描述：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_counterpart</span>(<span class="params">x1, y1, x2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Find y2 that makes x1-y1=x2-y2&quot;&quot;&quot;</span></span><br><span class="line">    x1_id = glove.stoi[x1]</span><br><span class="line">    y1_id = glove.stoi[y1]</span><br><span class="line">    x2_id = glove.stoi[x2]</span><br><span class="line">    x1, y1, x2 = glove.get_vecs_by_tokens([x1, y1, x2], <span class="literal">True</span>)</span><br><span class="line">    target = x2 - x1 + y1</span><br><span class="line">    max_sim = <span class="number">0</span></span><br><span class="line">    max_id = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(myvocab)):</span><br><span class="line">        vector = glove.get_vecs_by_tokens([myvocab[i]], <span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">        cossim = torch.dot(target, vector)</span><br><span class="line">        <span class="keyword">if</span> cossim &gt; max_sim <span class="keyword">and</span> i <span class="keyword">not</span> <span class="keyword">in</span> &#123;x1_id, y1_id, x2_id&#125;:</span><br><span class="line">            max_sim = cossim</span><br><span class="line">            max_id = i</span><br><span class="line">    <span class="keyword">return</span> myvocab[max_id]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(get_counterpart(<span class="string">&#x27;man&#x27;</span>, <span class="string">&#x27;woman&#x27;</span>, <span class="string">&#x27;king&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(get_counterpart(<span class="string">&#x27;more&#x27;</span>, <span class="string">&#x27;less&#x27;</span>, <span class="string">&#x27;long&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(get_counterpart(<span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;banana&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>在函数<code>get_counterpart</code>中，我们遍历所有向量，根据cosine相似度，找一个和x2-x1+y1最相近的向量（除三个输入向量之外）。使用这个函数，我们可以回答以下三组问题：</p>
<ul>
<li>man-woman, king-<strong>queen</strong></li>
<li>more-less, long-<strong>short</strong></li>
<li>apple-red, banana-<strong>yellow</strong></li>
</ul>
<p>词嵌入确实非常神奇，连反义词、水果的颜色这种抽象关系都能记录。当然，这里我只挑选了几组成功的例子。这种算法并不能认出单词的比较级（good-better, bad-worse）等更抽象的关系。</p>
<p>通过这一节的实践，我们认识了GloVe的基本用法。接下来，我们来看看怎么用词嵌入完成情感分析任务。</p>
<h2 id="基于GloVe的情感分析"><a href="#基于GloVe的情感分析" class="headerlink" title="基于GloVe的情感分析"></a>基于GloVe的情感分析</h2><h3 id="情感分析任务与数据集"><a href="#情感分析任务与数据集" class="headerlink" title="情感分析任务与数据集"></a>情感分析任务与数据集</h3><p>和猫狗分类类似，情感分析任务是一种比较简单的二分类NLP任务：给定一段话，输出这段话的情感是积极的还是消极的。</p>
<p>比如下面这段话：</p>
<p>I went and saw this movie last night after being coaxed to by a few friends of mine. I’ll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. ……</p>
<p>这是一段影评，大意说，这个观众本来不太想去看电影，因为他认为演员Kutcher只能演好喜剧。但是，看完后，他发现他错了，所有演员都演得非常好。这是一段积极的评论。</p>
<p>再比如这段话：</p>
<p>This is a pale imitation of ‘Officer and a Gentleman.’ There is NO chemistry between Kutcher and the unknown woman who plays his love interest. The dialog is wooden, the situations hackneyed. </p>
<p>这段影评说，这部剧是对《军官与绅士》的一个拙劣的模仿。Kutcher和那个成为他心上人的路人女性之间没有产生任何“化学反应”。对话太死板，场景太陈腐了。这是一段消极的评论。</p>
<p>这些评论都选自斯坦福大学的<a target="_blank" rel="noopener" href="https://ai.stanford.edu/~amaas/data/sentiment/">大型电影数据集</a>。它收录了IMDb上的电影评论，正面评论和负面评论各25000条。这个数据集是情感分析中最为常用的数据集，多数新手在学习NLP时都会用它训练一个情感分析模型。我们这个项目也会使用这个数据集。</p>
<p>这个数据集的文件结构大致如下：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├─test</span><br><span class="line">│  ├─neg</span><br><span class="line">│  │  ├ 0_2.txt</span><br><span class="line">│  │  ├ 1_3.txt</span><br><span class="line">│  │  └ ...</span><br><span class="line">│  └─pos</span><br><span class="line">├─train</span><br><span class="line">│   ├─neg</span><br><span class="line">│   └─pos</span><br><span class="line">└─imdb.vocab</span><br></pre></td></tr></table></figure>
<p>其中，<code>imdb.vocab</code>记录了数据集中的所有单词，一行一个。<code>test</code>和<code>train</code>是测试集和训练集，它们的<code>neg</code>和<code>pos</code>子文件夹分别记录了负面评论和正面评论。每一条评论都是一句话，存在一个txt文件里。</p>
<p>使用下面这个函数，我们就可以读取一个子文件夹里的所有评论：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_imdb</span>(<span class="params"><span class="built_in">dir</span>=<span class="string">&#x27;data/aclImdb&#x27;</span>, split=<span class="string">&#x27;pos&#x27;</span>, is_train=<span class="literal">True</span></span>):</span></span><br><span class="line">    subdir = <span class="string">&#x27;train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;test&#x27;</span></span><br><span class="line">    <span class="built_in">dir</span> = os.path.join(<span class="built_in">dir</span>, subdir, split)</span><br><span class="line">    lines = []</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="built_in">dir</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(<span class="built_in">dir</span>, file), <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            line = f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            lines.append(line)</span><br><span class="line">    <span class="keyword">return</span> lines</span><br></pre></td></tr></table></figure>
<p>这里，顺便介绍一下torchtext提供的分词工具。在NLP中，我们在得到一段文本时，一般需要对文本做一步预处理操作，把一段话变成“单词”的数组。这里的“单词”即可以是英文单词，也可以是数字序列、标点符号。在NLP中，这步预处理操作称为分词，“单词”叫做token（中文直译是“符号，记号”）。</p>
<p>使用torchtext把一段话转换成token数组的方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer(<span class="string">&#x27;a, b&#x27;</span>))</span><br><span class="line"><span class="comment"># &gt;&gt; [&#x27;a&#x27;, &#x27;,&#x27;, &#x27;b&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>有了它，我们可以验证读取IMDb数据集和分词的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> get_tokenizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    lines = read_imdb()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Length of the file:&#x27;</span>, <span class="built_in">len</span>(lines))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lines[0]:&#x27;</span>, lines[<span class="number">0</span>])</span><br><span class="line">    tokenizer = get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">    tokens = tokenizer(lines[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lines[0] tokens:&#x27;</span>, tokens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Length of the file: 12500</span><br><span class="line">lines[0]: This is a very light headed comedy about a wonderful ...</span><br><span class="line">lines[0] tokens: [&#x27;this&#x27;, &#x27;is&#x27;, &#x27;a&#x27;, &#x27;very&#x27;, &#x27;light&#x27;, &#x27;headed&#x27;, &#x27;comedy&#x27;, &#x27;about&#x27;, &#x27;a&#x27;, &#x27;wonderful&#x27;, ...</span><br></pre></td></tr></table></figure>
<h3 id="获取经GloVe预处理的数据"><a href="#获取经GloVe预处理的数据" class="headerlink" title="获取经GloVe预处理的数据"></a>获取经GloVe预处理的数据</h3><p>在这个项目中，我们的模型结构十分简单：输入序列经过词嵌入，送入单层RNN，之后输出结果。整个项目最难的部分是如何把token转换成GloVe词嵌入。在这一节里，我将介绍一种非常简单的实现方法。</p>
<p>torchtext其实还提供了一些更方便的NLP工具类（<code>Field</code>, <code>Vectors</code>等），用于管理词向量。但是，这些工具需要一定的学习成本。由于本文的主旨是介绍深度学习技术而非PyTorch使用技巧，本项目不会用到这些更高级的类。如果你以后要用PyTorch完成NLP任务，建议看完本文后参考<a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2021/09/sentiment-analysis-with-lstm-and-torchtext-with-code-and-explanation/">相关文章</a>进一步学习torchtext的用法。</p>
<p>PyTorch通常用<code>nn.Embedding</code>来表示词嵌入层。<code>nn.Embedding</code>其实就是一个矩阵，每一行都是一个词嵌入。每一个token都是整型索引，表示该token在词汇表里的序号。有了索引，有了矩阵，就可以得到token的词嵌入了。</p>
<p>但是，有些token在词汇表中并不存在。我们得对输入做处理，把词汇表里没有的token转换成<code>&lt;unk&gt;</code>这个表示未知字符的特殊token。同时，为了对齐序列的长度，我们还得添加<code>&lt;pad&gt;</code>这个特殊字符。而用GloVe直接生成的<code>nn.Embedding</code>里没有<code>&lt;unk&gt;</code>和<code>&lt;pad&gt;</code>字符。如果使用<code>nn.Embedding</code>的话，我们要编写非常复杂的预处理逻辑。</p>
<p>为此，我们可以用<code>GloVe</code>类的<code>get_vecs_by_tokens</code>直接获取token的词嵌入，以代替<code>nn.Embedding</code>。回忆一下前文提到的<code>get_vecs_by_tokens</code>的使用结果，所有没有出现的token都会被转换成零向量。这样，我们就不必操心数据预处理的事情了。</p>
<p><code>get_vecs_by_tokens</code>应该发生在数据读取之后，它可以直接被写在<code>Dataset</code>的读取逻辑里。我为此项目编写的<code>Dataset</code>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> torchtext.data <span class="keyword">import</span> get_tokenizer</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> GloVe</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.SentimentAnalysis.read_imdb <span class="keyword">import</span> read_imdb</span><br><span class="line"></span><br><span class="line">GLOVE_DIM = <span class="number">100</span></span><br><span class="line">GLOVE = GloVe(name=<span class="string">&#x27;6B&#x27;</span>, dim=GLOVE_DIM)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IMDBDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, is_train=<span class="literal">True</span>, <span class="built_in">dir</span>=<span class="string">&#x27;data/aclImdb&#x27;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.tokenizer = get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">        pos_lines = read_imdb(<span class="built_in">dir</span>, <span class="string">&#x27;pos&#x27;</span>, is_train)</span><br><span class="line">        neg_lines = read_imdb(<span class="built_in">dir</span>, <span class="string">&#x27;neg&#x27;</span>, is_train)</span><br><span class="line">        self.lines = pos_lines + neg_lines</span><br><span class="line">        self.pos_length = <span class="built_in">len</span>(pos_lines)</span><br><span class="line">        self.neg_length = <span class="built_in">len</span>(neg_lines)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.pos_length + self.neg_length</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        sentence = self.tokenizer(self.lines[index])</span><br><span class="line">        x = GLOVE.get_vecs_by_tokens(sentence)</span><br><span class="line">        label = <span class="number">1</span> <span class="keyword">if</span> index &lt; self.pos_length <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> x, label</span><br></pre></td></tr></table></figure>
<p>数据预处理的逻辑都在<code>__getitem__</code>里。每一段字符串会先被token化，之后由<code>GLOVE.get_vecs_by_tokens</code>得到词嵌入数组。</p>
<h3 id="对齐输入"><a href="#对齐输入" class="headerlink" title="对齐输入"></a>对齐输入</h3><p>使用一个batch的序列数据时常常会碰到序列不等长的问题。在我的<a href>上篇RNN代码实战文章</a>中，我曾计算了序列的最大长度，并手动为每个序列都创建了一个最大长度的向量。实际上，利用PyTorch <code>DataLoader</code>的<code>collate_fn</code>机制，还有一些更简洁的实现方法。</p>
<p>在这个项目中，我们可以这样创建<code>DataLoader</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader</span>(<span class="params"><span class="built_in">dir</span>=<span class="string">&#x27;data/aclImdb&#x27;</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">batch</span>):</span></span><br><span class="line">        x, y = <span class="built_in">zip</span>(*batch)</span><br><span class="line">        x_pad = pad_sequence(x, batch_first=<span class="literal">True</span>)</span><br><span class="line">        y = torch.Tensor(y)</span><br><span class="line">        <span class="keyword">return</span> x_pad, y</span><br><span class="line"></span><br><span class="line">    train_dataloader = DataLoader(IMDBDataset(<span class="literal">True</span>, <span class="built_in">dir</span>),</span><br><span class="line">                                  batch_size=<span class="number">32</span>,</span><br><span class="line">                                  shuffle=<span class="literal">True</span>,</span><br><span class="line">                                  collate_fn=collate_fn)</span><br><span class="line">    test_dataloader = DataLoader(IMDBDataset(<span class="literal">False</span>, <span class="built_in">dir</span>),</span><br><span class="line">                                 batch_size=<span class="number">32</span>,</span><br><span class="line">                                 shuffle=<span class="literal">True</span>,</span><br><span class="line">                                 collate_fn=collate_fn)</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, test_dataloader</span><br></pre></td></tr></table></figure>
<p>PyTorch <code>DataLoader</code>在获取<code>Dataset</code>的一个batch的数据时，实际上会先调用<code>Dataset.__getitem__</code>，获取若干个样本，再把所有样本拼接成一个batch。比如用<code>__getitem__</code>获取4个<code>[3, 10, 10]</code>的图片张量，再拼接成<code>[4, 3, 10, 10]</code>这一个batch。可是，序列数据通常长度不等，<code>__getitem__</code>可能会获得<code>[10, 100]</code>, <code>[15, 100]</code>这样不等长的词嵌入数组。</p>
<p>为了解决这个问题，我们要手动编写把所有张量拼成一个batch的函数。这个函数就是<code>DataLoader</code>的<code>collate_fn</code>函数。我们的<code>collate_fn</code>应该这样编写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">batch</span>):</span></span><br><span class="line">    x, y = <span class="built_in">zip</span>(*batch)</span><br><span class="line">    x_pad = pad_sequence(x, batch_first=<span class="literal">True</span>)</span><br><span class="line">    y = torch.Tensor(y)</span><br><span class="line">    <span class="keyword">return</span> x_pad, y</span><br></pre></td></tr></table></figure>
<p><code>collate_fn</code>的输入<code>batch</code>是每次<code>__getitem__</code>的结果的数组。比如在我们这个项目中，第一次获取了一个长度为10的积极的句子，<code>__getitem__</code>返回<code>(Tensor[10, 100], 1)</code>；第二次获取了一个长度为15的消极的句子，<code>__getitem__</code>返回<code>(Tensor[15, 100], 0)</code>。那么，输入<code>batch</code>的内容就是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(Tensor[<span class="number">10</span>, <span class="number">100</span>], <span class="number">1</span>), (Tensor[<span class="number">15</span>, <span class="number">100</span>], <span class="number">0</span>)]</span><br></pre></td></tr></table></figure>
<p>我们可以用<code>x, y = zip(*batch)</code>把它巧妙地转换成两个元组：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = (Tensor[<span class="number">10</span>, <span class="number">100</span>], Tensor[<span class="number">15</span>, <span class="number">100</span>])</span><br><span class="line">y = (<span class="number">1</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>之后，PyTorch的<code>pad_sequence</code>可以把不等长序列的数组按最大长度填充成一整个batch张量。也就是说，经过这个函数后，<code>x_pad</code>变成了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_pad = Tensor[<span class="number">2</span>, <span class="number">15</span>, <span class="number">100</span>]</span><br></pre></td></tr></table></figure>
<p><code>pad_sequence</code>的<code>batch_first</code>决定了<code>batch</code>是否在第一维。如果它为<code>False</code>，则结果张量的形状是<code>[15, 2, 100]</code>。</p>
<p><code>pad_sequence</code>还可以决定填充内容，默认填充0。在我们这个项目中，被填充的序列已经是词嵌入了，直接用全零向量表示<code>&lt;pad&gt;</code>没问题。</p>
<p>有了<code>collate_fn</code>，构建<code>DataLoader</code>就很轻松了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(IMDBDataset(<span class="literal">True</span>, <span class="built_in">dir</span>),</span><br><span class="line">            batch_size=<span class="number">32</span>,</span><br><span class="line">            shuffle=<span class="literal">True</span>,</span><br><span class="line">            collate_fn=collate_fn)</span><br></pre></td></tr></table></figure>
<p>注意，使用<code>shuffle=True</code>可以令<code>DataLoader</code>随机取数据构成batch。由于我们的<code>Dataset</code>十分工整，前一半的标签是1，后一半是0，必须得用随机的方式去取数据以提高训练效率。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型非常简单，就是单层RNN：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">64</span>, dropout_rate=<span class="number">0.5</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.drop = nn.Dropout(dropout_rate)</span><br><span class="line">        self.rnn = nn.GRU(GLOVE_DIM, hidden_units, <span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.linear = nn.Linear(hidden_units, <span class="number">1</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># x shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">        emb = self.drop(x)</span><br><span class="line">        output, _ = self.rnn(emb)</span><br><span class="line">        output = output[:, -<span class="number">1</span>]</span><br><span class="line">        output = self.linear(output)</span><br><span class="line">        output = self.sigmoid(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这里要注意一下，PyTorch的RNN会返回整个序列的输出。而在预测分类概率时，我们只需要用到最后一轮RNN计算的输出。因此，要用<code>output[:, -1]</code>取最后一次的输出。 </p>
<h3 id="训练、测试、推理"><a href="#训练、测试、推理" class="headerlink" title="训练、测试、推理"></a>训练、测试、推理</h3><p>项目的其他地方都比较简单，我把剩下的所有逻辑都写到<code>main</code>函数里了。</p>
<p>先准备好模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    device = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">    train_dataloader, test_dataloader = get_dataloader()</span><br><span class="line">    model = RNN().to(device)</span><br></pre></td></tr></table></figure>
<p>第一步是训练。训练照着普通RNN的训练模板写就行，没什么特别的。注意，在PyTorch中，使用二分类误差时，要在模型里用<code>nn.Sigmoid</code>，并使用<code>nn.BCELoss</code>作为误差函数。算误差前，得把序列长度那一维去掉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train</span></span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    citerion = torch.nn.BCELoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line"></span><br><span class="line">        loss_sum = <span class="number">0</span></span><br><span class="line">        dataset_len = <span class="built_in">len</span>(train_dataloader.dataset)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> train_dataloader:</span><br><span class="line">            batchsize = y.shape[<span class="number">0</span>]</span><br><span class="line">            x = x.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            hat_y = model(x)</span><br><span class="line">            hat_y = hat_y.squeeze(-<span class="number">1</span>)</span><br><span class="line">            loss = citerion(hat_y, y)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            loss_sum += loss * batchsize</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;dldemos/SentimentAnalysis/rnn.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>训练几十个epoch，模型就差不多收敛了。词嵌入对于训练还是有很大帮助的。</p>
<p>训练完了，接下来要测试精度。这些代码也很简单，跑完了模型和0.5比较得到预测结果，再和正确标签比较算一个准确度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model.load_state_dict(</span></span><br><span class="line"><span class="comment">#     torch.load(&#x27;dldemos/SentimentAnalysis/rnn.pth&#x27;, &#x27;cuda:0&#x27;))</span></span><br><span class="line"></span><br><span class="line">accuracy = <span class="number">0</span></span><br><span class="line">dataset_len = <span class="built_in">len</span>(test_dataloader.dataset)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> test_dataloader:</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    y = y.to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        hat_y = model(x)</span><br><span class="line">    hat_y.squeeze_(<span class="number">1</span>)</span><br><span class="line">    predictions = torch.where(hat_y &gt; <span class="number">0.5</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    score = torch.<span class="built_in">sum</span>(torch.where(predictions == y, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">    accuracy += score.item()</span><br><span class="line">accuracy /= dataset_len</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>我的精度达到了90%多。考虑到模型并不复杂，且并没有用验证集进行调参，这个精度已经非常棒了。</p>
<p>训练完了模型，我们来看看模型能不能在实际应用中排上用场。我去最近的财经新闻里摘抄了几句对美股的评论：</p>
<p>U.S. stock indexes fell Tuesday, driven by expectations for tighter Federal Reserve policy and an energy crisis in Europe. Stocks around the globe have come under pressure in recent weeks as worries about tighter monetary policy in the U.S. and a darkening economic outlook in Europe have led investors to sell riskier assets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inference</span></span><br><span class="line">tokenizer = get_tokenizer(<span class="string">&#x27;basic_english&#x27;</span>)</span><br><span class="line">article = ...</span><br><span class="line"></span><br><span class="line">x = GLOVE.get_vecs_by_tokens(tokenizer(article)).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    hat_y = model(x)</span><br><span class="line">hat_y = hat_y.squeeze_().item()</span><br><span class="line">result = <span class="string">&#x27;positive&#x27;</span> <span class="keyword">if</span> hat_y &gt; <span class="number">0.5</span> <span class="keyword">else</span> <span class="string">&#x27;negative&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<p>评论说，受到联邦政府更紧缩的保守经济政策和欧洲能源危机的影响，美国股市指数在周二下跌。近几周，全球股市都笼罩在对美国更紧缩的经济政策的担忧压力之下，欧洲灰暗的经济前景令投资者选择抛售高风险的资产。这显然是一段消极的评论。我的模型也很明智地输出了”negative”。看来，情感分析模型还是能在实际应用中发挥用场的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我介绍了GloVe词嵌入在PyTorch的一个应用。如果你只是想学习深度学习，建议多关注一下词嵌入的意义，不需要学习过多的API。如果你正在入门NLP，建议从这个简单的项目入手，体会一下词嵌入的作用。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/09/21/DLS-note-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/21/DLS-note-14/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十四）：循环神经网络基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-21 17:47:24" itemprop="dateCreated datePublished" datetime="2022-09-21T17:47:24+08:00">2022-09-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上一门课中，我们学习了如何用CNN处理网格状的数据。由于最常见的网格状数据是图像，我们主要学习了如何用CNN完成和图像相关的任务。而在这一门课中，我们要学习如何用循环神经网络（RNN）等序列模型处理序列数据。序列数据的种类就比较丰富多彩了：</p>
<p><img src="/2022/09/21/DLS-note-14/1.jpg" alt></p>
<p>如上图所示，对于语音识别、音乐生成、情绪分类、DNA序列分析、机器翻译、视频动作识别、命名实体识别这些任务，它们的输入和输出至少有一个是某类序列数据，它们都可以用序列模型来建模。</p>
<p>计算机科学中的自然语言处理（NLP）任务常常需要使用序列模型。我们在学这门课时，主要会围绕NLP问题进行讨论。</p>
<h2 id="符号标记"><a href="#符号标记" class="headerlink" title="符号标记"></a>符号标记</h2><p>序列数据需要用到一些新的符号标记。在开始正式学习之前，我们先以命名实体识别任务为例，认识一下这些新的符号标记。</p>
<p>命名实体识别任务要求找出句子中的有意义的人名、地名等特殊名词。以这个任务的一个训练样本为例，我们来看一看序列数据的符号标记。</p>
<p>设输入是 Harry Potter and Hermione Granger invented a new spell。</p>
<p>命名实体识别任务的输出$y$也是一个序列。序列的每一个元素是1或0，表示输入中对应的单词是否为命名实体。刚刚那个输入对应的输出应该是1 1 0 1 1 0 0 0 0。</p>
<p>输入输出都由单词或数字组成的序列。设输入为$x$，序列中的第$i$个单词记作$x^{&lt;  i  &gt;}$。比如$x^{&lt; 2 &gt;}$ = Potter。整个序列的长度$T_x=9$。同理，输出序列中的第$i$个元素记作$y^{&lt; i &gt;}$，整个序列的长度$T_y=9$。</p>
<p>对于第$i$个样本而言，其输入的长度为$T_x^{(i)}$，第$t$个单词为$x^{(i)&lt; t &gt;}$。值得注意的是，每个样本的长度可能不一致。</p>
<p>在表示这些数据时，还有一个问题：怎么表示一个单词？计算机可不认识一大堆字母。为了让只懂数字的计算机能够分清不同的单词，我们要先准备一个词汇表，并用单词在词汇表里的one-hot编码作为单词的表示。比如Harry是长度为10000的词汇表里第4075个单词，则Harry的表示是$[0, 0, 0…,0, 1, 0, …, 0]$，这个向量的长度是10000，只有第4075个元素是1，其他地方都是0。</p>
<blockquote>
<p>在大型的模型中，词汇表的大小会是30000至50000，甚至有100000的。</p>
</blockquote>
<p>有了这些符号标记，和序列数据有关的任务就可以完全用数学语言表示了。比如对于命名实体识别任务，每一条样本输入是一个向量序列，输出是一个01的数字序列。</p>
<h2 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h2><p>在序列问题上使用标准神经网络（全连接网络）会有几个问题：</p>
<ul>
<li>每个样本的长度可能不一致。尽管我们可以找到一个最大的长度，并对长度不足的样本进行零填充，但这种做法看上去就不太好。</li>
<li>同一个单词在不同的位置时应该算出类似的特征。而标准神经网络会把每个位置的输入区别对待，无法共享各个位置的知识。</li>
<li>和图像数据类似，序列数据的输入长度也很大。假如一个句子有10个单词，词汇表的大小是10000，则输入向量的大小就是100000。这样，神经网络第一层的参数量会很大，网络会过拟合。</li>
</ul>
<p>因此，我们要用一些其他的架构来处理序列数据以规避这些问题。其中一种可行的架构就是循环神经网络(Recurrent Neural Network, RNN)。</p>
<p>RNN运算过程如下图所示。在RNN中，对于一个样本，我们每次只输入一个单词$x^{&lt; t &gt;}$，得到一个输出$y^{&lt; t &gt;}$。除了输出$y^{&lt; t &gt;}$外，神经网络还会把中间激活输出$a^{&lt; t &gt;}$传递给下一轮计算，这个$a^{&lt; t &gt;}$记录了之前单词的某些信息。所有的输出按照这种方法依次计算。当然，第一轮计算时也会用到激活输出$a^{&lt; 0 &gt;}$，简单地令$a^{&lt; 0 &gt;}$为零张量即可。注意，所有的计算都是用同一个权重一样的神经网络。</p>
<p><img src="/2022/09/21/DLS-note-14/1.gif" alt></p>
<blockquote>
<p>有些文章会把一行RNN计算折叠成一个带循环箭头的神经网络，这只是另一种画图的方法：</p>
</blockquote>
<p><img src="/2022/09/21/DLS-note-14/2.jpg" alt></p>
<p>看完了示意图，让我们看看怎样用数学语言表示RNN。在第$t$轮计算中：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{ax} x^{< t >} + W_{aa} a^{< t - 1 >} + b_a) \\
\hat{y}^{< t >} &=g_2(W_{ya} a^{< t >} + b_y)
\end{aligned}</script><p>其中，$W_{ij}$表示用于计算$i$的，乘在$j$上的矩阵。$g$是激活函数。中间层激活函数多数情况下用tanh，也可以用ReLU。输出的激活函数视任务而定，在二分类的命名实体识别中，输出激活函数是sigmoid。</p>
<p>为了简化表示，可以把$W_{ax}, W_{aa}$拼一下，$x^{&lt; t &gt;}, a^{&lt; t - 1 &gt;}$拼一下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
W_a &= [W_{aa} | W_{ax}] \\
[a^{< t - 1 >}, x^{< t >}] &= \left[
  \begin{matrix}
  a^{< t - 1 >} \\
  --- \\
   x^{< t >}
  \end{matrix}
\right]
\end{aligned}</script><p>这样，原来的式子就可以化简了：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{a} [a^{< t - 1 >}, x^{< t >}] + b_a) \\
\hat{y}^{< t >} &=g_2(W_{y} a^{< t >} + b_y)
\end{aligned}</script><p>RNN也有一些问题。首先，显而易见，每个RNN的输出只能看到它之前的单词。这样是不太好的。比如一句话第一个单词是Teddy，你不知道这是一个人名，还是Teddy bear这样一个普通的物体。稍后我们会学习双向的RNN以解决此问题。</p>
<p>另外，RNN也会面临标准神经网络的梯度爆炸问题。后几节会介绍一些更高级的RNN架构。</p>
<h2 id="「穿越时空之反向传播」"><a href="#「穿越时空之反向传播」" class="headerlink" title="「穿越时空之反向传播」"></a>「穿越时空之反向传播」</h2><p>看完了正向传播，我们稍微看一下RNN反向传播的过程。</p>
<p>反向传播前，先要定义一个误差函数。对于命名实体识别这种结果是01的问题，可以继续采用交叉熵误差，即对于序列中每一个元素：</p>
<script type="math/tex; mode=display">
L^{< t >}(y^{< t >}, \hat{y}^{< t >}) = -(y^{< t >}log\hat{y}^{< t >}+(1 - y^{< t >})log(1 - \hat{y}^{< t >}))</script><p>对于一个样本：</p>
<script type="math/tex; mode=display">
L(y, \hat{y}) = -\Sigma_{t=1}^{T_y}(y^{< t >}log\hat{y}^{< t >}+(1 - y^{< t >})log(1 - \hat{y}^{< t >}))</script><p>接下来是反向传播的过程。RNN的计算虽然复杂了一些，但它本质上还是一个计算图。如下图所示，按照红色箭头的方向对变量反向求导即可：</p>
<p><img src="/2022/09/21/DLS-note-14/3.jpg" alt></p>
<p>使用了编程框架后，反向传播可以自动由框架完成，可以不需要关心里面的细节了。</p>
<p>由于序列数据有先后的概念，而RNN的反向传播又是从后面的数据向前面的数据进行，因此这样的反向传播有着「穿越时空之反向传播」的称呼。</p>
<h2 id="不同输入输出格式的RNN"><a href="#不同输入输出格式的RNN" class="headerlink" title="不同输入输出格式的RNN"></a>不同输入输出格式的RNN</h2><p>刚刚学习的那种RNN只能描述输入输出长度一致的任务。在那种架构的基础上稍作修改，我们就能得到描述各种输入输出格式的任务。</p>
<p><img src="/2022/09/21/DLS-note-14/4.jpg" alt></p>
<ul>
<li>一对一：其实一对一问题就是标准神经网络，可以不要那个激活输入$a$。</li>
<li>一对多：只把输入放入第一轮计算中，后续计算的输入是上一轮的输出。</li>
<li>多对一：只输出最后一轮计算的结果。</li>
<li>等长多对多：输入一个元素就输出一个元素。</li>
<li>不等长多对多：先做几轮不产生输出的计算（编码器），再做几轮只产生输出的计算（解码器）。解码器的输入也可以和一对多一样，来自于上一轮的输出（图中没有画出）。</li>
</ul>
<h2 id="RNN应用：语言模型"><a href="#RNN应用：语言模型" class="headerlink" title="RNN应用：语言模型"></a>RNN应用：语言模型</h2><p>为了加深对RNN的理解，我们来看一个基于RNN的应用——语言模型。</p>
<p>语言模型是NLP中的一个基础任务。一个语言模型能够输出某种语言某句话的出现概率。通过比较不同句子的出现概率，我们能够开发出很多应用。比如在英语里，同音的”apple and pear”比”apple and pair”的出现概率高（更可能是一个合理的句子）。当一个语音识别软件听到这句话时，可以分别写下这两句发音相近的句子，再根据语言模型断定这句话应该写成前者。</p>
<p>也就是说，对于一句话$x^{&lt; 1 &gt;}…x^{&lt; T_x &gt;}$，语言模型的输出是$P(x^{&lt; 1 &gt;},…, x^{&lt; T_x &gt;})$。这个式子也可以写成$P(x^{&lt; 1 &gt;}) \times P(x^{&lt; 2 &gt;} |x^{&lt; 1 &gt;}) \times  P(x^{&lt; 3 &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}) … \times  P(x^{&lt; T_x &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}, …, x^{&lt; T_x-1 &gt;})$，即一句话的出现概率，等于第一个单词出现在句首的概率，乘上第二个单词在第一个单词之后的概率，乘上第三个单词再第一、二个单词之后的概率，这样一直乘下去。</p>
<p>在训练语言模型时，我们一般要用到语料库(corpus)。语料库包含了某种语言大量的通顺的句子。我们希望一个模型能够学习到这些句子中的规律，知道一句话在这种语言中的出现概率是多少。</p>
<p>语言模型可以用RNN巧妙地实现。整个实现分两步：数据预处理和模型训练。</p>
<p>由于语料库中包含的是自然语言，而RNN的输入是one-hot编码，所以这中间要经过一个预处理的步骤。在NLP中，这一步骤叫做符号化(tokenize)。如我们在「符号标记」一节所学的，我们可以找来一个大小为10000的词汇表，根据每个单词在词汇表中的位置，生成一个one-hot编码。除了普通的词汇外，NLP中还有一些特殊的符号，比如表示句尾的<code>&lt;EOS&gt;</code> (End Of Sentence)，表示词汇表里没有的词的<code>&lt;UNK&gt;</code> (Unknown)。</p>
<p>经过预处理后，语料库里的每一句自然语言就变成了训练样本$x^{&lt; 1 &gt;} … x^{&lt; T_x &gt;}$。我们可以把每一句话输入RNN，巧妙地训练一个语言模型：</p>
<p><img src="/2022/09/21/DLS-note-14/5.jpg" alt></p>
<p>这个计算过程初次接触时有些令人费解，我们慢慢来看懂它。先竖着看一轮计算是怎么完成的。对于每一轮计算，都会给定一个单词编码$x^{&lt; i &gt;}$，输出一个softmax后的概率分布$\hat{y}^{&lt; i &gt;}$，它要对齐的训练标签是训练集某一句话的某个单词$y^{&lt; i &gt;}$。$\hat{y}$表示接收之前所有的输入单词后，此时刻应该输出某单词的概率分布，这个输出的含义和多分类中的类似。</p>
<p>算出这样的$\hat{y}$有什么用呢？别急，再横着看一遍。回忆一下，语言模型要求的概率可以写成$P(y^{&lt; 1 &gt;}) \times P(y^{&lt; 2 &gt;} |y^{&lt; 1 &gt;}) \times  P(y^{&lt; 3 &gt;} |y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;}) …$。RNN每一轮的输出，其实就是要拟合$P(y^{&lt; 1 &gt;})$, $P(y^{&lt; 2 &gt;} |y^{&lt; 1 &gt;})$, $P(y^{&lt; 3 &gt;} |y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;})$, …。每一个条件概率的条件，就是每一轮RNN的输入；每一个条件概率的待求事件，就是每一轮RNN的训练标签。比如$P(y^{&lt; 3 &gt;} |y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;})$这个条件概率，它的条件是$y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;}$，待求事件是$y^{&lt; 3 &gt;}$，所以第三轮RNN的标签是$y^{&lt; 3 &gt;}$，输入是$y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;}$（别忘了，在RNN中，前几轮的输入其实也影响了后续的计算）。当然，第一个概率$P(y^{&lt; 1 &gt;})$没有条件，所以第一轮的输入$x^{&lt; 1 &gt;}=0$。对softmax的结果依然使用的是交叉熵误差，一个序列的误差等于所有元素的误差之和。</p>
<p>刚刚介绍的是训练过程。在用这个模型计算某句子的概率时，只要把一个句子输入进这个RNN，再去softmax的概率分布里取出需要的概率，一乘，就能算出语言模型要求的整句话的概率了。比如<code>Cats average 15 hours of sleep a day. &lt; EOS &gt;</code>这个句子，我们要令$x^{&lt; 1 &gt;}=0$, $x^{&lt; 2 &gt;}=one_ hot(cats)$, $x^{&lt; 3 &gt;}=one_ hot(average)$，……。然后，从第一个输出概率分布$\hat{y}^{&lt; 1 &gt;}$里找出cats对应的概率，去$\hat{y}^{&lt; 2 &gt;}$里找到average对应的概率，去$\hat{y}^{&lt; 3 &gt;}$里找到15对应的概率，以此类推。最后把所有的概率乘起来。</p>
<p>通过这一节，我们学到了RNN的一种应用。是否真正理解语言模型这一任务，并不重要。重要的是，我们学到了RNN是怎么巧妙去完成一项任务的。在完成和序列数据有关的任务时，我们要精心定义RNN的输入序列和输出序列。一旦这两个序列定义好了，训练模型并解决任务就是很轻松的事情。</p>
<h2 id="用语言模型采样出全新的序列"><a href="#用语言模型采样出全新的序列" class="headerlink" title="用语言模型采样出全新的序列"></a>用语言模型采样出全新的序列</h2><p>给定一个别人训练好的RNN语言模型，我们可以弄出一个很好玩的应用：生成一个训练集里没有的句子。</p>
<p>我们刚刚学过，在计算一句话的概率时，RNN会把句子里的每一个单词输入，输出单词出现在前几个单词之后的概率分布$\hat{y}$。反过来想，我们可以根据RNN输出的概率分布，随机采样出某一个单词的下一个单词出来。具体来说，我们先随机生成句子里的第一个单词，把它输入RNN。再用RNN生成概率分布，对概率分布采样出下一个单词，采样出一个单词就输入一个单词，直到采样出<code>&lt; EOS &gt;</code>。这个过程就好像是在让AI生成句子一样。</p>
<blockquote>
<p>对概率分布采样，其实就是以某种概率随机挑选。比如我有两个骰子，我要计算两个骰子点数之和。这个点数之和就是一个概率分布，掷一轮骰子就是去分布里采样。我们可以快速地算出，点数之和为2的概率是$\frac{1}{36}$, 点数之和为3的概率是$\frac{2}{36}$。也就是说，我们在采样时，有$\frac{1}{36}$的概率取到2，$\frac{2}{36}$的概率取到3。</p>
</blockquote>
<p>如果把语料库的最小单元从单词换成字母，句子生成就变成了单词生成，我们可以让AI生成出从没出现过却看上去很合理的单词。</p>
<h2 id="RNN-的梯度问题"><a href="#RNN-的梯度问题" class="headerlink" title="RNN 的梯度问题"></a>RNN 的梯度问题</h2><p>在前几门课中，我们曾学过，过深的神经网络会有梯度过大/过小的问题。这些问题在RNN中也存在，毕竟RNN一般都是用来处理很长的序列数据的。</p>
<p>梯度过大的问题倒是有办法解决：设置一个梯度最大值，让所有梯度都不能超过这个值。</p>
<p>梯度过小的问题比较麻烦。想象一个很长的句子:The <strong>cat</strong>, which ate apples, pears, …., <strong>was</strong> full.这个cat和was存在着依赖关系。一旦梯度过小，一个句子前后的依赖关系就不是那么好传递了。</p>
<p>下面几节我们会学一些解决梯度问题的架构。</p>
<h2 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU (Gated Recurrent Unit)"></a>GRU (Gated Recurrent Unit)</h2><p>我们可以用GRU (Gated Recurrent Unit)来替代标准RNN中的计算单元，以解决梯度问题。</p>
<p>为了明确标准RNN的哪些模块被替换了，我们先回顾一下RNN原来的计算单元。</p>
<p><img src="/2022/09/21/DLS-note-14/6.jpg" alt></p>
<p>在标准的RNN单元中，$x^{&lt; t &gt;}$和$a^{&lt; t-1 &gt;}$一起决定了$a^{&lt; t &gt;}$，$a^{&lt; t &gt;}$又决定了$\hat{y}^{&lt; t &gt;}$。</p>
<p>先在脑中对这张图有个印象，稍后我们会看到GRU是怎样改进这个计算单元的。</p>
<p>上一节里，我们分析过，梯度消失会导致一句话后面的单词忘掉了前面的单词。那么，可不可以让网络的“记性”更好一点呢？我们可以参考一下人类的记忆行为。比如，当我们在接到了验证码短信后，要把验证码在脑子里记一段时间：读完了短信，要记住验证码；关闭短信应用，要记住验证码；打开需要验证码的应用，要记住验证码；输入验证吗时，要记住验证码；输完了验证码，总算可以忘记验证码了。这个过程中，我们一直在维护“验证码”这个信息，决定是记住它还是忘记它。我们可以让神经网络模型也按照这种思路记忆之前的信息。</p>
<p>在每轮计算更新中间变量$a$时，GRU还要使用到一个新的变量，表示中间变量$a$该不该忘记。这个变量越靠近0，就说明越应该保持之前的中间变量；越靠近1，就越靠近新的$a$。GRU的这个变量起到了电路中逻辑门的效果（GRU的第一个单词是gated）。</p>
<p>具体来说，一个简化版GRU的计算过程用数学符号表示如下：</p>
<p>我们把中间变量$a$临时更名为$c$，表示记忆单元memory cell。和之前的$a$一样，我们每轮要算一个新的$\tilde{c}$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{c}^{< t >} &= tanh(W_{c} [c^{< t - 1 >}, x^{< t >}] + b_c)
\end{aligned}</script><p>而同时，我们还要算一个决定是不是要用$\tilde{c}$去更新过去的$c$的“逻辑门” $\Gamma_u$:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Gamma_u &= sigmoid(W_{u} [c^{< t - 1 >}, x^{< t >}] + b_u)
\end{aligned}</script><p>注意，$\Gamma_u$和逻辑门不同，不是真的只能取0或1，而是取0~1中一个中间的值，表示更新的程度。</p>
<p>之后，每一轮的$c^{&lt; t &gt;}$是这样更新的：</p>
<script type="math/tex; mode=display">
c^{< t >} = \Gamma_u \ast \tilde{c}^{< t >} + (1 - \Gamma_u) \ast c^{< t - 1 >}</script><p>它的图示如下，其中紫色部分是更新操作。</p>
<p><img src="/2022/09/21/DLS-note-14/7.jpg" alt></p>
<p>在完整的GRU中，计算公式稍微复杂一点：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{c}^{< t >} &= tanh(W_{c} [\Gamma_r \ast c^{< t - 1 >}, x^{< t >}] + b_c) \\
\Gamma_u &= sigmoid(W_{u} [c^{< t - 1 >}, x^{< t >}] + b_u) \\
\Gamma_r &= sigmoid(W_{r} [c^{< t - 1 >}, x^{< t >}] + b_r) \\
c^{< t >} &= \Gamma_u \ast \tilde{c}^{< t >} + (1 - \Gamma_u) \ast c^{< t - 1 >}
\end{aligned}</script><p>唯一的区别是$c^{&lt; t - 1 &gt;}$多过了一道$\Gamma_r$。这种设计的好处很难从理论上解释。当时的研究者试了很多类似的GRU架构，最后发现这样的GRU是效果最好的。</p>
<h2 id="LSTM-long-short-term-memory-单元"><a href="#LSTM-long-short-term-memory-单元" class="headerlink" title="LSTM (long short term memory) 单元"></a>LSTM (long short term memory) 单元</h2><p>LSTM单元是另一种改进版的RNN单元。LSTM的核心思想和GRU一模一样，也是使用门来控制记忆变量的更新幅度，只是公式更复杂了一点。在LSTM中，要传递的中间变量有两个：$c$和$a$，使用的输出门也从2个增加到了3个。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{c}^{< t >} &= tanh(W_{c} [\Gamma_r \ast a^{< t - 1 >}, x^{< t >}] + b_c) \\
\Gamma_u &= sigmoid(W_{u} [a^{< t - 1 >}, x^{< t >}] + b_u) \\
\Gamma_f &= sigmoid(W_{f} [a^{< t - 1 >}, x^{< t >}] + b_f) \\
\Gamma_o &= sigmoid(W_{o} [a^{< t - 1 >}, x^{< t >}] + b_o) \\
c^{< t >} &= \Gamma_u \ast \tilde{c}^{< t >} + \Gamma_f \ast c^{< t - 1 >} \\
a^{< t >} &= \Gamma_o 
\ast tanh (c^{< t >})
\end{aligned}</script><p><img src="/2022/09/21/DLS-note-14/8.jpg" alt></p>
<blockquote>
<p>我觉得看图不如看公式看得清楚。</p>
</blockquote>
<p>我们不需要刻意去记LSTM的结构，也不要纠结为什么要在哪个地方用哪个门，只需要知道LSTM和GRU的区别，会用它们就行了。</p>
<p>虽然LSTM比GRU更复杂，但实际上LSTM很早（1997年）就有了，GRU是近几年才有的。二者的效果并没有显著的差别，一般认为LSTM功能更强大，GRU计算速度更快。碰到新任务无脑用LSTM即可，而如果要构建较大的网络则可以考虑使用性能更好的GRU。</p>
<h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p>之前提过，标准的RNN有一个问题：先出现的单词无法获取后续单词的信息。比如句首单词是Teddy，你不知道这是“泰迪熊”还是“泰迪”这个人名。为了解决这个问题，我们可以升级一下RNN的基础架构，使用双向RNN（BRNN）。在这个新架构下，GRU和LSTM的单元可以照用，不受影响。</p>
<p>BRNN的示意图如下：</p>
<p><img src="/2022/09/21/DLS-note-14/9.jpg" alt></p>
<p>假设一句话有4个单词。在BRNN中，除了会先从1-4正着输入一遍序列外，还会从4-1倒着输入一遍序列。正着传的中间变量叫$\overrightarrow{a}$，倒着传的中间变量叫做$\overleftarrow{a}$。每一轮输出满足$\hat{y}=g(W_y[ \overrightarrow{a}, \overleftarrow{a}  ] + b_y)$。</p>
<p>BRNN+LSTM通常是一个新序列任务的标配。当然，BRNN也有一个缺点：必须等一个序列输入完了才能返回结果，而不能实时返回结果。在语音识别等实时性较强的任务里，可能普通RNN更合适一点。</p>
<h2 id="深层RNN"><a href="#深层RNN" class="headerlink" title="深层RNN"></a>深层RNN</h2><p>到目前为止，我们学的RNN都是由几个简单的矩阵运算构成的，似乎和这套课的标题“深度学习”不沾边。实际上，也可以给基础的RNN多加一些参数，变成一个深层的RNN。</p>
<p>正如堆叠标准神经网络的隐藏层一样，我们可以堆叠RNN的基础模块，并传递多个中间变量。由于时序计算的计算成本很高，堆3层的计算量就已经很大了。</p>
<p><img src="/2022/09/21/DLS-note-14/10.jpg" alt></p>
<p>如果想进一步提升网络的拟合能力，可以修改计算输出$y$的结构，堆叠一些非时序的神经网络隐藏层。</p>
<p><img src="/2022/09/21/DLS-note-14/11.jpg" alt></p>
<blockquote>
<p>时序模块之所以计算缓慢，一大原因是无法并行。靠后的变量必须等之前的变量算好了才能计算。而在输出$y$的路径中添加一些隐藏层的运算代价没有那么大，因为这些运算是可以并行的。</p>
</blockquote>
<p>同样，使用深层RNN时，双向RNN，还有LSTM, GRU都是可以用的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这一课里，我们初次认识了序列数据，并学习了处理序列数据的RNN。利用RNN，我们可以开发出许多和序列数据相关的应用。基础的RNN存在不少问题，所以RNN存在着许多改进方法。让我们看一看这一课的具体知识点：</p>
<ul>
<li><p>序列数据及相关任务的示例</p>
<ul>
<li>语音识别：输入语音，输出文字</li>
<li>音乐生成：无输入，输出语音</li>
<li>机器翻译：输入某种文字，输出另一种文字</li>
<li>情绪分类：输入文字，输出1-5的分数</li>
<li>命名实体识别：输入文字，输出每个单词是否是命名实体</li>
</ul>
</li>
<li><p>单词的表示</p>
<ul>
<li>先准备好一个词汇表。比如大小为10000的词汇表（要包括<code>&lt;UNK&gt;, &lt;EOS&gt;</code>）。</li>
<li>每一个单词是一个长度10000的one-hot向量。单词在词汇表中的序号，就是one-hot向量中值为1的下标。</li>
</ul>
</li>
<li><p>循环神经网络（RNN）</p>
<ul>
<li>基本思想：用$a$表示上文信息</li>
<li>计算流程：循环输入序列元素，维护$a$</li>
<li>计算公式</li>
<li>反向传播的大概流程</li>
</ul>
</li>
<li><p>防止RNN梯度消失：GRU, LSTM</p>
<ul>
<li>基本思想：选择性更新$a$</li>
<li>大概了解GRU, LSTM的公式</li>
<li>GRU, LSTM的使用场景</li>
</ul>
</li>
<li><p>获取下文信息：BRNN</p>
<ul>
<li>基本思想与结构</li>
<li>使用场景</li>
</ul>
</li>
<li><p>增强表达能力：深层RNN</p>
<ul>
<li>怎么添加更多层RNN</li>
<li>怎么更好地拟合输出</li>
</ul>
</li>
<li><p>RNN应用：语言模型</p>
<ul>
<li>语言模型的定义</li>
<li>语言模型的训练与推理</li>
<li>对语言模型采样</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/09/21/DLS-note-14-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/09/21/DLS-note-14-2/" class="post-title-link" itemprop="url">你的第一个PyTorch RNN模型——字母级语言模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-21 17:46:13" itemprop="dateCreated datePublished" datetime="2022-09-21T17:46:13+08:00">2022-09-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>想要入门一项新技术，最快的方法就是写一个”Hello World”程序。入门CNN，大家一般会写一个简单的图片分类项目。可是，RNN的入门项目就比较少见了。自然语言处理任务要求的数据量都比较大，不是那么好设计一个入门项目。</p>
<p>在这篇文章中，我将展示一个入门级的RNN项目——字母级语言模型。这个项目的逻辑比较简单，要求的数据量不大，几分钟就可以训练完，非常适合新手入门。</p>
<p>这个项目使用的框架是PyTorch。首先，我会抛弃PyTorch的高级组件，仅使用线性层、自动求导机制来从头实现一个简单的RNN。之后，我还会用PyTorch的高级组件搭一个更通用的RNN。相信通过阅读这篇教程，大家不仅能够理解RNN的底层原理，还能够学到PyTorch中RNN组件的用法，能够自己搭建出各种各样的NLP任务模型。</p>
<h2 id="知识背景"><a href="#知识背景" class="headerlink" title="知识背景"></a>知识背景</h2><p>详细的知识介绍可以参考我的上篇文章：<a href>循环神经网络基础</a>。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>RNN 适用于处理序列数据。令$x^{&lt; i &gt;}$是序列的第$i$个元素，那么$x^{&lt; 1 &gt;} x^{&lt; 2 &gt;}…x^{&lt; T_x &gt;}$就是一个长度为$T_x$的序列。NLP中最常见的元素是单词，对应的序列是句子。</p>
<p>RNN使用同一个神经网络处理序列中的每一个元素。同时，为了表示序列的先后关系，RNN还有表示记忆的隐变量$a$，它记录了前几个元素的信息。对第$t$个元素的运算如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{ax} x^{< t >} + W_{aa} a^{< t - 1 >} + b_a) \\
\hat{y}^{< t >} &=g_2(W_{ya} a^{< t >} + b_y)
\end{aligned}</script><p>其中，$W, b$都是线性运算的参数，$g$是激活函数。隐藏层的激活函数一般用tanh，输出层的激活函数根据实际情况选用。另外，$a$得有一个初始值$a^{&lt; 1 &gt;}$，一般令$a^{&lt; 1 &gt;}=\vec0$。</p>
<h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>语言模型是NLP中的一个基础任务。假设我们以单词为基本元素，句子为序列，那么一个语言模型能够输出某句话的出现概率。通过比较不同句子的出现概率，我们能够开发出很多应用。比如在英语里，同音的”apple and pear”比”apple and pair”的出现概率高（更可能是一个合理的句子）。当一个语音识别软件听到这句话时，可以分别写下这两句发音相近的句子，再根据语言模型断定这句话应该写成前者。</p>
<p>规范地说，对于序列$x^{&lt; 1 &gt;}…x^{&lt; T_x &gt;}$，语言模型的输出是$P(x^{&lt; 1 &gt;},…, x^{&lt; T_x &gt;})$。这个式子也可以写成$P(x^{&lt; 1 &gt;}) \times P(x^{&lt; 2 &gt;} |x^{&lt; 1 &gt;}) \times  P(x^{&lt; 3 &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}) … \times  P(x^{&lt; T_x &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}, …, x^{&lt; T_x-1 &gt;})$，即一句话的出现概率，等于第一个单词出现在句首的概率，乘上第二个单词在第一个单词之后的概率，乘上第三个单词再第一、二个单词之后的概率，这样一直乘下去。</p>
<p>单词级的语言模型需要的数据量比较大，在这个项目中，我们将搭建一个字母级语言模型。即我们以字母为基本元素，单词为序列。语言模型会输出每个单词的概率。比如我们输入”apple”和”appll”，语言模型会告诉我们单词”apple”的概率更高，这个单词更可能是一个正确的英文单词。</p>
<h3 id="RNN-语言模型"><a href="#RNN-语言模型" class="headerlink" title="RNN 语言模型"></a>RNN 语言模型</h3><p>为了计算语言模型的概率，我们可以用RNN分别输出$P(x^{&lt; 1 &gt;})$, $P(x^{&lt; 2 &gt;} |x^{&lt; 1 &gt;})$, …，最后把这些概率乘起来。</p>
<p>$P(x^{&lt; t &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}, …, x^{&lt; t-1 &gt;})$这个式子，说白了就是给定前$t-1$个字母，猜一猜第$t$个字母最可能是哪个。比如给定了前四个字母”appl”，第五个单词构成”apply”, “apple”的概率比较大，构成”appll”, “appla”的概率较小。</p>
<p>为了让神经网络学会这个概率，我们可以令RNN的输入为<code>&lt;sos&gt; x_1, x_2, ..., x_T</code>，RNN的标签为<code>x_1, x_2, ..., x_T, &lt;eos&gt;</code>（<code>&lt;sos&gt;</code>和<code>&lt;eos&gt;</code>是句子开始和结束的特殊字符，实际实现中可以都用空格<code>&#39; &#39;</code>表示。<code>&lt;sos&gt;</code>也可以粗暴地用全零向量表示），即输入和标签都是同一个单词，只是它们的位置差了一格。模型每次要输出一个softmax的多分类概率，预测给定前几个字母时下一个字母的概率。这样，这个模型就能学习到前面那个条件概率了。</p>
<p><img src="/2022/09/21/DLS-note-14-2/DLS-note-14/5.jpg" alt></p>
<h2 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h2><p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN。">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN。</a></p>
<h3 id="数据集获取"><a href="#数据集获取" class="headerlink" title="数据集获取"></a>数据集获取</h3><p>为了搭建字母级语言模型，我们只需要随便找一个有很多单词的数据集。这里我选择了斯坦福大学的<a target="_blank" rel="noopener" href="https://ai.stanford.edu/~amaas/data/sentiment/">大型电影数据集</a>，它收录了IMDb上的电影评论，正面评论和负面评论各25000条。这个数据集本来是用于情感分类这一比较简单的NLP任务，拿来搭字母级语言模型肯定是没问题的。</p>
<p>这个数据集的文件结构大致如下：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├─test</span><br><span class="line">│  ├─neg</span><br><span class="line">│  │  ├ 0_2.txt</span><br><span class="line">│  │  ├ 1_3.txt</span><br><span class="line">│  │  └ ...</span><br><span class="line">│  └─pos</span><br><span class="line">├─train</span><br><span class="line">│   ├─neg</span><br><span class="line">│   └─pos</span><br><span class="line">└─imdb.vocab</span><br></pre></td></tr></table></figure>
<p>其中，<code>imdb.vocab</code>记录了数据集中的所有单词，一行一个。<code>test</code>和<code>train</code>是测试集和训练集，它们的<code>neg</code>和<code>pos</code>子文件夹分别记录了负面评论和正面评论。每一条评论都是一句话，存在一个txt文件里。</p>
<p>训练字母级语言模型时，直接拿词汇表来训练也行，从评论中截取一个个单词也行。我已经写好了这些读取数据集的代码，在<code>dldemos/BasicRNN/read_imdb.py</code>文件中。</p>
<p>在读取单词时，我们只需要26个字母和空格这一共27个字符。其他的字符全可以过滤掉。为了方便，我使用了正则表达式过滤出这27个字符：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words = re.sub(<span class="string">u&#x27;([^\u0020\u0061-\u007a])&#x27;</span>, <span class="string">&#x27;&#x27;</span>, words)</span><br></pre></td></tr></table></figure>
<p>这样，一个读取词汇表文件的函数就长这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_imdb_vocab</span>(<span class="params"><span class="built_in">dir</span>=<span class="string">&#x27;data/aclImdb&#x27;</span></span>):</span></span><br><span class="line">    fn = os.path.join(<span class="built_in">dir</span>, <span class="string">&#x27;imdb.vocab&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fn, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        word = f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>).replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        words = re.sub(<span class="string">u&#x27;([^\u0020\u0061-\u007a])&#x27;</span>, <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                       word.lower()).split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        filtered_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> <span class="built_in">len</span>(w) &gt; <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> filtered_words</span><br></pre></td></tr></table></figure>
<p>我写好了读取词汇表的函数<code>read_imdb_vocab</code>和<code>read_imdb_words</code>，它们都会返回一个单词的列表。我还写了一个读数据集整个句子的函数<code>read_imdb</code>。它们的用法和输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    vocab = read_imdb_vocab()</span><br><span class="line">    <span class="built_in">print</span>(vocab[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(vocab[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    lines = read_imdb()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Length of the file:&#x27;</span>, <span class="built_in">len</span>(lines))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lines[0]:&#x27;</span>, lines[<span class="number">0</span>])</span><br><span class="line">    words = read_imdb_words(n_files=<span class="number">100</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Length of the words:&#x27;</span>, <span class="built_in">len</span>(words))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(words[i])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">the</span><br><span class="line">and</span><br><span class="line">Length of the file: 12500</span><br><span class="line">lines[0]: Bromwell High is a cartoon ...</span><br><span class="line">Length of the words: 23425</span><br><span class="line">bromwell</span><br><span class="line">high</span><br><span class="line">is</span><br><span class="line">a</span><br><span class="line">cartoon</span><br></pre></td></tr></table></figure>
<h3 id="数据集读取"><a href="#数据集读取" class="headerlink" title="数据集读取"></a>数据集读取</h3><p>RNN的输入不是字母，而是表示字母的向量。最简单的字母表示方式是one-hot编码，每一个字母用一个某一维度为1，其他维度为0的向量表示。比如我有a, b, c三个字母，它们的one-hot编码分别为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">b: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">c: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>现在，我们只有单词数组。我们要把每个单词转换成这种one-hot编码的形式。</p>
<p>在转换之前，我准备了一些常量（<code>dldemos/BasicRNN/constant.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">EMBEDDING_LENGTH = <span class="number">27</span></span><br><span class="line">LETTER_MAP = &#123;<span class="string">&#x27; &#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">ENCODING_MAP = [<span class="string">&#x27; &#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">26</span>):</span><br><span class="line">    LETTER_MAP[<span class="built_in">chr</span>(<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>) + i)] = i + <span class="number">1</span></span><br><span class="line">    ENCODING_MAP.append(<span class="built_in">chr</span>(<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>) + i))</span><br><span class="line">LETTER_LIST = <span class="built_in">list</span>(LETTER_MAP.keys())</span><br></pre></td></tr></table></figure>
<p>我们一共有27个字符，0号字符是空格，剩余字母按照字母表顺序排列。<code>LETTER_MAP</code>和<code>ENCODING_MAP</code>分别完成了字母到数字的正向和反向映射。<code>LETTER_LIST</code>是所有字母的列表。</p>
<p>PyTorch提供了用于管理数据集读取的Dataset类。Dataset一般只会存储获取数据的信息，而非原始数据，比如存储图片路径。而每次读取时，Dataset才会去实际读取数据。在这个项目里，我们用Dataset存储原始的单词数组，实际读取时，每次返回一个one-hot编码的向量。</p>
<p>使用Dataset时，要继承这个类，实现<code>__len__</code>和<code>__getitem__</code>方法。前者表示获取数据集的长度，后者表示获取某项数据。我们的单词数据集<code>WordDataset</code>应该这样写（<code>dldemos/BasicRNN/main.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.constant <span class="keyword">import</span> EMBEDDING_LENGTH, LETTER_MAP</span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.read_imdb <span class="keyword">import</span> read_imdb_vocab, read_imdb_words</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, words, max_length, is_onehot=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function">        <span class="title">super</span>().<span class="title">__init__</span>()</span></span><br><span class="line"><span class="function">        <span class="title">n_words</span> = <span class="title">len</span>(<span class="params">words</span>)</span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">words</span> = <span class="title">words</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">n_words</span> = <span class="title">n_words</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">max_length</span> = <span class="title">max_length</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">is_onehot</span> = <span class="title">is_onehot</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.n_words</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;return the (one-hot) encoding vector of a word&quot;&quot;&quot;</span></span><br><span class="line">        word = self.words[index] + <span class="string">&#x27; &#x27;</span></span><br><span class="line">        word_length = <span class="built_in">len</span>(word)</span><br><span class="line">        <span class="keyword">if</span> self.is_onehot:</span><br><span class="line">            tensor = torch.zeros(self.max_length, EMBEDDING_LENGTH)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">                <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">                    tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tensor = torch.zeros(self.max_length, dtype=torch.long)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(word_length):</span><br><span class="line">                tensor[i] = LETTER_MAP[word[i]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>
<p>构造数据集的参数是<code>words, max_length, is_onehot</code>。<code>words</code>是单词数组。<code>max_length</code>表示单词的最大长度。在训练时，我们一般要传入一个batch的单词。可是，单词有长有短，我们不可能拿一个动态长度的数组去表示单词。为了统一地表达所有单词，我们可以记录单词的最大长度，把较短的单词填充空字符，直到最大长度。<code>is_onehot</code>表示是不是one-hot编码，我设计的这个数据集既能输出用数字标签表示的单词（比如abc表示成<code>[0, 1, 2]</code>），也能输出one-hoe编码表示的单词（比如abc表示成<code>[[1, 0, 0], [0, 1, 0], [0, 0, 1]]</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, words, max_length, is_onehot=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function">    <span class="title">super</span>().<span class="title">__init__</span>()</span></span><br><span class="line"><span class="function">    <span class="title">n_words</span> = <span class="title">len</span>(<span class="params">words</span>)</span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">words</span> = <span class="title">words</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">n_words</span> = <span class="title">n_words</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">max_length</span> = <span class="title">max_length</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">is_onehot</span> = <span class="title">is_onehot</span></span></span><br></pre></td></tr></table></figure>
<p>在获取数据集时，我们要根据是不是one-hot编码，先准备好一个全是0的输出张量。如果存的是one-hot编码，张量的形状是<code>[MAX_LENGTH, EMBEDDING_LENGTH]</code>，第一维是单词的最大长度，第二维是one-hot编码的长度。而如果是普通的标签数组，则张量的形状是<code>[MAX_LENGTH]</code>。准备好张量后，遍历每一个位置，令one-hot编码的对应位为1，或者填入数字标签。</p>
<p>另外，我们用空格表示单词的结束。要在处理前给单词加一个<code>&#39; &#39;</code>，保证哪怕最长的单词也会至少有一个空格。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;return the (one-hot) encoding vector of a word&quot;&quot;&quot;</span></span><br><span class="line">    word = self.words[index] + <span class="string">&#x27; &#x27;</span></span><br><span class="line">    word_length = <span class="built_in">len</span>(word)</span><br><span class="line">    <span class="keyword">if</span> self.is_onehot:</span><br><span class="line">        tensor = torch.zeros(self.max_length, EMBEDDING_LENGTH)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">            <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">                tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tensor = torch.zeros(self.max_length, dtype=torch.long)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(word_length):</span><br><span class="line">            tensor[i] = LETTER_MAP[word[i]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>
<p>注意！短单词的填充部分应该全是空字符。千万不要忘记给空字符的one-hot编码赋值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">    <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">        tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>有了数据集类，结合之前写好的数据集获取函数，可以搭建一个DataLoader。DataLoader是PyTorch提供的数据读取类，它可以方便地从Dataset的子类里读取一个batch的数据，或者以更高级的方式取数据（比如随机取数据）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader_and_max_length</span>(<span class="params">limit_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  is_onehot=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  is_vocab=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> is_vocab:</span><br><span class="line">        words = read_imdb_vocab()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        words = read_imdb_words(n_files=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    max_length = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        max_length = <span class="built_in">max</span>(max_length, <span class="built_in">len</span>(word))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> limit_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> max_length &gt; limit_length:</span><br><span class="line">        words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> <span class="built_in">len</span>(w) &lt;= limit_length]</span><br><span class="line">        max_length = limit_length</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for &lt;EOS&gt; (space)</span></span><br><span class="line">    max_length += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    dataset = WordDataset(words, max_length, is_onehot)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=<span class="number">256</span>), max_length</span><br></pre></td></tr></table></figure>
<p>这个函数会先调用之前编写的数据读取API获取单词数组。之后，函数会计算最长的单词长度。这里，我用<code>limit_length</code>过滤了过长的单词。据实验，这个数据集里最长的单词竟然有60多个字母，把短单词填充至60需要浪费大量的计算资源。因此，我设置了<code>limit_length</code>这个参数，不去读取那些过长的单词。</p>
<p>计算完最大长度后，别忘了+1，保证每个单词后面都有一个表示单词结束的空格。</p>
<p>最后，用<code>DataLoader(dataset, batch_size=256)</code>就可以得到一个DataLoader。<code>batch_size</code>就是指定batch size的参数。我们这个神经网络很小，输入数据也很小，可以选一个很大的batch size加速训练。</p>
<h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><p>模型的初始化函数和训练函数定义如下（<code>dldemos/BasicRNN/models.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.constant <span class="keyword">import</span> EMBEDDING_LENGTH, LETTER_LIST, LETTER_MAP</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN1</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">32</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line">        self.linear_a = nn.Linear(hidden_units + EMBEDDING_LENGTH,</span><br><span class="line">                                  hidden_units)</span><br><span class="line">        self.linear_y = nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">        self.tanh = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">        batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">        word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">        output = torch.empty_like(word)</span><br><span class="line"></span><br><span class="line">        a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">        x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">            next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">            hat_y = self.linear_y(next_a)</span><br><span class="line">            output[i] = hat_y</span><br><span class="line">            x = word[i]</span><br><span class="line">            a = next_a</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">        <span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>我们来一点一点地看看这个模型是怎么搭起来的。</p>
<p>回忆一下RNN的公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{ax} x^{< t >} + W_{aa} a^{< t - 1 >} + b_a) \\
\hat{y}^{< t >} &=g_2(W_{ya} a^{< t >} + b_y)
\end{aligned}</script><p>我们可以把第一行公式里的两个$W$合并一下，$x, a$拼接一下。这样，只需要两个线性层就可以描述RNN了。</p>
<p>因此，在初始化函数中，我们定义两个线性层<code>linear_a</code>，<code>linear_y</code>。另外，<code>hidden_units</code>表示隐藏层<code>linear_a</code>的神经元数目。<code>tanh</code>就是普通的tanh函数，它用作第一层的激活函数。</p>
<p><code>linear_a</code>就是公式的第一行，由于我们把输入<code>x</code>和状态<code>a</code>拼接起来了，这一层的输入通道数是<code>hidden_units + EMBEDDING_LENGTH</code>，输出通道数是<code>hidden_units</code>。第二层<code>linear_y</code>表示公式的第二行。我们希望RNN能预测下一个字母的出现概率，因此这一层的输出通道数是<code>EMBEDDING_LENGTH=27</code>，即字符个数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">32</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.hidden_units = hidden_units</span><br><span class="line">    self.linear_a = nn.Linear(hidden_units + EMBEDDING_LENGTH,</span><br><span class="line">                              hidden_units)</span><br><span class="line">    self.linear_y = nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">    self.tanh = nn.Tanh()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在描述模型运行的<code>forward</code>函数中，我们先准备好输出张量，再初始化好隐变量<code>a</code>和第一轮的输入<code>x</code>。根据公式，循环遍历序列的每一个字母，用<code>a, x</code>计算<code>hat_y</code>，并维护每一轮的<code>a, x</code>。最后，所有<code>hat_y</code>拼接成的<code>output</code>就是返回结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    output = torch.empty_like(word)</span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        hat_y = self.linear_y(next_a)</span><br><span class="line">        output[i] = hat_y</span><br><span class="line">        x = word[i]</span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    <span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>我们来看一看这个函数的细节。一开始，输入张量<code>word</code>的形状是<code>[batch数，最大单词长度，字符数=27]</code>。我们提前获取好形状信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>我们循环遍历的其实是单词长度那一维。为了方便理解代码，我们可以把单词长度那一维转置成第一维。根据这个新的形状，我们准备好同形状的输出张量。输出张量<code>output[i][j]</code>表示第j个batch的序列的第i个元素的27个字符预测结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">output = torch.empty_like(word)</span><br></pre></td></tr></table></figure>
<p>按照前文知识准备的描述，第一轮的输入是空字符，期待的输出是句子里的第一个字母；第二轮的输入的第一个字母，期待的输出是第二个字母……。因此，我们要把输入<code>x</code>初始化为空。理论上<code>x</code>应该是一个空字符，其one-hot编码是<code>[1, 0, 0, ...]</code>，但这里我们拿一个全0的向量表示句首也是可行的。除了初始化<code>x</code>，还要初始化一个全零隐变量<code>a</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br></pre></td></tr></table></figure>
<p>之后，按照顺序遍历每一个元素，计算<code>y_hat</code>并维护<code>a, x</code>。最后输出结果前别忘了把转置过的维度复原回去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">    next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">    hat_y = self.linear_y(next_a)</span><br><span class="line">    output[i] = hat_y</span><br><span class="line">    x = word[i]</span><br><span class="line">    a = next_a</span><br><span class="line"></span><br><span class="line"><span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line"><span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>从逻辑上讲，模型应该输出softmax的结果。但是，PyTorch的<code>CrossEntropyLoss</code>已经包含了softmax的计算，我们不用在模型里加softmax。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>main函数中完整的训练代码如下（<code>dldemos/BasicRNN/models.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_rnn1</span>():</span></span><br><span class="line">    device = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">    dataloader, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line"></span><br><span class="line">    model = RNN1().to(device)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    citerion = torch.nn.CrossEntropyLoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line"></span><br><span class="line">        loss_sum = <span class="number">0</span></span><br><span class="line">        dataset_len = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> dataloader:</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            hat_y = model(y)</span><br><span class="line">            n, Tx, _ = hat_y.shape</span><br><span class="line">            hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">            y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">            label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">            loss = citerion(hat_y, label_y)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            loss_sum += loss</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;dldemos/BasicRNN/rnn1.pth&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>首先，调用之前编写的函数，准备好<code>dataloader</code>和<code>model</code>。同时，准备好优化器<code>optimizer</code>和损失函数<code>citerion</code>。优化器和损失函数按照常见配置选择即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">dataloader, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line"></span><br><span class="line">model = RNN1().to(device)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">citerion = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<p>这个语言模型一下就能训练完，做5个epoch就差不多了。每一代训练中，<br>先调用模型求出<code>hat_y</code>，再调用损失函数<code>citerion</code>，最后反向传播并优化模型参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    dataset_len = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> dataloader:</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        hat_y = model(y)</span><br><span class="line"></span><br><span class="line">        n, Tx, _ = hat_y.shape</span><br><span class="line">        hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">        y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">        label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">        loss = citerion(hat_y, label_y)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        loss_sum += loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p>算损失函数前需要预处理一下数据，交叉熵损失函数默认<code>hat_y</code>的维度是<code>[batch数，类型数]</code>，<code>label_y</code>是一个一维整形标签数组。而模型的输出形状是<code>[batch数，最大单词长度，字符数]</code>，我们要把前两个维度融合在一起。另外，我们并没有提前准备好<code>label_y</code>，需要调用<code>argmax</code>把one-hot编码转换回标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hat_y = model(y)</span><br><span class="line">n, Tx, _ = hat_y.shape</span><br><span class="line">hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">loss = citerion(hat_y, label_y)</span><br></pre></td></tr></table></figure>
<p>之后就是调用PyTorch的自动求导功能。注意，为了防止RNN梯度过大，我们可以用<code>clip_grad_norm_</code>截取梯度的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>我还顺带输出了每一代的loss。当然这里我偷了个懒，这个loss并不能表示每一个样本的平均loss。不过，我们能通过这个loss得到模型的训练进度，这就够了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>我们可以手动为字母级语言模型写几个测试用例，看看每一个单词的概率是否和期望的一样。我的测试单词列表是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test_words = [</span><br><span class="line">    <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;appll&#x27;</span>, <span class="string">&#x27;appla&#x27;</span>, <span class="string">&#x27;apply&#x27;</span>, <span class="string">&#x27;bear&#x27;</span>, <span class="string">&#x27;beer&#x27;</span>, <span class="string">&#x27;berr&#x27;</span>, <span class="string">&#x27;beee&#x27;</span>, <span class="string">&#x27;car&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cae&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;cac&#x27;</span>, <span class="string">&#x27;caq&#x27;</span>, <span class="string">&#x27;query&#x27;</span>, <span class="string">&#x27;queee&#x27;</span>, <span class="string">&#x27;queue&#x27;</span>, <span class="string">&#x27;queen&#x27;</span>, <span class="string">&#x27;quest&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;quess&#x27;</span>, <span class="string">&#x27;quees&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>我构筑了几组长度一样，但是最后几个字母不太一样的“单词”。通过观察这些词的概率，我们能够验证语言模型的正确性。理论上来说，英文里的正确单词的概率会更高。</p>
<p>我们的模型只能输出每一个单词的softmax前结果。我们还要为模型另写一个求语言模型概率的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">language_model</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    <span class="comment"># word_label shape: [max_word_length, batch]</span></span><br><span class="line">    word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    word_label = torch.argmax(word, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [batch]</span></span><br><span class="line">    output = torch.ones(batch, device=word.device)</span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        tmp = self.linear_y(next_a)</span><br><span class="line">        hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br><span class="line">        probs = hat_y[torch.arange(batch), word_label[i]]</span><br><span class="line">        output *= probs</span><br><span class="line">        x = word[i]</span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这个函数和<code>forward</code>大致相同。只不过，这次我们的输出<code>output</code>要表示每一个单词的概率。因此，它被初始化成一个全1的向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output shape: [batch]</span></span><br><span class="line">output = torch.ones(batch, device=word.device)</span><br></pre></td></tr></table></figure>
<p>每轮算完最后一层的输出后，我们手动调用<code>F.softmax</code>得到softmax的概率值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmp = self.linear_y(next_a)</span><br><span class="line">hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们要根据每一个batch当前位置的单词，去<code>hat_y</code>里取出需要的概率。比如第2个batch当前的字母是<code>b</code>，我们就要取出<code>hat_y[2][2]</code>。</p>
<p>第<code>i</code>轮所有batch的字母可以用<code>word_label[i]</code>表示。根据这个信息，我们可以用<code>probs = hat_y[torch.arange(batch), word_label[i]]</code>神奇地从<code>hat_y</code>里取出每一个batch里<code>word_label[i]</code>处的概率。把这个概率乘到<code>output</code>上就算完成了一轮计算。</p>
<p>有了语言模型函数，我们可以测试一下开始那些单词的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_language_model</span>(<span class="params">model, is_onehot=<span class="literal">True</span>, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    _, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line">    <span class="keyword">if</span> is_onehot:</span><br><span class="line">        test_word = words_to_onehot(test_words, max_length)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        test_word = words_to_label_array(test_words, max_length)</span><br><span class="line">    test_word = test_word.to(device)</span><br><span class="line">    probs = model.language_model(test_word)</span><br><span class="line">    <span class="keyword">for</span> word, prob <span class="keyword">in</span> <span class="built_in">zip</span>(test_words, probs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;word&#125;</span>: <span class="subst">&#123;prob&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apple: <span class="number">9.39846032110836e-08</span></span><br><span class="line">appll: <span class="number">6.516307937687316e-09</span></span><br><span class="line">appla: <span class="number">3.6599331565412285e-08</span></span><br><span class="line">apply: <span class="number">1.2422759709806996e-07</span></span><br><span class="line">bear: <span class="number">1.6009346381906653e-06</span></span><br><span class="line">beer: <span class="number">1.6936465954131563e-06</span></span><br><span class="line">berr: <span class="number">9.99331746243115e-07</span></span><br><span class="line">beee: <span class="number">1.5601625591443735e-07</span></span><br><span class="line">car: <span class="number">1.8536804418545216e-05</span></span><br><span class="line">cae: <span class="number">1.8946409454656532e-06</span></span><br><span class="line">cat: <span class="number">1.875695670605637e-05</span></span><br><span class="line">cac: <span class="number">6.04180786467623e-06</span></span><br><span class="line">caq: <span class="number">3.6483314147517376e-08</span></span><br><span class="line">query: <span class="number">1.6811516161396867e-06</span></span><br><span class="line">queee: <span class="number">5.9459132728534314e-08</span></span><br><span class="line">queue: <span class="number">9.488831942405795e-09</span></span><br><span class="line">queen: <span class="number">5.990783051856852e-07</span></span><br><span class="line">quest: <span class="number">2.737341446845676e-06</span></span><br><span class="line">quess: <span class="number">4.7091912165342364e-06</span></span><br><span class="line">quees: <span class="number">1.3468336419464322e-06</span></span><br></pre></td></tr></table></figure>
<p>通过观察每一组用例，我们能发现，<code>apple, apply, bear, beer</code>这些正确的单词的概率确实会高一些。这个语言模型训练得不错。有趣的是，<code>caq</code>这种英语里几乎不存在的字母组合的概率也偏低。当然，语言模型对难一点的单词的判断就不太准了。<code>queen</code>和<code>queue</code>的出现概率就比较低。</p>
<h3 id="采样单词"><a href="#采样单词" class="headerlink" title="采样单词"></a>采样单词</h3><p>语言模型有一个很好玩的应用：我们可以根据语言模型输出的概率分布，采样出下一个单词；输入这一个单词，再采样下一个单词。这样一直采样，直到采样出空格为止。使用这种采样算法，我们能够让模型自动生成单词，甚至是英文里不存在，却看上去很像那么回事的单词。</p>
<p>我们要为模型编写一个新的方法<code>sample_word</code>，采样出一个最大长度为10的单词。这段代码的运行逻辑和之前的<code>forward</code>也很相似。只不过，这一次我们没有输入张量，每一轮的<code>x</code>要靠采样获得。<code>np.random.choice(LETTER_LIST, p=np_prob)</code>可以根据概率分布<code>np_prob</code>对列表<code>LETTER_LIST</code>进行采样。根据每一轮采样出的单词<code>letter</code>，我们重新生成一个<code>x</code>，给one-hot编码的对应位置赋值1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_word</span>(<span class="params">self, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    batch = <span class="number">1</span></span><br><span class="line">    output = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        tmp = self.linear_y(next_a)</span><br><span class="line">        hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        np_prob = hat_y[<span class="number">0</span>].detach().cpu().numpy()</span><br><span class="line">        letter = np.random.choice(LETTER_LIST, p=np_prob)</span><br><span class="line">        output += letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> letter == <span class="string">&#x27; &#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        x = torch.zeros(batch, EMBEDDING_LENGTH, device=device)</span><br><span class="line">        x[<span class="number">0</span>][LETTER_MAP[letter]] = <span class="number">1</span></span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>使用这个方法，我们可以写一个采样20次的脚本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">model</span>):</span></span><br><span class="line">    words = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        word = model.sample_word()</span><br><span class="line">        words.append(word)</span><br><span class="line">    <span class="built_in">print</span>(*words)</span><br></pre></td></tr></table></figure>
<p>我的一次输出是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movine  oaceniefke xumedfasss tinkly  cerawedaus meblilesen douteni  ttingieftu sinsceered inelid  tniblicl  krouthyych mochonalos memp  dendusmani sttywima  dosmmek  dring  diummitt  pormoxthin</span><br></pre></td></tr></table></figure>
<p>采样出来的单词几乎不会是英文里的正确单词。不过，这些单词的词缀很符合英文的造词规则，非常好玩。如果为采样函数加一些限制，比如只考虑概率前3的字母，那么算法应该能够采样出更正确的单词。</p>
<h2 id="PyTorch里的RNN函数"><a href="#PyTorch里的RNN函数" class="headerlink" title="PyTorch里的RNN函数"></a>PyTorch里的RNN函数</h2><p>刚刚我们手动编写了RNN的实现细节。实际上，PyTorch提供了更高级的函数，我们能够更加轻松地实现RNN。其他部分的代码逻辑都不怎么要改，我这里只展示一下要改动的关键部分。</p>
<blockquote>
<p>写这份代码时我参考了 <a target="_blank" rel="noopener" href="https://github.com/floydhub/word-language-model">https://github.com/floydhub/word-language-model</a></p>
</blockquote>
<p>新的模型的主要函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN2</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">64</span>, embeding_dim=<span class="number">64</span>, dropout_rate=<span class="number">0.2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.drop = nn.Dropout(dropout_rate)</span><br><span class="line">        self.encoder = nn.Embedding(EMBEDDING_LENGTH, embeding_dim)</span><br><span class="line">        self.rnn = nn.GRU(embeding_dim, hidden_units, <span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.decoder = torch.nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        nn.init.uniform_(self.encoder.weight, -initrange, initrange)</span><br><span class="line">        nn.init.zeros_(self.decoder.bias)</span><br><span class="line">        nn.init.uniform_(self.decoder.weight, -initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># word shape: [batch, max_word_length]</span></span><br><span class="line">        batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        first_letter = word.new_zeros(batch, <span class="number">1</span>)</span><br><span class="line">        x = torch.cat((first_letter, word[:, <span class="number">0</span>:-<span class="number">1</span>]), <span class="number">1</span>)</span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=word.device)</span><br><span class="line">        emb = self.drop(self.encoder(x))</span><br><span class="line">        output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        y = self.decoder(output.reshape(batch * Tx, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y.reshape(batch, Tx, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>初始化时，我们用<code>nn.Embedding</code>表示单词的向量。词嵌入（Embedding）是《深度学习专项-RNN》第二门课的内容，我会在下一篇笔记里介绍。这里我们把<code>nn.Embedding</code>看成一种代替one-hot编码的更高级的向量就行。这些向量和线性层参数<code>W</code>一样，是可以被梯度下降优化的。这样，不仅是RNN可以优化，每一个单词的表示方法也可以被优化。</p>
<p>注意，使用<code>nn.Embedding</code>后，输入的张量不再是one-hot编码，而是数字标签。代码中的其他地方也要跟着修改。</p>
<p><code>nn.GRU</code>可以创建GRU。其第一个参数是输入的维度，第二个参数是隐变量<code>a</code>的维度，第三个参数是层数，这里我们只构建1层RNN，<code>batch_first</code>表示输入张量的格式是<code>[batch, Tx, embedding_length]</code>还是<code>[Tx,  batch, embedding_length]</code>。</p>
<p>貌似RNN中常用的正则化是靠dropout实现的。我们要提前准备好dropout层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">64</span>, embeding_dim=<span class="number">64</span>, dropout_rate=<span class="number">0.2</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.drop = nn.Dropout(dropout_rate)</span><br><span class="line">    self.encoder = nn.Embedding(EMBEDDING_LENGTH, embeding_dim)</span><br><span class="line">    self.rnn = nn.GRU(embeding_dim, hidden_units, <span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">    self.decoder = torch.nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">    self.hidden_units = hidden_units</span><br><span class="line"></span><br><span class="line">    self.init_weights()</span><br></pre></td></tr></table></figure>
<p>准备好了计算层后，在forward里只要依次调用它们就行了。其底层原理和我们之前手写的是一样的。其中，<code>self.rnn(emb, hidden)</code>这个调用完成了循环遍历的计算。</p>
<p>由于输入格式改了，令第一轮输入为空字符的操作也更繁琐了一点。我们要先定义一个空字符张量，再把它和输入的第一至倒数第二个元素拼接起来，作为网络的真正输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    first_letter = word.new_zeros(batch, <span class="number">1</span>)</span><br><span class="line">    x = torch.cat((first_letter, word[:, <span class="number">0</span>:-<span class="number">1</span>]), <span class="number">1</span>)</span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=word.device)</span><br><span class="line">    emb = self.drop(self.encoder(x))</span><br><span class="line">    output, hidden = self.rnn(emb, hidden)</span><br><span class="line">    y = self.decoder(output.reshape(batch * Tx, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y.reshape(batch, Tx, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>PyTorch里的RNN用起来非常灵活。我们不仅能够给它一个序列，一次输出序列的所有结果，还可以只输入一个元素，得到一轮的结果。在采样单词时，我们不得不每次输入一个元素。有关采样的逻辑如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_word</span>(<span class="params">self, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    batch = <span class="number">1</span></span><br><span class="line">    output = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=device)</span><br><span class="line">    x = torch.zeros(batch, <span class="number">1</span>, device=device, dtype=torch.long)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        emb = self.drop(self.encoder(x))</span><br><span class="line">        rnn_output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        hat_y = self.decoder(rnn_output)</span><br><span class="line">        hat_y = F.softmax(hat_y, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        np_prob = hat_y[<span class="number">0</span>, <span class="number">0</span>].detach().cpu().numpy()</span><br><span class="line">        letter = np.random.choice(LETTER_LIST, p=np_prob)</span><br><span class="line">        output += letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> letter == <span class="string">&#x27; &#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        x = torch.zeros(batch, <span class="number">1</span>, device=device, dtype=torch.long)</span><br><span class="line">        x[<span class="number">0</span>] = LETTER_MAP[letter]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>以上就是PyTorch高级RNN组件的使用方法。在使用PyTorch的RNN时，主要的改变就是输入从one-hot向量变成了标签，数据预处理会更加方便一些。另外，PyTorch的RNN会自动完成循环，可以给它输入任意长度的序列。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我展示了一个字母级语言模型项目。这个项目涉及到的编程知识有：</p>
<ul>
<li>one-hot编码的处理</li>
<li>RNN的底层实现</li>
<li>如何用RNN对语言模型任务建模</li>
<li>如何用RNN求出语言模型的概率</li>
<li>如何对语言模型采样</li>
<li>PyTorch的RNN组件</li>
</ul>
<p>这篇文章只展示了部分关键代码。想阅读整个项目完整的代码，可以访问该项目的<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN">GitHub链接</a>。</p>
<p>如果大家正在学深度学习，强烈建议大家从头写一遍这个项目。编写代码能够学到很多细节，加深对RNN的理解。</p>
<p>在编写这个项目时，我总结了项目中几个比较有挑战性的部分。大家阅读代码或自己动手时可以格外注意这些部分。第一个比较难的部分是和batch有关的计算。RNN本身必须得顺序处理序列，效率较低，同时处理一个batch的数据是一个很重要的加速手段。我们的代码都得尽量符合向量化编程要求，一次处理一个batch。</p>
<p>另外，相比一般的数据，序列数据多了一个时间维度（或者说序列维度），在向量化计算中考虑这个维度是很耗费脑力的。我们可以在代码中加入对中间变量形状的注释。在使用PyTorch或者其他框架时，要注意是batch维度在前面，还是时间维度在前面。注意初始化RNN的<code>batch_first</code>这个参数。还有，一个张量到底是one-hot编码，还是embedding，还是标签序列，这个也要想清楚来。</p>
<blockquote>
<p>PyTorch里的<code>CrossEntropyLoss</code>自带了softmax操作，千万不能和softmax混用！我之前写了这个bug，调了很久才调出来，真是气死人了。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/15/20220813-SRGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/15/20220813-SRGAN/" class="post-title-link" itemprop="url">图像超分经典网络 SRGAN 解析 ~ 如何把 GAN 运用在其他视觉任务上</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-15 17:10:46" itemprop="dateCreated datePublished" datetime="2022-08-15T17:10:46+08:00">2022-08-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>生成对抗网络(GAN)是一类非常有趣的神经网络。借助GAN，计算机能够生成逼真的图片。近年来有许多“AI绘画”的新闻，这些应用大多是通过GAN实现的。实际上，GAN不仅能做图像生成，还能辅助其他输入信息不足的视觉任务。比如SRGAN，就是把GAN应用在超分辨率(SR)任务上的代表之作。</p>
<p>在这篇文章中，我将主要面向深度学习的初学者，介绍SRGAN[1]这篇论文，同时分享以下知识：</p>
<ul>
<li>GAN的原理与训练过程</li>
<li>感知误差(Perceptual Loss)</li>
<li>基于的GAN的SR模型框架</li>
</ul>
<p>讲完了知识后，我还会解读一下MMEditing的SRGAN的训练代码。看懂这份代码能够加深对SRGAN训练算法的理解。</p>
<h2 id="SRGAN-核心思想"><a href="#SRGAN-核心思想" class="headerlink" title="SRGAN 核心思想"></a>SRGAN 核心思想</h2><p>早期超分辨率方法的优化目标都是降低低清图像和高清图像之间的均方误差。降低均方误差，确实让增强图像和原高清图像的相似度更高。但是，图像的相似度指标高并不能代表图像的增强质量就很高。下图显示了插值、优化均方误差、SRGAN、原图这四个图像输出结果（括号里的相似度指标是PSNR和SSIM）。</p>
<p><img src="/2022/08/15/20220813-SRGAN/1.jpg" alt></p>
<p>从图中可以看出，优化均方误差虽然能让相似度指标升高，但图像的细节十分模糊，尤其是纹理比较密集的高频区域。相比之下，SRGAN增强出来的图像虽然相似度不高，但看起来更加清晰。</p>
<p>为什么SRGAN的增强结果那么清楚呢？这是因为SRGAN使用了一套新的优化目标。SRGAN使用的损失函数既包括了<strong>GAN误差</strong>，也包括了<strong>感知误差</strong>。这套新的优化目标能够让网络生成看起来更清楚的图片，而不仅仅是和原高清图像相似度更高的图片。</p>
<p>下面，我们来一步一步学习SRGAN的框架。</p>
<h2 id="GAN-的原理"><a href="#GAN-的原理" class="headerlink" title="GAN 的原理"></a>GAN 的原理</h2><p>GAN[2]是一套搭建神经网络的框架。给定一个图片数据集$p_g$，GAN的目的是训练出一个<strong>生成网络</strong>$G$，使得G能够凭空生成出和$p_g$中大多数图片都类似的图片。比如说$p_g$是一个小猫图片数据集，那么$G$就应该能凭空生成出小猫图片。当然，$G$不是真的没有任何输入，真的能够凭空生成一幅图片。为了生成出不一样的图片，$G$要求输入一个随机量，这个随机量叫做噪声$z$。这样，只要输入的噪声$z$变了，$G$的输出$G(z)$就变了，就能画出长相不一样的小猫了。</p>
<p>为了指导图像生成，$G$应该有一个“老师”告诉它该怎么画出更像的图片。这个“老师”叫做<strong>判别网络</strong>$D$。$D$就是一个二分类网络，它能够严格地判定出一幅图片是否来自数据集$p_g$。如果$p_g$是一个小猫数据集，那么$D$就应该能判定一张图片是不是小猫。这样，如果$G$生成出来的图片$G(z)$已经非常逼真，连$D$都觉得$G(z)$来自数据集$p_g$，那么$G$就是一个很成功的网络了。</p>
<p>如果只是生成小猫，我们直接拿小猫图片和其他图片就能训练出一个$D$了。问题是，大多数情况下我们只有数据集$p_g$，而难以获得一个$p_g$的反例数据集。GAN的想法，则巧妙地解决了这个问题：刚开始，$G$生成出来的图片肯定是很差的，这些图片肯定不像$p_g$。所以，我们以$G(z)$为反例，和$p_g$一起训练出一个$D$来。等$D$的判定能力强了以后，又拿$D$回头训练$G$。这样，$D$的审美水平逐渐提高，$G$的绘画能力也逐渐提高。最终，$D$能成功分辨出一幅图片是否来自$p_g$，而$G$生成出来的图片和$p_g$中的看起来完全相同，连$D$也分辨不出来。就这样，我们得到了一个很棒的生成网络$G$。</p>
<p>规范地来说，给定一个数据集$p_g$，我们希望训练出两个网络$D, G$。$D$能够判断一幅输入图片是否来自$p_g$:</p>
<script type="math/tex; mode=display">
D(x) = \left\{
\begin{aligned}
&1 & x \in p_g \\
&0 & x \notin p_g
\end{aligned}
\right.</script><p>$G$则能够根据来自噪声分布$p_z$的$z$生成一个真假难辨的图片$G(z)$，使得$D(G(z))=1$。</p>
<p>为了达到这个目标，二分类器$D$应该最小化这样一个的交叉熵误差：</p>
<script type="math/tex; mode=display">L(\hat{y}, y)=-(y \ log\hat{y} + (1-y) \ log(1-\hat{y}))</script><p>其中，$\hat{y}=D(x)$是预测结果为真的概率，$y$是0或1的标签。</p>
<p>对于来自数据集的图片$x \sim p_g$，$D$使用的标签$y$应该是1，误差公式化简为：</p>
<script type="math/tex; mode=display">
L(x)=-logD(x), x \sim p_g</script><p>对于$G$生成的图片$G(z)$，$D$使用的标签$y$应该是0，误差公式化简为：</p>
<script type="math/tex; mode=display">
L(z)=-log(1-D(G(z))), z \sim p_z</script><p>我们每步拿一张真图$x$和一张假图$G(z)$训练$D$。这样，每步的误差公式就是上面两个式子加起来：</p>
<script type="math/tex; mode=display">
L_D(x, z)=-(logD(x) + log(1-D(G(z)))), x \sim p_g, z \sim p_z</script><p>反过来，$G$应该和$D$对抗，最大化上面那个误差，想办法骗过$D$。这个“对抗”就是GAN的名称“生成对抗网络”的由来。但是，$G$不能改变$D(x)$那一项。因此，$G$使用的误差函数是：</p>
<script type="math/tex; mode=display">
L_G(z)=log(1-D(G(z))), z \sim p_z</script><p>使用上面这两种误差，就可以训练神经网络了。训练GAN时，每轮一般会训练$k(k&gt;=1)$次$D$，再训练1次$G$。这是为了先得到一个好的判别器，再用判别器去指导生成器。</p>
<p>GAN只是一套通用的框架，并没有指定神经网络$D, G$的具体结构。在不同任务中，$D, G$一般有不同的结构。</p>
<h2 id="基于GAN的超分辨率网络"><a href="#基于GAN的超分辨率网络" class="headerlink" title="基于GAN的超分辨率网络"></a>基于GAN的超分辨率网络</h2><p>如前文所述，以优化均方误差为目标的超分辨率模型难以复原图像的细节。其实，超分辨率任务和图像生成任务类似，都需要一个“老师”来指导优化目标。SRGAN把GAN框架运用到了超分辨率任务上。原来的生成器$G$随机生成图像，现在用来输出高清图像；原来的判定器$D$用来判定图像是否属于某数据集，现在$D$用来判断一幅图像是否是高清图像。</p>
<p>具体来说，相比基础的GAN，在SRGAN中，$D$的输入是高清图像$I^{HR}$。而$G$的输入从随机噪声$z$变成了高清图像退化后的低清图像$I^{LR}$。这样，$G$就不是在随机生成图像，而是在根据一幅低清图像生成一幅高清图像了。它们的误差函数分别是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L_D&=-(logD(I^{HR}) + log(1-D(G(I^{LR}))))\\
L_G&=log(1-D(G(I^{LR})))
\end{aligned}</script><p>借助GAN的架构，SRGAN能够利用$D$指导高清图像生成。但是，超分辨率任务毕竟和图像生成任务有一些区别，不能只用这种对抗误差来约束网络。因此，除了使用对抗误差外，SRGAN还使用了一种内容误差。这种内容误差用于让低清图片和高清图片的内容对齐，起到了和原均方误差一样的作用。</p>
<h2 id="基于感知的内容误差"><a href="#基于感知的内容误差" class="headerlink" title="基于感知的内容误差"></a>基于感知的内容误差</h2><p>在介绍SRGAN的内容误差之前，需要对“内容误差”和“感知误差”这两个名词做一个澄清。在SRGAN的原文章中，作者把内容误差和对抗误差之和叫做感知误差。但是，后续的大部分文献只把这种内容误差叫做感知误差，不会把内容误差和对抗误差放在一起称呼。在后文中，我也会用“感知误差”来指代SRGAN中的“内容误差”。</p>
<p>在深度卷积神经网络（CNN）火起来后，人们开始研究为什么CNN能够和人类一样识别出图像。经实验，人们发现两幅图像经VGG（一个经典的CNN）的某些中间层的输出越相似，两幅图像从观感上也越相似。这种相似度并不是基于某种数学指标，而是和人的感知非常类似。</p>
<p>VGG的这种“感知性”被运用在了风格迁移等任务上。也有人考虑把这种感知上的误差运用到超分辨率任务上，并取得了不错的结果[3]。下图是真值、插值、基于逐像素误差、基于感知误差的四个超分辨率结果。</p>
<p><img src="/2022/08/15/20220813-SRGAN/2.jpg" alt></p>
<p>SRGAN也使用了这种感知误差，以取代之前常常使用的逐像素均方误差。这种感知误差的计算方法如下：VGG有很多中间层，用于计算感知误差的中间层$i$是可调的。假如我们用$\phi_{i}(I)$表示图像$I$经VGG的第$i$层的中间输出结果，$\phi_{i}(I)_{x, y}$表示中间输出结果在坐标$(x, y)$处的值，则感知误差的公式如下：</p>
<script type="math/tex; mode=display">
L_{p}(I^{HR}, I^{LR})_{i}=\frac{1}{WH}\Sigma_{x=1}^{W}\Sigma_{y=1}^{H}(\phi_{i}(I^{HR})_{x, y}-\phi_{i}(G(I^{LR}))_{x, y})^2</script><p>直观上解释这个公式，就是先把高清图像$I^{HR}$送入VGG，再把高清图像退化出来的低清图像$I^{LR}$送入生成器，并把生成器的输出$G(I^{LR})$也送入VGG。两幅图片经VGG第$i$层生成的中间结果的逐像素均方误差，就是感知误差。</p>
<p>算上之前的对抗误差，一个图像超分辨率网络的总误差如下：</p>
<script type="math/tex; mode=display">
L_{SR}=L_p + w L_G</script><p>这里的$w$用于调整两个误差的相对权重，原论文使用$w=10^{-3}$。</p>
<h2 id="SRGAN的其他模块"><a href="#SRGAN的其他模块" class="headerlink" title="SRGAN的其他模块"></a>SRGAN的其他模块</h2><p>定义好了误差函数，只要在决定好网络结构就可以开始训练网络了。SRGAN使用的生成网络和判别网络的结构如下：</p>
<p><img src="/2022/08/15/20220813-SRGAN/3.jpg" alt></p>
<p>判别网络就是一个平平无奇的二分类网络，架构上没有什么创新。而生成网络则先用几个残差块提取特征，最后用一种超分辨率任务中常用的上采样模块PixelShuffle对原图像的尺寸翻倍两次，最后输出一个边长放大4倍的高清图像。</p>
<p>SRGAN的这种网络结构在当时确实取得了不错的结果。但是，很快就有后续研究提出了更好的网络架构。比如ESRGAN[4]去掉了生成网络的BN层，提出了一种叫做RRDB的高级模块。基于RRDB的生成网络有着更好的生成效果。</p>
<p>不仅是网络架构，SRGAN的其他细节也得到了后续研究的改进。GAN误差的公式、总误差的公式、高清图像退化成低清图像的数据增强算法……这些子模块都被后续研究改进了。但是，SRGAN这种基于GAN的训练架构一直没有发生改变。有了SRGAN的代码，想复现一些更新的超分辨率网络时，往往只需要换一下生成器的结构，或者改一改误差的公式就行了。大部分的训练代码是不用改变的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SRGAN是把GAN运用在超分辨率任务上的开山之作。如正文所述，SRGAN中的部分设计虽然已经过时，但它的整体训练架构被一直沿用了下来。现在去回顾SRGAN这篇论文时，只需要关注以下几点即可:</p>
<ul>
<li>如何把GAN套用在超分辨率任务上</li>
<li>GAN误差</li>
<li>感知误差</li>
</ul>
<p>通过阅读这篇论文，我们不仅应该学会GAN是怎样运用在SR上的，也应该能总结出如何把GAN应用在其他任务上。GAN的本质是去学习一个分布，令生成的$G(z)$看上去是来自分布$p_g$，而不是像图像分类等任务去学习一个$x \to y$的映射关系。因此，GAN会记忆一些和数据集相关的信息。在输入信息就已经比较完备的图像分类、目标检测等任务中，GAN可能没有什么用武之地。但是，在输入信息不足的超分辨率、图像补全等任务中，GAN记忆的数据集信息有很有用了。很多时候，GAN会“脑补”出输入图像中不够清楚的部分。</p>
<p>决定了要在某个任务中使用GAN时，我们可以在一个不使用GAN的架构上做以下改动：</p>
<ul>
<li>定义一个分类网络$D$。</li>
<li>在原loss中加一项由$D$算出来的GAN loss。</li>
<li>在训练流程中，加入训练$D$的逻辑。</li>
</ul>
<p>看完正文后，如果你对GAN在SR上的训练逻辑还是不太清楚，欢迎阅读附录中有关SRGAN训练代码的解读。</p>
<h2 id="附录：MMEditing-中的-SRGAN"><a href="#附录：MMEditing-中的-SRGAN" class="headerlink" title="附录：MMEditing 中的 SRGAN"></a>附录：MMEditing 中的 SRGAN</h2><p>MMEditing中的SRGAN写在<code>mmedit/models/restorers/srgan.py</code>这个文件里。学习训练逻辑时，我们只需要关注<code>SRGAN</code>类的<code>train_step</code>方法即可。</p>
<p>以下是<code>train_step</code>的源代码（我的mmedit版本是v0.15.1）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">self, data_batch, optimizer</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train step.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_batch (dict): A batch of data.</span></span><br><span class="line"><span class="string">        optimizer (obj): Optimizer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict: Returned output.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># data</span></span><br><span class="line">    lq = data_batch[<span class="string">&#x27;lq&#x27;</span>]</span><br><span class="line">    gt = data_batch[<span class="string">&#x27;gt&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generator</span></span><br><span class="line">    fake_g_output = self.generator(lq)</span><br><span class="line"></span><br><span class="line">    losses = <span class="built_in">dict</span>()</span><br><span class="line">    log_vars = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># no updates to discriminator parameters.</span></span><br><span class="line">    set_requires_grad(self.discriminator, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (self.step_counter % self.disc_steps == <span class="number">0</span></span><br><span class="line">            <span class="keyword">and</span> self.step_counter &gt;= self.disc_init_steps):</span><br><span class="line">        <span class="keyword">if</span> self.pixel_loss:</span><br><span class="line">            losses[<span class="string">&#x27;loss_pix&#x27;</span>] = self.pixel_loss(fake_g_output, gt)</span><br><span class="line">        <span class="keyword">if</span> self.perceptual_loss:</span><br><span class="line">            loss_percep, loss_style = self.perceptual_loss(</span><br><span class="line">                fake_g_output, gt)</span><br><span class="line">            <span class="keyword">if</span> loss_percep <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                losses[<span class="string">&#x27;loss_perceptual&#x27;</span>] = loss_percep</span><br><span class="line">            <span class="keyword">if</span> loss_style <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                losses[<span class="string">&#x27;loss_style&#x27;</span>] = loss_style</span><br><span class="line">        <span class="comment"># gan loss for generator</span></span><br><span class="line">        fake_g_pred = self.discriminator(fake_g_output)</span><br><span class="line">        losses[<span class="string">&#x27;loss_gan&#x27;</span>] = self.gan_loss(</span><br><span class="line">            fake_g_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># parse loss</span></span><br><span class="line">        loss_g, log_vars_g = self.parse_losses(losses)</span><br><span class="line">        log_vars.update(log_vars_g)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimize</span></span><br><span class="line">        optimizer[<span class="string">&#x27;generator&#x27;</span>].zero_grad()</span><br><span class="line">        loss_g.backward()</span><br><span class="line">        optimizer[<span class="string">&#x27;generator&#x27;</span>].step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># discriminator</span></span><br><span class="line">    set_requires_grad(self.discriminator, <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># real</span></span><br><span class="line">    real_d_pred = self.discriminator(gt)</span><br><span class="line">    loss_d_real = self.gan_loss(</span><br><span class="line">        real_d_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">True</span>)</span><br><span class="line">    loss_d, log_vars_d = self.parse_losses(<span class="built_in">dict</span>(loss_d_real=loss_d_real))</span><br><span class="line">    optimizer[<span class="string">&#x27;discriminator&#x27;</span>].zero_grad()</span><br><span class="line">    loss_d.backward()</span><br><span class="line">    log_vars.update(log_vars_d)</span><br><span class="line">    <span class="comment"># fake</span></span><br><span class="line">    fake_d_pred = self.discriminator(fake_g_output.detach())</span><br><span class="line">    loss_d_fake = self.gan_loss(</span><br><span class="line">        fake_d_pred, target_is_real=<span class="literal">False</span>, is_disc=<span class="literal">True</span>)</span><br><span class="line">    loss_d, log_vars_d = self.parse_losses(<span class="built_in">dict</span>(loss_d_fake=loss_d_fake))</span><br><span class="line">    loss_d.backward()</span><br><span class="line">    log_vars.update(log_vars_d)</span><br><span class="line"></span><br><span class="line">    optimizer[<span class="string">&#x27;discriminator&#x27;</span>].step()</span><br><span class="line"></span><br><span class="line">    self.step_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    log_vars.pop(<span class="string">&#x27;loss&#x27;</span>)  <span class="comment"># remove the unnecessary &#x27;loss&#x27;</span></span><br><span class="line">    outputs = <span class="built_in">dict</span>(</span><br><span class="line">        log_vars=log_vars,</span><br><span class="line">        num_samples=<span class="built_in">len</span>(gt.data),</span><br><span class="line">        results=<span class="built_in">dict</span>(lq=lq.cpu(), gt=gt.cpu(), output=fake_g_output.cpu()))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure></p>
<p>一开始，图像输出都在词典<code>data_batch</code>里。函数先把低清图<code>lq</code>和高清的真值<code>gt</code>从词典里取出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># data</span></span><br><span class="line">lq = data_batch[<span class="string">&#x27;lq&#x27;</span>]</span><br><span class="line">gt = data_batch[<span class="string">&#x27;gt&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>之后，函数计算了$G(I^{lq})$，为后续loss的计算做准备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generator</span></span><br><span class="line">fake_g_output = self.generator(lq)</span><br></pre></td></tr></table></figure>
<p>接下来，是优化生成器<code>self.generator</code>的逻辑。这里面有一些函数调用，我们可以不管它们的实现，大概理解整段代码的意思就行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">losses = <span class="built_in">dict</span>()</span><br><span class="line">log_vars = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># no updates to discriminator parameters.</span></span><br><span class="line">set_requires_grad(self.discriminator, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (self.step_counter % self.disc_steps == <span class="number">0</span></span><br><span class="line">        <span class="keyword">and</span> self.step_counter &gt;= self.disc_init_steps):</span><br><span class="line">    <span class="keyword">if</span> self.pixel_loss:</span><br><span class="line">        losses[<span class="string">&#x27;loss_pix&#x27;</span>] = self.pixel_loss(fake_g_output, gt)</span><br><span class="line">    <span class="keyword">if</span> self.perceptual_loss:</span><br><span class="line">        loss_percep, loss_style = self.perceptual_loss(</span><br><span class="line">            fake_g_output, gt)</span><br><span class="line">        <span class="keyword">if</span> loss_percep <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            losses[<span class="string">&#x27;loss_perceptual&#x27;</span>] = loss_percep</span><br><span class="line">        <span class="keyword">if</span> loss_style <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            losses[<span class="string">&#x27;loss_style&#x27;</span>] = loss_style</span><br><span class="line">    <span class="comment"># gan loss for generator</span></span><br><span class="line">    fake_g_pred = self.discriminator(fake_g_output)</span><br><span class="line">    losses[<span class="string">&#x27;loss_gan&#x27;</span>] = self.gan_loss(</span><br><span class="line">        fake_g_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse loss</span></span><br><span class="line">    loss_g, log_vars_g = self.parse_losses(losses)</span><br><span class="line">    log_vars.update(log_vars_g)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># optimize</span></span><br><span class="line">    optimizer[<span class="string">&#x27;generator&#x27;</span>].zero_grad()</span><br><span class="line">    loss_g.backward()</span><br><span class="line">    optimizer[<span class="string">&#x27;generator&#x27;</span>].step()</span><br></pre></td></tr></table></figure>
<p>为了只训练生成器，要用下面的代码关闭判别器的训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># no updates to discriminator parameters.</span></span><br><span class="line">set_requires_grad(self.discriminator, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>正文说过，训练GAN时一般要先训好判别器，且训练判别器多于训练生成器。因此，下面的if语句可以让判别器训练了<code>self.disc_init_steps</code>步后，每训练<code>self.disc_steps</code>步判别器再训练一步生成器。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if (self.step_counter % self.disc_steps == 0</span><br><span class="line">    and self.step_counter &gt;= self.disc_init_steps):</span><br></pre></td></tr></table></figure><br>if语句块里分别计算了逐像素误差（比如均方误差和L1误差）、感知误差、GAN误差。虽然SRGAN完全抛弃了逐像素误差，但实际训练时我们还是可以按一定比例加上这个误差。这些误差最后会用于训练生成器。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.pixel_loss:</span><br><span class="line">    losses[<span class="string">&#x27;loss_pix&#x27;</span>] = self.pixel_loss(fake_g_output, gt)</span><br><span class="line"><span class="keyword">if</span> self.perceptual_loss:</span><br><span class="line">    loss_percep, loss_style = self.perceptual_loss(</span><br><span class="line">        fake_g_output, gt)</span><br><span class="line">    <span class="keyword">if</span> loss_percep <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        losses[<span class="string">&#x27;loss_perceptual&#x27;</span>] = loss_percep</span><br><span class="line">    <span class="keyword">if</span> loss_style <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        losses[<span class="string">&#x27;loss_style&#x27;</span>] = loss_style</span><br><span class="line"><span class="comment"># gan loss for generator</span></span><br><span class="line">fake_g_pred = self.discriminator(fake_g_output)</span><br><span class="line">losses[<span class="string">&#x27;loss_gan&#x27;</span>] = self.gan_loss(</span><br><span class="line">    fake_g_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># parse loss</span></span><br><span class="line">loss_g, log_vars_g = self.parse_losses(losses)</span><br><span class="line">log_vars.update(log_vars_g)</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimize</span></span><br><span class="line">optimizer[<span class="string">&#x27;generator&#x27;</span>].zero_grad()</span><br><span class="line">loss_g.backward()</span><br><span class="line">optimizer[<span class="string">&#x27;generator&#x27;</span>].step()</span><br></pre></td></tr></table></figure></p>
<p>训练完生成器后，要训练判别器。和生成器的误差计算方法类似，判别器的训练代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># discriminator</span></span><br><span class="line">set_requires_grad(self.discriminator, <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># real</span></span><br><span class="line">real_d_pred = self.discriminator(gt)</span><br><span class="line">loss_d_real = self.gan_loss(</span><br><span class="line">    real_d_pred, target_is_real=<span class="literal">True</span>, is_disc=<span class="literal">True</span>)</span><br><span class="line">loss_d, log_vars_d = self.parse_losses(<span class="built_in">dict</span>(loss_d_real=loss_d_real))</span><br><span class="line">optimizer[<span class="string">&#x27;discriminator&#x27;</span>].zero_grad()</span><br><span class="line">loss_d.backward()</span><br><span class="line">log_vars.update(log_vars_d)</span><br><span class="line"><span class="comment"># fake</span></span><br><span class="line">fake_d_pred = self.discriminator(fake_g_output.detach())</span><br><span class="line">loss_d_fake = self.gan_loss(</span><br><span class="line">    fake_d_pred, target_is_real=<span class="literal">False</span>, is_disc=<span class="literal">True</span>)</span><br><span class="line">loss_d, log_vars_d = self.parse_losses(<span class="built_in">dict</span>(loss_d_fake=loss_d_fake))</span><br><span class="line">loss_d.backward()</span><br><span class="line">log_vars.update(log_vars_d)</span><br><span class="line"></span><br><span class="line">optimizer[<span class="string">&#x27;discriminator&#x27;</span>].step()</span><br></pre></td></tr></table></figure>
<p>这段代码有两个重点：</p>
<ol>
<li>在训练判别器时，要用<code>set_requires_grad(self.discriminator, True)</code>开启判别器的梯度计算。</li>
<li><code>fake_d_pred = self.discriminator(fake_g_output.detach())</code>这一行的<code>detach()</code>很关键。<code>detach()</code>可以中断某张量的梯度跟踪。<code>fake_g_output</code>是由生成器算出来的，如果不把这个张量的梯度跟踪切断掉，在优化判别器时生成器的参数也会跟着优化。</li>
</ol>
<p>函数的最后部分是一些和MMEditing其他代码逻辑的交互，和SRGAN本身没什么关联。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">self.step_counter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">log_vars.pop(<span class="string">&#x27;loss&#x27;</span>)  <span class="comment"># remove the unnecessary &#x27;loss&#x27;</span></span><br><span class="line">outputs = <span class="built_in">dict</span>(</span><br><span class="line">    log_vars=log_vars,</span><br><span class="line">    num_samples=<span class="built_in">len</span>(gt.data),</span><br><span class="line">    results=<span class="built_in">dict</span>(lq=lq.cpu(), gt=gt.cpu(), output=fake_g_output.cpu()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>只要理解了本文的误差计算公式，再看懂了这段代码是如何训练判别器和生成器的，就算是完全理解了SRGAN的核心思想了。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] (SRGAN): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04802">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a></p>
<p>[2] (GAN): <a target="_blank" rel="noopener" href="http://papers.neurips.cc/paper/5423-generative-adversarial-nets.pdf">Generative Adversarial Nets</a></p>
<p>[3] (Perceptual Loss)：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.08155">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></p>
<p>[4] (ESRGAN): <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.00219">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/20220712-custom-op-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/20220712-custom-op-2/" class="post-title-link" itemprop="url">PyTorch 自定义算子：复现CPU和CUDA版的二维卷积</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:20:00" itemprop="dateCreated datePublished" datetime="2022-08-09T14:20:00+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我之前的<a href="https://zhouyifan.net/2022/03/18/20220315-custom-op/">一篇文章</a>介绍了如何给PyTorch添加CPU上的简单的加法算子。在这篇文章里，我将继续展示一个更具体的PyTorch自定义算子示例——自己动手复现二维卷积算子。这个示例是基于PyTorch Extension的，在迁移项目时，不需要自己生成动态库，只需要用<code>setup.py</code>重新编译一遍即可。我会同时介绍CPU版和CUDA版的实现。</p>
<p>许多前沿的神经网络都会对卷积进行一些修改。比如大名鼎鼎的可变形卷积(deformable convolution)。相信看完这篇文章后，大家能看懂PyTorch卷积的实现代码，并大概了解如何修改卷积的实现细节，并把新写好的卷积运用到自己的PyTorch项目中。</p>
<h1 id="PyTorch-Extension-实现二维卷积"><a href="#PyTorch-Extension-实现二维卷积" class="headerlink" title="PyTorch Extension 实现二维卷积"></a>PyTorch Extension 实现二维卷积</h1><h2 id="搭建项目"><a href="#搭建项目" class="headerlink" title="搭建项目"></a>搭建项目</h2><p>在开始写代码前，要准备一个崭新的目录，在这个文件夹里搭建项目。</p>
<p>在根目录下，先创建一个<code>setup.py</code>，之后要填写这份安装文件。</p>
<p>之后，创建一个文件夹，其名字是项目名。在这个文件夹里合适的地方新建一个子文件夹，专门用来放和算子相关的文件。我的项目名叫做<code>panoflow</code>，算子相关文件放在了<code>panoflow/core/op</code>子文件夹下。</p>
<p>接下来，和算子实现相关的文件都应该放在算子文件夹里。使用和测试算子的文件可以放在项目文件夹的其他地方。</p>
<p>由于在实现中我借用了MMCV的代码，还要提前准备好一些头文件。首先新建一个文件<code>pytorch_cpp_helper.hpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> PYTORCH_CPP_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTORCH_CPP_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> at;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.device().is_cuda(), #x <span class="meta-string">&quot; must be a CUDA tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CPU(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(!x.device().is_cuda(), #x <span class="meta-string">&quot; must be a CPU tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CONTIGUOUS(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.is_contiguous(), #x <span class="meta-string">&quot; must be contiguous&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CUDA(x);            \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CPU_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CPU(x);            \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// PYTORCH_CPP_HELPER</span></span></span><br></pre></td></tr></table></figure>
<p>再创建一个文件<code>pytorch_cuda_helper.hpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> PYTORCH_CUDA_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTORCH_CUDA_HELPER</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/ATen.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/cuda/CUDAContext.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;c10/cuda/CUDAGuard.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/cuda/CUDAApplyUtils.cuh&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;THC/THCAtomics.cuh&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;common_cuda_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> at::Half;</span><br><span class="line"><span class="keyword">using</span> at::Tensor;</span><br><span class="line"><span class="keyword">using</span> phalf = at::Half;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __PHALF(x) (x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// PYTORCH_CUDA_HELPER</span></span></span><br></pre></td></tr></table></figure>
<p>还有一个<code>common_cuda_helper.hpp</code>：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> COMMON_CUDA_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> COMMON_CUDA_HELPER</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_1D_KERNEL_LOOP(i, n)                              \</span></span><br><span class="line"><span class="meta">  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n); \</span></span><br><span class="line"><span class="meta">       i += blockDim.x * gridDim.x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_2D_KERNEL_LOOP(i, n, j, m)                             \</span></span><br><span class="line"><span class="meta">  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n);   \</span></span><br><span class="line"><span class="meta">       i += blockDim.x * gridDim.x)                                 \</span></span><br><span class="line"><span class="meta">    for (size_t j = blockIdx.y * blockDim.y + threadIdx.y; j &lt; (m); \</span></span><br><span class="line"><span class="meta">         j += blockDim.y * gridDim.y)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_2D_KERNEL_BLOCK_LOOP(i, n, j, m)          \</span></span><br><span class="line"><span class="meta">  for (size_t i = blockIdx.x; i &lt; (n); i += gridDim.x) \</span></span><br><span class="line"><span class="meta">    for (size_t j = blockIdx.y; j &lt; (m); j += gridDim.y)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THREADS_PER_BLOCK 512</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">GET_BLOCKS</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> num_threads = THREADS_PER_BLOCK)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> optimal_block_num = (N + num_threads - <span class="number">1</span>) / num_threads;</span><br><span class="line">  <span class="keyword">int</span> max_block_num = <span class="number">4096</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">min</span>(optimal_block_num, max_block_num);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ T <span class="title">bilinear_interpolate</span><span class="params">(<span class="keyword">const</span> T* input, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="keyword">const</span> <span class="keyword">int</span> width, T y, T x,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="keyword">const</span> <span class="keyword">int</span> index <span class="comment">/* index for debug only*/</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// deal with cases that inverse elements are out of feature map boundary</span></span><br><span class="line">  <span class="keyword">if</span> (y &lt; <span class="number">-1.0</span> || y &gt; height || x &lt; <span class="number">-1.0</span> || x &gt; width) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y &lt;= <span class="number">0</span>) y = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (x &lt;= <span class="number">0</span>) x = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> y_low = (<span class="keyword">int</span>)y;</span><br><span class="line">  <span class="keyword">int</span> x_low = (<span class="keyword">int</span>)x;</span><br><span class="line">  <span class="keyword">int</span> y_high;</span><br><span class="line">  <span class="keyword">int</span> x_high;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y_low &gt;= height - <span class="number">1</span>) &#123;</span><br><span class="line">    y_high = y_low = height - <span class="number">1</span>;</span><br><span class="line">    y = (T)y_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    y_high = y_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (x_low &gt;= width - <span class="number">1</span>) &#123;</span><br><span class="line">    x_high = x_low = width - <span class="number">1</span>;</span><br><span class="line">    x = (T)x_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    x_high = x_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T ly = y - y_low;</span><br><span class="line">  T lx = x - x_low;</span><br><span class="line">  T hy = <span class="number">1.</span> - ly, hx = <span class="number">1.</span> - lx;</span><br><span class="line">  <span class="comment">// do bilinear interpolation</span></span><br><span class="line">  T v1 = input[y_low * width + x_low];</span><br><span class="line">  T v2 = input[y_low * width + x_high];</span><br><span class="line">  T v3 = input[y_high * width + x_low];</span><br><span class="line">  T v4 = input[y_high * width + x_high];</span><br><span class="line">  T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;</span><br><span class="line"></span><br><span class="line">  T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">bilinear_interpolate_gradient</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> height, <span class="keyword">const</span> <span class="keyword">int</span> width, T y, T x, T&amp; w1, T&amp; w2, T&amp; w3, T&amp; w4,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span>&amp; x_low, <span class="keyword">int</span>&amp; x_high, <span class="keyword">int</span>&amp; y_low, <span class="keyword">int</span>&amp; y_high,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> index <span class="comment">/* index for debug only*/</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// deal with cases that inverse elements are out of feature map boundary</span></span><br><span class="line">  <span class="keyword">if</span> (y &lt; <span class="number">-1.0</span> || y &gt; height || x &lt; <span class="number">-1.0</span> || x &gt; width) &#123;</span><br><span class="line">    <span class="comment">// empty</span></span><br><span class="line">    w1 = w2 = w3 = w4 = <span class="number">0.</span>;</span><br><span class="line">    x_low = x_high = y_low = y_high = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y &lt;= <span class="number">0</span>) y = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (x &lt;= <span class="number">0</span>) x = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  y_low = (<span class="keyword">int</span>)y;</span><br><span class="line">  x_low = (<span class="keyword">int</span>)x;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y_low &gt;= height - <span class="number">1</span>) &#123;</span><br><span class="line">    y_high = y_low = height - <span class="number">1</span>;</span><br><span class="line">    y = (T)y_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    y_high = y_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (x_low &gt;= width - <span class="number">1</span>) &#123;</span><br><span class="line">    x_high = x_low = width - <span class="number">1</span>;</span><br><span class="line">    x = (T)x_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    x_high = x_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T ly = y - y_low;</span><br><span class="line">  T lx = x - x_low;</span><br><span class="line">  T hy = <span class="number">1.</span> - ly, hx = <span class="number">1.</span> - lx;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// reference in forward</span></span><br><span class="line">  <span class="comment">// T v1 = input[y_low * width + x_low];</span></span><br><span class="line">  <span class="comment">// T v2 = input[y_low * width + x_high];</span></span><br><span class="line">  <span class="comment">// T v3 = input[y_high * width + x_low];</span></span><br><span class="line">  <span class="comment">// T v4 = input[y_high * width + x_high];</span></span><br><span class="line">  <span class="comment">// T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);</span></span><br><span class="line"></span><br><span class="line">  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// COMMON_CUDA_HELPER</span></span></span><br></pre></td></tr></table></figure></p>
<p>这些文件添加了CPU和CUDA实现时需要的头文件和定义，后面的C++源码会用到它们。</p>
<h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="C-实现"><a href="#C-实现" class="headerlink" title="C++实现"></a>C++实现</h3><p>在用C++实现一个算子时，我们要编写一个形如这样的文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">my_add</span><span class="params">(torch::Tensor t1, torch::Tensor t2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">return</span> t1 + t2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">	m.<span class="built_in">def</span>(<span class="string">&quot;my_add&quot;</span>, my_add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个C++文件主要包含两部分内容：算子的实现函数和C++接口绑定。在实现卷积时，也是要实现这两部分内容。</p>
<p>在修改一个现有的算子时，最好的方法不是从头写一个，而是去开源库里找一份实现，并在这个基础上进行修改。</p>
<blockquote>
<p>我在MMCV的仓库里找到了<a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops">可变形卷积的实现</a>，并把它拆解回了普通的卷积。我参考了这篇教程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464492627">手把手教你如何高效地在 MMCV 中贡献算子</a>。另外，这份笔记还参考了<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension">PyTorch官方Extension教程</a>。</p>
</blockquote>
<p>找到了卷积的实现后，在算子文件夹下新建一个cpp源文件。比如我的文件路径就是<code>panoflow/core/op/my_conv.cpp</code>。这样一个普通卷积的实现如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;pytorch_cpp_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_shape_check</span><span class="params">(at::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">                         at::Tensor weight, <span class="keyword">int</span> kH,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> kW, <span class="keyword">int</span> dH, <span class="keyword">int</span> dW, <span class="keyword">int</span> padH, <span class="keyword">int</span> padW,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> dilationH, <span class="keyword">int</span> dilationW, <span class="keyword">int</span> group)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">        weight.<span class="built_in">ndimension</span>() == <span class="number">4</span>,</span><br><span class="line">        <span class="string">&quot;4D weight tensor (nOutputPlane,nInputPlane,kH,kW) expected, but got: %s&quot;</span>,</span><br><span class="line">        weight.<span class="built_in">ndimension</span>());</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(weight.<span class="built_in">is_contiguous</span>(), <span class="string">&quot;weight tensor has to be contiguous&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(kW &gt; <span class="number">0</span> &amp;&amp; kH &gt; <span class="number">0</span>,</span><br><span class="line">                <span class="string">&quot;kernel size should be greater than zero, but got kH: %d kW: %d&quot;</span>,</span><br><span class="line">                kH, kW);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>((weight.<span class="built_in">size</span>(<span class="number">2</span>) == kH &amp;&amp; weight.<span class="built_in">size</span>(<span class="number">3</span>) == kW),</span><br><span class="line">                <span class="string">&quot;kernel size should be consistent with weight, &quot;</span>,</span><br><span class="line">                <span class="string">&quot;but got kH: %d kW: %d weight.size(2): %d, weight.size(3): %d&quot;</span>,</span><br><span class="line">                kH, kW, weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(dW &gt; <span class="number">0</span> &amp;&amp; dH &gt; <span class="number">0</span>,</span><br><span class="line">                <span class="string">&quot;stride should be greater than zero, but got dH: %d dW: %d&quot;</span>, dH,</span><br><span class="line">                dW);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">        dilationW &gt; <span class="number">0</span> &amp;&amp; dilationH &gt; <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;dilation should be greater than 0, but got dilationH: %d dilationW: %d&quot;</span>,</span><br><span class="line">        dilationH, dilationW);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> ndim = input.<span class="built_in">ndimension</span>();</span><br><span class="line">    <span class="keyword">int</span> dimf = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> dimh = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> dimw = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ndim == <span class="number">4</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        dimf++;</span><br><span class="line">        dimh++;</span><br><span class="line">        dimw++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(ndim == <span class="number">3</span> || ndim == <span class="number">4</span>,</span><br><span class="line">                <span class="string">&quot;3D or 4D input tensor expected but got: %s&quot;</span>, ndim);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> nInputPlane = weight.<span class="built_in">size</span>(<span class="number">1</span>) * group;</span><br><span class="line">    <span class="keyword">long</span> inputHeight = input.<span class="built_in">size</span>(dimh);</span><br><span class="line">    <span class="keyword">long</span> inputWidth = input.<span class="built_in">size</span>(dimw);</span><br><span class="line">    <span class="keyword">long</span> nOutputPlane = weight.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> outputHeight =</span><br><span class="line">        (inputHeight + <span class="number">2</span> * padH - (dilationH * (kH - <span class="number">1</span>) + <span class="number">1</span>)) / dH + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> outputWidth =</span><br><span class="line">        (inputWidth + <span class="number">2</span> * padW - (dilationW * (kW - <span class="number">1</span>) + <span class="number">1</span>)) / dW + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (outputWidth &lt; <span class="number">1</span> || outputHeight &lt; <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">AT_ERROR</span>(</span><br><span class="line">            <span class="string">&quot;Given input size: (%ld x %ld x %ld). &quot;</span></span><br><span class="line">            <span class="string">&quot;Calculated output size: (%ld x %ld x %ld). Output size is too small&quot;</span>,</span><br><span class="line">            nInputPlane, inputHeight, inputWidth, nOutputPlane, outputHeight,</span><br><span class="line">            outputWidth);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(input.<span class="built_in">size</span>(<span class="number">1</span>) == nInputPlane,</span><br><span class="line">                <span class="string">&quot;invalid number of input planes, expected: %d, but got: %d&quot;</span>,</span><br><span class="line">                nInputPlane, input.<span class="built_in">size</span>(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>((inputHeight &gt;= kH &amp;&amp; inputWidth &gt;= kW),</span><br><span class="line">                <span class="string">&quot;input image is smaller than kernel&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_forward</span><span class="params">(Tensor input, Tensor weight, Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Tensor output, Tensor columns, <span class="keyword">int</span> kW,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> kH, <span class="keyword">int</span> dW, <span class="keyword">int</span> dH, <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> dilationW, <span class="keyword">int</span> dilationH, <span class="keyword">int</span> group,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> im2col_step)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> isCuda = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (input.<span class="built_in">device</span>().<span class="built_in">is_cuda</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(input);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(weight);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(bias);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(output);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(columns);</span><br><span class="line">        isCuda = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(input);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(weight);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(bias);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(output);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(columns);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">my_conv_shape_check</span>(input, weight, kH, kW, dH, dW, padH,</span><br><span class="line">                        padW, dilationH, dilationW, group);</span><br><span class="line">    <span class="function">at::DeviceGuard <span class="title">guard</span><span class="params">(input.device())</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> batch = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (input.<span class="built_in">ndimension</span>() == <span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Force batch</span></span><br><span class="line">        batch = <span class="number">0</span>;</span><br><span class="line">        input.<span class="built_in">unsqueeze_</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> batchSize = input.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> nInputPlane = input.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">long</span> inputHeight = input.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">long</span> inputWidth = input.<span class="built_in">size</span>(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> nOutputPlane = weight.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> outputWidth =</span><br><span class="line">        (inputWidth + <span class="number">2</span> * padW - (dilationW * (kW - <span class="number">1</span>) + <span class="number">1</span>)) / dW + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> outputHeight =</span><br><span class="line">        (inputHeight + <span class="number">2</span> * padH - (dilationH * (kH - <span class="number">1</span>) + <span class="number">1</span>)) / dH + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    output = output.<span class="built_in">view</span>(&#123;batchSize / im2col_step, im2col_step, nOutputPlane,</span><br><span class="line">                          outputHeight, outputWidth&#125;);</span><br><span class="line">    columns = at::<span class="built_in">zeros</span>(</span><br><span class="line">        &#123;nInputPlane * kW * kH, im2col_step * outputHeight * outputWidth&#125;,</span><br><span class="line">        input.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    input = input.<span class="built_in">view</span>(&#123;batchSize / im2col_step, im2col_step, nInputPlane,</span><br><span class="line">                        inputHeight, inputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    Tensor output_buffer = at::<span class="built_in">zeros</span>(&#123;batchSize / im2col_step, nOutputPlane,</span><br><span class="line">                                      im2col_step * outputHeight, outputWidth&#125;,</span><br><span class="line">                                     output.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(</span><br><span class="line">        &#123;output_buffer.<span class="built_in">size</span>(<span class="number">0</span>), group, output_buffer.<span class="built_in">size</span>(<span class="number">1</span>) / group,</span><br><span class="line">         output_buffer.<span class="built_in">size</span>(<span class="number">2</span>), output_buffer.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize / im2col_step; elt++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (isCuda)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">my_conv_im2col_cuda</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                            inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                            dilationW, im2col_step, columns);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">my_conv_im2col_cpu</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                            inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                            dilationW, im2col_step, columns);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        columns = columns.<span class="built_in">view</span>(&#123;group, columns.<span class="built_in">size</span>(<span class="number">0</span>) / group, columns.<span class="built_in">size</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">        weight = weight.<span class="built_in">view</span>(&#123;group, weight.<span class="built_in">size</span>(<span class="number">0</span>) / group, weight.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">                              weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group; g++)</span><br><span class="line">        &#123;</span><br><span class="line">            output_buffer[elt][g] = output_buffer[elt][g]</span><br><span class="line">                                        .<span class="built_in">flatten</span>(<span class="number">1</span>)</span><br><span class="line">                                        .<span class="built_in">addmm_</span>(weight[g].<span class="built_in">flatten</span>(<span class="number">1</span>), columns[g])</span><br><span class="line">                                        .<span class="built_in">view_as</span>(output_buffer[elt][g]);</span><br><span class="line">        &#125;</span><br><span class="line">        columns =</span><br><span class="line">            columns.<span class="built_in">view</span>(&#123;columns.<span class="built_in">size</span>(<span class="number">0</span>) * columns.<span class="built_in">size</span>(<span class="number">1</span>), columns.<span class="built_in">size</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">        weight = weight.<span class="built_in">view</span>(&#123;weight.<span class="built_in">size</span>(<span class="number">0</span>) * weight.<span class="built_in">size</span>(<span class="number">1</span>), weight.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">                              weight.<span class="built_in">size</span>(<span class="number">3</span>), weight.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(</span><br><span class="line">        &#123;output_buffer.<span class="built_in">size</span>(<span class="number">0</span>), output_buffer.<span class="built_in">size</span>(<span class="number">1</span>) * output_buffer.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">         output_buffer.<span class="built_in">size</span>(<span class="number">3</span>), output_buffer.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(&#123;batchSize / im2col_step, nOutputPlane,</span><br><span class="line">                                        im2col_step, outputHeight, outputWidth&#125;);</span><br><span class="line">    output_buffer.<span class="built_in">transpose_</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    output.<span class="built_in">copy_</span>(output_buffer);</span><br><span class="line">    output = output.<span class="built_in">view</span>(&#123;batchSize, nOutputPlane, outputHeight, outputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    bias = bias.<span class="built_in">view</span>(&#123;<span class="number">1</span>, bias.<span class="built_in">size</span>(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>&#125;);</span><br><span class="line">    output.<span class="built_in">add_</span>(bias);</span><br><span class="line"></span><br><span class="line">    input = input.<span class="built_in">view</span>(&#123;batchSize, nInputPlane, inputHeight, inputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (batch == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        output = output.<span class="built_in">view</span>(&#123;nOutputPlane, outputHeight, outputWidth&#125;);</span><br><span class="line">        input = input.<span class="built_in">view</span>(&#123;nInputPlane, inputHeight, inputWidth&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; index++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_cpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">      m.<span class="built_in">def</span>(<span class="string">&quot;my_conv_forward&quot;</span>, my_conv_forward, <span class="string">&quot;my_conv_forward&quot;</span>,</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;input&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;weight&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;bias&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;output&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;columns&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;kW&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;kH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dW&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;padW&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;padH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dilationW&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dilationH&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;group&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;im2col_step&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这份实现非常长，我挑一些重点的内容讲解。</p>
<p>从最下面的<code>PYBIND11_MODULE(my_ops, m)</code>看起。这里的<code>my_ops</code>是生成的库名，可以随便取名。待会要import这个库名。代码块里<code>m.def</code>用于定义C++函数的Python接口。<code>&quot;my_conv_forward&quot;</code>是Python调用时的函数名称，<code>my_conv_forward</code>是被Python代码调用的这份代码里的C++函数名称。也就是说，这份卷积实现的入口函数就是<code>my_conv_forward</code>。我们从这个函数看起。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_forward</span><span class="params">(Tensor input, Tensor weight, Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Tensor output, Tensor columns, <span class="keyword">int</span> kW,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> kH, <span class="keyword">int</span> dW, <span class="keyword">int</span> dH, <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> dilationW, <span class="keyword">int</span> dilationH, <span class="keyword">int</span> group,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> im2col_step)</span></span></span><br></pre></td></tr></table></figure>
<p><code>my_conv_forward</code>就是卷积的主函数。它的参数除了PyTorch的<code>Conv2d</code>传入的参数外，还多了两个参数<code>output, columus</code>。这两个张量是保存中间结果的，在PyTorch侧是看不到的。<code>output</code>用于保存卷积输出，<code>columns</code>用于保存卷积时的列矩阵。底层实现卷积时，会先把图像转换成一个用列表示的矩阵，再把卷积操作当成一个矩阵乘法来完成。其中，第一步操作叫做”im2col”。对此原理不熟的话可以参考这篇文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63974249。">https://zhuanlan.zhihu.com/p/63974249。</a></p>
<p><code>my_conv_forward</code>函数的大部分内容都是在做类型检查和张量形状转换。在修改卷积实现时，这些东西都可以不用改。整个卷积操作的核心都在这一部分：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize / im2col_step; elt++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (isCuda)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">my_conv_im2col_cuda</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                        inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                        dilationW, im2col_step, columns);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">my_conv_im2col_cpu</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                        inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                        dilationW, im2col_step, columns);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    columns = columns.<span class="built_in">view</span>(&#123;group, columns.<span class="built_in">size</span>(<span class="number">0</span>) / group, columns.<span class="built_in">size</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">    weight = weight.<span class="built_in">view</span>(&#123;group, weight.<span class="built_in">size</span>(<span class="number">0</span>) / group, weight.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">                          weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group; g++)</span><br><span class="line">    &#123;</span><br><span class="line">        output_buffer[elt][g] = output_buffer[elt][g]</span><br><span class="line">                                    .<span class="built_in">flatten</span>(<span class="number">1</span>)</span><br><span class="line">                                    .<span class="built_in">addmm_</span>(weight[g].<span class="built_in">flatten</span>(<span class="number">1</span>), columns[g])</span><br><span class="line">                                    .<span class="built_in">view_as</span>(output_buffer[elt][g]);</span><br><span class="line">    &#125;</span><br><span class="line">    columns =</span><br><span class="line">        columns.<span class="built_in">view</span>(&#123;columns.<span class="built_in">size</span>(<span class="number">0</span>) * columns.<span class="built_in">size</span>(<span class="number">1</span>), columns.<span class="built_in">size</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">    weight = weight.<span class="built_in">view</span>(&#123;weight.<span class="built_in">size</span>(<span class="number">0</span>) * weight.<span class="built_in">size</span>(<span class="number">1</span>), weight.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">                          weight.<span class="built_in">size</span>(<span class="number">3</span>), weight.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码先做了<code>im2col</code>操作，再做了矩阵乘法。其实，包括可变形卷积在内，各种稀奇古怪的卷积操作通过靠修改<code>im2col</code>来完成的。CPU和CUDA版卷积的主要区别，也体现在<code>im2col</code>中（后面的矩阵乘法在CPU和CUDA上都能用）。</p>
<p>由于是讲CPU实现，这里的CUDA实现我暂时放了一个空函数。<code>my_conv_im2col_cpu</code>的内容如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_cpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数其实只是处理了一下输入，真正的实现在<code>my_conv_im2col_cpu_kernel</code>里。<code>AT_DISPATCH_FLOATING_TYPES_AND_HALF</code>可以让实现兼容半精度和普通float，所以实现<code>my_conv_im2col_cpu_kernel</code>得写成一个模板函数。</p>
<p><code>my_conv_im2col_cpu_kernel</code>的实现如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; index++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>它的作用就是把图像里的数据搬到做卷积运算的<code>column</code>里。循环遍历每一次卷积的每一个位置，把待运算的量填入<code>column</code>。卷积里的所有参数(pad, stride, …)都是在这段函数里生效的。想实现可变形卷积等改进，也要修改这个函数。</p>
<h3 id="Python封装"><a href="#Python封装" class="headerlink" title="Python封装"></a>Python封装</h3><p>实现好了后，如果编译完了的话，刚刚的卷积接口可以通过以下方式在Python里调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> my_ops</span><br><span class="line">my_ops.my_conv_forward(...)</span><br></pre></td></tr></table></figure>
<p>这里的<code>my_ops</code>这个名称必须和开始<code>PYBIND11_MODULE(my_ops, m)</code>里面那个库名称对应。</p>
<p>基于这个接口，可以仿照PyTorch中<code>Conv2d</code>的接口，编写一个和<code>Conv2d</code>等价的<code>torch.nn.Module</code>出来。我的这个Python文件的路径是<code>panoflow/core/op/my_conv.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.utils <span class="keyword">import</span> _pair</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> my_ops</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyConvF</span>(<span class="params">Function</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="built_in">input</span>: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                weight,</span></span></span><br><span class="line"><span class="params"><span class="function">                bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                groups=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                im2col_step=<span class="number">32</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">input</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">input</span>.dim() != <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&#x27;Expected 4D tensor as input, got <span class="subst">&#123;<span class="built_in">input</span>.dim()&#125;</span>D tensor \</span></span><br><span class="line"><span class="string">                  instead.&#x27;</span>)</span><br><span class="line">        ctx.stride = _pair(stride)</span><br><span class="line">        ctx.padding = _pair(padding)</span><br><span class="line">        ctx.dilation = _pair(dilation)</span><br><span class="line">        ctx.groups = groups</span><br><span class="line">        ctx.im2col_step = im2col_step</span><br><span class="line"></span><br><span class="line">        weight = weight.type_as(<span class="built_in">input</span>)</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>, weight)</span><br><span class="line"></span><br><span class="line">        output = <span class="built_in">input</span>.new_empty(MyConvF._output_size(ctx, <span class="built_in">input</span>, weight))</span><br><span class="line"></span><br><span class="line">        ctx.bufs_ = [<span class="built_in">input</span>.new_empty(<span class="number">0</span>), <span class="built_in">input</span>.new_empty(<span class="number">0</span>)]  <span class="comment"># columns, ones</span></span><br><span class="line"></span><br><span class="line">        cur_im2col_step = <span class="built_in">min</span>(ctx.im2col_step, <span class="built_in">input</span>.size(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">assert</span> (<span class="built_in">input</span>.size(<span class="number">0</span>) % cur_im2col_step</span><br><span class="line">                ) == <span class="number">0</span>, <span class="string">&#x27;batch size must be divisible by im2col_step&#x27;</span></span><br><span class="line"></span><br><span class="line">        my_ops.my_conv_forward(</span><br><span class="line">            <span class="built_in">input</span>,</span><br><span class="line">            weight,</span><br><span class="line">            bias,</span><br><span class="line">            output,</span><br><span class="line">            ctx.bufs_[<span class="number">0</span>],</span><br><span class="line">            kW=weight.size(<span class="number">3</span>),</span><br><span class="line">            kH=weight.size(<span class="number">2</span>),</span><br><span class="line">            dW=ctx.stride[<span class="number">1</span>],</span><br><span class="line">            dH=ctx.stride[<span class="number">0</span>],</span><br><span class="line">            padW=ctx.padding[<span class="number">1</span>],</span><br><span class="line">            padH=ctx.padding[<span class="number">0</span>],</span><br><span class="line">            dilationW=ctx.dilation[<span class="number">1</span>],</span><br><span class="line">            dilationH=ctx.dilation[<span class="number">0</span>],</span><br><span class="line">            group=ctx.groups,</span><br><span class="line">            im2col_step=cur_im2col_step)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_output_size</span>(<span class="params">ctx, <span class="built_in">input</span>, weight</span>):</span></span><br><span class="line">        channels = weight.size(<span class="number">0</span>)</span><br><span class="line">        output_size = (<span class="built_in">input</span>.size(<span class="number">0</span>), channels)</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">input</span>.dim() - <span class="number">2</span>):</span><br><span class="line">            in_size = <span class="built_in">input</span>.size(d + <span class="number">2</span>)</span><br><span class="line">            pad = ctx.padding[d]</span><br><span class="line">            kernel = ctx.dilation[d] * (weight.size(d + <span class="number">2</span>) - <span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">            stride_ = ctx.stride[d]</span><br><span class="line">            output_size += ((in_size + (<span class="number">2</span> * pad) - kernel) // stride_ + <span class="number">1</span>, )</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">all</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> s: s &gt; <span class="number">0</span>, output_size)):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&#x27;convolution input is too small (output would be &#x27;</span> +</span><br><span class="line">                <span class="string">&#x27;x&#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, output_size)) + <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> output_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_conv = MyConvF.apply</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyConv2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 in_channels: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 out_channels: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 kernel_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 bias: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        kernel_size_ = _pair(kernel_size)</span><br><span class="line">        stride_ = _pair(stride)</span><br><span class="line">        padding_ = _pair(padding)</span><br><span class="line">        dilation_ = _pair(dilation)</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.kernel_size = kernel_size_</span><br><span class="line">        self.stride = stride_</span><br><span class="line">        self.padding = padding_</span><br><span class="line">        self.dilation = dilation_</span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.weight = Parameter(</span><br><span class="line">            torch.Tensor(out_channels, in_channels // groups, *kernel_size_))</span><br><span class="line">        self.bias = Parameter(torch.Tensor(out_channels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Useless attributes</span></span><br><span class="line">        self.transposed = <span class="literal">None</span></span><br><span class="line">        self.output_padding = <span class="literal">None</span></span><br><span class="line">        self.padding_mode = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">return</span> my_conv(<span class="built_in">input</span>, self.weight, self.bias, self.stride,</span><br><span class="line">                       self.padding, self.dilation, self.groups)</span><br></pre></td></tr></table></figure>
<p>以后，用自己的卷积<code>MyConv2d</code>就和用普通的<code>Conv2d</code>一样了。</p>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>打开外面的<code>setup.py</code>，填写以下内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">src_root = <span class="string">&#x27;panoflow/core/op&#x27;</span></span><br><span class="line">cpp_src = [<span class="string">&#x27;my_conv.cpp&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    include_dirs = [<span class="string">&#x27;panoflow/core/op&#x27;</span>]</span><br><span class="line">    cpp_path = [os.path.join(src_root, src) <span class="keyword">for</span> src <span class="keyword">in</span> cpp_src]</span><br><span class="line"></span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;panoflow&#x27;</span>,</span><br><span class="line">        ext_modules=[</span><br><span class="line">            cpp_extension.CppExtension(</span><br><span class="line">                <span class="string">&#x27;my_ops&#x27;</span>, cpp_path, include_dirs=include_dirs)</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>其中的路径要根据自己的实际情况修改。</p>
<p>和编译相关的内容都写在<code>cpp_extension.CppExtension</code>里。其中，源文件要写在第二个参数里，头文件目录要写在<code>include_dirs</code>。由于我的源文件放在<code>panoflow/core/op</code>里，我写了个源文件名数组<code>cpp_src</code>，在传参前把路径组合了一下。由于<code>include_dirs</code>和源文件在同一个目录下，我也填的是<code>panoflow/core/op</code>。</p>
<p>写完了<code>setup.py</code>后，运行<code>python setup.py develop</code>，就能一键编译和安装。如果运行后没有报编译错误，就可以把实现的卷积用起来了。</p>
<h3 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h3><p>用单元测试可以快速地验证卷积是否实现成功。我写了一个简单的单元测试文件，在任意一个文件夹下创建该文件即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> panoflow.core.op.my_conv <span class="keyword">import</span> MyConv2d</span><br><span class="line"></span><br><span class="line">inc = <span class="number">3</span></span><br><span class="line">outc = <span class="number">4</span></span><br><span class="line">img_shaspe = (<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># device_name = &#x27;cuda:0&#x27;</span></span><br><span class="line">device_name = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">open_bias = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one</span>():</span></span><br><span class="line">    ts = torch.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_two</span>():</span></span><br><span class="line">    ts = torch.rand([<span class="number">1</span>, inc, *img_shaspe]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test_one()</span><br><span class="line">    test_two()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中，<code>panoflow.core.op.my_conv</code>是我刚刚放<code>MyConv2d</code>的Python模块。</p>
<p>直接运行这个Python文件，如果没有任何输出（报错信息），就说明卷积实现成功了。</p>
<h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h2><h3 id="C-实现-1"><a href="#C-实现-1" class="headerlink" title="C++实现"></a>C++实现</h3><p>在刚刚的实现中，有一个<code>my_conv_im2col_cuda</code>的实现是空着的。在CUDA版本中，我们要实现这个函数。不过，这个函数要放在一个用<code>nvcc</code>编译的<code>.cu</code>文件里。<strong>注意！注意！注意！</strong> 因此，<code>my_conv.cpp</code>里那个空的<code>my_conv_im2col_cuda</code>实现应该全部删掉。</p>
<p>新建一个文件<code>my_conv_cuda.cu</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Modify from https://github.com/open-mmlab/mmcv/blob/my_conv/mmcv/ops/csrc/common/cuda/deform_conv_cuda_kernel.cuh</span></span><br><span class="line"><span class="comment">// Copyright (c) OpenMMLab. All rights reserved.</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/types.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;pytorch_cuda_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">my_conv_im2col_gpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CUDA_1D_KERNEL_LOOP</span>(index, n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;my_conv_im2col_gpu&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_gpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;&lt;&lt;&lt;<span class="built_in">GET_BLOCKS</span>(num_kernels),</span><br><span class="line">                                                THREADS_PER_BLOCK, <span class="number">0</span>,</span><br><span class="line">                                                at::cuda::<span class="built_in">getCurrentCUDAStream</span>()&gt;&gt;&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_CUDA_CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和CPU版的类似，<code>my_conv_im2col_cuda</code>也是预处理了输入，并调用核函数<code>my_conv_im2col_gpu_kernel</code>来实现<code>im2col</code>。</p>
<p>CUDA实现和CPU几乎一样，唯一的区别就是for循环变成了<code>CUDA_1D_KERNEL_LOOP(index, n)</code>。这个宏是头文件里帮我们定义的，它简化了CUDA的一维循环。</p>
<h3 id="编译-1"><a href="#编译-1" class="headerlink" title="编译"></a>编译</h3><p>修改<code>setup.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">src_root = <span class="string">&#x27;panoflow/core/op&#x27;</span></span><br><span class="line">cpp_src = [<span class="string">&#x27;my_conv.cpp&#x27;</span>, <span class="string">&#x27;my_conv_cuda.cu&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    include_dirs = [<span class="string">&#x27;panoflow/core/op&#x27;</span>]</span><br><span class="line">    cpp_path = [os.path.join(src_root, src) <span class="keyword">for</span> src <span class="keyword">in</span> cpp_src]</span><br><span class="line"></span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;panoflow&#x27;</span>,</span><br><span class="line">        ext_modules=[</span><br><span class="line">            cpp_extension.CUDAExtension(</span><br><span class="line">                <span class="string">&#x27;my_ops&#x27;</span>, cpp_path, include_dirs=include_dirs)</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>首先，要把源文件加入<code>cpp_src</code>里。之后，把<code>CppExtension</code>改成<code>CUDAExtension</code>。这样，就能编译新写的CUDA文件了。</p>
<p>写完了之后，再次<code>python setup.py develop</code>编译即可。</p>
<blockquote>
<p>编译小技巧：不拿IDE直接写C++和CUDA源代码是很容易出错误的。但如果你想只用<code>setup.py</code>来验证代码的正确性，可以<code>python setup.py develop &gt; tmp.txt</code>把编译输出重定向到一个文件里来查看。由于编译时的信息过多，在命令行里很难从一堆编译warning里找到最重要的error。</p>
</blockquote>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>由于Python部分在之前都已经写好了，可以直接用刚刚的单元测试文件测试了。只要把刚刚那份文件的<code>device_name</code>改成<code>cuda:0</code>即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> panoflow.core.op.my_conv <span class="keyword">import</span> MyConv2d</span><br><span class="line"></span><br><span class="line">inc = <span class="number">3</span></span><br><span class="line">outc = <span class="number">4</span></span><br><span class="line">img_shaspe = (<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">device_name = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line"><span class="comment"># device_name = &#x27;cpu&#x27;</span></span><br><span class="line">open_bias = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one</span>():</span></span><br><span class="line">    ts = torch.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_two</span>():</span></span><br><span class="line">    ts = torch.rand([<span class="number">1</span>, inc, *img_shaspe]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test_one()</span><br><span class="line">    test_two()</span><br></pre></td></tr></table></figure>
<p>同样，没报错就说明写对了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/20220807-ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/20220807-ResNet/" class="post-title-link" itemprop="url">ResNet 论文概览与精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:19:36" itemprop="dateCreated datePublished" datetime="2022-08-09T14:19:36+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">论文阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ResNet是CV中的经典网络。在这篇文章中，我将按照阅读论文的通用方法由粗至精地解读这篇文章。如果你对ResNet不熟，最好对着原论文阅读本文。如果你已经很熟悉ResNet了，也可以通过这篇文章查缺补漏。</p>
<h2 id="粗读"><a href="#粗读" class="headerlink" title="粗读"></a>粗读</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>摘要的前三句话开门见山地介绍本文要解决的问题以及解决的方法。</p>
<ul>
<li>问题：<strong>较深</strong>的神经网络很难训练。</li>
<li>本文的工作：提出了一种能够轻松训练比以往的网络要深得多的残差学习框架。</li>
<li>本文方法的进一步解释：神经网络的层将拟合一个基于输入的残差函数，而不是一个没有参考的函数。</li>
</ul>
<p>随后，摘要展示了这种方法取得的成就：</p>
<ul>
<li>经实验而非理论证明，增加深度后，模型的训练效果和测试效果都得到了提升。</li>
<li>精度打败了当时所有的分类模型。</li>
<li>深度高达152，大幅超过了当时较火的19层的VGG，同时并没有增加多少计算量。</li>
<li>精度打败了当时所有的检测、分割等任务的模型。这证明这种方法的泛化性强。</li>
</ul>
<p>这篇摘要没有罗列贡献，而是在提出问题后轻轻一点，介绍了本文的方法及其作用。随后，所有的篇幅都在秀这种方法的成效。看完这段话，我们可能还不知道“残差”是怎么算的，但我们知道了这种方法很厉害，测试精度、训练难易度、泛化性都十分优秀，成功解决了较深的模型难以训练的问题。</p>
<p>在这轮粗读中，我们可以带着以下问题去概览论文：</p>
<ul>
<li>文章要解决的具体是一个怎样的问题？为什么较深的神经网络很难训练？</li>
<li>文章提出的“残差”是什么？它为什么能解决深模型难训练的问题？</li>
<li>凭什么说深模型难训练的问题被解决了？实验是怎么做的？</li>
<li>这篇文章提出的模型比其他模型好在哪？</li>
</ul>
<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>引言用十分清晰的逻辑阐明了本文的核心思想——<strong>深度残差学习</strong>框架。引言先介绍“训练更深的神经网络”这一问题的由来，再抽丝剥茧地讨论该问题的解决方法，最后自然过渡到了本文的核心思想上。看完引言，我们就应该看懂这篇文章。</p>
<p>深度卷积神经网络的有效性，得益于它的“深度”。那么，是不是层数更深的网络就更好呢？过去的实验显示：不是的。</p>
<p>更深的网络面临的第一个问题是梯度弥散/爆炸，这些问题会令网络难以收敛。但是，通过参数归一初始化和归一化层，这一问题以及得到了有效缓解。证据就是，较深的网络能够收敛。</p>
<p>这时，较深网络又暴露出了第二个问题——<strong>退化</strong>：增加网络的深度，反而会降低网络的精度。这种退化和过拟合还不同，因为退化不仅导致精度降低，还提升了模型的训练误差。</p>
<p>这种退化，是不是说明较深的网络本质上就比不过较浅的网络呢？其实并不是，退化只是因为较深的网络不是那么容易优化。考虑一个较浅的网络，我们往它的后面增加几层全等映射(y=x)。这个变深的网络与原来的网络完全等价。从这个角度看，更深的网络最少不会比更浅的网络更差才对。因此，我们要想办法让学习算法能够更好地优化更深的网络，最起码优化出一个不比较浅网络更差的网络。也就是说，我们要想办法让学习算法能够轻松学会恒等映射。</p>
<p>文章提出了一种深度残差学习框架。假设一层网络表示的映射是$H(x)$，则该层的非线性层要拟合另一个表示残差的映射$F(x) := H(x) - x$。换个角度看，就是一层网络的输出由原来的$F(x)$变成了$F(x)+x$，多加上了一个$x$。这样，只要令$F(x)=0$，网络就能轻松描述恒等映射，较深的网络最起码不比较浅的网络更差了。</p>
<p>多个数据集上的实验表明，这种方法不仅容易训练，还能得到更高的精度。在多项任务的CV竞赛中，这种方法都独占鳌头。</p>
<p>看完了引言，我们基本能知道文章在解决怎样的问题，也大致明白了残差学习的原理。接下来，我们来过一过这篇文章的实验，看看这种方法的效果究竟如何。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>我们主要看几组ImageNet上的实验。为了方便称呼，作者把不使用残差连接的普通网络叫做“平坦(plain)”网络。第一组实验对比了不同深度的平坦网络的训练误差（细线）和验证误差（粗线）。</p>
<p><img src="/2022/08/09/20220807-ResNet/1.jpg" alt></p>
<p>可以看出，更深的网络有更高的训练误差和验证误差。作者排除了梯度问题的影响。这些网络在训练时使用了Batch Normalization (BN)，经调试，它们的梯度值也一切正常。因此，这种问题不是梯度消失导致的。引言中对于退化问题的描述得到了验证。</p>
<p>第二组实验是对残差网络ResNet的实验。我们可以把它的结果和平坦网络的放在一起对比。</p>
<p><img src="/2022/08/09/20220807-ResNet/2.jpg" alt></p>
<p>从图中可以看出，使用残差学习后，更深的网络果然有了更低的训练误差和验证误差。同时，从表中可以看出，残差网络的测试误差也降低了一大截。这说明残差学习很好地解决了前文提到的退化问题，且这种有效性在测试数据上依然能保持。</p>
<p>由于这轮粗读我们没读方法部分，和方法有关的实验结果得跳过。可以直接翻页到和其他模型的对比表格处：</p>
<p><img src="/2022/08/09/20220807-ResNet/3.jpg" alt></p>
<p>ResNet 在各个评价指标上打败了当时的其他网络，确实很厉害。</p>
<p>后面的表格显示，不仅是图像分类，ResNet在检测和分割等任务中也取得了第一名。</p>
<p>一般看到这里，一轮粗读就差不多完成了。从这轮粗读中，我们看懂了残差学习的核心思想，基本上理解了本文的核心创新点。当然，粗读深度学习相关的论文时，还可以扫一眼网络的核心模块和模型结构。精读的时候，我们再仔细理解文章的方法。</p>
<h2 id="精读"><a href="#精读" class="headerlink" title="精读"></a>精读</h2><h3 id="残差公式"><a href="#残差公式" class="headerlink" title="残差公式"></a>残差公式</h3><p>设多个层构成的模块在拟合一个映射$H(x)$，其中第一层的输入是$x$，则我们希望网络的非线性层去拟合另一个残差函数$F(x) := H(x) - x$，或者说整个模块在拟合$F(x) + x$。由于神经网络的多个非线性层能拟合任意复杂的映射，拟合一个残差函数$H(x)-x$也是可以实现的。</p>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>有了整体的思路，我们来具体看看每一个带残差的模块是怎么搭建的。具体而言，一个模块可以由输入$x$和参数集合${W_i}$计算得到（为了简化表达，bias被省略了）:</p>
<script type="math/tex; mode=display">
y = F(x, \lbrace W_i \rbrace) +x</script><p>我们主要关心$F(x, {W_i})$是怎么搭建的。文章中给出了一种简单的双层残差块示例，其中，$x$是以短路的形式连接到输出上的：</p>
<script type="math/tex; mode=display">
F = W_2\sigma(W_1x), \sigma = ReLU</script><p><img src="/2022/08/09/20220807-ResNet/4.jpg" alt></p>
<p>注意这里的$y = W_2\sigma(W_1x)$是一个未经激活的结果。整个模块送入下一层的输出是$\sigma(y)$，还要加上激活函数。</p>
<p>残差学习是一个框架，每个残差块可以有更多层。比如本文在实验部分还测试了另一种三层残差块（下图是一个示例，实际上通道数可以任意改变）。</p>
<p><img src="/2022/08/09/20220807-ResNet/5.jpg" alt></p>
<p>多层的残差块都是可行的。但是，单层的残差块$y = W_1x + x$和线性层几乎一样，不能提升网络的有效性。</p>
<p>上述的输入$x$是直接加到激活前的输出上的。这个加法操作有一个前提：模块的输入输出通道数是一样的。在输入输出通道数不同时，可以在短路连接上加一个变换维度的线性运算$W_sx$。原来直接做加法的操作叫做全等映射，这里加上一个线性运算再做加法的操作叫做投影映射。</p>
<script type="math/tex; mode=display">
y = F(x, \lbrace W_i \rbrace) + W_sx</script><p><img src="/2022/08/09/20220807-ResNet/6.jpg" alt></p>
<p>这里的符号标记都是基于全连接层的。这些计算对卷积层来说也一样，把矩阵乘法替换成卷积即可。</p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>本文先借鉴VGG的思路，先搭建了一个图像大小逐渐减半，深度逐渐翻倍的平坦网络。之后，在平坦网络连续的3x3卷积层上添加残差连接。下图中的实线代表普通的短路连接，虚线表示需要变换张量形状的短路连接。</p>
<p><img src="/2022/08/09/20220807-ResNet/7.jpg" alt></p>
<p>在虚线表示的残差连接中，图像的大小和深度都发生了改变。对于深度的增加，即可以使用上一节提到的投影映射，也可以直接往多出来的通道数里填0（全等映射）。对于大小的减半，无论是用怎样的深度变化方法，都可以使用步幅为2的操作来实现大小减半。</p>
<blockquote>
<p>在投影时，要进行卷积操作，卷积的步幅为2很好理解。但是，文章没有详细介绍步幅为2的全等映射是怎么做的。直观上理解，就是以步幅2跳着去输入张量里取值。</p>
</blockquote>
<p>文章还提出了层数不同的ResNet架构。对于较深的网络，文章使用了3层残差块。</p>
<p><img src="/2022/08/09/20220807-ResNet/8.jpg" alt></p>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>训练的大部分配置都借鉴自AlexNet。如果你是刚入门图像识别且以后要从事这方面的研究，可以多关注这些细节。</p>
<p><strong>数据处理</strong>：</p>
<ul>
<li>随机缩放图像短边至 [256, 480] 中的均匀随机值</li>
<li>随机翻转</li>
<li>裁剪成 224x224</li>
<li>像素归一化（同AlexNet）</li>
<li>标准颜色增强（同AlexNet）</li>
</ul>
<p><strong>归一化</strong></p>
<ul>
<li>激活前使用BN</li>
<li>参数初始化使用 He Initialization</li>
</ul>
<p><strong>优化配置</strong></p>
<ul>
<li>batch size 256 的 mini-batch 梯度下降</li>
<li>学习率初始化为0.1，发现误差不动了就除以10</li>
<li>迭代$60 \times 10^4$轮</li>
<li>weight decay=0.0001, momentum=0.9</li>
<li>没有dropout</li>
</ul>
<p>此外，为了提高比赛中的精度，在测试时使用了10-crop（对图像裁剪10次，把输入分别输入网络，取结果的平均值）。同时，图像以不同的尺寸输入进了网络，结果取所有运算的平均值。</p>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>读懂了方法，我们再来详细读一遍实验部分。</p>
<p>再看一次平坦网络和残差网络的对比。</p>
<p><img src="/2022/08/09/20220807-ResNet/2.jpg" alt></p>
<p>在这份对比实验中，残差网络相对平坦网络没有添加任何参数。当通道数改变时，残差网络用的是0填充的全等映射。这样，平坦网络和残差网络的唯一区别就是是否有短路连接，整个对比实验非常公平。实验的结果证明了残差连接的有效性。</p>
<p>这篇工作还做了另一个比较平坦网络的实验：在CIFAR-10数据集上统计了平坦网络和残差网络的3x3卷积输出的标准差。标准差大，能说明输出的数值较大。</p>
<p><img src="/2022/08/09/20220807-ResNet/9.jpg" alt></p>
<p>从这份对比结果中，我们能看出残差网络的输出标准差小于平坦网络。这符合残差学习的设计初衷：残差块至少是一个不会使性能变差的全等映射，其效果能够在全等映射的基础上优化。也因此，残差网络的输出大小会比平坦网络更靠近0。</p>
<p>看完了和平坦网络的对比，再看一看不同配置的ResNet直接的对比。文章先比较了全等映射和投影映射的短路连接。本文探讨了短路连接的3种配置：A) 全部使用全等连接 B) 有通道数变动的地方才使用投影连接 C) 全部使用投影连接。它们的表现如下：</p>
<p><img src="/2022/08/09/20220807-ResNet/10.jpg" alt></p>
<p>可以看出，投影用得越多，效果越好。作者认为，B相对A更好，是因为通道数变化时0填充的部分没有残差学习（也就是没有做$+x$的操作）；C相对B更好，是因为参数更多了。这些实验结果说明，投影连接并不能解决退化问题，出于性能的考虑，本文没有在其他实验中使用C。</p>
<blockquote>
<p>后来，B是公认的ResNet标配。</p>
</blockquote>
<p>文章还讨论了构筑更深网络的bottleneck结构。如前文所述，50层以上的ResNet在每个残差块里使用了3个1x1, 3x3, 1x3的卷积。这种设计主要是为了缩短训练时间。ResNet-50和ResNet-34有着同样的时间复杂度，却因为深度更大，精度更高。</p>
<p>这篇论文的主要实验就是这些。论文后面还展示了一个比较有趣的实验：在CIFAR-10数据集上训练超过1000层的ResNet。实验结果显示，1000多层的ResNet仍能成功被优化，训练误差也降到了很低，但是测试误差没有110层的网络好。作者认为，这是因为训练数据太少，网络过拟合了。</p>
<p><img src="/2022/08/09/20220807-ResNet/11.jpg" alt></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ResNet是基于深度学习的计算机视觉中里程碑式的工作。通过阅读这篇论文，我们发现，ResNet使用的残差结构本身并不十分复杂。这篇工作真正出彩之处，是发现了深度神经网络的一个普遍存在的问题，并用精彩的推理设计出了一套能够解决此问题的方案。这种方案确实很有效，基于残差学习的ResNet击败了同时代的所有网络。残差连接被用在了后续几乎所有网络架构中，使用ResNet为backbone也成了其他CV任务较为常用的初始配置。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/DLS-note-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/DLS-note-13/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十三）：CNN应用：人脸识别与风格迁移</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:13:53" itemprop="dateCreated datePublished" datetime="2022-08-09T14:13:53+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这堂课里，我们要学习两个具体的应用：人脸识别、风格迁移。</p>
<p>相信大家已经在很多地方见识过人脸识别应用了：在火车站，要通过身份证和人脸核实身份；办理业务时，可以用手机完成人脸识别以核实身份；进办公楼时，员工只要刷脸通过就可以打开门禁。通过这节课的学习，我们能够学会如何用CNN完成人脸识别。</p>
<p>神经网络风格迁移是一项有趣的应用。它利用了CNN捕捉图像抽象信息的特点，能够把一幅图像的风格转移到另一幅图像上，从而生成一幅新的图像。这项技术不需要从头训练网络，学完这门课后，我们能快速地用代码实现神经网络风格迁移。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><p>准确来说，在人脸识别(Face Recognition)任务中，会给定一个有$K$个人的数据库。之后，每一次识别都会输入一张图片，输出这张图片是$K$个人中的哪一个，或者没有检测到相关人士。</p>
<p>有一个与这个相关的任务叫做人脸验证(Face Verification)。这个任务稍微简单一些，输入是一张图片和一个标记身份的数据（比如身份证号），要求输出图片中的人是否符合该身份。</p>
<h3 id="单样本学习"><a href="#单样本学习" class="headerlink" title="单样本学习"></a>单样本学习</h3><p>按我们之前学的方法，假如我们在$K$个人的数据库上做识别（分类）任务，应该套用一个CNN模型，并在模型最后接一个$K+1$类的softmax，表示输入图片是K个人中的哪一个，或者都不是。</p>
<p>但是，这样的架构不适合人脸识别任务。以公司的门禁识别为例，这种方法有如下的缺点：</p>
<ol>
<li>每来一个新同事，模型就要重新训练一次。</li>
<li>每个人都得上传大量的个人照片供网络训练。</li>
</ol>
<p>理想情况下，我们希望模型能从一张人脸照片中学会分辨这张人脸，这样每个新同事只需要上传一张照片即可。这叫做单样本学习(One-shot Learning)。</p>
<p>为了完成单样本学习，我们可以从另一个角度来建模人脸识别问题：如果输入的人脸和数据库里某张人脸极为相似，我们就说识别出了这张人脸；否则，就说没有识别到有效的人脸。</p>
<p>这样，人脸识别问题就被转换为了一个求两张图片相似度的问题。我们可以让网络学习一个输入是两张图片，输出是二者相似度的一个映射。</p>
<h3 id="孪生网络"><a href="#孪生网络" class="headerlink" title="孪生网络"></a>孪生网络</h3><p>在完成和相似度有关的问题时，一种常见的做法是使用孪生网络（Siamese Network)。</p>
<p>假设网络的倒数第二层有128个神经元。在普通分类网络中，这128个神经元输出的长度为128的向量会被输入进最后的softmax层。而在孪生网络中，我们要去掉softmax层，并用这个没有sofrmax的网络$f$分别输出两张图片$x^{(1)}, x^{(2)}$对应的128维向量$f(x^{(1)}), f(x^{(2)})$。这样，每张图片有了唯一对应的一个128维向量，这个向量可以看成是该图片的编码(encoding)。而我们又知道，对向量求相似度是很方便的。我们可以利用两张图片的编码求出相似度。</p>
<p><img src="/2022/08/09/DLS-note-13/1.jpg" alt></p>
<p>说起向量的相似度，最容易想到是向量间的距离$||v - u||^2$。因此，我们可以i设法让网络学会这样一种关系：</p>
<ul>
<li>若$x^{(i)}, x^{(j)}$是同一人，则$||f(x^{(i)}) - f(x^{(j)})||^2$很小。</li>
<li>若$x^{(i)}, x^{(j)}$不是同一人，则$||f(x^{(i)}) - f(x^{(j)})||^2$很大。、</li>
</ul>
<p>为了达成这个目标，我们来看看应该用怎样的误差来指导网络的优化方向。</p>
<h3 id="三元组误差"><a href="#三元组误差" class="headerlink" title="三元组误差"></a>三元组误差</h3><p>在让网络学习基于距离的相似度时，一种常用的误差是三元组误差(Triplet Loss)。</p>
<p>在一轮训练中，我们要用到3张图片：一张基准图(anchor)$A$和与其相对的正负样本$P, N$各一张。设$d(A, B)$为图片$A, B$的编码的距离，则我们希望$d(A, P) \leq d(A, N)$。</p>
<blockquote>
<p>左边的人是吴恩达的妻子，这几张图片已经出现了好几次。</p>
</blockquote>
<p><img src="/2022/08/09/DLS-note-13/2.jpg" alt></p>
<p>移个项，即我们希望：</p>
<script type="math/tex; mode=display">
d(A, P) - d(A, N) \leq 0.</script><p>但这个条件太容易满足了，令$f(x)=0$恒成立的话无论怎样的输入都可以使上式左侧为0了。因此，我们要加一点小小的约束：</p>
<script type="math/tex; mode=display">
d(A, P) - d(A, N) + \alpha \leq 0,</script><p>这个$\alpha$是一个较小的数，比如可以令$\alpha=0.2$。为了达成这个目标，我们可以设置以下的误差函数</p>
<script type="math/tex; mode=display">
L(A, P, N)=max(d(A, P) - d(A, N) + \alpha, 0).</script><p>这里取max的直观解释是：只要满足开始那个不等式，让正样本和负样本能够分清楚就行了。至于两类样本要不要分辨得那么分明，我们并不关心。</p>
<p>为了利用这个误差训练网络，我们要在训练集里为同一个人准备多张照片。比如1000个人，每人100张照片，共100000张照片构成训练集。</p>
<p>在提出此设计的FaceNet中，还有一些小细节：为了加大训练难度，让模型效果更好，每次训练时应该选择较难的三元组$A, P, N$。详情请阅读原论文。</p>
<h3 id="人脸验证与二分类问题"><a href="#人脸验证与二分类问题" class="headerlink" title="人脸验证与二分类问题"></a>人脸验证与二分类问题</h3><p>其实，判断两张图片的编码是否相似的问题可以简单地转化成一个二分类问题：把两张图片的编码“拼起来”，输入进一个逻辑回归的单元，让网络输入两张图片是否相似。</p>
<p>这里把两张图片“拼起来”有很多的实现方式。一种简单的方式是求两个编码的绝对值（L1距离）。</p>
<p>另外，在部署时，由于比较的图像的编码是可以预处理的，只需要用神经网络跑一遍输入图片即可。</p>
<h2 id="神经网络风格迁移"><a href="#神经网络风格迁移" class="headerlink" title="神经网络风格迁移"></a>神经网络风格迁移</h2><blockquote>
<p>我之前写了一篇<a href="https://zhouyifan.net/2022/06/01/20220531-styletransfer/">详细介绍神经网络风格迁移的文章</a>，我认为那篇文章比这堂课更好理解，非常推荐大家阅读。因此，这部分笔记我会写得潦草一些。</p>
</blockquote>
<h3 id="什么是神经网络风格迁移？"><a href="#什么是神经网络风格迁移？" class="headerlink" title="什么是神经网络风格迁移？"></a>什么是神经网络风格迁移？</h3><p><img src="/2022/08/09/DLS-note-13/3.jpg" alt></p>
<p>如上图所示，在神经网络风格迁移中，输入一张表示内容的图(C)和一张表示画家风格的图(S)，我们可以借助神经网络生成一幅融合内容与风格的图片(G)。</p>
<p>接下来实现神经网络风格迁移时，我们会关注CNN浅层和深层提取出来的特征。因此，在具体介绍风格迁移的实现之前，我们先来看看CNN各层网络究竟学到了什么。</p>
<h3 id="深度卷积网络学到了什么？"><a href="#深度卷积网络学到了什么？" class="headerlink" title="深度卷积网络学到了什么？"></a>深度卷积网络学到了什么？</h3><p>为了设法可视化神经网络每一层的输出，我们可以考虑把整个训练集喂入网络，找出使某一层某一神经元激活输出最大的几个像素块。</p>
<p>以AlexNet为例，令第一层某几个神经元激活输出最大的像素块是：</p>
<p><img src="/2022/08/09/DLS-note-13/4.jpg" alt></p>
<p>可以看出，浅层的神经网络更关注不同方向轮廓信息。</p>
<p>用同样的方式可视化更深的层，可以看出网络关注的信息越来越抽象。</p>
<p><img src="/2022/08/09/DLS-note-13/5.jpg" alt></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>和优化一个网络的参数不同，在神经网络风格迁移中，我们要把一幅图像当成可以优化的参数，通过修改图像的像素值来最小化一个损失函数。让我们看看这个损失函数是怎么定义的。</p>
<p>损失函数由两部分构成：生成图像(G)和内容图像(C)的<strong>内容损失</strong>与和风格图像(S)的<strong>风格损失</strong>。二者之间的比例靠比例系数$\alpha, \beta$控制。</p>
<script type="math/tex; mode=display">
J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G).</script><p>我们来看一个优化这个损失函数的步骤示例：</p>
<ol>
<li>生成一个随机图像$G: 100 \times 100 \times 3$</li>
<li>使用梯度下降更新$G$，最小化$J(G)$</li>
</ol>
<p><img src="/2022/08/09/DLS-note-13/6.jpg" alt></p>
<h3 id="内容损失"><a href="#内容损失" class="headerlink" title="内容损失"></a>内容损失</h3><p>内容损失的计算方式如下：</p>
<ol>
<li>使用一个预训练的网络（最常用的是VGG）。</li>
<li>选择神经网络中的某一层$l$，这一层相关的数据会用来计算内容损失。$l$越浅，表示越具体的图像；$l$越深，表示越抽象的图像。一般选取适中的$l$。</li>
<li>令$a^{<a href="C">l</a>}, a^{[l][G]}$为图像$C, G$网络第$l$层的激活输出。我们认为，如果这两个值很相似，则两幅图像的内容就很相似。</li>
<li>令$J_{content}(C, G) = ||a^{<a href="C">l</a>} - a^{[l][G]}||^2$</li>
</ol>
<h3 id="风格损失"><a href="#风格损失" class="headerlink" title="风格损失"></a>风格损失</h3><p>看完了内容损失，现在来看风格损失的计算方式。我们刚刚一直在用名词“风格”。这个词放在美术里，可以指画家的绘画风格。而对于神经网络来说，“风格”又是什么意思呢？</p>
<p>和刚刚的内容损失类似，计算风格损失时，我们也要选择CNN的某层$l$，计算这一层卷积激活输出的<strong>通道相关量</strong>。</p>
<p>这里“通道相关量”反映的是图像各个通道间两两的相关关系。这句话可能有点拗口，让我们看一个详细的通道相关量示例。</p>
<p><img src="/2022/08/09/DLS-note-13/7.jpg" alt></p>
<p>假设一个激活输出有5个通道，我们用颜色“红-黄-紫-绿-蓝”称呼它们。每个通道表示的是一类特征，比如红色的通道表示图像有没有竖着的条纹，黄色通道表示图像有没有橙色的色块。我们想计算红黄通道之间的相关量，其实就是计算图像每个像素处红色通道的值和绿色通道的值之间的相关关系，看看它们是会同时出现，还是一个出现了另一个就不出现。（两个数值的相关量就是它们的乘积，具体的数学表示会在后文中给出）</p>
<p>为什么这样的相关关系能够捕捉到风格信息呢？红色通道和黄色通道的相关关系，就是是不是有竖条纹的地方就有橙色色块。这样一种线条和颜色的关联，就可以代表图片的风格。（下图中红色的框和黄色的框分别圈出了两种特征）</p>
<p><img src="/2022/08/09/DLS-note-13/8.jpg" alt></p>
<p>从直觉上认识了风格，现在我们来看一看风格的具体计算方法。在计算两个向量的相关量时，可以直接求两个向量的积。因此，我们要用到一种叫做”风格矩阵“的中间量，它计算了所有通道向量的乘积：</p>
<script type="math/tex; mode=display">
G^{[l]}_{kk'}=\Sigma_i^{H} \Sigma_i^{W} a^{[l]}_{i, j, k} a^{[l]}_{i, j, k'}</script><p>其中，$a^{[l]}_{i, j, k}$是第$i$行$j$列第$k$个通道的激活输出，风格矩阵$G$的形状是$n_c^{[l]} \times n_c^{[l]}$，$G^{[l]}_{kk’}$描述的是第$l$层激活输出中，第$k, k’$个通道间的相关量。</p>
<blockquote>
<p>在数学中这个矩阵叫做gram矩阵。</p>
</blockquote>
<p>有了风格矩阵，就可以算风格损失了。为了简化表示，我们用$G^{[l]}(S), G^{[l]}(G)$分别表示风格图像S和生成图像G的风格矩阵。和刚刚一样，我们选择一层$l$，并计算风格误差：</p>
<script type="math/tex; mode=display">
J^{[l]}_{style}(S, G) = ||G^{[l]}(S)- G^{[l]}(G)||_F^2</script><p>这是一层上的风格误差。其实，我们还可以指定多个层上的风格误差，以使生成图像能够拟合风格图像在多个卷积层上（抽象程度不同）的风格。多个层的风格误差就是各层风格误差之和。</p>
<h2 id="1D和3D上的卷积"><a href="#1D和3D上的卷积" class="headerlink" title="1D和3D上的卷积"></a>1D和3D上的卷积</h2><p>在结束这门课之前，我们来学习最后一项内容——1D和3D数据上的卷积。之前，我们只关注2D的图像数据，当维度不是2时，卷积有怎样的变化呢？</p>
<p>1D数据可以是心电图。和可以用2D卷积捕捉2D图像的边缘一样，我们可以用下图中那个尖尖的1D卷积来捕获高频的数据。1D卷积前后的形状变化规律和2D一样，比如用16个长度为5的卷积卷一个$14 \times 1$的1D图像，可以得到$10 \times 16$的1D图像（第一维是图像长度，第二维是通道数）。 </p>
<p><img src="/2022/08/09/DLS-note-13/9.jpg" alt></p>
<p>3D数据也是类似的道理。用16个$5 \times 5 \times 5$的卷积核卷一个单通道$14 \times 14 \times 14$的单通道图像，可以得到一个$5 \times 5 \times 5 \times 16$的图像。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这节课主要介绍了人脸识别和神经网络风格迁移两项任务，最后顺便介绍了1D和3D上的卷积。</p>
<ul>
<li>人脸识别<ul>
<li>人脸验证与人脸识别任务的定义</li>
<li>如何对单样本学习建模</li>
<li>孪生网络</li>
<li>基于三元组误差和二分类误差的人脸识别网络</li>
</ul>
</li>
<li>神经网络风格迁移<ul>
<li>神经网络风格迁移的输入输出</li>
<li>利用小技巧可视化 CNN 学到的图像信息</li>
<li>生成风格迁移图像时的内容误差与风格误差</li>
</ul>
</li>
</ul>
<p>人脸识别任务依赖于数据集，项目搭建起来比较麻烦。对于这一周的内容，我就不展示其他的代码实战笔记了，欢迎阅读我之前写的<a href="https://zhouyifan.net/2022/06/01/20220531-styletransfer/">神经网络风格迁移的实现</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/DLS-note-12-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/DLS-note-12-2/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》代码实战（十二）：目标检测 NMS 的 Python 实现（附算法动图）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:09:41" itemprop="dateCreated datePublished" datetime="2022-08-09T14:09:41+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这篇文章中，我将给出一份带运行示例的NMS Python脚本，并对算法和代码进行详细解说。相信大家看完这篇文章后，能够轻松地掌握NMS的底层原理。</p>
<p>如果你对目标检测的基础知识不太熟，欢迎先阅读我的上篇文章：<a href="https://zhouyifan.net/2022/07/26/DLS-note-12/">目标检测简介</a></p>
<p>示例脚本（包括可视化的代码）链接：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/nms">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/nms</a> </p>
<h2 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h2><p>在目标检测算法中，为了尽量不漏掉物体，会输出大量的检测结果（每一条结果由检测概率与检测框组成）。这些检测结果很可能有重复的，即会有多个框标出了同一个物体。下图就是一个例子，算法正确识别出了两辆车，但却有多个检测框标出了同一辆车。</p>
<p><img src="/2022/08/09/DLS-note-12-2/1.jpg" alt></p>
<p>我们需要一个算法来过滤多余的检测框。最常用的算法就是NMS(Non-Maximum Suppresion, 非极大值抑制)。该算法的思路很简单：只保留局部概率最大的检测框，与其重合的其他检测框都会被舍去。</p>
<p>算法的伪代码如下：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">输入所有检测概率、检测框</span><br><span class="line">当还有检测框没有处理：</span><br><span class="line">    从剩下的框里挑出检测概率最大的框 bbox_a</span><br><span class="line">    遍历所有没有处理的框bbox_i：</span><br><span class="line">        如果 bbox_i != bbox_a 且 bbox_i 与 bbox_a 重合：</span><br><span class="line">            舍去 bbox_i</span><br><span class="line">    把 bbox_a 输出成一个检测结果</span><br></pre></td></tr></table></figure>
<p>当然，这个算法的描述还不够准确：究竟该怎么定义两个检测框是“重合”呢？如果两个检测框有交集就说它们重合是行不通的，因为图片中很可能会有挨得很近的物体，它们的检测框就是相交的。因此，我们一般用IoU（交并比）来描述两个框的重合程度，如果IoU超出某个阈值，就说这两个框是“重合”的。</p>
<p>IoU的计算很简单，算出两个矩形的「交面积」，再用两个矩形面积之和减去「交面积」就可以得到「并面积」，「交面积」比「并面积」就是IoU。</p>
<p><img src="/2022/08/09/DLS-note-12-2/1.gif" alt></p>
<p>在NMS之前，一般还会先做一步预处理：对于预测概率小于某个阈值的检测框，我们不信任它们，会在进行NMS之前就把它们舍去。在代码实现时这部分逻辑常常会放到NMS的函数里。这样，整个NMS的流程是这样的：</p>
<p><img src="/2022/08/09/DLS-note-12-2/2.gif" alt></p>
<p>上述的NMS针对是识别一种物体的任务，在推广到多分类NMS时，只要把输入的检测概率换成有物体的概率乘上概率最大的类别的概率即可。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><blockquote>
<p>理论上这段代码是兼容任何格式的张量的。经测试，NumPy, PyTorch 的张量都可以正确运行。</p>
</blockquote>
<p>先看一下IoU的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span>(<span class="params">b1: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>], b2: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">    intersection = box_intersection(b1, b2)</span><br><span class="line">    inter_area = area(intersection)</span><br><span class="line">    union_area = area(b1) + area(b2) - inter_area</span><br><span class="line">    <span class="keyword">return</span> inter_area / union_area</span><br></pre></td></tr></table></figure>
<p>所有的检测框用一个长度为4的元组表示。<code>box_intersection</code>用于计算两个框的相交框，<code>area</code>用于计算框的面积。这段代码和之前描述得一样，先获取相交区域，计算交面积，再计算并面积，最后算除法。</p>
<p><code>box_intersection</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_intersection</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        b1: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">        b2: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]:</span></span><br><span class="line">    x11, y11, x12, y12 = b1</span><br><span class="line">    x21, y21, x22, y22 = b2</span><br><span class="line"></span><br><span class="line">    xl = <span class="built_in">max</span>(x11, x21)</span><br><span class="line">    xr = <span class="built_in">min</span>(x12, x22)</span><br><span class="line">    yt = <span class="built_in">max</span>(y11, y21)</span><br><span class="line">    yb = <span class="built_in">min</span>(y12, y22)</span><br><span class="line">    <span class="keyword">return</span> (xl, yt, xr, yb)</span><br></pre></td></tr></table></figure>
<p>算相交区域，可以理解成分别求出x, y方向上的相交部分，再把两部分合成一个框。而求直线的相交，就是取最大的左端点和最小的右端点。</p>
<p><code>area</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">area</span>(<span class="params">box: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">    x1, y1, x2, y2 = box</span><br><span class="line">    width = <span class="built_in">max</span>(x2 - x1, <span class="number">0</span>)</span><br><span class="line">    height = <span class="built_in">max</span>(y2 - y1, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> width * height</span><br></pre></td></tr></table></figure>
<p>如果两个框不相交，<code>x2 - x1</code>和<code>y2 - y1</code>会出现小于0的情况。因此，要保证它们最小为0，再算面积就不会有错了。</p>
<p>有了<code>iou</code>，就可以实现了NMS了。我的NMS函数定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">predicts: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">        score_thresh: <span class="built_in">float</span> = <span class="number">0.6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        iou_thresh: <span class="built_in">float</span> = <span class="number">0.3</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Non-Maximum Suppression</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        predicts (np.ndarray): Tensor of shape [n, 5]. The second demesion</span></span><br><span class="line"><span class="string">            includes 1 probability and 4 numbers x, y, w, h denoting a bounding</span></span><br><span class="line"><span class="string">            box.</span></span><br><span class="line"><span class="string">        score_thresh (float): The boxes with probability lower than</span></span><br><span class="line"><span class="string">            score_threash will be discarded.</span></span><br><span class="line"><span class="string">        iou_thresh (float): The threshold determining whether two boxes are</span></span><br><span class="line"><span class="string">            &quot;overlapped&quot;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (np.ndarray, List[int]): The filtered predictions and the indices of</span></span><br><span class="line"><span class="string">            remaining boxes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<p>NMS算法需要输入检测结果<code>predicts</code>，输出过滤后的检测结果<code>filtered_predicts</code>。此外，NMS算法有两个输入属性<code>score_thresh</code>， <code>iou_thresh</code>，分别表示被选定的检测框最小需要的概率、判断两个框是否重合的IoU阈值。为了方便其他的计算（比如多分类NMS），我还输出了一个索引数组indices，表示被选中的框的索引。这方面的逻辑可以根据自己的项目要求进行优化，没有统一的规定。</p>
<p>NMS的实现和开始的伪代码几乎一模一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">predicts: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">        score_thresh: <span class="built_in">float</span> = <span class="number">0.6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        iou_thresh: <span class="built_in">float</span> = <span class="number">0.3</span></span>):</span></span><br><span class="line">    n_remainder = <span class="built_in">len</span>(predicts)</span><br><span class="line">    vis = [<span class="literal">False</span>] * n_remainder</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Filter predicts with low probability</span></span><br><span class="line">    <span class="keyword">for</span> i, predict <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> predict[<span class="number">0</span>] &lt; score_thresh:</span><br><span class="line">            vis[i] = <span class="literal">True</span></span><br><span class="line">            n_remainder -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># NMS</span></span><br><span class="line">    output_predicts = []</span><br><span class="line">    output_indices = []</span><br><span class="line">    <span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">        max_pro = -<span class="number">1</span></span><br><span class="line">        max_index = <span class="number">0</span></span><br><span class="line">        <span class="comment"># Find argmax</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">                <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                    max_index = i</span><br><span class="line">                    max_pro = p[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append output</span></span><br><span class="line">        max_p = predicts[max_index]</span><br><span class="line">        output_predicts.append(max_p)</span><br><span class="line">        output_indices.append(max_index)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Suppress</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">                <span class="keyword">if</span> iou(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; iou_thresh:</span><br><span class="line">                    vis[i] = <span class="literal">True</span></span><br><span class="line">                    n_remainder -= <span class="number">1</span></span><br><span class="line">        vis[max_index] = <span class="literal">True</span></span><br><span class="line">        n_remainder -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_predicts, output_indices</span><br></pre></td></tr></table></figure>
<p>一开始，先声明一些辅助的变量。<code>n_remainder</code>表示还有多少个框没被访问，<code>vis[i]</code>表示第<code>i</code>个框是否被访问。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_remainder = <span class="built_in">len</span>(predicts)</span><br><span class="line">vis = [<span class="literal">False</span>] * n_remainder</span><br></pre></td></tr></table></figure>
<p>一开始，先过滤那些概率过小的框：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, predict <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">    <span class="keyword">if</span> predict[<span class="number">0</span>] &lt; score_thresh:</span><br><span class="line">        vis[i] = <span class="literal">True</span></span><br><span class="line">        n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>之后，正式进入NMS，先准备好输出的列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NMS</span></span><br><span class="line">output_predicts = []</span><br><span class="line">output_indices = []</span><br><span class="line">    <span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br></pre></td></tr></table></figure>
<p>在还有没访问的框时，先找出剩下的框中概率最大的那个框：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    max_pro = -<span class="number">1</span></span><br><span class="line">    max_index = <span class="number">0</span></span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">            <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                max_index = i</span><br><span class="line">                max_pro = p[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>之后，抑制掉和概率最大框“重合”的框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    max_p = predicts[max_index]</span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">            <span class="keyword">if</span> iou(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; iou_thresh:</span><br><span class="line">                vis[i] = <span class="literal">True</span></span><br><span class="line">                n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>最后，把这个结果添加进输出，并维护好<code>vis</code>和<code>n_remainder</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append output</span></span><br><span class="line">    output_predicts.append(max_p)</span><br><span class="line">    output_indices.append(max_index)</span><br><span class="line">    vis[max_index] = <span class="literal">True</span></span><br><span class="line">    n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里可以把<code>vis[max_index] = True</code>那两行移到抑制操作的前面，这样判断里就不用加<code>i != max_index</code>了。我这样写是为了强调一下，判断重合的时候不需要判断自己。</p>
</blockquote>
<p>实现完了，返回结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> output_predicts, output_indices</span><br></pre></td></tr></table></figure>
<h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>做单元测试的时候，最好是找一份现有的实现用做对照。为了让整个测试过程更贴合实际一些，我用MMDetection的YOLOv3跑了一个NMS前和NMS后的检测框结果，并把NMS前的检测框输入进了自己实现的NMS，比较了两份NMS的输出。以下是具体的测试过程。</p>
<p>照着MMDetection的官方文档，下载好YOLOv3模型，用下面的代码即可对MMDetection的demo图片进行推理并保存结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mmdet.apis <span class="keyword">import</span> inference_detector, init_detector, show_result_pyplot</span><br><span class="line"><span class="keyword">from</span> mmdet.models.detectors <span class="keyword">import</span> BaseDetector</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose to use a config and initialize the detector</span></span><br><span class="line">config = <span class="string">&#x27;configs/yolo/yolov3_mobilenetv2_320_300e_coco.py&#x27;</span></span><br><span class="line"><span class="comment"># Setup a checkpoint file to load</span></span><br><span class="line">checkpoint = <span class="string">&#x27;checkpoints/yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth&#x27;</span></span><br><span class="line"><span class="comment"># initialize the detector</span></span><br><span class="line">model: BaseDetector = init_detector(config, checkpoint, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img = <span class="string">&#x27;demo/demo.jpg&#x27;</span></span><br><span class="line">result = inference_detector(model, img)</span><br><span class="line"></span><br><span class="line">model.show_result(img, result, score_thr=<span class="number">0.3</span>, out_file=<span class="string">&#x27;work_dirs/1.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/08/09/DLS-note-12-2/d_1.jpg" alt></p>
<p>为了得到NMS前的输入，我在<code>mmdet/models/dense_head/YOLOV3Head.get_bboxes</code>里面插入了一段输出结果的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">save_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;bboxes&#x27;</span>: bboxes,</span><br><span class="line">    <span class="string">&#x27;cls_scores&#x27;</span>: scores,</span><br><span class="line">    <span class="string">&#x27;prob&#x27;</span>: objectness</span><br><span class="line">&#125;</span><br><span class="line">torch.save(save_dict, <span class="string">&#x27;work_dirs/bboxes.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">det_bboxes, det_labels = multiclass_nms(</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>得到NMS前的输入是<br><img src="/2022/08/09/DLS-note-12-2/d_2.jpg" alt></p>
<p>把这份数据输入进我们的NMS中，得到的可视化结果如下：</p>
<p><img src="/2022/08/09/DLS-note-12-2/3.jpg" alt></p>
<p>这跟开始那份输出一模一样。看来我们实现的这份NMS完全正确。</p>
<p>如果你对测试过程、可视化、多分类NMS感兴趣，欢迎直接阅读我的项目源码。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我给出了一份NMS的简洁而靠近底层的Python实现，并对NMS算法进行了介绍。通过阅读这篇文章，相信大家已经完全理解了NMS的原理，并且能够用任何一种语言实现NMS。一般的NMS开源实现支持的参数更多，代码会更复杂一些，但它们的核心和我的这份实现是一样的。</p>
<p>这份NMS的实现还有很大的改进空间。比如每轮求概率最大的框时，可以先排好序，或者用优先队列，这样均摊下来每次获取概率最大的框的复杂度是<code>O(logn)</code>。但是后面判断重复的框一定有一个<code>O(n)</code>的计算，这部分的优化并不显著。大家有余力可以参考成熟的CV项目里NMS是怎么高效用C++实现的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/26/DLS-note-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/26/DLS-note-12/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十二）：目标检测与语义分割简介 (YOLO, U-Net)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-26 21:00:00" itemprop="dateCreated datePublished" datetime="2022-07-26T21:00:00+08:00">2022-07-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这节课中，我们要学习计算机视觉中最重要的任务之一——目标检测任务。我们会先认识目标定位和关键点检测这两个比较简单的任务，慢慢过度到目标检测任务。之后，我们会详细学习目标检测的经典算法YOLO。最后，我们会稍微认识一下语义分割任务及适用于此问题的U-Net架构。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p>在<strong>图像分类问题</strong>中，给定一幅图片，我们只要说出图片里的物体是什么就行了。在这堂课要讨论的任务中，我们还要多做一件事——定位。我们要先用边框圈出图中的物体，再说出框里的物体是什么。这叫做<strong>带定位(localization)的分类问题</strong>。更进一步，如果我们不再是只讨论一个物体，而是要把图片中所有物体都框出来，并标出每一个物体的类别，这就是<strong>目标检测</strong>问题，</p>
<p><img src="/2022/07/26/DLS-note-12/1.jpg" alt></p>
<p>我们对分类任务的神经网络结构已经很熟悉了。那么，带定位的分类该使用怎样的网络呢？实际上，一个边框可以用边框中心和边框宽高这四个量表示。除了softmax出来的分类结果外，我们只要让网络再多输出四个数就行了。如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/2.jpg" alt></p>
<p>这里，要统一一下对于边框的定义。我们用$b_x, b_y$表示边框的中心坐标，$b_h, b_w$表示边框的高、宽。</p>
<p>来看一下标签$y$的具体写法。假设一共有四类物体：行人、汽车、摩托车、背景（没有物体）。那么，标签$y$应该用$y=[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3]^T$表示。其中，$p_c$表示图中有没有物体。若$p_c=1$，则$c_1, c_2, c_3$分别表示物体属于除背景外的哪一类；若$p_c=0$，则其他值无意义。</p>
<p>这样，在算误差时，也需要分类讨论。若$p_c=1$，则算估计值与标签8个分量两两之间的均方误差；若$p_c=0$，只算$p_c$的均方误差，不用管其他量。</p>
<p><img src="/2022/07/26/DLS-note-12/3.jpg" alt></p>
<p>只要更换一下神经网络的输出格式，我们就能得到一个完成目标定位问题的网络。</p>
<h2 id="关键点检测"><a href="#关键点检测" class="headerlink" title="关键点检测"></a>关键点检测</h2><p>我们刚刚学了用2个点表示一个边框。其实，拓展一下边框检测，就是一个关键点（英文有时叫做”landmark”，是“地标”的意思）检测问题。</p>
<p>比如，在人脸关键点检测中，我们可以定义一堆关键点，分别表示眼睛、鼻子、嘴巴……的位置。我们还是让网络先输出一个数，表示图中有没有人脸；再输出2n个数，表示n个人脸关键点。这样，网络就能学习怎么标出人脸关键点了。</p>
<p><img src="/2022/07/26/DLS-note-12/4.jpg" alt></p>
<p>很多应用都基于人脸关键点检测技术。比如我们检测到了眼睛周围的关键点后，就可以给人“戴上”墨镜。</p>
<p>总之，通过这一节的学习，我们要知道，目标定位中输出2个坐标只是关键点检测的一个特例。只要训练数据按照某种规律标出了关键点，不管这些关键点是表示一个框，还是人脸上各器官的位置，网络都能学习这种规律。</p>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>有了之前的知识储备，现在我们来正式学习目标检测。目标检测可以用一种叫做“滑动窗口”的技术实现。</p>
<p>假设我们要构建一个汽车的目标检测系统。我们可以先构造一个汽车分类数据集——数据集的x是一些等大的图片，y表示图片里是不是有汽车。如果图片里有汽车，汽车应该占据图片的大部分位置。</p>
<p><img src="/2022/07/26/DLS-note-12/5.jpg" alt></p>
<p>通过学习，网络就能够判断一个框里的物体是不是汽车了。这样，我们可以用一个边框框出图片的一部分，裁剪下来，让网络看看图片的这一部分是不是汽车。只要我们尝试的次数够多，总能找出图中的汽车。</p>
<p><img src="/2022/07/26/DLS-note-12/6.jpg" alt></p>
<p>在遍历边框时，我们是通过“滑动”的方法：遍历边框的大小，选择好大小后把框放到左上角，先往右移，再往下移。所以这种方法叫做“滑动窗口”。</p>
<p>滑动窗口算法有一个缺点：如果我们移动窗口的步伐过小，则运行分类器的次数会很多；如果移动窗口的步伐过大，则算法的精度会受到影响。在深度学习时代之前，分类器都是一些简单的线性函数，能够快速算完，多遍历一些滑动窗口没有问题。而使用了深度CNN后，遍历滑动窗口的代价就很大了。</p>
<p>幸好，滑动窗口也可以通过卷积来生成，而不一定要遍历出来。让我们看下去。</p>
<h2 id="基于卷积的滑动窗口"><a href="#基于卷积的滑动窗口" class="headerlink" title="基于卷积的滑动窗口"></a>基于卷积的滑动窗口</h2><p>滑动窗口其实可以通过执行巧妙的卷积来生成。在那之前，我们先学一门前置技能：怎么把全连接层变成卷积层。</p>
<p>前两周学习CNN时，我们学过，卷积结束后，卷积的输出会被喂入全连接层中。实际上，我们可以用卷积来等价实现全连接层。比如下图中，一个$5 \times 5 \times 16$的体积块想变成一个长度为400的向量，可以通过执行400个$5 \times 5$的卷积来实现。</p>
<p><img src="/2022/07/26/DLS-note-12/7.jpg" alt></p>
<p>知道了这一点后，我们就可以利用卷积来快速实现滑动窗口了。</p>
<p>假设我们按照上一节的算法，先实现了对$14 \times 14$的小图片进行分类的分类器。之后，我们输入了一张$16 \times 16$的大图片。我们遍历滑动窗口，令步幅为2。这样，理论上，有4个合法的滑动窗口，应该执行4次分类器的运算，如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/8-2.jpg" alt></p>
<p>可是，仔细一想，在执行4次分类器的过程中，有很多重复的运算。比如，对于4个滑动窗口中间那共有的$12 \times 12$个像素，它们的卷积结果被算了4次。理想情况下，只需要对它们做一次卷积就行了。这该怎么优化呢？</p>
<p>其实，很简单，我们可以利用卷积本身的特性来优化。卷积层只定义了卷积核，而没有规定输入图像的大小。我们可以拿出之前在$14 \times 14$的图像上训练好的卷积层，把它们用在$16 \times 16$的图片的卷积上。经过相同的网络，$16 \times 16$的图片会生成一个$2 \times 2$大小的分类结果，而不是$1 \times 1$的。这$2 \times 2$个分类结果，恰好就是那4个滑动窗口的分类结果。通过这样巧妙地利用卷积操作，我们规避了遍历滑动窗口带来的重复计算。</p>
<p><img src="/2022/07/26/DLS-note-12/8.jpg" alt></p>
<p>不过，这个方法还是有一些缺陷的。在刚才那个例子中，$16 \times 16$的图片其实可以放下9个$14 \times 14$大小的边框。但是，由于分类网络中max pool的存在，我们只能生成4个分类结果，也就是步幅为2的滑动窗口的分类结果。同时，最准确的检测框也不一定是正方形的，而可能是长方形的。为了让生成的滑动窗口更准确一些，我们要用到其他方法。</p>
<h2 id="预测边框"><a href="#预测边框" class="headerlink" title="预测边框"></a>预测边框</h2><p>在这一节，我们要使用YOLO(You Only Look Once)算法解决上一节中碰到的问题。还记得这周课开头学的目标定位问题吗？我们可以把滑动窗口和目标定位结合一下。</p>
<p>给定一幅图像，我们可以把图像分成$3 \times 3$个格子。训练模型前，我们要对训练数据做预处理。根据每个训练样本中物体的中心点所在的格子，我们把物体分配到每一个格子中。也就是说，不管一个物体的边框跨越了几个格子，它的中心点在哪，它就属于哪个格子。比如对于下图的训练样本，右边那辆车就属于橙色的格子。之后，我们给每个格子标上标签$y$。这个标签$y$就是目标定位中那个表示图片中是否有物体、物体的边框、物体的类别的标签向量。对于这个$3 \times 3$的格子，有9个标签向量，整个标签张量的形状是$3 \times 3 \times 8$。</p>
<p><img src="/2022/07/26/DLS-note-12/9.jpg" alt></p>
<p>这样，每一幅图像的输出和标签一样，也是一个$3 \times 3 \times 8$的张量了。输入一幅图片后，我们利用上一节学的卷积滑动窗口，同时预测出每个格子里的物体边框。</p>
<p>另外，这里要详细讨论一下$b_x, b_y, b_h, b_w$的表示方法。由于我们只关心框相对于格子的位置，因此我们可以把规定一个格子的边长为1。这样，就满足$0 \leq b_x, b_y \leq 1$了。不过，由于物体的边框可以超出小框，$b_h, b_w &gt; 1$是很有可能的。</p>
<p>看到这，大家可能会有一些疑问：如果一个格子里有多个物体呢？的确，这个算法无法输出一个格子里的多个物体。一种解决方法是，我们可以把格子分得更细一点，比如$19 \times 19$个格子。这样，可以被检测到物体会多一些。但是，增加格子数又会引入一个新的问题——多个格子检测到了同一个物体。下面的两节里我们会尝试解决这个新的问题。</p>
<blockquote>
<p>吴恩达老师说，YOLO这篇论文很难读懂，他和其他几个资深研究者都花了很大的功夫才读懂这篇论文。</p>
</blockquote>
<h2 id="IoU-交并比"><a href="#IoU-交并比" class="headerlink" title="IoU(交并比)"></a>IoU(交并比)</h2><p>在目标检测中，有一个微妙的问题：框出一个物体的边框有无数个，想精确框出标签的边框是不可能的。怎么判定一个输出结果和标签里的边框“差不多”呢？这就要用到<strong>IoU(Intersection over Union，交并比)</strong> 这个概念。</p>
<p>IoU，顾名思义，二者的交集比上二者的并集，很好理解。比如下图中，网络的输出是紫框，真值是红框。二者的并集是绿色区域，交集是橙色区域。则IoU就是橙色比绿色。</p>
<p><img src="/2022/07/26/DLS-note-12/10.jpg" alt></p>
<p>依照惯例，如果IoU$\geq 0.5$，我们就认为网络的输出是正确的。当然，想更严格一点，0.6,0.7也是可以的。</p>
<blockquote>
<p>IOU 也是 “I owe you(我欠了你的钱)”的缩写，哈哈哈。</p>
</blockquote>
<h2 id="NMS-非极大值抑制"><a href="#NMS-非极大值抑制" class="headerlink" title="NMS(非极大值抑制)"></a>NMS(非极大值抑制)</h2><p>假设在YOLO中，我们用$19 \times 19$个小格来检测物体。可是，由于小格子太多了，算法得到了多个重复的检测框（以及每个框中有物体的概率）。这该怎么办呢？</p>
<p><img src="/2022/07/26/DLS-note-12/11.jpg" alt></p>
<p>NMS(Non-Maximum Suppresion，非极大值抑制)就是解决这个问题的算法。这个算法的名字听起来很奇怪，但大家理解了这个算法的实现后，就知道这个“抑制”是什么意思了。</p>
<blockquote>
<p>讲起算法我就不困了。我会抛弃视频中的讲解思路，用我自己的逻辑讲一遍。讲算法，千万不能一上来就讲算法的步骤，一定要先讲清楚算法的思路。</p>
</blockquote>
<p>在学NMS之前，我们先动动脑，看看在去掉重复的框时，我们期望得到怎样的去重输出结果。</p>
<p>首先，既然是去重，那么就不能出现两个框过度重合的情况。其次，我们希望留下来的框的预测概率尽可能大。</p>
<p>在这两个要求下，我们来看看上面那幅图的输出应该是怎样的。<br>我们一眼就能看出，对于左边那辆车，我们应该保留0.8的框；对于右边那辆车，我们应该保留0.9的框。</p>
<p>为什么我们能“一眼看出”呢？这是因为左边两个框、右边三个框恰好都分别表示了一辆车。我们能够快速地把这些框分成两类。但是，在情况比较复杂时，我们就难以快速找出最好的框了。比如下面这种情况中，两辆车很近，有些框甚至同时标出了两辆车：</p>
<p><img src="/2022/07/26/DLS-note-12/11-2.jpg" alt></p>
<p>为了处理这种复杂的情况，我们必须想出一种万全的算法，以筛选出那些概率比较大的框。</p>
<p>稍微思考一下，其实这样的算法非常简单：找出最大的框，去掉所有和它过度重合的框；在剩下的框中，找出最大的框，去掉所有和它过度重合的框；……。一直重复直到没有未处理的框为止。这就是NMS算法。</p>
<p>还是让我们来看看刚刚那个例子。使用NMS时，我们会先找到0.9这个框，“抑制”掉右边0.6和0.7的框。在剩下的框中，最大的是0.8这个框，它会“抑制”掉左边那个0.7的框。</p>
<p><img src="/2022/07/26/DLS-note-12/11-3.jpg" alt></p>
<p>接下来，让我们来严格描述一下这个算法。假设我们有$19 \times 19=361$个输出结果，每个输出结果是一个长度为5的向量$[p_c, b_x, b_y, b_h, b_w]$，分别表示有物体的概率、边框的中心和高宽（我们先不管检测多个物体的情况。事实上，当推广到多个物体时，只要往这个输出结果里多加几个概率就行了）。我们要用NMS输出应该保留的检测结果。“过度重合”，就是两个框的IoU大于0.5。</p>
<p>首先，先做一步初筛，扔掉概率$p_c$小于0.6的结果。</p>
<p>之后，对于没有遍历的框，重复执行：找出概率最大的框，把它加入输出结果；去掉所有和它IoU大于0.5的框。</p>
<p>这个过程用伪代码表示如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Input and preprocessing</span></span><br><span class="line"><span class="built_in">input</span> predicts of size [<span class="number">19</span>, <span class="number">19</span>, <span class="number">5</span>]</span><br><span class="line">resize predicts to [<span class="number">361</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Filter predicts with low probability</span></span><br><span class="line">filtered_predicts = []</span><br><span class="line"><span class="keyword">for</span> predict <span class="keyword">in</span> predicts:</span><br><span class="line">    <span class="comment"># drop p_c &lt; 0.6</span></span><br><span class="line">    <span class="keyword">if</span> predict[<span class="number">0</span>] &gt;= <span class="number">0.6</span>:  </span><br><span class="line">        filtered_predicts.append(predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># NMS</span></span><br><span class="line">n_remainder  = <span class="built_in">len</span>(filtered_predicts)</span><br><span class="line">vis = [<span class="literal">False</span>] * n_remainder <span class="comment"># False for unvisited item</span></span><br><span class="line">output_predicts = []</span><br><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    max_pro = -<span class="number">1</span></span><br><span class="line">    max_index = <span class="number">0</span></span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(filtered_predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">            <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                max_index = i</span><br><span class="line">                max_pro = p[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append output</span></span><br><span class="line">    max_p = filtered_predicts[max_index]</span><br><span class="line">    output_predicts.append(max_p)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(filtered_predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">            <span class="keyword">if</span> get_IoU(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; <span class="number">0.5</span>:</span><br><span class="line">                vis[i] = <span class="literal">True</span></span><br><span class="line">                n_remainder -= <span class="number">1</span></span><br><span class="line">    vis[max_index] = <span class="literal">True</span></span><br><span class="line">    n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>假设进NMS的框有$N$个。算法里求当前最大框那一步可以用优先队列来优化，这一步复杂度是$O(logN)$。但是“抑制”那一步必须要遍历一遍剩下的框，还是有一个$O(N)$复杂度（我暂时想不出朴素的低复杂度的算法）。因此，不用优先队列优化也差不多。算上外层的循环，整个算法的复杂度是$O(N^2)$。在实际的应用中，送入NMS的结果没那么多，不会超过10000个。而且，随着框被不断过滤，外层循环的次数会减少不少。这个算法的性能瓶颈不在输入数$N$上，而在于求IoU实现的细节上。</p>
</blockquote>
<h2 id="锚框（Anchor-Boxes"><a href="#锚框（Anchor-Boxes" class="headerlink" title="锚框（Anchor Boxes)"></a>锚框（Anchor Boxes)</h2><p>为了让一个格子能够检测到多个物体，YOLO论文还提出了一种叫做锚框(Anchor Boxes)的技术。</p>
<p>假设一个格子里同时包含了两个物体：一个“竖着”的人和一个“横着”的车。那么，我们可以以这个格子的中心点为“锚”，画一个竖的框和横的框，让每个格子可以检测到两个物体。这样，人和车都能被检测了。</p>
<p><img src="/2022/07/26/DLS-note-12/12.jpg" alt></p>
<p>严谨地描述，锚框技术是这样做改进的：</p>
<ul>
<li>之前，每一个格子只能包含<strong>一个</strong>样本。训练数据中每一个标签框会被分配到<strong>它中点所在</strong>的<strong>格子</strong>。</li>
<li>现在，每一个格子能包含<strong>多个</strong>样本。每个格子都会预定义几个不同形状的锚框，有几个锚框，就最多能检测到几个物体。训练数据的每一个标签框会被分配到<strong>和它交并比最大的</strong>的<strong>锚框</strong>。</li>
</ul>
<p>注意，之前的最小单元是格子，现在是锚框，所以说现在每个样本被分配到锚框上而不是格子上。可以看下面这两个样本的例子，第一个例子是两个物体都检测到了，第二个是只有锚框2里有物体。和之前一样，如果有某个锚框里没有物体，则除了$p_c$外全填问号即可。</p>
<p><img src="/2022/07/26/DLS-note-12/13.jpg" alt></p>
<p>锚框技术实际上只是对训练数据做了一个约束，改变了训练数据的格式。检测算法本身没有什么改变。</p>
<h2 id="YOLO-算法总结"><a href="#YOLO-算法总结" class="headerlink" title="YOLO 算法总结"></a>YOLO 算法总结</h2><p>让我们把前几节的内容总结一下，看一下YOLO算法的全貌。</p>
<p>在训练前，我们要对数据做预处理。首先，我们要指定以下超参数：图片切分成多大的格子、每个格子里有多少个锚框。之后，根据这些信息，我们可以得到每一个训练标签张量的形状。比如$3 \times 3 \times 2 \times 8$的一个训练标签，表示图片被切成了$3 \times 3$的格子，每个格子有两个锚框。这是一个三分类问题，对于每一个检测出来的物体，都可以用一个长度为$8$的向量表示。其中，$p_c$表示这个锚框里有没有物体, $(b_x, b_y), (b_h, b_w)$分别表示中心点坐标、框的高宽，$c_1, c_2, c_3$分别表示是否为该类物体。</p>
<p>有了预处理好的训练数据，就可以训练一个CNN了。</p>
<p><img src="/2022/07/26/DLS-note-12/14.jpg" alt></p>
<p>在网络给出了输出后，由于输出的框往往多于标签中的框，还要对输出结果进行筛选。筛选的过程如前文所述，先去掉概率过小的框，再分别对每一类物体的框做NMS。</p>
<blockquote>
<p>课堂上没有介绍loss。loss的组成比较复杂，建议阅读原论文。</p>
</blockquote>
<h2 id="区域提案"><a href="#区域提案" class="headerlink" title="区域提案"></a>区域提案</h2><p>YOLO算法是在一堆固定的框里找物体。实际上，我们还可以用神经网络来找出候选框，再在这些框里详细检测。这种技术就叫做区域提案(region proposal)，相关的网络叫做R-CNN(Region with CNN)。</p>
<p>R-CNN 系列网络有多个改进版本：</p>
<ul>
<li>R-CNN: 使用区域提案，但是每次只对一个区域里的物体做分类。</li>
<li>Fast R-CNN: 使用区域提案，并使用基于卷积的滑动窗口加速各区域里物体的分类。</li>
<li>Faster R-CNN: 前两个算法都是用传统方法提案区域，Faster R-CNN用CNN来提案区域，进一步令算法加速。</li>
</ul>
<p>吴恩达老师认为，虽然区域提案的方法很酷，但把目标检测分两步来完成还是太麻烦了，一步到位的YOLO系列算法已经挺方便了。</p>
<h2 id="基于U-Net的语义分割"><a href="#基于U-Net的语义分割" class="headerlink" title="基于U-Net的语义分割"></a>基于U-Net的语义分割</h2><blockquote>
<p>最早这门课是没有这一节的，估计U-Net的架构太常用了，吴恩达老师把基于U-Net的语义分割加入了这周的课中。</p>
</blockquote>
<p>语义分割也是应用非常广泛的一项CV任务。相较于只把物体框出来的目标检测，语义分割会把每一类物体的每个像素都精确地标出来。如下图的示例所示，输入一张图片，语义分割会把每一类物体准确地用同一种颜色表示。</p>
<p><img src="/2022/07/26/DLS-note-12/15.jpg" alt></p>
<p>具体来说，语义分割的输出是一个单通道图片。图片的数字表示此处像素的类别。</p>
<p><img src="/2022/07/26/DLS-note-12/16.jpg" alt></p>
<p>在分类模型中，图像会越卷越小，最后压平放进全连接层并输出多个类别的分类概率。而在语义分割模型中，由于模型的输出也是一幅图像，在输入图像被卷小了以后，应该还有一个放大的过程。</p>
<p><img src="/2022/07/26/DLS-note-12/17.jpg" alt></p>
<p>目前，我们还没有学过带学习参数的可以放大图像分辨率的结构。下一节介绍的反卷积能够完成这件事。</p>
<h2 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h2><p><img src="/2022/07/26/DLS-note-12/1.gif" alt></p>
<p>反卷积和卷积的输入输出大小彻底相反。让我们看看反卷积的形状是怎么计算的。</p>
<p><img src="/2022/07/26/DLS-note-12/18.jpg" alt></p>
<p>如上图所示，反卷积也有卷积核大小、步幅、填充这些参数。不过这些参数都是在输出图像上做的。也就是说，我们会在输出图像上做填充，并且每次在输出图像上一步一步移动。我们把正卷积的输出大小计算公式套到反卷积上的输出上，就能算出反卷积的输入的大小。</p>
<p>在卷积时，我们是把卷积核与图像对应位置的数字乘起来，再求和，算出一个输出值；反卷积则是反了过来，把一个输入值乘到卷积核的每个位置上，再把乘法结果放再输出的对应位置上。一趟反卷积计算如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/19.jpg" alt></p>
<p>这里我们只需要知道反卷积可以做上采样就行了，不需要纠结底层实现细节。</p>
<blockquote>
<p>本课对反卷积的介绍甚少。实际上，反卷积可以通过正卷积来实现。我扫了一圈没看到讲解得比较好的相关文章，有兴趣的可以自行查找资料。</p>
</blockquote>
<h2 id="U-Net-架构"><a href="#U-Net-架构" class="headerlink" title="U-Net 架构"></a>U-Net 架构</h2><p>学完了反卷积，可以来看U-Net的结构了。</p>
<p>U-Net除了对图像使用了先缩小再放大的卷积外，还使用了一种跳连（不是ResNet中残差连接的跳连，而是把两份输入拼接在了一起）。这样，在反卷积层中，不仅有来自上一层的输入，还有来自前面相同大小的正卷积的结果。这样做的好处是，后半部分的网络既能获得前一个卷积的抽象、高级（比如类别）的输入，又能获得前半部分网络中具体，低级的特征（比如形状）。这样，后面的层能够更好地生成输出。</p>
<p><img src="/2022/07/26/DLS-note-12/20.jpg" alt></p>
<p>U-Net具体的结构如下：</p>
<p><img src="/2022/07/26/DLS-note-12/21.jpg" alt></p>
<p>这幅图中，做运算的图像张量被表示成了一个二维矩形，矩形的高度是图像的宽高，矩形的宽度是通道数。U-Net的前半部分和常见的CNN一样，缩小图像大小，增大图像通道数。而在后半部分中，每次上采样时，一半的通道来自上一层的输出，另一半的通道来自于网络前半部分。</p>
<p>从图中能看出，U-Net的结构图是一个“U”型，因此它才被叫做U-Net。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们主要学习了以下内容：</p>
<ul>
<li>任务定义与输出格式<ul>
<li>目标定位</li>
<li>关键点检测</li>
<li>目标检测</li>
<li>语义分割</li>
</ul>
</li>
<li>YOLO<ul>
<li>用卷积实现全连接</li>
<li>用卷积实现滑动窗口</li>
<li>锚框</li>
<li>IoU</li>
<li>NMS</li>
<li>YOLO算法</li>
</ul>
</li>
<li>U-Net<ul>
<li>反卷积</li>
<li>U-Net架构</li>
</ul>
</li>
</ul>
<p>这周的代码实战中，我会详细讲解NMS的实现。时间允许的话，我还会展示一下如何在COCO上训练YOLO。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/20220717-chinese-internet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/20220717-chinese-internet/" class="post-title-link" itemprop="url">从我的公众号被诬告抄袭想到的：中国互联网不配有未来</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-24 00:33:06" itemprop="dateCreated datePublished" datetime="2022-07-24T00:33:06+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天，刚收到公众号官方的两条通知，说我的文章涉嫌整合他人内容，被暂时取消原创声明功能。</p>
<p><img src="/2022/07/24/20220717-chinese-internet/1.jpg" alt></p>
<p>一开始，我还以为是系统的检测算法出了故障，赶紧低声下气地提交了申诉，说明全网上我的文章都是由我自己完成的。</p>
<p>当我回过头来阅读通知时，愕然发现了“经用户投诉且经平台审核”这几个字，一时间怒火中烧：哦，原来我是被诬告了。</p>
<p>我是两三个月前才在开始在公开媒体上创作的，对一些规则可能不熟。有些长期做自媒体的人或许会劝我：唉，这种事正常啊。只要去申诉，给你恢复就行了。而且你看，就封了两天，之后不就正常了？忍一忍就过去了。你不要玻璃心了。</p>
<p>是啊，从利益的角度来看，这件事不就是让我两天发不了“原创”的文章？两天过后一切损失都抹平了。</p>
<p>才怪呢。</p>
<p>这件事对我真正的影响，是损害了我的名誉。</p>
<p>说实话，你可以说我水平差劲，说我没钱没势，说我狂妄自大。背后说，当面讲，拿着个大喇叭对全国人民喊。我都会不以为然。</p>
<p>问题是，对于我的作品，对于我辛辛苦苦创作出来的受到了客观认可的作品，你不能诋毁它。甚至不是去挖苦文章的内容，而是拿最恶劣的抄袭来指控我。这是对我名声的侮辱，对所有有尊严的创作者的侮辱，也是你们创作平台自己的耻辱。</p>
<p>通知里说“有用户举报”。我不知道是不是真的有人举报。如果是真的，那我也奈何不了那个人。在这件事上，我是弱势的一方。我也不知道是谁干的，也没有受到什么严重的经济损失，没有任何追责的可能。可能别人就是觉得好玩，顺手按了个举报按钮呢？我除了骂一句“此人卑鄙无耻”以外，也做不了什么。</p>
<p>真正有问题的是微信公众号的官方。你们的审核人员心慵意懒，玩忽职守。手握审核大权而不知善用，身着公正之衣而不辩是非。不察之下竟把抄袭之罪强加于光明磊落的原创作者，以至于颠倒是非，污人清白，真是岂有此理！</p>
<p>你以为你们平台做起来靠的是什么？靠的是你们掌握的数以亿计的流量？别开玩笑了。给你们带来价值的，是会下金蛋的鸡。看着满棚的金鸡，几位手持饲料的奴仆倒好像也长出了翅膀，以为自己也能下出金蛋一般，觉得随手杀掉一两只鸡也无所谓。真是可笑至极。</p>
<p>我这里还要好心奉劝一下所有的创作平台，烦请你们给审核人员的评估指标中加一个错审率，加大造成冤情的惩罚。同时，在认定冤情的申诉通过后，把“对不起”三个大字好好地打在私信里。</p>
<p>仔细一想，这事也怪不了公众号平台，整个环境毕竟就是这样的。</p>
<p>每天在平台上发送的内容那么多，审核员能够把每篇文章都过一遍都实属不易，出几个纰漏也是情理之中。这些道理我肯定都懂，也可以理解。</p>
<p>但趁着这口气，我还要发表一下对于中国内容创作平台的看法。</p>
<p>以前，去网上查编程知识的时候，查出来的全是低劣的复制粘贴文章。想要搜个教程，还要跳过那万年不变的前几个网站，去后面几个搜索结果的跟帖中翻出学习资源来。想在网络中找精品资源，可谓是沙里淘金，海底捞针。</p>
<p>现在，我学有小成，想在网上分享一些学习的心得。可是，又关注者寥寥。</p>
<p>是我不会用搜索引擎吗？是我写不出好的文章吗？</p>
<p>我看，是这个互联网的运行机制有问题啊。</p>
<p>在“后来者居上”的论坛中，优秀的帖子还是会被顶起，随后贴上“精品”的标签，供后人赏读。</p>
<p>而在以推荐机制为主的封闭创作平台当道之后，本来就稀有的精品内容便沉入了泥潭之下。只推荐自己喜欢的内容，有谁不乐意呢？坐揽着源源不断的流量，那哪平台不开心呢？这就是大势所趋啊。</p>
<p>平台只知道流量，只知道赚钱。但这也没有办法。很多平台看似规模宏大，实际上，他们还烧着投资人的钱，他们自己还身陷囹圄，入不敷出。因此，他们只能想尽一切办法，赶快扩大规模，赶快收割流量，赶快盈利。然而，哪怕真有一日，他们开始盈利了，也只会在只知道赚钱的道路上转不过弯，忘记了当年平台是怎么火起来的。</p>
<p>公益性地维护一个优质的内容平台。这种看上去吃力不讨好的事情，小平台不会做，大平台也不会做。</p>
<p>按他们这样下去，中国互联网上优质的文章只会越来越少见，看不到更好的未来啊。</p>
<p>质量和金钱，真的就是互斥的关系吗？</p>
<p>我看，只是运营这些平台的人太菜了吧。</p>
<p>一来，他们过于浮躁。在指定最优化目标时，只想到了赚钱，却不知道往里面加一点点的“情怀”。</p>
<p>二来，他们水平低下。但凡掺入了一些不赚钱的因素，就觉得要运营不下去了。</p>
<p>三来，他们目光短浅。以为创造没有利润的精品是在浪费时间，实际上有内涵的事物在多年后能够带来超出金钱的价值。</p>
<p>等我有钱了，我能够把这一切都做好。</p>
<p>我知道，十多年的寒窗苦读，对多数人来讲并不是什么愉快的经历。很多时候，并不是自己没有学好，而是教育的方法有问题。这一问题在大学之后尤为突出。倘若当年能够收获一些优质的知识，也不至于会走那么多的弯路。</p>
<p>等我有钱了，我会设法建立一个吸引优秀创作者的平台，把优质的内容结合并组织起来，把名声打响，让大家都能来这里学习。我不仅要做一些“公益”的事情，我还要赚钱，我要把平台持久地运营下去。我会扶正互联网的创作风气，还互联网一个蓬勃发展的未来。</p>
<p>在这篇文章里，我也只能随口嚷嚷。诸君把这些话当作笑谈即可。不过，在当下，我还是会慢慢地行动着，创作着。</p>
<p>如果未来优秀的中文内容越来越多，说不定不再是我们计算机学生抱着一堆机械工业出版社的黑书，而是美国的教授拿着一本本从中文英化过去的参考书。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">135</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">58</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
