<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/en/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/en/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/en/images/logo.svg" color="#222">

<link rel="stylesheet" href="/en/css/main.css">


<link rel="stylesheet" href="/en/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/en/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="前段时间我写了一篇VQVAE的解读，现在再补充一篇VQVAE的PyTorch实现教程。在这个项目中，我们会实现VQVAE论文，在MNIST和CelebAHQ两个数据集上完成图像生成。具体来说，我们会先实现并训练一个图像压缩网络VQVAE，它能把真实图像编码成压缩图像，或者把压缩图像解码回真实图像。之后，我们会训练一个生成压缩图像的生成网络PixelCNN。 代码仓库：https:&#x2F;&#x2F;github.">
<meta property="og:type" content="article">
<meta property="og:title" content="VQVAE PyTorch 实现教程">
<meta property="og:url" content="https://zhouyifan.net/en/2023/07/01/20230622-VQVAE-2/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="前段时间我写了一篇VQVAE的解读，现在再补充一篇VQVAE的PyTorch实现教程。在这个项目中，我们会实现VQVAE论文，在MNIST和CelebAHQ两个数据集上完成图像生成。具体来说，我们会先实现并训练一个图像压缩网络VQVAE，它能把真实图像编码成压缩图像，或者把压缩图像解码回真实图像。之后，我们会训练一个生成压缩图像的生成网络PixelCNN。 代码仓库：https:&#x2F;&#x2F;github.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/3.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/4.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/5.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/6.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/7.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/8.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/9.jpg">
<meta property="article:published_time" content="2023-07-01T04:35:10.000Z">
<meta property="article:modified_time" content="2023-09-27T16:15:02.881Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2023/07/01/20230622-VQVAE-2/1.jpg">

<link rel="canonical" href="https://zhouyifan.net/en/2023/07/01/20230622-VQVAE-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>VQVAE PyTorch 实现教程 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/en/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/en/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/en/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/en/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/en/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/en/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net" rel="section"><i class="fa fa-language fa-fw"></i>简体中文</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2023/07/01/20230622-VQVAE-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          VQVAE PyTorch 实现教程
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-07-01 12:35:10" itemprop="dateCreated datePublished" datetime="2023-07-01T12:35:10+08:00">2023-07-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>前段时间我写了一篇VQVAE的解读，现在再补充一篇VQVAE的PyTorch实现教程。在这个项目中，我们会实现VQVAE论文，在MNIST和CelebAHQ两个数据集上完成图像生成。具体来说，我们会先实现并训练一个图像压缩网络VQVAE，它能把真实图像编码成压缩图像，或者把压缩图像解码回真实图像。之后，我们会训练一个生成压缩图像的生成网络PixelCNN。</p>
<p>代码仓库：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/VQVAE">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/VQVAE</a></p>
<h2 id="项目运行示例"><a href="#项目运行示例" class="headerlink" title="项目运行示例"></a>项目运行示例</h2><p>如果你只是想快速地把项目运行起来，可以只阅读本节。</p>
<p>在本地安装好项目后，运行<code>python dldemos/VQVAE/dataset.py</code>来下载MNIST数据集。之后运行<code>python dldemos/VQVAE/main.py</code>，这个脚本会完成以下四个任务：</p>
<ol>
<li>训练VQVAE</li>
<li>用VQVAE重建数据集里的随机数据</li>
<li>训练PixelCNN</li>
<li>用PixelCNN+VQVAE随机生成图片</li>
</ol>
<p>第二步得到的重建结果大致如下（每对图片中左图是原图，右图是重建结果）：</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/1.jpg" alt></p>
<p>第四步得到的随机生成结果大致如下：</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/2.jpg" alt></p>
<p>如果你要使用CelebAHQ数据集，请照着下一节的指示把CelebAHQ下载到指定目录，再执行<code>python dldemos/VQVAE/main.py -c 4</code>。</p>
<h2 id="数据集准备"><a href="#数据集准备" class="headerlink" title="数据集准备"></a>数据集准备</h2><p>MNIST数据集可以用PyTorch的API自动下载。我们可以用下面的代码下载MNIST数据集并查看数据的格式。从输出中可知，MNIST的图片形状为<code>[1, 28, 28]</code>，颜色取值范围为<code>[0, 1]</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download_mnist</span>():</span></span><br><span class="line">    mnist = torchvision.datasets.MNIST(root=<span class="string">&#x27;data/mnist&#x27;</span>, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;length of MNIST&#x27;</span>, <span class="built_in">len</span>(mnist))</span><br><span class="line">    <span class="built_in">id</span> = <span class="number">4</span></span><br><span class="line">    img, label = mnist[<span class="built_in">id</span>]</span><br><span class="line">    <span class="built_in">print</span>(img)</span><br><span class="line">    <span class="built_in">print</span>(label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># On computer with monitor</span></span><br><span class="line">    <span class="comment"># img.show()</span></span><br><span class="line"></span><br><span class="line">    img.save(<span class="string">&#x27;work_dirs/tmp_mnist.jpg&#x27;</span>)</span><br><span class="line">    tensor = transforms.ToTensor()(img)</span><br><span class="line">    <span class="built_in">print</span>(tensor.shape)</span><br><span class="line">    <span class="built_in">print</span>(tensor.<span class="built_in">max</span>())</span><br><span class="line">    <span class="built_in">print</span>(tensor.<span class="built_in">min</span>())</span><br></pre></td></tr></table></figure>
<p>我们可以用下面的代码把它封成简单的<code>Dataset</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNISTImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, img_shape=(<span class="params"><span class="number">28</span>, <span class="number">28</span></span>)</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.img_shape = img_shape</span><br><span class="line">        self.mnist = torchvision.datasets.MNIST(root=<span class="string">&#x27;data/mnist&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.mnist)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span></span><br><span class="line">        img = self.mnist[index][<span class="number">0</span>]</span><br><span class="line">        pipeline = transforms.Compose(</span><br><span class="line">            [transforms.Resize(self.img_shape),</span><br><span class="line">             transforms.ToTensor()])</span><br><span class="line">        <span class="keyword">return</span> pipeline(img)</span><br></pre></td></tr></table></figure>
<p>接下来准备CelebAHQ。CelebAHQ数据集原本的图像大小是1024x1024，但我们这个项目用不到这么大的图片。我在kaggle上找到了一个256x256的CelebAHQ (<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256)，所有文件加起来只有300MB左右，很适合我们项目。请在该页面下载压缩包，并把压缩包解压到项目的`data/celebA/celeba_hq_256`目录下。">https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256)，所有文件加起来只有300MB左右，很适合我们项目。请在该页面下载压缩包，并把压缩包解压到项目的`data/celebA/celeba_hq_256`目录下。</a></p>
<p>下载完数据后，我们可以写一个简单的从目录中读取图片的<code>Dataset</code>类。和MNIST的预处理流程不同，我这里给CelebAHQ的图片加了一个中心裁剪的操作，一来可以让人脸占比更大，便于模型学习，二来可以让该类兼容CelebA数据集（CelebA数据集的图片不是正方形，需要裁剪）。这个操作是可选的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CelebADataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root, img_shape=(<span class="params"><span class="number">64</span>, <span class="number">64</span></span>)</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.root = root</span><br><span class="line">        self.img_shape = img_shape</span><br><span class="line">        self.filenames = <span class="built_in">sorted</span>(os.listdir(root))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.filenames)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span></span><br><span class="line">        path = os.path.join(self.root, self.filenames[index])</span><br><span class="line">        img = Image.<span class="built_in">open</span>(path)</span><br><span class="line">        pipeline = transforms.Compose([</span><br><span class="line">            transforms.CenterCrop(<span class="number">168</span>),</span><br><span class="line">            transforms.Resize(self.img_shape),</span><br><span class="line">            transforms.ToTensor()</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">return</span> pipeline(img)</span><br></pre></td></tr></table></figure>
<p>有了数据集类后，我们可以用它们生成<code>Dataloader</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">CELEBA_DIR = <span class="string">&#x27;data/celebA/img_align_celeba&#x27;</span></span><br><span class="line">CELEBA_HQ_DIR = <span class="string">&#x27;data/celebA/celeba_hq_256&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader</span>(<span class="params"><span class="built_in">type</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                   img_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   dist_train=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   num_workers=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                   **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span> == <span class="string">&#x27;CelebA&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> img_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            kwargs[<span class="string">&#x27;img_shape&#x27;</span>] = img_shape</span><br><span class="line">        dataset = CelebADataset(CELEBA_DIR, **kwargs)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;CelebAHQ&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> img_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            kwargs[<span class="string">&#x27;img_shape&#x27;</span>] = img_shape</span><br><span class="line">        dataset = CelebADataset(CELEBA_HQ_DIR, **kwargs)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&#x27;MNIST&#x27;</span>:</span><br><span class="line">        <span class="keyword">if</span> img_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            dataset = MNISTImageDataset(img_shape)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dataset = MNISTImageDataset()</span><br><span class="line">    <span class="keyword">if</span> dist_train:</span><br><span class="line">        sampler = DistributedSampler(dataset)</span><br><span class="line">        dataloader = DataLoader(dataset,</span><br><span class="line">                                batch_size=batch_size,</span><br><span class="line">                                sampler=sampler,</span><br><span class="line">                                num_workers=num_workers)</span><br><span class="line">        <span class="keyword">return</span> dataloader, sampler</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dataloader = DataLoader(dataset,</span><br><span class="line">                                batch_size=batch_size,</span><br><span class="line">                                shuffle=<span class="literal">True</span>,</span><br><span class="line">                                num_workers=num_workers)</span><br><span class="line">        <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure>
<p>我们可以利用<code>Dataloader</code>来查看CelebAHQ数据集的内容及数据格式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> os.path.exists(CELEBA_HQ_DIR):</span><br><span class="line">    dataloader = get_dataloader(<span class="string">&#x27;CelebAHQ&#x27;</span>, <span class="number">16</span>)</span><br><span class="line">    img = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloader))</span><br><span class="line">    <span class="built_in">print</span>(img.shape)</span><br><span class="line">    N = img.shape[<span class="number">0</span>]</span><br><span class="line">    img = einops.rearrange(img,</span><br><span class="line">                            <span class="string">&#x27;(n1 n2) c h w -&gt; c (n1 h) (n2 w)&#x27;</span>,</span><br><span class="line">                            n1=<span class="built_in">int</span>(N**<span class="number">0.5</span>))</span><br><span class="line">    <span class="built_in">print</span>(img.shape)</span><br><span class="line">    <span class="built_in">print</span>(img.<span class="built_in">max</span>())</span><br><span class="line">    <span class="built_in">print</span>(img.<span class="built_in">min</span>())</span><br><span class="line">    img = transforms.ToPILImage()(img)</span><br><span class="line">    img.save(<span class="string">&#x27;work_dirs/tmp_celebahq.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure><br>从输出中可知，CelebAHQ的颜色取值范围同样是<code>[0, 1]</code>。经我们的预处理流水线得到的图片如下。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/3.jpg" alt></p>
<h2 id="实现并训练-VQVAE"><a href="#实现并训练-VQVAE" class="headerlink" title="实现并训练 VQVAE"></a>实现并训练 VQVAE</h2><p>要用VQVAE做图像生成，其实要训练两个模型：一个是用于压缩图像的VQVAE，另一个是生成压缩图像的PixelCNN。这两个模型是可以分开训练的。我们先来实现并训练VQVAE。</p>
<p>VQVAE的架构非常简单：一个编码器，一个解码器，外加中间一个嵌入层。损失函数为图像的重建误差与编码器输出与其对应嵌入之间的误差。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/4.jpg" alt></p>
<p>VQVAE的编码器和解码器的结构也很简单，仅由普通的上/下采样层和残差块组成。具体来说，编码器先是有两个3x3卷积+2倍下采样卷积的模块，再有两个残差块(ReLU, 3x3卷积, ReLU, 1x1卷积)；解码器则反过来，先有两个残差块，再有两个3x3卷积+2倍上采样反卷积的模块。为了让代码看起来更清楚一点，我们不用过度封装，仅实现一个残差块模块，再用残差块和PyTorch自带模块拼成VQVAE。</p>
<p>先实现残差块。注意，由于模型比较简单，残差块内部和VQVAE其他地方都可以不使用BatchNorm。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.conv1 = nn.Conv2d(dim, dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(dim, dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        tmp = self.relu(x)</span><br><span class="line">        tmp = self.conv1(tmp)</span><br><span class="line">        tmp = self.relu(tmp)</span><br><span class="line">        tmp = self.conv2(tmp)</span><br><span class="line">        <span class="keyword">return</span> x + tmp</span><br></pre></td></tr></table></figure>
<p>有了残差块类后，我们可以直接实现VQVAE类。我们先在初始化函数里把模块按顺序搭好。编码器和解码器的结构按前文的描述搭起来即可。嵌入空间(codebook)其实就是个普通的嵌入层。此处我仿照他人代码给嵌入层显式初始化参数，但实测下来和默认的初始化参数方式差别不大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VQVAE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_dim, dim, n_embedding</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.encoder = nn.Sequential(nn.Conv2d(input_dim, dim, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.ReLU(), nn.Conv2d(dim, dim, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.ReLU(), nn.Conv2d(dim, dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                     ResidualBlock(dim), ResidualBlock(dim))</span><br><span class="line">        self.vq_embedding = nn.Embedding(n_embedding, dim)</span><br><span class="line">        self.vq_embedding.weight.data.uniform_(-<span class="number">1.0</span> / n_embedding,</span><br><span class="line">                                               <span class="number">1.0</span> / n_embedding)</span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Conv2d(dim, dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            ResidualBlock(dim), ResidualBlock(dim),</span><br><span class="line">            nn.ConvTranspose2d(dim, dim, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>), nn.ReLU(),</span><br><span class="line">            nn.ConvTranspose2d(dim, input_dim, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        self.n_downsample = <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>之后，我们来实现模型的前向传播。这里的逻辑就略显复杂了。整体来看，这个函数完成了编码、取最近邻、解码这三步。其中，取最近邻的部分最为复杂。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="comment"># encode</span></span><br><span class="line">    ze = self.encoder(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ze: [N, C, H, W]</span></span><br><span class="line">    <span class="comment"># embedding [K, C]</span></span><br><span class="line">    embedding = self.vq_embedding.weight.data</span><br><span class="line">    N, C, H, W = ze.shape</span><br><span class="line">    K, _ = embedding.shape</span><br><span class="line">    embedding_broadcast = embedding.reshape(<span class="number">1</span>, K, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    ze_broadcast = ze.reshape(N, <span class="number">1</span>, C, H, W)</span><br><span class="line">    distance = torch.<span class="built_in">sum</span>((embedding_broadcast - ze_broadcast)**<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    nearest_neighbor = torch.argmin(distance, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># make C to the second dim</span></span><br><span class="line">    zq = self.vq_embedding(nearest_neighbor).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># stop gradient</span></span><br><span class="line">    decoder_input = ze + (zq - ze).detach()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># decode</span></span><br><span class="line">    x_hat = self.decoder(decoder_input)</span><br><span class="line">    <span class="keyword">return</span> x_hat, ze, zq</span><br></pre></td></tr></table></figure><br>我们来详细看一看取最近邻的实现。取最近邻时，我们要用到两块数据：编码器输出<code>ze</code>与嵌入矩阵<code>embedding</code>。<code>ze</code>可以看成一个形状为<code>[N, H, W]</code>的数组，数组存储了长度为<code>C</code>的向量。而嵌入矩阵里有<code>K</code>个长度为<code>C</code>的向量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ze: [N, C, H, W]</span></span><br><span class="line"><span class="comment"># embedding [K, C]</span></span><br><span class="line">embedding = self.vq_embedding.weight.data</span><br><span class="line">N, C, H, W = ze.shape</span><br><span class="line">K, _ = embedding.shape</span><br></pre></td></tr></table></figure><br>为了求<code>N*H*W</code>个向量在嵌入矩阵里的最近邻，我们要先算这每个向量与嵌入矩阵里<code>K</code>个向量的距离。在算距离前，我们要把<code>embedding</code>和<code>ze</code>的形状变换一下，保证<code>(embedding_broadcast - ze_broadcast)**2</code>的形状为<code>[N, K, C, H, W]</code>。我们对这个临时结果的第2号维度（<code>C</code>所在维度）求和，得到形状为<code>[N, K, H, W]</code>的<code>distance</code>。它的含义是，对于<code>N*H*W</code>个向量，每个向量到嵌入空间里<code>K</code>个向量的距离分别是多少。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embedding_broadcast = embedding.reshape(<span class="number">1</span>, K, C, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">ze_broadcast = ze.reshape(N, <span class="number">1</span>, C, H, W)</span><br><span class="line">distance = torch.<span class="built_in">sum</span>((embedding_broadcast - ze_broadcast)**<span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>有了距离张量后，我们再对其1号维度（<code>K</code>所在维度）求最近邻所在下标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nearest_neighbor = torch.argmin(distance, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>有了下标后，我们可以用<code>self.vq_embedding(nearest_neighbor)</code>从嵌入空间取出最近邻了。别忘了，<code>nearest_neighbor</code>的形状是<code>[N, H, W]</code>，<code>self.vq_embedding(nearest_neighbor)</code>的形状会是<code>[N, H, W, C]</code>。我们还要把<code>C</code>维度转置一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make C to the second dim</span></span><br><span class="line">zq = self.vq_embedding(nearest_neighbor).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>最后，我们用论文里提到的停止梯度算子，把<code>zq</code>变形一下。这样，算误差的时候用的是<code>zq</code>，算梯度时<code>ze</code>会接收解码器传来的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># stop gradient</span></span><br><span class="line">decoder_input = ze + (zq - ze).detach()</span><br></pre></td></tr></table></figure>
<p>求最近邻的部分就到此结束了。最后再补充一句，前向传播函数不仅返回了重建结果<code>x_hat</code>，还返回了<code>ze</code>, <code>zq</code>。这是因为我们待会要在训练时根据<code>ze</code>, <code>zq</code>求损失函数。</p>
<p>准备好了模型类后，假设我们已经用某些超参数初始化好了模型<code>model</code>，我们可以用下面的代码训练VQVAE。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_vqvae</span>(<span class="params">model: VQVAE,</span></span></span><br><span class="line"><span class="params"><span class="function">                img_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                ckpt_path=<span class="string">&#x27;dldemos/VQVAE/model.pth&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dataset_type=<span class="string">&#x27;MNIST&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                lr=<span class="number">1e-3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_epochs=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                l_w_embedding=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                l_w_commitment=<span class="number">0.25</span></span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;batch size:&#x27;</span>, batch_size)</span><br><span class="line">    dataloader = get_dataloader(dataset_type,</span><br><span class="line">                                batch_size,</span><br><span class="line">                                img_shape=img_shape,</span><br><span class="line">                                use_lmdb=USE_LMDB)</span><br><span class="line">    model.to(device)</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr)</span><br><span class="line">    mse_loss = nn.MSELoss()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> dataloader:</span><br><span class="line">            current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">            x = x.to(device)</span><br><span class="line"></span><br><span class="line">            x_hat, ze, zq = model(x)</span><br><span class="line">            l_reconstruct = mse_loss(x, x_hat)</span><br><span class="line">            l_embedding = mse_loss(ze.detach(), zq)</span><br><span class="line">            l_commitment = mse_loss(ze, zq.detach())</span><br><span class="line">            loss = l_reconstruct + \</span><br><span class="line">                l_w_embedding * l_embedding + l_w_commitment * l_commitment</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            total_loss += loss.item() * current_batch_size</span><br><span class="line">        total_loss /= <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">        toc = time.time()</span><br><span class="line">        torch.save(model.state_dict(), ckpt_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;e&#125;</span> loss: <span class="subst">&#123;total_loss&#125;</span> elapsed <span class="subst">&#123;(toc - tic):<span class="number">.2</span>f&#125;</span>s&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Done&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>先看一下训练函数的参数。其他参数都没什么特别的，只有误差权重<code>l_w_embedding=1,l_w_commitment=0.25</code>需要讨论一下。误差函数有三项，但论文只给了第三项的权重（0.25），默认第二项的权重为1。我在实现时把第二项的权重<code>l_w_embedding</code>也加上了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_vqvae</span>(<span class="params">model: VQVAE,</span></span></span><br><span class="line"><span class="params"><span class="function">                img_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                ckpt_path=<span class="string">&#x27;dldemos/VQVAE/model.pth&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dataset_type=<span class="string">&#x27;MNIST&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                lr=<span class="number">1e-3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_epochs=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                l_w_embedding=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                l_w_commitment=<span class="number">0.25</span></span>):</span></span><br></pre></td></tr></table></figure><br>再来把函数体过一遍。一开始，我们可以用传来的参数把<code>dataloader</code>初始化一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;batch size:&#x27;</span>, batch_size)</span><br><span class="line">dataloader = get_dataloader(dataset_type,</span><br><span class="line">                            batch_size,</span><br><span class="line">                            img_shape=img_shape,</span><br><span class="line">                            use_lmdb=USE_LMDB)</span><br></pre></td></tr></table></figure>
<p>再把模型的状态调好，并准备好优化器和算均方误差的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.to(device)</span><br><span class="line">model.train()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr)</span><br><span class="line">mse_loss = nn.MSELoss()</span><br></pre></td></tr></table></figure>
<p>准备好变量后，进入训练循环。训练的过程比较常规，唯一要注意的就是误差计算部分。由于我们把复杂的逻辑都放在了模型类中，这里我们可以直接先用<code>model(x)</code>得到重建图像<code>x_hat</code>和算误差的<code>ze, zq</code>，再根据论文里的公式算3个均方误差，最后求一个加权和，代码比较简明。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> dataloader:</span><br><span class="line">        current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = x.to(device)</span><br><span class="line"></span><br><span class="line">        x_hat, ze, zq = model(x)</span><br><span class="line">        l_reconstruct = mse_loss(x, x_hat)</span><br><span class="line">        l_embedding = mse_loss(ze.detach(), zq)</span><br><span class="line">        l_commitment = mse_loss(ze, zq.detach())</span><br><span class="line">        loss = l_reconstruct + \</span><br><span class="line">            l_w_embedding * l_embedding + l_w_commitment * l_commitment</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure></p>
<p>训练完毕后，我们可以用下面的代码来测试VQVAE的重建效果。所谓重建，就是模拟训练的过程，随机取一些图片，先编码后解码，看解码出来的图片和原图片是否一致。为了获取重建后的图片，我们只需要直接执行前向传播函数<code>model(x)</code>即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span>(<span class="params">model, x, device, dataset_type=<span class="string">&#x27;MNIST&#x27;</span></span>):</span></span><br><span class="line">    model.to(device)</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        x_hat, _, _ = model(x)</span><br><span class="line">    n = x.shape[<span class="number">0</span>]</span><br><span class="line">    n1 = <span class="built_in">int</span>(n**<span class="number">0.5</span>)</span><br><span class="line">    x_cat = torch.concat((x, x_hat), <span class="number">3</span>)</span><br><span class="line">    x_cat = einops.rearrange(x_cat, <span class="string">&#x27;(n1 n2) c h w -&gt; (n1 h) (n2 w) c&#x27;</span>, n1=n1)</span><br><span class="line">    x_cat = (x_cat.clip(<span class="number">0</span>, <span class="number">1</span>) * <span class="number">255</span>).cpu().numpy().astype(np.uint8)</span><br><span class="line">    <span class="keyword">if</span> dataset_type == <span class="string">&#x27;CelebA&#x27;</span> <span class="keyword">or</span> dataset_type == <span class="string">&#x27;CelebAHQ&#x27;</span>:</span><br><span class="line">        x_cat = cv2.cvtColor(x_cat, cv2.COLOR_RGB2BGR)</span><br><span class="line">    cv2.imwrite(<span class="string">f&#x27;work_dirs/vqvae_reconstruct_<span class="subst">&#123;dataset_type&#125;</span>.jpg&#x27;</span>, x_cat)</span><br><span class="line"></span><br><span class="line">vqvae = ...</span><br><span class="line">dataloader = get_dataloader(...)</span><br><span class="line">img = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloader)).to(device)</span><br><span class="line">reconstruct(vqvae, img, device, cfg[<span class="string">&#x27;dataset_type&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h2 id="训练压缩图像生成模型-PixelCNN"><a href="#训练压缩图像生成模型-PixelCNN" class="headerlink" title="训练压缩图像生成模型 PixelCNN"></a>训练压缩图像生成模型 PixelCNN</h2><p>有了一个VQVAE后，我们要用另一个模型对VQVAE的离散空间采样，也就是训练一个能生成压缩图片的模型。我们可以按照VQVAE论文的方法，使用PixelCNN来生成压缩图片。</p>
<p>PixelCNN 的原理及实现方法就不在这里过多介绍了。详情可以参见我之前的PixelCNN解读文章。简单来说，PixelCNN给每个像素从左到右，从上到下地编了一个序号，让每个像素仅由之前所有像素决定。采样时，PixelCNN按序号从左上到右下逐个生成图像的每一个像素；训练时，PixelCNN使用了某种掩码机制，使得每个像素只能看到编号更小的像素，并行地输出每一个像素的生成结果。</p>
<p>PixelCNN具体的训练示意图如下。模型的输入是一幅图片，每个像素的取值是0~255；模型给图片的每个像素输出了一个概率分布，即表示此处颜色取0，取1，……，取255的概率。由于神经网络假设数据的输入符合标准正态分布，我们要在数据输入前把整型的颜色转换成0~1之间的浮点数。最简单的转换方法是除以255。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/5.jpg" alt></p>
<p>以上是训练PixelCNN生成普通图片的过程。而在训练PixelCNN生成压缩图片时，上述过程需要修改。压缩图片的取值是离散编码。离散编码和颜色值不同，它不是连续的。你可以说颜色1和颜色0、2相近，但不能说离散编码1和离散编码0、2相近。因此，为了让PixelCNN建模离散编码，需要把原来的除以255操作换成一个嵌入层，使得网络能够读取离散编码。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/6.jpg" alt></p>
<p>反映在代码中，假设我们已经有了一个普通的PixelCNN模型<code>GatedPixelCNN</code>，我们需要在整个模型的最前面套一个嵌入层，嵌入层的嵌入个数等于离散编码的个数(<code>color_level</code>)，嵌入长度等于模型的特征长度(<code>p</code>)。由于嵌入层会直接输出一个长度为<code>p</code>的向量，我们还需要把第一个模块的输入通道数改成<code>p</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dldemos.pixelcnn.model <span class="keyword">import</span> GatedPixelCNN, GatedBlock</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PixelCNNWithEmbedding</span>(<span class="params">GatedPixelCNN</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_blocks, p, linear_dim, bn=<span class="literal">True</span>, color_level=<span class="number">256</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(n_blocks, p, linear_dim, bn, color_level)</span><br><span class="line">        self.embedding = nn.Embedding(color_level, p)</span><br><span class="line">        self.block1 = GatedBlock(<span class="string">&#x27;A&#x27;</span>, p, p, bn)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().forward(x)</span><br></pre></td></tr></table></figure>
<p>有了一个能处理离散编码的PixelCNN后，我们可以用下面的代码来训练PixelCNN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_generative_model</span>(<span class="params">vqvae: VQVAE,</span></span></span><br><span class="line"><span class="params"><span class="function">                           model,</span></span></span><br><span class="line"><span class="params"><span class="function">                           img_shape=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           ckpt_path=<span class="string">&#x27;dldemos/VQVAE/gen_model.pth&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           dataset_type=<span class="string">&#x27;MNIST&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                           n_epochs=<span class="number">50</span></span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;batch size:&#x27;</span>, batch_size)</span><br><span class="line">    dataloader = get_dataloader(dataset_type,</span><br><span class="line">                                batch_size,</span><br><span class="line">                                img_shape=img_shape,</span><br><span class="line">                                use_lmdb=USE_LMDB)</span><br><span class="line">    vqvae.to(device)</span><br><span class="line">    vqvae.<span class="built_in">eval</span>()</span><br><span class="line">    model.to(device)</span><br><span class="line">    model.train()</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), <span class="number">1e-3</span>)</span><br><span class="line">    loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">    tic = time.time()</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> dataloader:</span><br><span class="line">            current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                x = x.to(device)</span><br><span class="line">                x = vqvae.encode(x)</span><br><span class="line"></span><br><span class="line">            predict_x = model(x)</span><br><span class="line">            loss = loss_fn(predict_x, x)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            total_loss += loss.item() * current_batch_size</span><br><span class="line">        total_loss /= <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">        toc = time.time()</span><br><span class="line">        torch.save(model.state_dict(), ckpt_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;e&#125;</span> loss: <span class="subst">&#123;total_loss&#125;</span> elapsed <span class="subst">&#123;(toc - tic):<span class="number">.2</span>f&#125;</span>s&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Done&#x27;</span>)</span><br><span class="line">gen_model = PixelCNNWithEmbedding(cfg[<span class="string">&#x27;pixelcnn_n_blocks&#x27;</span>],</span><br><span class="line">                                      cfg[<span class="string">&#x27;pixelcnn_dim&#x27;</span>],</span><br><span class="line">                                      cfg[<span class="string">&#x27;pixelcnn_linear_dim&#x27;</span>], <span class="literal">True</span>,</span><br><span class="line">                                      cfg[<span class="string">&#x27;n_embedding&#x27;</span>])</span><br><span class="line">vqvae.load_state_dict(torch.load(cfg[<span class="string">&#x27;vqvae_path&#x27;</span>]))</span><br><span class="line">train_generative_model(vqvae,</span><br><span class="line">                        gen_model,</span><br><span class="line">                        img_shape=(img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]),</span><br><span class="line">                        device=device,</span><br><span class="line">                        ckpt_path=cfg[<span class="string">&#x27;gen_model_path&#x27;</span>],</span><br><span class="line">                        dataset_type=cfg[<span class="string">&#x27;dataset_type&#x27;</span>],</span><br><span class="line">                        batch_size=cfg[<span class="string">&#x27;batch_size_2&#x27;</span>],</span><br><span class="line">                        n_epochs=cfg[<span class="string">&#x27;n_epochs_2&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>训练部分的核心代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> dataloader:</span><br><span class="line">    current_batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        x = vqvae.encode(x)</span><br><span class="line"></span><br><span class="line">    predict_x = model(x)</span><br><span class="line">    loss = loss_fn(predict_x, x)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>这段代码的意思是说，从训练集里随机取图片<code>x</code>，再将图片压缩成离散编码<code>x = vqvae.encode(x)</code>。这时，<code>x</code>既是PixelCNN的输入，也是PixelCNN的拟合目标。把它输入进PixelCNN，PixelCNN会输出每个像素的概率分布。用交叉熵损失函数约束输出结果即可。</p>
<p>训练完毕后，我们可以用下面的函数来完成整套图像生成流水线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_imgs</span>(<span class="params">vqvae: VQVAE,</span></span></span><br><span class="line"><span class="params"><span class="function">                gen_model,</span></span></span><br><span class="line"><span class="params"><span class="function">                img_shape,</span></span></span><br><span class="line"><span class="params"><span class="function">                n_sample=<span class="number">81</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                device=<span class="string">&#x27;cuda&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dataset_type=<span class="string">&#x27;MNIST&#x27;</span></span>):</span></span><br><span class="line">    vqvae = vqvae.to(device)</span><br><span class="line">    vqvae.<span class="built_in">eval</span>()</span><br><span class="line">    gen_model = gen_model.to(device)</span><br><span class="line">    gen_model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    C, H, W = img_shape</span><br><span class="line">    H, W = vqvae.get_latent_HW((C, H, W))</span><br><span class="line">    input_shape = (n_sample, H, W)</span><br><span class="line">    x = torch.zeros(input_shape).to(device).to(torch.long)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">                output = gen_model(x)</span><br><span class="line">                prob_dist = F.softmax(output[:, :, i, j], -<span class="number">1</span>)</span><br><span class="line">                pixel = torch.multinomial(prob_dist, <span class="number">1</span>)</span><br><span class="line">                x[:, i, j] = pixel[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    imgs = vqvae.decode(x)</span><br><span class="line"></span><br><span class="line">    imgs = imgs * <span class="number">255</span></span><br><span class="line">    imgs = imgs.clip(<span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">    imgs = einops.rearrange(imgs,</span><br><span class="line">                            <span class="string">&#x27;(n1 n2) c h w -&gt; (n1 h) (n2 w) c&#x27;</span>,</span><br><span class="line">                            n1=<span class="built_in">int</span>(n_sample**<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line">    imgs = imgs.detach().cpu().numpy().astype(np.uint8)</span><br><span class="line">    <span class="keyword">if</span> dataset_type == <span class="string">&#x27;CelebA&#x27;</span> <span class="keyword">or</span> dataset_type == <span class="string">&#x27;CelebAHQ&#x27;</span>:</span><br><span class="line">        imgs = cv2.cvtColor(imgs, cv2.COLOR_RGB2BGR)</span><br><span class="line"></span><br><span class="line">    cv2.imwrite(<span class="string">f&#x27;work_dirs/vqvae_sample_<span class="subst">&#123;dataset_type&#125;</span>.jpg&#x27;</span>, imgs)</span><br></pre></td></tr></table></figure>
<p>抛掉前后处理，和图像生成有关的代码如下。一开始，我们要随便创建一个空图片<code>x</code>，用于储存PixelCNN生成的压缩图片。之后，我们按顺序遍历每个像素，把当前图片输入进PixelCNN，让PixelCNN预测下一个像素的概率分布<code>prob_dist</code>。我们再用<code>torch.multinomial</code>从概率分布中采样，把采样的结果填回图片。遍历结束后，我们用VQVAE的解码器把压缩图片变成真实图片。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">C, H, W = img_shape</span><br><span class="line">H, W = vqvae.get_latent_HW((C, H, W))</span><br><span class="line">input_shape = (n_sample, H, W)</span><br><span class="line">x = torch.zeros(input_shape).to(device).to(torch.long)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">            output = gen_model(x)</span><br><span class="line">            prob_dist = F.softmax(output[:, :, i, j], -<span class="number">1</span>)</span><br><span class="line">            pixel = torch.multinomial(prob_dist, <span class="number">1</span>)</span><br><span class="line">            x[:, i, j] = pixel[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">imgs = vqvae.decode(x)</span><br></pre></td></tr></table></figure><br>至此，我们已经实现了用VQVAE做图像生成的四个任务：训练VQVAE、重建图像、训练PixelCNN、随机生成图像。完整的<code>main</code>函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    os.makedirs(<span class="string">&#x27;work_dirs&#x27;</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;-c&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;-d&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    cfg = get_cfg(args.c)</span><br><span class="line"></span><br><span class="line">    device = <span class="string">f&#x27;cuda:<span class="subst">&#123;args.d&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line">    img_shape = cfg[<span class="string">&#x27;img_shape&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    vqvae = VQVAE(img_shape[<span class="number">0</span>], cfg[<span class="string">&#x27;dim&#x27;</span>], cfg[<span class="string">&#x27;n_embedding&#x27;</span>])</span><br><span class="line">    gen_model = PixelCNNWithEmbedding(cfg[<span class="string">&#x27;pixelcnn_n_blocks&#x27;</span>],</span><br><span class="line">                                      cfg[<span class="string">&#x27;pixelcnn_dim&#x27;</span>],</span><br><span class="line">                                      cfg[<span class="string">&#x27;pixelcnn_linear_dim&#x27;</span>], <span class="literal">True</span>,</span><br><span class="line">                                      cfg[<span class="string">&#x27;n_embedding&#x27;</span>])</span><br><span class="line">    <span class="comment"># 1. Train VQVAE</span></span><br><span class="line">    train_vqvae(vqvae,</span><br><span class="line">                img_shape=(img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]),</span><br><span class="line">                device=device,</span><br><span class="line">                ckpt_path=cfg[<span class="string">&#x27;vqvae_path&#x27;</span>],</span><br><span class="line">                batch_size=cfg[<span class="string">&#x27;batch_size&#x27;</span>],</span><br><span class="line">                dataset_type=cfg[<span class="string">&#x27;dataset_type&#x27;</span>],</span><br><span class="line">                lr=cfg[<span class="string">&#x27;lr&#x27;</span>],</span><br><span class="line">                n_epochs=cfg[<span class="string">&#x27;n_epochs&#x27;</span>],</span><br><span class="line">                l_w_embedding=cfg[<span class="string">&#x27;l_w_embedding&#x27;</span>],</span><br><span class="line">                l_w_commitment=cfg[<span class="string">&#x27;l_w_commitment&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Test VQVAE by visualizaing reconstruction result</span></span><br><span class="line">    vqvae.load_state_dict(torch.load(cfg[<span class="string">&#x27;vqvae_path&#x27;</span>]))</span><br><span class="line">    dataloader = get_dataloader(cfg[<span class="string">&#x27;dataset_type&#x27;</span>],</span><br><span class="line">                                <span class="number">16</span>,</span><br><span class="line">                                img_shape=(img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]))</span><br><span class="line">    img = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloader)).to(device)</span><br><span class="line">    reconstruct(vqvae, img, device, cfg[<span class="string">&#x27;dataset_type&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Train Generative model (Gated PixelCNN in our project)</span></span><br><span class="line">    vqvae.load_state_dict(torch.load(cfg[<span class="string">&#x27;vqvae_path&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">    train_generative_model(vqvae,</span><br><span class="line">                           gen_model,</span><br><span class="line">                           img_shape=(img_shape[<span class="number">1</span>], img_shape[<span class="number">2</span>]),</span><br><span class="line">                           device=device,</span><br><span class="line">                           ckpt_path=cfg[<span class="string">&#x27;gen_model_path&#x27;</span>],</span><br><span class="line">                           dataset_type=cfg[<span class="string">&#x27;dataset_type&#x27;</span>],</span><br><span class="line">                           batch_size=cfg[<span class="string">&#x27;batch_size_2&#x27;</span>],</span><br><span class="line">                           n_epochs=cfg[<span class="string">&#x27;n_epochs_2&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Sample VQVAE</span></span><br><span class="line">    vqvae.load_state_dict(torch.load(cfg[<span class="string">&#x27;vqvae_path&#x27;</span>]))</span><br><span class="line">    gen_model.load_state_dict(torch.load(cfg[<span class="string">&#x27;gen_model_path&#x27;</span>]))</span><br><span class="line">    sample_imgs(vqvae,</span><br><span class="line">                gen_model,</span><br><span class="line">                cfg[<span class="string">&#x27;img_shape&#x27;</span>],</span><br><span class="line">                device=device,</span><br><span class="line">                dataset_type=cfg[<span class="string">&#x27;dataset_type&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>VQVAE有两个超参数：嵌入个数<code>n_embedding</code>、特征向量长度<code>dim</code>。论文中<code>n_embedding=512</code>，<code>dim=256</code>。而经我实现发现，用更小的参数量也能达到不错的效果。</p>
<p>所有实验的配置文件我都放在了该项目目录下<code>config.py</code>文件中。对于MNIST数据集，我使用的模型超参数为：<code>dim=32, n_embedding=32</code>。VQVAE重建结果如下所示。可以说重建得几乎完美（每对图片左图为原图，右图为重建结果）。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/1.jpg" alt></p>
<p>而对于CelebAHQ数据集，我测试了不同输入尺寸下的不同VQVAE，共有4组配置。</p>
<ol>
<li><code>shape=(3, 128, 128) dim=128 n_embedding=64</code></li>
<li><code>shape=(3, 128, 128) dim=128 n_embedding=128</code></li>
<li><code>shape=(3, 64, 64) dim=128 n_embedding=64</code></li>
<li><code>shape=(3, 64, 64) dim=128 n_embedding=32</code></li>
</ol>
<p>实验的结果很好预测。对于同尺寸的图片，嵌入数越多重建效果越好。这里我只展示下第一组和第二组的重建结果。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/7.jpg" alt></p>
<p>可以看出，VQVAE的重建效果还不错。但由于只使用了均方误差，重建图片在细节上还是比较模糊。重建效果还是很重要的，它决定了该方法做图像生成的质量上限。后续有很多工作都试图提升VQVAE的重建效果。</p>
<p>接下来来看一下随机图像生成的实验。PixelCNN主要有模块数<code>n_blocks</code>、特征长度<code>dim</code>，输出线性层特征长度<code>linear_dim</code>这三个超参数。其中模块数一般是固定的，而输出线性层就被用了一次，其特征长度的影响不大。最需要调节的是特征长度<code>dim</code>。对于MNIST，我的超参数设置为</p>
<ul>
<li><code>n_blocks=15 dim=128 linear_dim=32</code>.</li>
</ul>
<p>对于CelebAHQ，我的超参数设置为</p>
<ul>
<li><code>n_blocks=15 dim=384 linear_dim=256</code>.</li>
</ul>
<p>PixelCNN的训练时间主要由输入图片尺寸和<code>dim</code>决定，训练难度主要由VQVAE的嵌入个数（即多分类的类别数）决定。PixelCNN训起来很花时间。如果时间有限，在CelebAHQ上建议只训练最小最简单的第4组配置。我在项目中提供了PixelCNN的并行训练脚本，比如用下面的命令可以用4张卡在1号配置下并行训练。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=4 dldemos/VQVAE/dist_train_pixelcnn.py -c 1</span><br></pre></td></tr></table></figure>
<p>来看一下实验结果。MNIST上的采样结果还是非常不错的。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/2.jpg" alt></p>
<p>CelebAHQ上的结果会差一点。以下是第4组配置(图像边长<code>64</code>，嵌入数<code>32</code>)的采样结果。大部分图片都还行，起码看得出是一张人脸。但<code>64x64</code>的图片本来就分辨率不高，加上VQVAE解码的损耗，放大来看人脸还是比较模糊的。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/8.jpg" alt></p>
<p>第1组配置（图像边长<code>128</code>，嵌入数<code>64</code>）的PixelCNN实在训练得太慢了，我只训了一个半成品模型。由于部分生成结果比较吓人，我只挑了几个还能看得过去的生成结果。可以看出，如果把模型训完的话，边长128的模型肯定比边长64的模型效果更好。</p>
<p><img src="/2023/07/01/20230622-VQVAE-2/9.jpg" alt></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>网上几乎找不到在CelebAHQ上训练的VQVAE PyTorch项目。我在实现这份代码时，参考了以下项目：</p>
<ul>
<li>官方TensorFlow实现 <a target="_blank" rel="noopener" href="https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb">https://github.com/deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb</a> 。主要代码都写在一个notebook里。</li>
<li>官方实现的PyTorch复现 <a target="_blank" rel="noopener" href="https://github.com/MishaLaskin/vqvae。">https://github.com/MishaLaskin/vqvae。</a></li>
<li>苏剑林的TensorFlow实现。用的生成模型不是PixelCNN而是Transformer。<a target="_blank" rel="noopener" href="https://github.com/bojone/vae/blob/master/vq_vae_keras.py">https://github.com/bojone/vae/blob/master/vq_vae_keras.py</a></li>
</ul>
<h2 id="实验经历分享"><a href="#实验经历分享" class="headerlink" title="实验经历分享"></a>实验经历分享</h2><p>别看VQVAE的代码不难，我做这些实验时还是经历了不少波折的。</p>
<p>一开始，我花一天就把代码写完了，并完成了MNIST上的实验。我觉得在MNIST上做实验的难度太低，不过瘾，就准备把数据集换成CelebA再做一些实验。结果这一做就是两个星期。</p>
<p>换成CelebA后，我碰到的第一个问题是VQVAE训练速度太慢。我尝试减半模型参数，训练时间却减小得不明显。我大致猜出是数据读取占用了大量时间，用性能分析工具一查，果然如此。原来我在<code>DataLoader</code>中一直只用了一个线程，加上<code>num_workers=4</code>就好了。我还把数据集打包成LMDB格式进一步加快数据读取速度。</p>
<p>之后，我又发现VQVAE在CelebA上的重建效果很差。我尝试增加模型参数，没起作用。我又怀疑是64x64的图片质量太低，模型学不到东西，就尝试把输入尺寸改成128x128，并把数据集从CelebA换成CelebAHQ，重建效果依然不行。我调了很多参数，发现了一些奇怪的现象：在嵌入层前使用和不使用BatchNorm对结果的影响很大，且显式初始化嵌入层会让模型的误差一直居高不下。我实在是找不到问题，就拿代码对着别人的PyTorch实现一行一行比较过去。总算，我发现我在使用嵌入层时是用<code>vq_embedding.weight.data[x]</code>（因为前面已经获取了这个矩阵，这样写比较自然），别人是用<code>vq_embedding(x)</code>。我的写法会把嵌入层排除在梯度计算外，嵌入层根本得不到优化。我说怎么换了一个嵌入层的初始化方法模型就根本训不动了。改完bug之后，只训了5个epoch，新模型的误差比原来训练数小时的模型要低了。新模型的重建效果非常好。</p>
<p>总算，任务完成了一半，现在只剩PixelCNN要训练了。我先尝试训练输入为128x128，嵌入数64的模型，采样结果很差。为了加快实验速度，我把输入尺寸减小到64x64，再次训练，采样结果还是不行。根据我之前的经验，PixelCNN的训练难度主要取决于类别数。于是，我把嵌入的数量从64改成了32，并大幅增加PixelCNN的参数量，再次训练。过了很久，训练误差终于降到0.08左右。我一测，这次的采样结果还不错。</p>
<p>这样看来，之前的采样效果不好，是输入128x128，嵌入数64的实验太难了。我毕竟只是想做一个demo，在一个小型实验上成功就行了，没必要花时间去做更耗时的实验。按理说，我应该就此收手。但是，我就是咽不下这一口气，就是想在128x128的实验上成功。我再次加大了PixelCNN的参数量，用128x128的配置，大火慢炖，训练了一天一夜。第二天一早起来，我看到这回的误差也降到了0.08。上次的实验误差降到这个程度时实验已经成功了。我迫不及待地去测试采样效果，却发现采样效果还是稀烂。没办法，我选择投降，开始写这篇文章，准备收工。</p>
<p>写到PixelCNN介绍的那一章节时，我正准备讲解代码。看到PixelCNN训练之前预处理除以<code>color_level</code>那一行时，我楞了一下：这行代码是用来做什么的来着？这段代码全是从PixelCNN项目里复制过来的。当时是做普通图片的图像生成，所以要对输入颜色做一个预处理，把整数颜色变成0~1之间的浮点数。但现在是在生成压缩图片，不能这样处理啊！我恍然大悟，知道是在处理离散输入时做错了。应该多加一个嵌入层，把离散值转换成向量。由于VQVAE的重点不在生成模型上，原论文根本没有强调PixelCNN在离散编码上的实现细节。网上几乎所有文章也都没谈这一点。因此，我在实现PixelCNN时，直接不假思索地把原来的代码搬了过来，根本没想过这种地方会出现bug。</p>
<p>把这处bug改完后，我再次开启训练。这下所有模型的采样结果都正常了。误差降到0.5左右就已经有不错的采样结果了，原来我之前把误差降到0.08完全是无用功。太气人了。</p>
<p>这次的实验让我学到了很多东西。首先是PyTorch编程上的一些注意事项：</p>
<ul>
<li>调用<code>embedding.weight.data[x]</code>是传不了梯度的。</li>
<li>如果读数据时有费时的处理操作（读写硬盘、解码），要在<code>Dataloader</code>里设置<code>num_workers</code>。</li>
</ul>
<p>另外，在测试一个模型是否实现成功时有一个重要的准则：</p>
<ul>
<li>不要仅在简单的数据集（如MNIST）上测试。测试成功可能只是暴力拟合的结果。只有在一个难度较大的数据集上测试成功才能说模型没有问题。</li>
</ul>
<p>在观察模型是否训成功时，还需要注意：</p>
<ul>
<li>训练误差降低不代表模型更优。训练误差的评价方法和模型实际使用方法可能完全不同。不能像我这样偷懒不加测试指标。</li>
</ul>
<p>除了学到的东西外，我还有一些感想。在别人的项目的基础上修改、照着他人代码复现、完全自己动手从零开始写，对于深度学习项目来说，这三种实现方式的难度是依次递增的。改别人的项目，你可能去配置文件里改一两个数字就行了。而照着他人代码复现，最起码你能把代码改成和他人的代码一模一样，然后再去比较哪一块错了。自己动手写，则是有bug都找不到可以参考的地方了。说深度学习的算法难以调试，难就难在这里。效果不好，你很难说清是训练代码错了、超参数没设置好、训练流程错了，或是测试代码错了。可以出错的地方太多了，通常的代码调试手段难以用在深度学习项目上。</p>
<p>对于想要在深度学习上有所建树的初学者，我建议一定要从零动手复现项目。很多工程经验是难以总结的，只有踩了一遍坑才能知道。除了凭借经验外，还可以掌握一些特定的工程方法来减少bug的出现。比如运行训练之前先拿性能工具分析一遍，看看代码是否有误，是否可以提速；又比如可以训练几步后看所有可学习参数是否被正确修改。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/en/tags/Python/" rel="tag"># Python</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/en/2023/06/19/20230605-VQGAN/" rel="prev" title="VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型">
      <i class="fa fa-chevron-left"></i> VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型
    </a></div>
      <div class="post-nav-item">
    <a href="/en/2023/07/07/20230330-diffusion-model/" rel="next" title="扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现">
      扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E8%BF%90%E8%A1%8C%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.</span> <span class="nav-text">项目运行示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87"><span class="nav-number">2.</span> <span class="nav-text">数据集准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E5%B9%B6%E8%AE%AD%E7%BB%83-VQVAE"><span class="nav-number">3.</span> <span class="nav-text">实现并训练 VQVAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E5%8E%8B%E7%BC%A9%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B-PixelCNN"><span class="nav-number">4.</span> <span class="nav-text">训练压缩图像生成模型 PixelCNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">5.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">6.</span> <span class="nav-text">参考资料</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%8F%E5%8E%86%E5%88%86%E4%BA%AB"><span class="nav-number">7.</span> <span class="nav-text">实验经历分享</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/en/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/en/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/en/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/en/lib/anime.min.js"></script>
  <script src="/en/lib/velocity/velocity.min.js"></script>
  <script src="/en/lib/velocity/velocity.ui.min.js"></script>

<script src="/en/js/utils.js"></script>

<script src="/en/js/motion.js"></script>


<script src="/en/js/schemes/muse.js"></script>


<script src="/en/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
