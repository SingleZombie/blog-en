<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/en/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/en/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/en/images/logo.svg" color="#222">

<link rel="stylesheet" href="/en/css/main.css">


<link rel="stylesheet" href="/en/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/en/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="2022年中旬，以扩散模型为核心的图像生成模型将AI绘画带入了大众的视野。实际上，在更早的一年之前，就有了一个能根据文字生成高清图片的模型——VQGAN。VQGAN不仅本身具有强大的图像生成能力，更是传承了前作VQVAE把图像压缩成离散编码的思想，推广了「先压缩，再生成」的两阶段图像生成思路，启发了无数后续工作。  在这篇文章中，我将对VQGAN的论文和源码中的关键部分做出解读，提炼出VQGAN中">
<meta property="og:type" content="article">
<meta property="og:title" content="VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型">
<meta property="og:url" content="https://zhouyifan.net/en/2023/06/19/20230605-VQGAN/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="2022年中旬，以扩散模型为核心的图像生成模型将AI绘画带入了大众的视野。实际上，在更早的一年之前，就有了一个能根据文字生成高清图片的模型——VQGAN。VQGAN不仅本身具有强大的图像生成能力，更是传承了前作VQVAE把图像压缩成离散编码的思想，推广了「先压缩，再生成」的两阶段图像生成思路，启发了无数后续工作。  在这篇文章中，我将对VQGAN的论文和源码中的关键部分做出解读，提炼出VQGAN中">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/0.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/3.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/4.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/5.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/6.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/7.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/8.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/9.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/10.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/11.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/12.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/13.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/19.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/16.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/17.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/18.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/14.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/15.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/20.jpg">
<meta property="og:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/21.jpg">
<meta property="article:published_time" content="2023-06-19T09:53:59.000Z">
<meta property="article:modified_time" content="2024-02-21T08:51:41.486Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2023/06/19/20230605-VQGAN/0.jpg">

<link rel="canonical" href="https://zhouyifan.net/en/2023/06/19/20230605-VQGAN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/en/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/en/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/en/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/en/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/en/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/en/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net" rel="section"><i class="fa fa-language fa-fw"></i>简体中文</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2023/06/19/20230605-VQGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-06-19 17:53:59" itemprop="dateCreated datePublished" datetime="2023-06-19T17:53:59+08:00">2023-06-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>2022年中旬，以扩散模型为核心的图像生成模型将AI绘画带入了大众的视野。实际上，在更早的一年之前，就有了一个能根据文字生成高清图片的模型——VQGAN。VQGAN不仅本身具有强大的图像生成能力，更是传承了前作VQVAE把图像压缩成离散编码的思想，推广了「先压缩，再生成」的两阶段图像生成思路，启发了无数后续工作。</p>
<p><img src="/2023/06/19/20230605-VQGAN/0.jpg" alt="VQGAN生成出的高清图片"></p>
<p>在这篇文章中，我将对VQGAN的论文和源码中的关键部分做出解读，提炼出VQGAN中的关键知识点。由于VQGAN的核心思想和VQVAE如出一辙，我不会过多地介绍VQGAN的核心思想，强烈建议读者先去学懂VQVAE，再来看VQGAN。</p>
<h2 id="VQGAN-核心思想"><a href="#VQGAN-核心思想" class="headerlink" title="VQGAN 核心思想"></a>VQGAN 核心思想</h2><p>VQGAN的论文名为<em>Taming Transformers for High-Resolution Image Synthesis</em>，直译过来是「驯服Transformer模型以实现高清图像合成」。可以看出，该方法是在用Transformer生成图像。可是，为什么这个模型叫做VQGAN，是一个GAN呢？这是因为，VQGAN使用了两阶段的图像生成方法：</p>
<ul>
<li>训练时，先训练一个图像压缩模型（包括编码器和解码器两个子模型），再训练一个生成压缩图像的模型。</li>
<li>生成时，先用第二个模型生成出一个压缩图像，再用第一个模型复原成真实图像。</li>
</ul>
<p>其中，第一个图像压缩模型叫做VQGAN，第二个压缩图像生成模型是一个基于Transformer的模型。</p>
<p>为什么会有这种乍看起来非常麻烦的图像生成方法呢？要理解VQGAN的这种设计动机，有两条路线可以走。两条路线看待问题的角度不同，但实际上是在讲同一件事。</p>
<p>第一条路线是从Transformer入手。Transformer已经在文本生成领域大展身手。同时，Transformer也在视觉任务中开始崭露头角。相比擅长捕捉局部特征的CNN，Transformer的优势在于它能更好地融合图像的全局信息。可是，Transformer的自注意力操作开销太大，只能生成一些分辨率较低的图像。因此，作者认为，可以综合CNN和Transformer的优势，先用基于CNN的VQGAN把图像压缩成一个尺寸更小、信息更丰富的小图像，再用Transformer来生成小图像。</p>
<p>第二条路线是从VQVAE入手。VQVAE是VQGAN的前作，它有着和VQGAN一模一样两阶段图像生成方法。不同的是，VQVAE没有使用GAN结构，且其配套的压缩图像生成模型是基于CNN的。为提升VQVAE的生成效果，作者提出了两项改进策略：1) 图像压缩模型VQVAE仅使用了均方误差，压缩图像的复原结果较为模糊，可以把图像压缩模型换成GAN；2) 在生成压缩图片这个任务上，基于CNN的图像生成模型比不过Transformer，可以用Transformer代替原来的CNN。</p>
<p>第一条思路是作者在论文的引言中描述的，听起来比较高大上；而第二条思路是读者读过文章后能够自然总结出来的，相对来说比较清晰易懂。如果你已经理解了VQVAE，你能通过第二条思路瞬间弄懂VQGAN的原理。说难听点，VQGAN就是一个改进版的VQVAE。然而，VQGAN的改进非常有效，且使用了若干技巧来实现带约束（比如根据文字描述）的高清图像生成，有非常多地方值得学习。</p>
<p>在下文中，我将先补充VQVAE的背景以方便讨论，再介绍VQGAN论文的四大知识点：VQGAN的设计细节、生成压缩图像的Transformer的设计细节、带约束图像生成的实现方法、高清图像生成的实现方法。</p>
<h2 id="VQVAE-背景知识补充"><a href="#VQVAE-背景知识补充" class="headerlink" title="VQVAE 背景知识补充"></a>VQVAE 背景知识补充</h2><p>VQVAE的学习目标是用一个编码器把图像压缩成离散编码，再用一个解码器把图像尽可能地还原回原图像。</p>
<p><img src="/2023/06/19/20230605-VQGAN/1.jpg" alt></p>
<p>通俗来说，VQVAE就是把一幅真实图像压缩成一个小图像。这个小图像和真实图像有着一些相同的性质：小图像的取值和像素值（0-255的整数）一样，都是离散的；小图像依然是二维的，保留了某些空间信息。因此，VQVAE的示意图画成这样会更形象一些：</p>
<p><img src="/2023/06/19/20230605-VQGAN/2.jpg" alt></p>
<p>但小图像和真实图像有一个关键性的区别：与像素值不同，小图像的离散取值之间没有关联。真实图像的像素值其实是一个连续颜色的离散采样，相邻的颜色值也更加相似。比如颜色254和颜色253和颜色255比较相似。而小图像的取值之间是没有关联的，你不能说编码为1与编码为0和编码为2比较相似。由于神经网络不能很好地处理这种离散量，在实际实现中，编码并不是以整数表示的，而是以类似于NLP中的嵌入向量的形式表示的。VAE使用了嵌入空间（又称codebook）来完成整数序号到向量的转换。</p>
<p><img src="/2023/06/19/20230605-VQGAN/3.jpg" alt></p>
<p>为了让任意一个编码器输出向量都变成一个固定的嵌入向量，VQVAE采取了一种离散化策略：把每个输出向量$z_e(x)$替换成嵌入空间中最近的那个向量$z_q(x)$。$z_e(x)$的离散编码就是$z_q(x)$在嵌入空间的下标。这个过程和把254.9的输出颜色值离散化成255的整数颜色值的原理类似。</p>
<p><img src="/2023/06/19/20230605-VQGAN/4.jpg" alt></p>
<p>VQVAE的损失函数由两部分组成：重建误差和嵌入空间误差。</p>
<script type="math/tex; mode=display">
L_{VQ} = L_{reconstruct} + L_{embedding}</script><p>其中，重建误差就是输入和输出之间的均方误差。</p>
<script type="math/tex; mode=display">
L_{reconstruct} = ||x - \hat{x}||_2^2</script><p>嵌入空间误差为解码器输出向量$z_e(x)$和它在嵌入空间对应向量$z_q(x)$的均方误差。</p>
<script type="math/tex; mode=display">
L_{embedding} = ||z_e(x) - z_q(x)||_2^2</script><p>作者在误差中还使用了一种「停止梯度」的技巧。这个技巧在VQGAN中被完全保留，此处就不过多介绍了。</p>
<h2 id="图像压缩模型-VQGAN"><a href="#图像压缩模型-VQGAN" class="headerlink" title="图像压缩模型 VQGAN"></a>图像压缩模型 VQGAN</h2><p>回顾了VQVAE的背景知识后，我们来正式认识VQGAN的几个创新点。第一点，图像压缩模型VQVAE被改进成了VQGAN。</p>
<p>一般VAE重建出来出来的图像都会比较模糊。这是因为VAE只使用了均方误差，而均方误差只能保证像素值尽可能接近，却不能保证图像的感知效果更加接近。为此，作者把GAN的一些方法引入VQVAE，改造出了VQGAN。</p>
<p>具体来说，VQGAN有两项改进。第一，作者用感知误差(perceptual loss)代替原来的均方误差作为VQGAN的重建误差。第二，作者引入了GAN的对抗训练机制，加入了一个基于图块的判别器，把GAN误差加入了总误差。</p>
<blockquote>
<p>计算感知误差的方法如下：把两幅图像分别输入VGG，取出中间某几层卷积层的特征，计算特征图像之间的均方误差。如果你之前没学过相关知识，请搜索”perceptual loss”。</p>
<p>基于图块的判别器，即判别器不为整幅图输出一个真或假的判断结果，而是把图像拆成若干图块，分别输出每个图块的判断结果，再对所有图块的判断结果取一个均值。这只是GAN的一种改进策略而已，没有对GAN本身做太大的改动。如果你之前没学过相关知识，请搜索”PatchGAN”。</p>
</blockquote>
<p>这样，总的误差可以写成：</p>
<script type="math/tex; mode=display">
L = L_{VQ} + \lambda L_{GAN}</script><p>其中，$\lambda$是控制两种误差比例的权重。作者在论文中使用了一个公式来自适应地设置$\lambda$。和普通的GAN一样，VQGAN的编码器、解码器（即生成器）、codebook会最小化误差，判别器会最大化误差。</p>
<p>用VQGAN代替VQVAE后，重建图片中的模糊纹理清晰了很多。</p>
<p><img src="/2023/06/19/20230605-VQGAN/5.jpg" alt></p>
<p>有了一个保真度高的图像压缩模型，我们可以进入下一步，训练一个生成压缩图像的模型。</p>
<h2 id="基于-Transformer-的压缩图像生成模型"><a href="#基于-Transformer-的压缩图像生成模型" class="headerlink" title="基于 Transformer 的压缩图像生成模型"></a>基于 Transformer 的压缩图像生成模型</h2><p>如前所述，经VQGAN得到的压缩图像与真实图像有一个本质性的不同：真实图像的像素值具有连续性，相邻的颜色更加相似，而压缩图像的像素值则没有这种连续性。压缩图像的这一特性让寻找一个压缩图像生成模型变得异常困难。多数强大的真实图像生成模型（比如GAN）都是输出一个连续的浮点颜色值，再做一个浮点转整数的操作，得到最终的像素值。而对于压缩图像来说，这种输出连续颜色的模型都不适用了。因此，之前的VQVAE使用了一个能建模离散颜色的PixelCNN模型作为压缩图像生成模型。但PixelCNN的表现不够优秀。</p>
<p>恰好，功能强大的Transformer天生就支持建模离散的输出。在NLP中，每个单词都可以用一个离散的数字表示。Transformer会不断生成表示单词的数字，以达到生成句子的效果。</p>
<p><img src="/2023/06/19/20230605-VQGAN/6.jpg" alt="Transformer 随机生成句子的过程"></p>
<p>为了让Transformer生成图像，我们可以把生成句子的一个个单词，变成生成压缩图像的一个个像素。但是，要让Transformer生成二维图像，还需要克服一个问题：在生成句子时，Transformer会先生成第一个单词，再根据第一个单词生成第二个单词，再根据第一、第二个单词生成第三个单词……。也就是说，Transformer每次会根据<strong>之前</strong>所有的单词来生成下一单词。而图像是二维数据，没有先后的概念，怎样让像素和文字一样有先后顺序呢？</p>
<p>VQGAN的作者使用了自回归图像生成模型的常用做法，给图像的每个像素从左到右，从上到下规定一个顺序。有了先后顺序后，图像就可以被视为一个一维句子，可以用Transfomer生成句子的方式来生成图像了。在第$i$步，Transformer会根据前$i - 1$个像素$s_{ &lt; i}$生成第$i$个像素$s_i$，</p>
<p><img src="/2023/06/19/20230605-VQGAN/7.jpg" alt></p>
<h2 id="带约束的图像生成"><a href="#带约束的图像生成" class="headerlink" title="带约束的图像生成"></a>带约束的图像生成</h2><p>在生成新图像时，我们更希望模型能够根据我们的需求生成图像。比如，我们希望模型生成「一幅优美的风景画」，又或者希望模型在一幅草图的基础上作画。这些需求就是模型的约束。为了实现带约束的图像生成，一般的做法是先有一个无约束（输入是随机数）的图像生成模型，再在这个模型的基础上把一个表示约束的向量插入进图像生成的某一步。</p>
<p>把约束向量插入进模型的方法是需要设计的，插入约束向量的方法往往和模型架构有着密切关系。比如假设一个生成模型是U-Net架构，我们可以把约束向量和当前特征图拼接在一起，输入进U-Net的每一大层。</p>
<p>为了实现带约束的图像生成，VQGAN的作者再次借鉴了Transformer实现带约束文字生成的方法。许多自然语言处理任务都可以看成是带约束的文字生成。比如机器翻译，其实可以看成在给定一种语言的句子的前提下，让模型「随机」生成一个另一种语言的句子。比如要把「简要访问非洲」翻译成英语，我们可以对之前无约束文字生成的Transformer做一些修改。</p>
<p><img src="/2023/06/19/20230605-VQGAN/8.jpg" alt></p>
<p>也就是说，给定约束的句子$c$，在第$i$步，Transformer会根据前$i-1$个输出单词$s_{ &lt; i}$以及$c$生成第$i$个单词$s_i$。表示约束的单词被添加到了所有输出之前，作为这次「随机生成」的额外输入。</p>
<blockquote>
<p>上述方法并不是唯一的文字生成方法。这种文字生成方法被称为”decoder-only”。实际上，也有使用一个编码器来额外维护约束信息的文字生成方法。最早的Transformer就用到了带编码器的方法。</p>
</blockquote>
<p>我们同样可以把这种思想搬到压缩图像生成里。比如对于MNIST数据集，我们希望模型只生成0~9这些数字中某一个数字的手写图像。也就是说，约束是类别信息，约束的取值是0~9。我们就可以把这个0~9的约束信息添加到Transformer的输入$s_{ &lt; i}$之前，以实现由类别约束的图像生成。</p>
<p><img src="/2023/06/19/20230605-VQGAN/9.jpg" alt></p>
<p>但这种设计又会产生一个新的问题。假设约束条件不能简单地表示成整数，而是一些其他类型的数据，比如语义分割图像，那该怎么办呢？对于这种以图像形式表示的约束，作者的做法是，再训练另一个VQGAN，把约束图像压缩成另一套压缩图片。这一套压缩图片和生成图像的压缩图片有着不同的codebook，就像两种语言有着不同的单词一样。这样，约束图像也变成了一系列的整数，可以用之前的方法进行带约束图像生成了。</p>
<p><img src="/2023/06/19/20230605-VQGAN/10.jpg" alt></p>
<h2 id="生成高清图像"><a href="#生成高清图像" class="headerlink" title="生成高清图像"></a>生成高清图像</h2><p>由于Transformer注意力计算的开销很大，作者在所有配置中都只使用了$16 \times 16$的压缩图像，再增大压缩图像尺寸的话计算资源就不够了。而另一方面，每张图像在VQGAN中的压缩比例是有限的。如果图像压缩得过多，则VQGAN的重建质量就不够好了。因此，设边长压缩了$f$倍，则该方法一次能生成的图片的最大尺寸是$16f \times 16f$。在多项实验中，$f=16$的表现都较好。这样算下来，该方法一次只能生成$256 \times 256$的图片。这种尺寸的图片还称不上高清图片。</p>
<p>为了生成更大尺寸的图片，作者先训练好了一套能生成$256 \times 256$的图片的VQGAN+Transformer，再用了一种基于滑动窗口的采样机制来生成大图片。具体来说，作者把待生成图片划分成若干个$16\times16$像素的图块，每个图块对应压缩图像的一个像素。之后，在每一轮生成时，只有待生成图块周围的$16\times16$个图块（$256\times256$个像素）会被输入进VQGAN和Transformer，由Transformer生成一个新的压缩图像像素，再把该压缩图像像素解码成图块。(在下面的示意图中，每个方块是一个图块，transformer的输入是$3\times3$个图块)</p>
<p><img src="/2023/06/19/20230605-VQGAN/11.jpg" alt></p>
<p>这个滑动窗口算法不是那么好理解，需要多想一下才能理解它的具体做法。在理解这个算法时，你可能会有这样的问题：上面的示意图中，待生成像素有的时候在最左边，有的时候在中间，有的时候在右边，每次约束它的像素都不一样。这么复杂的约束逻辑怎么编写？其实，Transformer自动保证了每个像素只会由之前的像素约束，而看不到后面的像素。因此，在实现时，只需要把待生成像素框起来，直接用Transformer预测待生成像素即可，不需要编写额外的约束逻辑。</p>
<blockquote>
<p>如果你没有学过Transformer的话，理解这部分会有点困难。Transformer可以根据第1~k-1个像素并行地生成第2~k个像素，且保证生成每个像素时不会偷看到后面像素的信息。因此，假设我们要生成第i个像素，其实是预测了所有第2~k个像素的结果，再取出第i个结果，填回待生成图像。</p>
</blockquote>
<p>由于论文篇幅有限，作者没有对滑动窗口机制做过多的介绍，也没有讲带约束的滑动窗口是怎么实现的。如果你在理解这一部分时碰到了问题，不用担心，这很正常。稍后我们会在代码阅读章节彻底理解滑动窗口的实现方法。我也是看了代码才看懂此处的做法。</p>
<p>作者在论文中解释了为什么用滑动窗口生成高清图像是合理的。作者先是讨论了两种情况，只要满足这两种情况中的任意一种，拿滑动窗口生成图像就是合理的。第一种情况是数据集的统计规律是几乎空间不变，也就是说训练集图片每$256\times256$个像素的统计规律是类似的。这和我们拿$3\times3$卷积卷图像是因为图像每$3\times3$个像素的统计规律类似的原理是一样的。第二种情况是有空间上的约束信息。比如之前提到的用语义分割图来指导图像生成。由于语义分割也是一张图片，它给每个待生成像素都提供了额外信息。这样，哪怕是用滑动窗口，在局部语义的指导下，模型也足以生成图像了。</p>
<p>若是两种情况都不满足呢？比如在对齐的人脸数据集上做无约束生成。在对齐的人脸数据集里，每张图片中人的五官所在的坐标是差不多的，图片的空间不变性不满足；做无约束生成，自然也没有额外的空间信息。在这种情况下，我们可以人为地添加一个坐标约束，即从左到右、从上到下地给每个像素标一个序号，把每个滑动窗口里的坐标序号做为约束。有了坐标约束后，就还原成了上面的第二种情况，每个像素有了额外的空间信息，基于滑动窗口的方法依然可行。</p>
<p><img src="/2023/06/19/20230605-VQGAN/12.jpg" alt></p>
<p>学完了论文的四大知识点，我们知道VQGAN是怎么根据约束生成高清图像的了。接下来，我们来看看论文的实验部分，看看作者是怎么证明方法的有效性的。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在实验部分，作者先是分别验证了基于Transformer的压缩图像生成模型较以往模型的优越性（4.1节）、VQGAN较以往模型的优越性（4.4节末尾）、使用VQGAN做图像压缩的必要性及相关消融实验（4.3节），再把整个生成方法综合起来，在多项图像生成任务上与以往的图像生成模型做定量对比（4.4节），最后展示了该方法惊艳的带约束生成效果（4.2节）。</p>
<p>在论文4.1节中，作者验证了基于Transformer的压缩图像生成模型的有效性。之前，压缩图像都是使用能输出离散分布的PixelCNN系列模型来生成的。PixelCNN系列的最强模型是PixelSNAIL。为确保公平，作者对比了相同训练时间、相同训练步数下两个网络在不同训练集下的负对数似然（NLL）指标。结果表明，基于Transformer的模型确实训练得更快。</p>
<blockquote>
<p>对于直接能建模离散分布的模型来说，NLL就是交叉熵损失函数。</p>
</blockquote>
<p><img src="/2023/06/19/20230605-VQGAN/13.jpg" alt></p>
<p>在论文4.4节末尾，作者将VQGAN和之前的图像压缩模型对比，验证了引入感知误差和GAN结构的有效性。作者汇报了各模型重建图像集与原数据集（ImageNet的训练集和验证集）的FID（指标FID是越低越好）。同时，结果也说明，增大codebook的尺寸或者编码种类都能提升重建效果。</p>
<p><img src="/2023/06/19/20230605-VQGAN/19.jpg" alt></p>
<p>在论文4.3节中，作者验证了使用VQGAN的必要性。作者训了两个模型，一个直接让Transformer做真实图像生成，一个用VQGAN把图像边长压缩2倍，再用Transformer生成压缩图像。经比较，使用了VQGAN后，图像生成速度快了10多倍，且图像生成效果也有所提升。</p>
<p>另外，作者还做了有关图像边长压缩比例$f$的消融实验。作者固定让Transformer生成$16 \times 16$的压缩图片，即每次训练时用到的图像尺寸都是$16f \times 16f$。之后，作者训练训练了不同$f$下的模型，用各个模型来生成图片。结果显示$f=16$时效果最好。这是因为，在固定Transformer的生成分辨率的前提下，$f$越小，Transformer的感受野越小。如果Transformer的感受野过小，就学习不到足够的信息。</p>
<p><img src="/2023/06/19/20230605-VQGAN/16.jpg" alt></p>
<p>在论文4.4节中，作者探究了VQGAN+Transformer在多项基准测试（benchmark）上的结果。</p>
<p>首先是语义图像合成（根据语义分割图像来生成）任务。本文的这套方法还不错。</p>
<p><img src="/2023/06/19/20230605-VQGAN/17.jpg" alt></p>
<p>接着是人脸生成任务。这套方法表现还行，但还是比不过专精于某一任务的GAN。</p>
<p><img src="/2023/06/19/20230605-VQGAN/18.jpg" alt></p>
<p>作者还比较了各模型在ImageNet上的生成结果。这一比较的数据量较多，欢迎大家自行阅读原论文。</p>
<p>在论文4.2节中，作者展示了多才多艺的VQGAN+Transformer在各种约束下的图像生成结果。这些图像都是按照默认配置生成的，大小为$256\times256$。</p>
<p><img src="/2023/06/19/20230605-VQGAN/14.jpg" alt></p>
<p>作者还展示了使用了滑动窗口算法后，模型生成的不同分辨率的图像。</p>
<p><img src="/2023/06/19/20230605-VQGAN/15.jpg" alt></p>
<p>本文开头的那张高清图片也来自论文。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>VQGAN是一个改进版的VQVAE，它将感知误差和GAN引入了图像压缩模型，把压缩图像生成模型替换成了更强大的Transformer。相比纯种的GAN（如StyleGAN），VQGAN的强大之处在于它支持带约束的高清图像生成。VQGAN借助NLP中”decoder-only”策略实现了带约束图像生成，并使用滑动窗口机制实现了高清图像生成。虽然在某些特定任务上VQGAN还是落后于其他GAN，但VQGAN的泛化性和灵活性都要比纯种GAN要强。它的这些潜力直接促成了Stable Diffusion的诞生。</p>
<p>如果你是读完了VQVAE再来读的VQGAN，为了完全理解VQGAN，你只需要掌握本文提到的4个知识点：VQVAE到VQGAN的改进方法、使用Transformer做图像生成的方法、使用”decoder-only”策略做带约束图像生成的方法、用滑动滑动窗口生成任意尺寸的图片的思想。</p>
<h2 id="代码阅读"><a href="#代码阅读" class="headerlink" title="代码阅读"></a>代码阅读</h2><p>在代码阅读章节中，我将先简略介绍官方源码的项目结构以方便大家学习，再介绍代码中的几处核心代码。具体来说，我会介绍模型是如何组织配置文件的、模型的定义代码在哪、训练代码在哪、采样代码在哪，同时我会主要分析VQGAN的结构、Transformer的结构、损失函数、滑动窗口采样算法这几部分的代码。</p>
<p>官方源码地址：<a target="_blank" rel="noopener" href="https://github.com/CompVis/taming-transformers。">https://github.com/CompVis/taming-transformers。</a></p>
<p>官方的Git仓库里有很多很大的图片，且git记录里还藏了一些很大的数据，整个Git仓库非常大。如果你的网络不好，建议以zip形式下载仓库，或者只把代码部分下载下来。</p>
<h2 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">├─assets</span><br><span class="line">├─configs</span><br><span class="line">├─scripts</span><br><span class="line">└─taming</span><br><span class="line">    ├─data</span><br><span class="line">    │  └─conditional_builder</span><br><span class="line">    ├─models</span><br><span class="line">    └─modules</span><br><span class="line">        ├─diffusionmodules</span><br><span class="line">        ├─discriminator</span><br><span class="line">        ├─losses</span><br><span class="line">        ├─misc</span><br><span class="line">        ├─transformer</span><br><span class="line">        └─vqvae</span><br></pre></td></tr></table></figure>
<p><code>configs</code>目录下存放的是模型配置文件。VQGAN和Transformer的模型配置是分开来放的。每个模型配置文件都会指向一个Python模型类，比如<code>taming.models.vqgan.VQModel</code>，配置里的参数就是模型类的初始化参数。我们可用通过阅读配置文件找到模型的定义位置。</p>
<p>运行脚本包括根目录下的<code>main.py</code>和<code>scripts</code>文件夹下的脚本。<code>main.py</code>是用于训练的。<code>scripts</code>文件夹下有各种采样脚本和数据集可视化脚本。</p>
<p><code>taming</code>是源代码的主目录。其<code>data</code>子文件夹下放置了各数据集的预处理代码，<code>models</code>放置了VQGAN和Transformer PyTorch模型的定义代码，<code>modules</code>则放置了模型中用到的模块，主要包括VQGAN编码解码模块（<code>diffusionmodules</code>）、判别器模块（<code>discriminator</code>）、误差模块（<code>losses</code>）、Transformer模块（<code>transformer</code>）、codebook模块（<code>vqvae</code>）。</p>
<h2 id="VQGAN-模型结构"><a href="#VQGAN-模型结构" class="headerlink" title="VQGAN 模型结构"></a>VQGAN 模型结构</h2><p>打开<code>configs\faceshq_vqgan.yaml</code>，我们能够找到高清人脸生成任务使用的VQGAN模型配置。我们来学习一下这个模型的定义方法。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">base_learning_rate:</span> <span class="number">4.5e-6</span></span><br><span class="line">  <span class="attr">target:</span> <span class="string">taming.models.vqgan.VQModel</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">embed_dim:</span> <span class="number">256</span></span><br><span class="line">    <span class="attr">n_embed:</span> <span class="number">1024</span></span><br><span class="line">    <span class="attr">ddconfig:</span></span><br><span class="line">      <span class="string">...</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">lossconfig:</span></span><br><span class="line">      <span class="attr">target:</span> <span class="string">taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator</span></span><br><span class="line">      <span class="attr">params:</span></span><br><span class="line">        <span class="string">...</span></span><br></pre></td></tr></table></figure>
<p>从配置文件的<code>target</code>字段中，我们知道VQGAN定义在模块<code>taming.models.vqgan.VQModel</code>中。我们可以打开<code>taming\models\vqgan.py</code>这个文件，查看其中<code>VQModel</code>类的代码。</p>
<p>首先先看一下初始化函数。初始化函数主要是初始化了<code>encoder</code>、<code>decoder</code>、<code>loss</code>、<code>quantize</code>这几个模块，我们可以从文件开头的import语句中找到这几个模块的定义位置。不过，先不急，我们来继续看一下模型的前向传播函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> taming.modules.diffusionmodules.model <span class="keyword">import</span> Encoder, Decoder</span><br><span class="line"><span class="keyword">from</span> taming.modules.vqvae.quantize <span class="keyword">import</span> VectorQuantizer2 <span class="keyword">as</span> VectorQuantizer</span><br><span class="line"><span class="keyword">from</span> taming.modules.vqvae.quantize <span class="keyword">import</span> GumbelQuantize</span><br><span class="line"><span class="keyword">from</span> taming.modules.vqvae.quantize <span class="keyword">import</span> EMAVectorQuantizer</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VQModel</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ddconfig,</span></span></span><br><span class="line"><span class="params"><span class="function">                 lossconfig,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_embed,</span></span></span><br><span class="line"><span class="params"><span class="function">                 embed_dim,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ckpt_path=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ignore_keys=[],</span></span></span><br><span class="line"><span class="params"><span class="function">                 image_key=<span class="string">&quot;image&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 colorize_nlabels=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 monitor=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 remap=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 sane_index_shape=<span class="literal">False</span>,  <span class="comment"># tell vector quantizer to return indices as bhw</span></span></span></span><br><span class="line"><span class="params"><span class="function">                 </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.image_key = image_key</span><br><span class="line">        self.encoder = Encoder(**ddconfig)</span><br><span class="line">        self.decoder = Decoder(**ddconfig)</span><br><span class="line">        self.loss = instantiate_from_config(lossconfig)</span><br><span class="line">        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=<span class="number">0.25</span>,</span><br><span class="line">                                        remap=remap, sane_index_shape=sane_index_shape)</span><br><span class="line">        self.quant_conv = torch.nn.Conv2d(ddconfig[<span class="string">&quot;z_channels&quot;</span>], embed_dim, <span class="number">1</span>)</span><br><span class="line">        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[<span class="string">&quot;z_channels&quot;</span>], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)</span><br><span class="line">        self.image_key = image_key</span><br><span class="line">        <span class="keyword">if</span> colorize_nlabels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">type</span>(colorize_nlabels)==<span class="built_in">int</span></span><br><span class="line">            self.register_buffer(<span class="string">&quot;colorize&quot;</span>, torch.randn(<span class="number">3</span>, colorize_nlabels, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> monitor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.monitor = monitor</span><br></pre></td></tr></table></figure>
<p>模型的前向传播逻辑非常清晰。<code>self.encoder</code>可以把一张图片变为特征，<code>self.decoder</code>可以把特征变回图片。<code>self.quant_conv</code>和<code>post_quant_conv</code>则分别完成了编码器到codebook、codebook到解码器的通道数转换。<code>self.quantize</code>实现了VQVAE和VQGAN中那个找codebook里的最近邻、替换成最近邻的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    h = self.encoder(x)</span><br><span class="line">    h = self.quant_conv(h)</span><br><span class="line">    quant, emb_loss, info = self.quantize(h)</span><br><span class="line">    <span class="keyword">return</span> quant, emb_loss, info</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, quant</span>):</span></span><br><span class="line">    quant = self.post_quant_conv(quant)</span><br><span class="line">    dec = self.decoder(quant)</span><br><span class="line">    <span class="keyword">return</span> dec</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">    quant, diff, _ = self.encode(<span class="built_in">input</span>)</span><br><span class="line">    dec = self.decode(quant)</span><br><span class="line">    <span class="keyword">return</span> dec, diff</span><br></pre></td></tr></table></figure>
<p>接下来，我们再看一看VQGAN的各个模块的定义。编码器和解码器的定义都可以在<code>taming\modules\diffusionmodules\model.py</code>里找到。VQGAN使用的编码器和解码器基于DDPM论文中的U-Net架构（而此架构又可以追溯到PixelCNN++的模型架构）。相比于最经典的U-Net，此U-Net每一层由若干个残差块和若干个自注意力块构成。为了把这个U-Net用到VQGAN里，U-Net的下采样部分和上采样部分被拆开，分别做成了VQGAN的编码器和解码器。</p>
<p>此处代码过长，我就只贴出部分关键代码了。以下是编码器的<code>__init__</code>函数和<code>forward</code>函数的关键代码。<code>self.down</code>存储了U-Net各层的模块。对于第i层，<code>down[i].block</code>是所有残差块，<code>down[i].attn</code>是所有自注意力块，<code>down[i].downsample</code>是下采样操作。它们在<code>forward</code>里会被依次调用。解码器的结构与之类似，只不过下采样变成了上采样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *, ch, out_ch, ch_mult=(<span class="params"><span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">8</span></span>), num_res_blocks,</span></span></span><br><span class="line"><span class="params"><span class="function">                 attn_resolutions, dropout=<span class="number">0.0</span>, resamp_with_conv=<span class="literal">True</span>, in_channels,</span></span></span><br><span class="line"><span class="params"><span class="function">                 resolution, z_channels, double_z=<span class="literal">True</span>, **ignore_kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line">        self.down = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(self.num_resolutions):</span><br><span class="line">            block = nn.ModuleList()</span><br><span class="line">            attn = nn.ModuleList()</span><br><span class="line">            block_in = ch*in_ch_mult[i_level]</span><br><span class="line">            block_out = ch*ch_mult[i_level]</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(self.num_res_blocks):</span><br><span class="line">                block.append(ResnetBlock(in_channels=block_in,</span><br><span class="line">                                         out_channels=block_out,</span><br><span class="line">                                         temb_channels=self.temb_ch,</span><br><span class="line">                                         dropout=dropout))</span><br><span class="line">                block_in = block_out</span><br><span class="line">                <span class="keyword">if</span> curr_res <span class="keyword">in</span> attn_resolutions:</span><br><span class="line">                    attn.append(AttnBlock(block_in))</span><br><span class="line">            down = nn.Module()</span><br><span class="line">            down.block = block</span><br><span class="line">            down.attn = attn</span><br><span class="line">            <span class="keyword">if</span> i_level != self.num_resolutions-<span class="number">1</span>:</span><br><span class="line">                down.downsample = Downsample(block_in, resamp_with_conv)</span><br><span class="line">                curr_res = curr_res // <span class="number">2</span></span><br><span class="line">            self.down.append(down)</span><br><span class="line"></span><br><span class="line">       ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        hs = [self.conv_in(x)]</span><br><span class="line">        <span class="keyword">for</span> i_level <span class="keyword">in</span> <span class="built_in">range</span>(self.num_resolutions):</span><br><span class="line">            <span class="keyword">for</span> i_block <span class="keyword">in</span> <span class="built_in">range</span>(self.num_res_blocks):</span><br><span class="line">                h = self.down[i_level].block[i_block](hs[-<span class="number">1</span>], temb)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(self.down[i_level].attn) &gt; <span class="number">0</span>:</span><br><span class="line">                    h = self.down[i_level].attn[i_block](h)</span><br><span class="line">                hs.append(h)</span><br><span class="line">            <span class="keyword">if</span> i_level != self.num_resolutions-<span class="number">1</span>:</span><br><span class="line">                hs.append(self.down[i_level].downsample(hs[-<span class="number">1</span>]))</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> h</span><br></pre></td></tr></table></figure>
<p>之后，我们再看看离散化层的代码，即把编码器的输出变成codebook里的嵌入的实现代码。作者在<code>taming\modules\vqvae\quantize.py</code>中提供了VQVAE原版的离散化操作以及若干个改进过的离散化操作。我们就来看一下原版的离散化模块<code>VectorQuantizer</code>是怎么实现的。</p>
<p>离散化模块的初始化非常简洁，主要是初始化了一个嵌入层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VectorQuantizer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_e, e_dim, beta</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VectorQuantizer, self).__init__()</span><br><span class="line">        self.n_e = n_e</span><br><span class="line">        self.e_dim = e_dim</span><br><span class="line">        self.beta = beta</span><br><span class="line"></span><br><span class="line">        self.embedding = nn.Embedding(self.n_e, self.e_dim)</span><br><span class="line">        self.embedding.weight.data.uniform_(-<span class="number">1.0</span> / self.n_e, <span class="number">1.0</span> / self.n_e)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在前向传播时，作者先是算出了编码器输出<code>z</code>和所有嵌入的距离<code>d</code>，再用<code>argmin</code>算出了最近邻嵌入的下标<code>min_encodings</code>，最后根据下标取出解码器输入<code>z_q</code>。同时，该函数还计算了其他几个可能用到的量，比如和codebook有关的误差 <code>loss</code>。注意，在计算<code>loss</code>和<code>z_q</code>时，作者都使用到了停止梯度算子（<code>.detach()</code>）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, z</span>):</span></span><br><span class="line">    z = z.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">    z_flattened = z.view(-<span class="number">1</span>, self.e_dim)</span><br><span class="line">    <span class="comment"># distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z</span></span><br><span class="line"></span><br><span class="line">    d = torch.<span class="built_in">sum</span>(z_flattened ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + \</span><br><span class="line">        torch.<span class="built_in">sum</span>(self.embedding.weight**<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">2</span> * \</span><br><span class="line">        torch.matmul(z_flattened, self.embedding.weight.t())</span><br><span class="line"></span><br><span class="line">    <span class="comment">## could possible replace this here</span></span><br><span class="line">    <span class="comment"># #\start...</span></span><br><span class="line">    <span class="comment"># find closest encodings</span></span><br><span class="line">    min_encoding_indices = torch.argmin(d, dim=<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    min_encodings = torch.zeros(</span><br><span class="line">        min_encoding_indices.shape[<span class="number">0</span>], self.n_e).to(z)</span><br><span class="line">    min_encodings.scatter_(<span class="number">1</span>, min_encoding_indices, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)</span><br><span class="line">    <span class="comment">#.........\end</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute loss for embedding</span></span><br><span class="line">    loss = torch.mean((z_q.detach()-z)**<span class="number">2</span>) + self.beta * \</span><br><span class="line">        torch.mean((z_q - z.detach()) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># preserve gradients</span></span><br><span class="line">    z_q = z + (z_q - z).detach()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perplexity</span></span><br><span class="line">    e_mean = torch.mean(min_encodings, dim=<span class="number">0</span>)</span><br><span class="line">    perplexity = torch.exp(-torch.<span class="built_in">sum</span>(e_mean * torch.log(e_mean + <span class="number">1e-10</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape back to match original input shape</span></span><br><span class="line">    z_q = z_q.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> z_q, loss, (perplexity, min_encodings, min_encoding_indices)</span><br></pre></td></tr></table></figure>
<p>VQGAN的三个主要模块已经看完了。最后，我们来看一下误差的定义。误差的定义在<code>taming\modules\losses\vqperceptual.py</code>的<code>VQLPIPSWithDiscriminator</code>类里。误差类名里的LPIPS（Learned Perceptual Image Patch Similarity，学习感知图像块相似度）就是感知误差的全称，”WithDiscriminator”表示误差是带了判定器误差的。我们来把这两类误差分别看一下。</p>
<p>说实话，这个误差模块乱得一塌糊涂，一边自己在算误差，一边又维护了codebook误差和重建误差的权重，最后会把自己维护的两个误差和其他误差合在一起输出。功能全部耦合在一起。我们就跳过这个类的实现细节，主要关注<code>self.perceptual_loss</code>和<code>self.discriminator</code>是怎么调用其他模块的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> taming.modules.losses.lpips <span class="keyword">import</span> LPIPS</span><br><span class="line"><span class="keyword">from</span> taming.modules.discriminator.model <span class="keyword">import</span> NLayerDiscriminator, weights_init</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VQLPIPSWithDiscriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.perceptual_loss = LPIPS().<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">        self.discriminator = NLayerDiscriminator...</span><br></pre></td></tr></table></figure>
<p>感知误差模块在<code>taming\modules\losses\vqperceptual.py</code>文件里。这个文件来自GitHub项目 PerceptualSimilarity。</p>
<p>感知误差可以简单地理解为两张图片在VGG中几个卷积层输出的误差的加权和。加权的权重是可以学习的。作者使用的是已经学习好的感知误差。感知误差的初始化函数如下。其中，<code>self.lin0</code>等模块就是算权重的模块，<code>self.net</code>是VGG。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LPIPS</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># Learned perceptual metric</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, use_dropout=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.scaling_layer = ScalingLayer()</span><br><span class="line">        self.chns = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>, <span class="number">512</span>]  <span class="comment"># vg16 features</span></span><br><span class="line">        self.net = vgg16(pretrained=<span class="literal">True</span>, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.lin0 = NetLinLayer(self.chns[<span class="number">0</span>], use_dropout=use_dropout)</span><br><span class="line">        self.lin1 = NetLinLayer(self.chns[<span class="number">1</span>], use_dropout=use_dropout)</span><br><span class="line">        self.lin2 = NetLinLayer(self.chns[<span class="number">2</span>], use_dropout=use_dropout)</span><br><span class="line">        self.lin3 = NetLinLayer(self.chns[<span class="number">3</span>], use_dropout=use_dropout)</span><br><span class="line">        self.lin4 = NetLinLayer(self.chns[<span class="number">4</span>], use_dropout=use_dropout)</span><br><span class="line">        self.load_from_pretrained()</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.parameters():</span><br><span class="line">            param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>在算误差时，先是把图像<code>input</code>和<code>target</code>都输入进VGG，获取各层输出<code>outs0, outs1</code>，再求出两个图像的输出的均方误差<code>diffs</code>，最后用<code>lins</code>给各层误差加权，求和。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, target</span>):</span></span><br><span class="line">    in0_input, in1_input = (self.scaling_layer(<span class="built_in">input</span>), self.scaling_layer(target))</span><br><span class="line">    outs0, outs1 = self.net(in0_input), self.net(in1_input)</span><br><span class="line">    feats0, feats1, diffs = &#123;&#125;, &#123;&#125;, &#123;&#125;</span><br><span class="line">    lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]</span><br><span class="line">    <span class="keyword">for</span> kk <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.chns)):</span><br><span class="line">        feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])</span><br><span class="line">        diffs[kk] = (feats0[kk] - feats1[kk]) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    res = [spatial_average(lins[kk].model(diffs[kk]), keepdim=<span class="literal">True</span>) <span class="keyword">for</span> kk <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.chns))]</span><br><span class="line">    val = res[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(self.chns)):</span><br><span class="line">        val += res[l]</span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure>
<p>GAN的判别器写在<code>taming\modules\discriminator\model.py</code>文件里。这个文件来自GitHub上的 pytorch-CycleGAN-and-pix2pix 项目。这个判别器非常简单，就是一个全卷积网络。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLayerDiscriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Defines a PatchGAN discriminator as in Pix2Pix</span></span><br><span class="line"><span class="string">        --&gt; see https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_nc=<span class="number">3</span>, ndf=<span class="number">64</span>, n_layers=<span class="number">3</span>, use_actnorm=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Construct a PatchGAN discriminator</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            input_nc (int)  -- the number of channels in input images</span></span><br><span class="line"><span class="string">            ndf (int)       -- the number of filters in the last conv layer</span></span><br><span class="line"><span class="string">            n_layers (int)  -- the number of conv layers in the discriminator</span></span><br><span class="line"><span class="string">            norm_layer      -- normalization layer</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(NLayerDiscriminator, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> use_actnorm:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            norm_layer = ActNorm</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(norm_layer) == functools.partial:  <span class="comment"># no need to use bias as BatchNorm2d has affine parameters</span></span><br><span class="line">            use_bias = norm_layer.func != nn.BatchNorm2d</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            use_bias = norm_layer != nn.BatchNorm2d</span><br><span class="line"></span><br><span class="line">        kw = <span class="number">4</span></span><br><span class="line">        padw = <span class="number">1</span></span><br><span class="line">        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=<span class="number">2</span>, padding=padw), nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>)]</span><br><span class="line">        nf_mult = <span class="number">1</span></span><br><span class="line">        nf_mult_prev = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_layers):  <span class="comment"># gradually increase the number of filters</span></span><br><span class="line">            nf_mult_prev = nf_mult</span><br><span class="line">            nf_mult = <span class="built_in">min</span>(<span class="number">2</span> ** n, <span class="number">8</span>)</span><br><span class="line">            sequence += [</span><br><span class="line">                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=<span class="number">2</span>, padding=padw, bias=use_bias),</span><br><span class="line">                norm_layer(ndf * nf_mult),</span><br><span class="line">                nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>)</span><br><span class="line">            ]</span><br><span class="line"></span><br><span class="line">        nf_mult_prev = nf_mult</span><br><span class="line">        nf_mult = <span class="built_in">min</span>(<span class="number">2</span> ** n_layers, <span class="number">8</span>)</span><br><span class="line">        sequence += [</span><br><span class="line">            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=<span class="number">1</span>, padding=padw, bias=use_bias),</span><br><span class="line">            norm_layer(ndf * nf_mult),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="literal">True</span>)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        sequence += [</span><br><span class="line">            nn.Conv2d(ndf * nf_mult, <span class="number">1</span>, kernel_size=kw, stride=<span class="number">1</span>, padding=padw)]  <span class="comment"># output 1 channel prediction map</span></span><br><span class="line">        self.main = nn.Sequential(*sequence)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Standard forward.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.main(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Transformer-模型结构"><a href="#Transformer-模型结构" class="headerlink" title="Transformer 模型结构"></a>Transformer 模型结构</h2><p>此方法使用的Transformer是GPT2。我们先看一下该项目封装Transformer的模型类<code>taming.models.cond_transformer.Net2NetTransformer</code>，再稍微看一下GPT类<code>taming.modules.transformer.mingpt.GPT</code>的具体实现。</p>
<p><code>Net2NetTransformer</code>主要是实现了论文中提到的带约束生成。它会把输入<code>x</code>和约束<code>c</code>分别用一个VQGAN转成压缩图像，把图像压扁成一维，再调用GPT。我们来看一下这个类的主要内容。</p>
<p>初始化函数主要是初始化了输入图像的VQGAN <code>self.first_stage_model</code>、约束图像的VQGAN <code>self.cond_stage_model</code>、Transformer <code>self.transformer</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net2NetTransformer</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 transformer_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cond_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 permuter_config=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ckpt_path=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ignore_keys=[],</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_stage_key=<span class="string">&quot;image&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cond_stage_key=<span class="string">&quot;depth&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 downsample_cond_size=-<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pkeep=<span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 sos_token=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 unconditional=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.be_unconditional = unconditional</span><br><span class="line">        self.sos_token = sos_token</span><br><span class="line">        self.first_stage_key = first_stage_key</span><br><span class="line">        self.cond_stage_key = cond_stage_key</span><br><span class="line">        self.init_first_stage_from_ckpt(first_stage_config)</span><br><span class="line">        self.init_cond_stage_from_ckpt(cond_stage_config)</span><br><span class="line">        <span class="keyword">if</span> permuter_config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            permuter_config = &#123;<span class="string">&quot;target&quot;</span>: <span class="string">&quot;taming.modules.transformer.permuter.Identity&quot;</span>&#125;</span><br><span class="line">        self.permuter = instantiate_from_config(config=permuter_config)</span><br><span class="line">        self.transformer = instantiate_from_config(config=transformer_config)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)</span><br><span class="line">        self.downsample_cond_size = downsample_cond_size</span><br><span class="line">        self.pkeep = pkeep</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_first_stage_from_ckpt</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        model = instantiate_from_config(config)</span><br><span class="line">        model = model.<span class="built_in">eval</span>()</span><br><span class="line">        model.train = disabled_train</span><br><span class="line">        self.first_stage_model = model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_cond_stage_from_ckpt</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.cond_stage_model = ...</span><br></pre></td></tr></table></figure>
<p>模型的前向传播函数如下。一开始，函数调用<code>encode_to_z</code>和<code>encode_to_c</code>，根据<code>self.cond_stage_model</code>和<code>self.first_stage_model</code>把约束图像和输入图像编码成压扁至一维的压缩图像。之后函数做了一个类似Dropout的操作，根据<code>self.pkeep</code>随机替换掉约束编码。最后，函数把约束编码和输入编码拼接起来，使用通常方法调用Transformer。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, c</span>):</span></span><br><span class="line">    <span class="comment"># one step to produce the logits</span></span><br><span class="line">    _, z_indices = self.encode_to_z(x)</span><br><span class="line">    _, c_indices = self.encode_to_c(c)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.training <span class="keyword">and</span> self.pkeep &lt; <span class="number">1.0</span>:</span><br><span class="line">        mask = torch.bernoulli(self.pkeep*torch.ones(z_indices.shape,</span><br><span class="line">                                                      device=z_indices.device))</span><br><span class="line">        mask = mask.<span class="built_in">round</span>().to(dtype=torch.int64)</span><br><span class="line">        r_indices = torch.randint_like(z_indices, self.transformer.config.vocab_size)</span><br><span class="line">        a_indices = mask*z_indices+(<span class="number">1</span>-mask)*r_indices</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        a_indices = z_indices</span><br><span class="line"></span><br><span class="line">    cz_indices = torch.cat((c_indices, a_indices), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># target includes all sequence elements (no need to handle first one</span></span><br><span class="line">    <span class="comment"># differently because we are conditioning)</span></span><br><span class="line">    target = z_indices</span><br><span class="line">    <span class="comment"># make the prediction</span></span><br><span class="line">    logits, _ = self.transformer(cz_indices[:, :-<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># cut off conditioning outputs - output i corresponds to p(z_i | z_&#123;&lt;i&#125;, c)</span></span><br><span class="line">    logits = logits[:, c_indices.shape[<span class="number">1</span>]-<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logits, target</span><br></pre></td></tr></table></figure>
<p>GPT2的结构不是本文的重点，我们就快速把模型结构过一遍了。GPT2的模型定义在<code>taming.modules.transformer.mingpt.GPT</code>里。GPT2的结构并不复杂，就是一个只有解码器的Transformer。前向传播时，数据先通过嵌入层<code>self.tok_emb</code>，再经过若干个Transformer模块<code>self.blocks</code>，最后过一个LayerNorm层<code>self.ln_f</code>和线性层<code>self.head</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GPT</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, idx, embeddings=<span class="literal">None</span>, targets=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># forward the GPT model</span></span><br><span class="line">        token_embeddings = self.tok_emb(idx) <span class="comment"># each index maps to a (learnable) vector</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> embeddings <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># prepend explicit embeddings</span></span><br><span class="line">            token_embeddings = torch.cat((embeddings, token_embeddings), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        t = token_embeddings.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">assert</span> t &lt;= self.block_size, <span class="string">&quot;Cannot forward, model block size is exhausted.&quot;</span></span><br><span class="line">        position_embeddings = self.pos_emb[:, :t, :] <span class="comment"># each position maps to a (learnable) vector</span></span><br><span class="line">        x = self.drop(token_embeddings + position_embeddings)</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.ln_f(x)</span><br><span class="line">        logits = self.head(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if we are given some desired targets also calculate the loss</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = F.cross_entropy(logits.view(-<span class="number">1</span>, logits.size(-<span class="number">1</span>)), targets.view(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits, loss</span><br></pre></td></tr></table></figure>
<p>每个Transformer块就是非常经典的自注意力加全连接层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; an unassuming Transformer block &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ln1 = nn.LayerNorm(config.n_embd)</span><br><span class="line">        self.ln2 = nn.LayerNorm(config.n_embd)</span><br><span class="line">        self.attn = CausalSelfAttention(config)</span><br><span class="line">        self.mlp = nn.Sequential(</span><br><span class="line">            nn.Linear(config.n_embd, <span class="number">4</span> * config.n_embd),</span><br><span class="line">            nn.GELU(),  <span class="comment"># nice</span></span><br><span class="line">            nn.Linear(<span class="number">4</span> * config.n_embd, config.n_embd),</span><br><span class="line">            nn.Dropout(config.resid_pdrop),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, layer_past=<span class="literal">None</span>, return_present=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> check that training still works</span></span><br><span class="line">        <span class="keyword">if</span> return_present: <span class="keyword">assert</span> <span class="keyword">not</span> self.training</span><br><span class="line">        <span class="comment"># layer past: tuple of length two with B, nh, T, hs</span></span><br><span class="line">        attn, present = self.attn(self.ln1(x), layer_past=layer_past)</span><br><span class="line"></span><br><span class="line">        x = x + attn</span><br><span class="line">        x = x + self.mlp(self.ln2(x))</span><br><span class="line">        <span class="keyword">if</span> layer_past <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">or</span> return_present:</span><br><span class="line">            <span class="keyword">return</span> x, present</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="基于滑动窗口的带约束图像生成"><a href="#基于滑动窗口的带约束图像生成" class="headerlink" title="基于滑动窗口的带约束图像生成"></a>基于滑动窗口的带约束图像生成</h2><p>看完了所有模型的结构，我们最后来学习一下论文中没能详细介绍的滑动窗口算法。在<code>scripts\taming-transformers.ipynb</code>里有一个采样算法的最简实现，我们就来学习一下这份代码。</p>
<p>这份代码可以根据一幅语义分割图像来生成高清图像。一开始，代码会读入模型和语义分割图像。大致的代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> taming.models.cond_transformer <span class="keyword">import</span> Net2NetTransformer</span><br><span class="line">model = Net2NetTransformer(**config.model.params)</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">segmentation_path = <span class="string">&quot;data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png&quot;</span></span><br><span class="line">segmentation = Image.<span class="built_in">open</span>(segmentation_path)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/19/20230605-VQGAN/20.jpg" alt><br>之后，代码把约束图像用对应的VQGAN编码进压缩空间，得到<code>c_indices</code>。由于待生成图像为空，我们可以随便生成一个待生成图像的压缩图像<code>z_indices</code>，代码中使用了<code>randint</code>初始化待生成的压缩图像。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">c_code, c_indices = model.encode_to_c(segmentation)</span><br><span class="line">z_indices = torch.randint(codebook_size, z_indices_shape, device=model.device)</span><br><span class="line"></span><br><span class="line">idx = z_indices</span><br><span class="line">idx = idx.reshape(z_code_shape[<span class="number">0</span>],z_code_shape[<span class="number">2</span>],z_code_shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">cidx = c_indices</span><br><span class="line">cidx = cidx.reshape(c_code.shape[<span class="number">0</span>],c_code.shape[<span class="number">2</span>],c_code.shape[<span class="number">3</span>])</span><br></pre></td></tr></table></figure><br>最后就是最关键的滑动窗口采样部分了。我们先稍微浏览一遍代码，再详细地一行一行看过去。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">temperature = <span class="number">1.0</span></span><br><span class="line">top_k = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, z_code_shape[<span class="number">2</span>]-<span class="number">0</span>):</span><br><span class="line">  <span class="keyword">if</span> i &lt;= <span class="number">8</span>:</span><br><span class="line">    local_i = i</span><br><span class="line">  <span class="keyword">elif</span> z_code_shape[<span class="number">2</span>]-i &lt; <span class="number">8</span>:</span><br><span class="line">    local_i = <span class="number">16</span>-(z_code_shape[<span class="number">2</span>]-i)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    local_i = <span class="number">8</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,z_code_shape[<span class="number">3</span>]-<span class="number">0</span>):</span><br><span class="line">    <span class="keyword">if</span> j &lt;= <span class="number">8</span>:</span><br><span class="line">      local_j = j</span><br><span class="line">    <span class="keyword">elif</span> z_code_shape[<span class="number">3</span>]-j &lt; <span class="number">8</span>:</span><br><span class="line">      local_j = <span class="number">16</span>-(z_code_shape[<span class="number">3</span>]-j)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      local_j = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">    i_start = i-local_i</span><br><span class="line">    i_end = i_start+<span class="number">16</span></span><br><span class="line">    j_start = j-local_j</span><br><span class="line">    j_end = j_start+<span class="number">16</span></span><br><span class="line">    </span><br><span class="line">    patch = idx[:,i_start:i_end,j_start:j_end]</span><br><span class="line">    patch = patch.reshape(patch.shape[<span class="number">0</span>],-<span class="number">1</span>)</span><br><span class="line">    cpatch = cidx[:, i_start:i_end, j_start:j_end]</span><br><span class="line">    cpatch = cpatch.reshape(cpatch.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">    patch = torch.cat((cpatch, patch), dim=<span class="number">1</span>)</span><br><span class="line">    logits,_ = model.transformer(patch[:,:-<span class="number">1</span>])</span><br><span class="line">    logits = logits[:, -<span class="number">256</span>:, :]</span><br><span class="line">    logits = logits.reshape(z_code_shape[<span class="number">0</span>],<span class="number">16</span>,<span class="number">16</span>,-<span class="number">1</span>)</span><br><span class="line">    logits = logits[:,local_i,local_j,:]</span><br><span class="line"></span><br><span class="line">    logits = logits/temperature</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      logits = model.top_k_logits(logits, top_k)</span><br><span class="line"></span><br><span class="line">    probs = torch.nn.functional.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    idx[:,i,j] = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x_sample = model.decode_to_img(idx, z_code_shape)</span><br><span class="line">show_image(x_sample)</span><br></pre></td></tr></table></figure><br>一开始的<code>temperature</code>和<code>top_k</code>是得到logit后的采样参数，和滑动窗口算法无关。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">temperature = <span class="number">1.0</span></span><br><span class="line">top_k = <span class="number">100</span></span><br></pre></td></tr></table></figure></p>
<p>进入生成图像循环后，<code>i, j</code>分别表示压缩图像的竖索引和横索引，<code>i_start, i_end, j_start, j_end</code>是滑动窗口上下左右边界。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, z_code_shape[<span class="number">2</span>]-<span class="number">0</span>):</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,z_code_shape[<span class="number">3</span>]-<span class="number">0</span>):</span><br><span class="line">    ...</span><br><span class="line">    i_start = i-local_i</span><br><span class="line">    i_end = i_start+<span class="number">16</span></span><br><span class="line">    j_start = j-local_j</span><br><span class="line">    j_end = j_start+<span class="number">16</span></span><br></pre></td></tr></table></figure>
<p>为了获取这四个滑动窗口的范围，代码用了若干条件语句计算待生成像素在滑动窗口里的相对位置<code>local_i, local_j</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, z_code_shape[<span class="number">2</span>]-<span class="number">0</span>):</span><br><span class="line">  <span class="keyword">if</span> i &lt;= <span class="number">8</span>:</span><br><span class="line">    local_i = i</span><br><span class="line">  <span class="keyword">elif</span> z_code_shape[<span class="number">2</span>]-i &lt; <span class="number">8</span>:</span><br><span class="line">    local_i = <span class="number">16</span>-(z_code_shape[<span class="number">2</span>]-i)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    local_i = <span class="number">8</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,z_code_shape[<span class="number">3</span>]-<span class="number">0</span>):</span><br><span class="line">    <span class="keyword">if</span> j &lt;= <span class="number">8</span>:</span><br><span class="line">      local_j = j</span><br><span class="line">    <span class="keyword">elif</span> z_code_shape[<span class="number">3</span>]-j &lt; <span class="number">8</span>:</span><br><span class="line">      local_j = <span class="number">16</span>-(z_code_shape[<span class="number">3</span>]-j)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      local_j = <span class="number">8</span></span><br></pre></td></tr></table></figure><br>得到了滑动窗口的边界后，代码用滑动窗口从约束图像的压缩图像和待生成图像的压缩图像上各取出一个图块，并拼接起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">patch = idx[:,i_start:i_end,j_start:j_end]</span><br><span class="line">patch = patch.reshape(patch.shape[<span class="number">0</span>],-<span class="number">1</span>)</span><br><span class="line">cpatch = cidx[:, i_start:i_end, j_start:j_end]</span><br><span class="line">cpatch = cpatch.reshape(cpatch.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">patch = torch.cat((cpatch, patch), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>之后，只需要把拼接的图块直接输入进Transformer，得到输出<code>logits</code>，再用<code>local_i,local_j</code>去输出图块的对应位置取出下一个压缩图像像素的概率分布，就可以随机生成下一个压缩图像像素了。如前文所述，Transformer类会把二维的图块压扁到一维，输入进GPT。同时，GPT会自动保证前面的像素看不到后面的像素，我们不需要人为地指定约束像素。这个地方的调用逻辑其实非常简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">logits,_ = model.transformer(patch[:,:-<span class="number">1</span>])</span><br><span class="line">logits = logits[:, -<span class="number">256</span>:, :]</span><br><span class="line">logits = logits.reshape(z_code_shape[<span class="number">0</span>],<span class="number">16</span>,<span class="number">16</span>,-<span class="number">1</span>)</span><br><span class="line">logits = logits[:,local_i,local_j,:]</span><br></pre></td></tr></table></figure>
<p>最后只要从<code>logits</code>里采样，把采样出的压缩图像像素填入<code>idx</code>，就完成了一步生成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">logits = logits/temperature</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    logits = model.top_k_logits(logits, top_k)</span><br><span class="line"></span><br><span class="line">probs = torch.nn.functional.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">idx[:,i,j] = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>反复执行循环，就能将压缩图像生成完毕。最后将压缩图像过一遍VQGAN的解码器即可得到最终的生成图像。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_sample = model.decode_to_img(idx, z_code_shape)</span><br><span class="line">show_image(x_sample)</span><br></pre></td></tr></table></figure>
<p><img src="/2023/06/19/20230605-VQGAN/21.jpg" alt></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>VQGAN论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09841">https://arxiv.org/abs/2012.09841</a></p>
<p>VQGAN GitHub：<a target="_blank" rel="noopener" href="https://github.com/CompVis/taming-transformers">https://github.com/CompVis/taming-transformers</a></p>
<p>如果你需要补充学习早期工作，欢迎阅读我之前的文章。</p>
<p>Transformer解读</p>
<p>PixelCNN解读</p>
<p>VQVAE解读</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/en/2023/06/11/20221106-transformer-pytorch/" rel="prev" title="PyTorch Transformer 英中翻译超详细教程">
      <i class="fa fa-chevron-left"></i> PyTorch Transformer 英中翻译超详细教程
    </a></div>
      <div class="post-nav-item">
    <a href="/en/2023/07/01/20230622-VQVAE-2/" rel="next" title="VQVAE PyTorch 实现教程">
      VQVAE PyTorch 实现教程 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#VQGAN-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="nav-number">1.</span> <span class="nav-text">VQGAN 核心思想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VQVAE-%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86%E8%A1%A5%E5%85%85"><span class="nav-number">2.</span> <span class="nav-text">VQVAE 背景知识补充</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9%E6%A8%A1%E5%9E%8B-VQGAN"><span class="nav-number">3.</span> <span class="nav-text">图像压缩模型 VQGAN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E-Transformer-%E7%9A%84%E5%8E%8B%E7%BC%A9%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.</span> <span class="nav-text">基于 Transformer 的压缩图像生成模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%A6%E7%BA%A6%E6%9D%9F%E7%9A%84%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-number">5.</span> <span class="nav-text">带约束的图像生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E9%AB%98%E6%B8%85%E5%9B%BE%E5%83%8F"><span class="nav-number">6.</span> <span class="nav-text">生成高清图像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">7.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">8.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB"><span class="nav-number">9.</span> <span class="nav-text">代码阅读</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84"><span class="nav-number">10.</span> <span class="nav-text">项目结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VQGAN-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">11.</span> <span class="nav-text">VQGAN 模型结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">12.</span> <span class="nav-text">Transformer 模型结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E5%B8%A6%E7%BA%A6%E6%9D%9F%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="nav-number">13.</span> <span class="nav-text">基于滑动窗口的带约束图像生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">14.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/en/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/en/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/en/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/en/lib/anime.min.js"></script>
  <script src="/en/lib/velocity/velocity.min.js"></script>
  <script src="/en/lib/velocity/velocity.ui.min.js"></script>

<script src="/en/js/utils.js"></script>

<script src="/en/js/motion.js"></script>


<script src="/en/js/schemes/muse.js"></script>


<script src="/en/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
