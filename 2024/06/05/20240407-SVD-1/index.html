<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/en/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/en/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/en/images/logo.svg" color="#222">

<link rel="stylesheet" href="/en/css/main.css">


<link rel="stylesheet" href="/en/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/en/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="在上篇文章中，我们浏览了 Stable Video Diffusion (SVD) 的论文，并特别学习了没有在论文中提及的模型结构、噪声调度器这两个模块。在这篇文章中，让我们来看看 SVD 在 Diffusers 中的源码实现。我们会先学习 SVD 的模型结构，再学习 SVD 的采样流水线。在本文的多数章节中，我都会将 SVD 的结构与 Stable Diffusion (SD) 的做对比，帮助之">
<meta property="og:type" content="article">
<meta property="og:title" content="Stable Video Diffusion 源码解读 (Diffusers 版)">
<meta property="og:url" content="https://zhouyifan.net/en/2024/06/05/20240407-SVD-1/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="在上篇文章中，我们浏览了 Stable Video Diffusion (SVD) 的论文，并特别学习了没有在论文中提及的模型结构、噪声调度器这两个模块。在这篇文章中，让我们来看看 SVD 在 Diffusers 中的源码实现。我们会先学习 SVD 的模型结构，再学习 SVD 的采样流水线。在本文的多数章节中，我都会将 SVD 的结构与 Stable Diffusion (SD) 的做对比，帮助之">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2024/06/05/20240407-SVD-1/rocket.gif">
<meta property="og:image" content="https://zhouyifan.net/2024/06/05/20240407-SVD-1/0-1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/06/05/20240407-SVD-1/0-2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/06/05/20240407-SVD-1/1-1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/06/05/20240407-SVD-1/1-2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/06/05/20240407-SVD-1/1-3.jpg">
<meta property="article:published_time" content="2024-06-05T07:52:35.000Z">
<meta property="article:modified_time" content="2024-08-19T12:59:46.019Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="扩散模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2024/06/05/20240407-SVD-1/rocket.gif">

<link rel="canonical" href="https://zhouyifan.net/en/2024/06/05/20240407-SVD-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Stable Video Diffusion 源码解读 (Diffusers 版) | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/en/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/en/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/en/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/en/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/en/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/en/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net" rel="section"><i class="fa fa-language fa-fw"></i>简体中文</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/06/05/20240407-SVD-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Stable Video Diffusion 源码解读 (Diffusers 版)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-05 15:52:35" itemprop="dateCreated datePublished" datetime="2024-06-05T15:52:35+08:00">2024-06-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">代码阅读</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在上篇文章中，我们浏览了 Stable Video Diffusion (SVD) 的论文，并特别学习了没有在论文中提及的模型结构、噪声调度器这两个模块。在这篇文章中，让我们来看看 SVD 在 Diffusers 中的源码实现。我们会先学习 SVD 的模型结构，再学习 SVD 的采样流水线。在本文的多数章节中，我都会将 SVD 的结构与 Stable Diffusion (SD) 的做对比，帮助之前熟悉 SD 的读者快速理解 SVD 的性质。强烈建议读者在阅读本文前先熟悉 SD 及其在 Diffusers 中的实现。</p>
<p><a href="https://zhouyifan.net/2024/01/23/20230713-SD3/">Stable Diffusion Diffusers 实现源码解读</a></p>
<h2 id="简单采样实验"><a href="#简单采样实验" class="headerlink" title="简单采样实验"></a>简单采样实验</h2><p>目前开源的 SVD 仅有图生视频模型，即给定视频首帧，模型生成视频的后续内容。在首次开源时，SVD 有 1.0 和 1.0-xt 两个版本。二者模型结构配置相同，主要区别在于训练数据上。SVD 1.0 主要用于生成 14 帧 576x1024 的视频，而 1.0-xt 版本由 1.0 模型微调而来，主要用于生成 25 帧 576x1024 的视频。后来，开发团队又开源了 SVD 1.1-xt，该模型在固定帧率的视频数据上微调，输出视频更加连贯。为了做实验方便，在这篇文章中，我们将使用最基础的 SVD 1.0 模型。</p>
<p>参考 Diffusers 官方文档: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/main/en/using-diffusers/svd">https://huggingface.co/docs/diffusers/main/en/using-diffusers/svd</a> ，我们来创建一个关于 SVD 的 “Hello World” 项目。如果你的电脑可以访问 HuggingFace 原站的话，直接运行下面的脚本就行了；如果不能访问原网站，可以尝试取消代码里的那行注释，访问 HuggingFace 镜像站；如果还是不行，则需要手动下载 “stabilityai/stable-video-diffusion-img2vid” 仓库，并将仓库路径改成本地下载的仓库路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.environ[&#x27;HF_ENDPOINT&#x27;] = &#x27;https://hf-mirror.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableVideoDiffusionPipeline</span><br><span class="line"><span class="keyword">from</span> diffusers.utils <span class="keyword">import</span> load_image, export_to_video</span><br><span class="line"></span><br><span class="line">pipe = StableVideoDiffusionPipeline.from_pretrained(</span><br><span class="line">    <span class="string">&quot;stabilityai/stable-video-diffusion-img2vid&quot;</span>, torch_dtype=torch.float16, variant=<span class="string">&quot;fp16&quot;</span></span><br><span class="line">)</span><br><span class="line">pipe.enable_model_cpu_offload()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the conditioning image</span></span><br><span class="line">image = load_image(<span class="string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png&quot;</span>)</span><br><span class="line">image = image.resize((<span class="number">1024</span>, <span class="number">576</span>))</span><br><span class="line"></span><br><span class="line">generator = torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">frames = pipe(image, decode_chunk_size=<span class="number">8</span>, generator=generator).frames[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">export_to_video(frames, <span class="string">&quot;generated.mp4&quot;</span>, fps=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<p>成功运行后，我们能得到这样的一个火箭升空视频。它的第一帧会和我们的输入图片一模一样。</p>
<p><img src="/2024/06/05/20240407-SVD-1/rocket.gif" alt></p>
<h2 id="SVD-概览"><a href="#SVD-概览" class="headerlink" title="SVD 概览"></a>SVD 概览</h2><p>由于 SVD 并没有在论文里对其图生视频模型做详细的介绍，我们没有官方资料可以参考，只能靠阅读源码来了解 SVD 的实现细节。为了让大家在读代码时不会晕头转向，我会在读代码前简单概述一下 SVD 的模型结构和采样方法。</p>
<p>SVD 和 SD 一样，是一个隐扩散模型（Latent Diffusion Model, LDM）。图像（视频帧）的生成由两个阶段组成：先由扩散模型生成压缩图像，再由 VAE 解码成真实图像。</p>
<p>扩散模型在生成图像时，会用一个去噪 U-Net $\epsilon_\theta$ 反复对纯噪声图像 $z_T$ 去噪，直至得到一幅有意义的图片 $z$。为了让模型输出我们想要的图像，我们会用一些额外的信息来约束模型，或者说将约束信息也输入进 U-Net。对于文生图 SD 来说，额外约束是文本。对于图生视频 SVD 来说，额外约束是图像。LDM 提出了两种输入约束信息的方式：与输入噪声图像拼接、作为交叉注意力模块的 K, V。SD 仅使用了交叉注意力的方式，而 SVD 同时使用了两种方式。</p>
<p><img src="/2024/06/05/20240407-SVD-1/0-1.jpg" alt></p>
<p>上面这两种添加约束信息的方法适用于信息量比较大的约束。实际上，还有一种更简单的输入实数约束信息的方法。除了噪声输入外，去噪模型还必须输入当前的去噪时刻 $t$。自最早的 DDPM 以来，时刻 $t$ 都是先被转换成位置编码，再输入进 U-Net 的所有残差块中。仿照这种输入机制，如果有其他的约束信息和 $t$ 一样可以用一个实数表示，则不必像前面那样将这种约束信息与输入拼接或输入交叉注意力层，只需要把约束也转换成位置编码，再与 $t$ 的编码加在一起。</p>
<p>SVD 给模型还添加了三种额外约束：噪声增强程度、帧率、运动程度。这三种约束都是用和时刻编码相加的形式实现的。</p>
<p><img src="/2024/06/05/20240407-SVD-1/0-2.jpg" alt></p>
<p>即使现在不完全理解这三种额外约束的意义也不要紧。稍后我们会在学习 U-Net 结构时看到这种额外约束是怎么添加进 U-Net 的，在学习采样流水线时了解这三种约束的意义。</p>
<p>总结一下，除了添加了少数模块外，SVD 和 SD 的整体架构一样，都是以去噪 U-Net 为核心的 LDM。除了原本扩散模型要求的噪声、去噪时刻这两种输入外，SVD 还加入了 4 种约束信息：约束图像（视频首帧）、噪声增强程度、帧率、运动程度。约束图像是最主要的约束信息，它会与噪声输入拼接，且输入进 U-Net 的交叉注意力层中。后三种额外约束会以和处理去噪时刻类似的方式输入进 U-Net 中。</p>
<h2 id="去噪模型结构"><a href="#去噪模型结构" class="headerlink" title="去噪模型结构"></a>去噪模型结构</h2><p>接下来，我们来学习 SVD 的去噪模型的结构。在 Diffusers 中，一个扩散模型的参数、配置全部放在一个模型文件夹里，该文件夹的各个子文件夹存储了模型的各个模块，如自编码器、去噪模型、调度器等。我们可以在 <code>https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/tree/main</code> 找到 SVD 的模型文件夹，或者访问我们本地下载好的模型文件夹。</p>
<p>SVD 的去噪 U-Net 放在模型文件夹的 <code>unet</code> 子文件夹里。通过阅读子文件夹里的 <code>config.json</code>，我们就能知道模型类的名字是什么，并知道初始化模型的参数有哪些。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;UNetSpatioTemporalConditionModel&quot;</span>,</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;down_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;DownBlockSpatioTemporal&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;up_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;UpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>通过在本地 Diffusers 库文件夹里搜索类名 <code>UNetSpatioTemporalConditionModel</code>，或者利用 IDE 的 Python 智能提示功能，在前文的示例脚本里跳转到 <code>StableVideoDiffusionPipeline</code> 所在文件，再跳转到 <code>UNetSpatioTemporalConditionModel</code> 所在文件，我们就能知道 SVD 的去噪 U-Net 类定义在 <code>diffusers/models/unet_spatio_temporal_condition.py</code> 里。我们可以对照位于 <code>diffusers/models/unet_2d_condition.py</code> 的 SD 的 2D U-Net 类 <code>UNet2DConditionModel</code> 来看一下 SVD 的 U-Net 有何不同。</p>
<p>先来看 <code>__init__</code> 构造函数。SVD U-Net 几乎就是一个写死了许多参数的特化版 2D U-Net，其构造函数也基本上是 SD 2D U-Net 的构造函数的子集。比如 2D U-Net 允许用 <code>act_fn</code> 来指定模型的激活函数，默认为 <code>&quot;silu&quot;</code>，而 SVD U-Net 直接把所有模块的激活函数写死成 <code>&quot;silu&quot;</code>。经过简化后，SVD U-Net 的构造函数可读性高了很多。我们从参数开始读起，逐一了解构造函数每一个参数的意义：</p>
<ul>
<li><code>sample_size=None</code>：隐空间图片边长。供其他代码调用，与 U-Net 无关。</li>
<li><code>in_channels=8</code>：输入通道数。</li>
<li><code>out_channels=4</code>: 输出通道数。</li>
<li><code>down_block_types</code>：每一大层下采样模块的类名。</li>
<li><code>up_block_types</code>：每一大层上采样模块的类名。</li>
<li><code>block_out_channels = (320, 640, 1280, 1280)</code>：每一大层的通道数。</li>
<li><code>addition_time_embed_dim=256</code>: 每个额外约束的通道数。</li>
<li><code>projection_class_embeddings_input_dim=768</code>: 所有额外约束的通道数。</li>
<li><code>layers_per_block=2</code>: 每一大层有几个结构相同的模块。</li>
<li><code>cross_attention_dim=1024</code>: 交叉注意力层的通道数。</li>
<li><code>transformer_layers_per_block=1</code>: 每一大层的每一个模块里有几个 Transformer 层。</li>
<li><code>num_attention_heads=(5, 10, 10, 20)</code>: 各大层多头注意力层的头数。</li>
<li><code>num_frames=25</code>: 训练时的帧数。供其他代码调用，与 U-Net 无关。</li>
</ul>
<p>SVD U-Net 的参数基本和 SD 的一致，不同之处有：1）稍后我们会在采样流水线里看到，SVD 把图像约束拼接到了噪声图像上，所以整个噪声输入的通道数是原来的两倍，从 4 变为 8；2）多了一个给采样代码用的 <code>num_frames</code> 参数，它其实没有被 U-Net 用到。</p>
<p>我们再来大致过一下构造函数的实现细节。SVD U-Net 的整体结构和 2D U-Net 的几乎一致。数据先经过下采样模块，再经过中间模块，最后过上采样模块。下采样模块和上采样模块之间有短路连接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, down_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(down_block_types):</span><br><span class="line">    ...</span><br><span class="line">    down_block = get_down_block(...)</span><br><span class="line">    self.down_blocks.append(down_block)</span><br><span class="line"></span><br><span class="line">self.mid_block = UNetMidBlockSpatioTemporal(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, up_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(up_block_types):</span><br><span class="line">    ...</span><br><span class="line">    up_block = get_up_block(...)</span><br><span class="line">    self.up_blocks.append(up_block)</span><br><span class="line"></span><br><span class="line">self.conv_norm_out = nn.GroupNorm(...)</span><br><span class="line">self.conv_act = nn.SiLU()</span><br><span class="line">self.conv_out = nn.Conv2d(...)</span><br></pre></td></tr></table></figure>
<p>扩散模型还需要处理去噪时刻约束 $t$。U-Net 会先用正弦编码（Transformer 里的位置编码）<code>time_proj</code> 来将时刻转为向量，再用一系列线性层 <code>time_embedding</code> 预处理这个编码。该编码后续会输入进 U-Net 主体的每一个模块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.time_proj = Timesteps(block_out_channels[<span class="number">0</span>], <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">timestep_input_dim = block_out_channels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)</span><br></pre></td></tr></table></figure>
<p>除了多数扩散模型都有的 U-Net 模块外，SVD 还加入了额外约束模块。如前文所述，对于能用一个实数表示的约束，可以使用和处理时刻类似的方式，先让其过位置编码层，再过线性层，最后把得到的输出编码和时刻编码加起来。所以，和这种额外约束相关的模块在代码里叫做 <code>add_time</code>。在 2D U-Net 里，额外约束是可选的。SD 没有用到额外约束。而 SVD 把额外约束设为了必选模块。稍后我们会在采样流水线里看到，SVD 将视频的帧率、运动程度、噪声增强强度作为了生成时的额外约束。这些约束都是用这种与时刻编码相加的形式实现的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.add_time_proj = Timesteps(addition_time_embed_dim, <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)</span><br></pre></td></tr></table></figure></p>
<p>构造函数的代码就看完了。在构造函数中，我们认识了 SVD U-Net 的各个模块，但对其工作原理或许还存在着些许疑惑。我们来模型的前向传播函数 <code>forward</code> 里看一下各个模块是怎么处理输入的。</p>
<p>看代码前，我们先回顾一下概念，整理一下 U-Net 的数据处理流程。下面是我之前给 SD U-Net 画的示意图。该图对 SVD 同样适用。和 SD 相比，SVD 的输入 <code>x</code> 不仅包括噪声图像（准确说是多个表示视频帧的图像），还包括作为约束的首帧图像； <code>c</code> 换成了首帧图像的 CLIP 编码；<code>t</code> 不仅包括时刻，还包括一些额外约束。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-1.jpg" alt></p>
<p>和上图所示的一样，SVD U-Net 的 <code>forward</code> 方法的输入包含图像 <code>sample</code>，时刻 <code>timestep</code>，交叉注意力层约束（图像编码） <code>encoder_hidden_states</code> , 额外约束 <code>added_time_ids</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    sample: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    timestep: <span class="type">Union</span>[torch.Tensor, <span class="built_in">float</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    encoder_hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    added_time_ids: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    return_dict: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<p>方法首先会处理去噪时刻和额外参数，我们来看一下这两个输入是怎么拼到一起的。</p>
<p>做完一系列和形状相关的处理后，输入时刻 <code>timestep</code> 变成了 <code>timesteps</code>。随后，该变量会先过正弦编码（位置编码）层 <code>time_proj</code>，再过一些线性层 <code>time_embedding</code>，得到最后输入 U-Net 主体的时刻嵌入 <code>emb</code>。这两个模块的命名非常容易混淆，千万别弄反了。类似地，额外约束也是先过正弦编码层 <code>add_time_proj</code>，再过一些线性层 <code>add_embedding</code>，最后其输出 <code>aug_emb</code> 会加到 <code>emb</code> 上。当然，为了确保结果可以相加，<code>time_embedding</code> 和 <code>add_time_proj</code> 的输出通道数是相同的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># preprocessing</span></span><br><span class="line"><span class="comment"># timesteps = timestep</span></span><br><span class="line"></span><br><span class="line">t_emb = self.time_proj(timesteps)</span><br><span class="line">t_emb = t_emb.to(dtype=sample.dtype)</span><br><span class="line"></span><br><span class="line">emb = self.time_embedding(t_emb)</span><br><span class="line"></span><br><span class="line">time_embeds = self.add_time_proj(added_time_ids.flatten())</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">time_embeds = time_embeds.to(emb.dtype)</span><br><span class="line">aug_emb = self.add_embedding(time_embeds)</span><br><span class="line">emb = emb + aug_emb</span><br></pre></td></tr></table></figure>
<p>这里有关额外约束的处理写得很差，逻辑也很难读懂。在构造函数里，额外约束的正弦编码层 <code>add_time_proj</code> 的输出通道数 <code>addition_time_embed_dim</code> 是 256, 线性模块 <code>add_embedding</code> 的输入通道数 <code>projection_class_embeddings_input_dim</code> 是 768。两个通道数不一样的模块是怎么接起来的？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">  ...</span></span></span><br><span class="line"><span class="params"><span class="function">  addition_time_embed_dim: <span class="built_in">int</span> = <span class="number">256</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">  projection_class_embeddings_input_dim: <span class="built_in">int</span> = <span class="number">768</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">  ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">self</span>.<span class="title">add_time_proj</span> = <span class="title">Timesteps</span>(<span class="params">addition_time_embed_dim, <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">self</span>.<span class="title">add_embedding</span> = <span class="title">TimestepEmbedding</span>(<span class="params">projection_class_embeddings_input_dim, time_embed_dim</span>)</span></span><br></pre></td></tr></table></figure>
<p>原来，在下面这份模块前向传播代码中，<code>added_time_ids</code> 的形状是 <code>[batch_size, 3]</code>。其中的 <code>3</code> 表示有三个额外约束。做了 <code>flatten()</code> 再过 <code>add_time_proj</code> 后，可以得到形状为 <code>[3 * batch_size, 256]</code> 的正弦编码 <code>time_embeds</code>。之所以三个约束可以用同一个模块来处理，是因为正弦编码没有学习参数，对所有输入都会产生同样的输出。得到 <code>time_embeds</code> 后，再根据从输入噪声图像里得到的 <code>batch_size</code>，用 <code>reshape</code> 把 <code>time_embeds</code> 的形状变成 <code>[batch_size, 768]</code>。这样，<code>time_embeds</code> 就可以输入进 <code>add_embedding</code> 里了。 <code>add_embedding</code> 是有可学习参数的，三个约束必须分别处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">time_embeds = self.add_time_proj(added_time_ids.flatten())</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">time_embeds = time_embeds.to(emb.dtype)</span><br><span class="line">aug_emb = self.add_embedding(time_embeds)</span><br></pre></td></tr></table></figure>
<p>这些代码不应该这样写的。当前的写法不仅可读性差，还不利于维护。比较好的写法是在构造函数里把输入参数从<code>projection_class_embeddings_input_dim</code> 改为 <code>num_add_time</code>，表示额外约束的数量。之后，把 <code>add_embedding</code> 的输入通道数改成 <code>num_add_time * addition_time_embed_dim</code>。这样，使用者不必手动设置合理的 <code>add_embedding</code> 的输入通道数（比如保证 768 必须是 256 的 3 倍），只设置有几个额外约束就行了。这样改了之后，为了提升可读性，还可以像下面那样把 <code>reshape</code> 里的那个 <code>-1</code> 写清楚来。Diffusers 采用这种比较混乱的写法，估计是因为这段代码是从 2D U-Net 里摘抄出来的。而原 2D U-Net 需要兼容更复杂的情况，所以 <code>add_time_proj</code> 和 <code>add_embedding</code> 的通道数需要分别指定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">-&gt;</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, self.num_add_time * self.addition_time_embed_dim))</span><br></pre></td></tr></table></figure>
<p>预处理完时刻和额外约束后，方法还会修改所有输入的形状，使得它们第一维的长度都是 <code>batch_size</code> 乘视频帧数。正如我们在上一篇文章中学到的，为了兼容图像模型里的模块，我们要先把视频长度那一维和 batch 那一维合并，等到了和时序相关的模块再对视频长度那一维单独处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Flatten the batch and frames dimensions</span></span><br><span class="line"><span class="comment"># sample: [batch, frames, channels, height, width] -&gt; [batch * frames, channels, height, width]</span></span><br><span class="line">sample = sample.flatten(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># Repeat the embeddings num_video_frames times</span></span><br><span class="line"><span class="comment"># emb: [batch, channels] -&gt; [batch * frames, channels]</span></span><br><span class="line">emb = emb.repeat_interleave(num_frames, dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># encoder_hidden_states: [batch, 1, channels] -&gt; [batch * frames, 1, channels]</span></span><br><span class="line">encoder_hidden_states = encoder_hidden_states.repeat_interleave(num_frames, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>后面的代码就和 2D U-Net 的几乎一样了。数据依次经过下采样块、中间块、上采样块。下采样块的中间结果还会保存在栈 <code>down_block_res_samples</code> 里，作为上采样模块的额外输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sample = self.conv_in(sample)</span><br><span class="line"></span><br><span class="line">image_only_indicator = torch.zeros(batch_size, num_frames, dtype=sample.dtype, device=sample.device)</span><br><span class="line"></span><br><span class="line">down_block_res_samples = (sample,)</span><br><span class="line"><span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">    sample, res_samples = downsample_block(...)</span><br><span class="line">    down_block_res_samples += res_samples</span><br><span class="line"></span><br><span class="line">sample = self.mid_block(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">    res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">    down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">    sample = upsample_block(...)</span><br><span class="line"></span><br><span class="line">sample = self.conv_norm_out(sample)</span><br><span class="line">sample = self.conv_act(sample)</span><br><span class="line">sample = self.conv_out(sample)</span><br></pre></td></tr></table></figure>
<p>光看 U-Net 类，我们还看不出 SVD 的 3D U-Net 和 2D U-Net 的区别。接下来，我们来看一看 U-Net 中某一个具体的模块是怎么实现的。由于 U-Net 下采样块、中间块、上采样块的结构是类似的，我们只挑某一大层的下采样模块类 <code>CrossAttnDownBlockSpatioTemporal</code> 来学习。</p>
<p>在 <code>CrossAttnDownBlockSpatioTemporal</code> 类中，我们可以看到 SVD U-Net 的每一个子模块都可以拆成残差卷积块和 Transformer 块。数据在经过子模块时，会先过残差块，再过 Transformer 块。我们来继续深究时序残差块类 <code>SpatioTemporalResBlock</code> 和时序 Transformer 块 <code>TransformerSpatioTemporalModel</code> 的实现细节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __init__</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">    in_channels = in_channels <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> out_channels</span><br><span class="line">    resnets.append(</span><br><span class="line">        SpatioTemporalResBlock(...)</span><br><span class="line">    )</span><br><span class="line">    attentions.append(</span><br><span class="line">        TransformerSpatioTemporalModel(...)</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">blocks = <span class="built_in">list</span>(<span class="built_in">zip</span>(self.resnets, self.attentions))</span><br><span class="line"><span class="keyword">for</span> resnet, attn <span class="keyword">in</span> blocks:</span><br><span class="line">    hidden_states = resnet(hidden_states, ...)</span><br><span class="line">    hidden_states = attn(hidden_states, ...)</span><br></pre></td></tr></table></figure>
<p>在开始看代码之前，我们再回顾一下论文里有关 3D U-Net 块的介绍。SVD 的 U-Net 是从 Video LDM 的 U-Net 改过来的。下面的模块结构图源自 Video LDM 论文，我将其改成了能描述 SVD U-Net 块的图。图中红框里的模块表示在原 SD 2D U-Net 块的基础上新加入的模块。可以看出，SVD 实际上就是在原来的 2D 残差块后面加了一个 3D 卷积层，原空间注意力块后面加了一个时序注意力层。旧模块输出和新模块输出之间用一个比例 $\alpha$ 来线性混合。中间数据形状变换的细节我们已经在上篇文章里学过了，这篇文章里我们主要关心这些模块在代码里大概是怎么定义的。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-2.jpg" alt></p>
<p>3D 残差块类 <code>SpatioTemporalResBlock</code> 在 <code>diffusers/models/resnet.py</code> 文件中。它有三个子模块，分别对应上文示意图中的 2D 残差块、时序残差块（3D 卷积）、混合模块。在运算时，旧模块的输出会缓存到<br> <code>hidden_states_mix</code> 中，新模块的输出为 <code>hidden_states</code>，二者最终会送入混合模块 <code>time_mixer</code> 做一个线性混合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpatioTemporalResBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.spatial_res_block = ResnetBlock2D(...)</span><br><span class="line">        self.temporal_res_block = TemporalResnetBlock(...)</span><br><span class="line">        self.time_mixer = AlphaBlender(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        hidden_states = self.spatial_res_block(hidden_states, temb)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        hidden_states_mix = hidden_states</span><br><span class="line">        </span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        hidden_states = self.temporal_res_block(hidden_states, temb)</span><br><span class="line">        hidden_states = self.time_mixer(</span><br><span class="line">            x_spatial=hidden_states_mix,</span><br><span class="line">            x_temporal=hidden_states,</span><br><span class="line">        )</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p><code>ResnetBlock2D</code> 是 SD 2D U-Net 的残差模块，我们在这篇文章里就不去学习它了。 时序残差块 <code>TemporalResnetBlock</code> 和 2D 残差块的结构几乎完全一致，唯一的区别在于 2D 卷积被换成了 3D 卷积。从代码中我们可以知道，这个模块是一个标准的残差块，数据会依次过两个卷积层，并在最后输出前与输入相加。扩散模型中的时刻约束 <code>temb</code> 会在数据过完第一个卷积层后，加到数据上。值得注意的是，虽然类里面的卷积层名字叫 3D 卷积，但实际上它的卷积核形状为 <code>(3, 1, 1)</code>，这说明这个卷积层实际上只是一个时序维度上窗口大小为 3 的 1D 卷积层。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-3.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemporalResnetBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        kernel_size = (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        padding = [k // <span class="number">2</span> <span class="keyword">for</span> k <span class="keyword">in</span> kernel_size]</span><br><span class="line"></span><br><span class="line">        self.norm1 = torch.nn.GroupNorm(...)</span><br><span class="line">        self.conv1 = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temb_channels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.time_emb_proj = nn.Linear(temb_channels, out_channels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.time_emb_proj = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.norm2 = torch.nn.GroupNorm(...)</span><br><span class="line"></span><br><span class="line">        self.dropout = torch.nn.Dropout(<span class="number">0.0</span>)</span><br><span class="line">        self.conv2 = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">        self.nonlinearity = get_activation(<span class="string">&quot;silu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.use_in_shortcut = self.in_channels != out_channels</span><br><span class="line"></span><br><span class="line">        self.conv_shortcut = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.use_in_shortcut:</span><br><span class="line">            self.conv_shortcut = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_tensor, temb</span>):</span></span><br><span class="line">        hidden_states = input_tensor</span><br><span class="line"></span><br><span class="line">        hidden_states = self.norm1(hidden_states)</span><br><span class="line">        hidden_states = self.nonlinearity(hidden_states)</span><br><span class="line">        hidden_states = self.conv1(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.time_emb_proj <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            temb = self.nonlinearity(temb)</span><br><span class="line">            temb = self.time_emb_proj(temb)[:, :, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            temb = temb.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">            hidden_states = hidden_states + temb</span><br><span class="line"></span><br><span class="line">        hidden_states = self.norm2(hidden_states)</span><br><span class="line">        hidden_states = self.nonlinearity(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.conv2(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.conv_shortcut <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_tensor = self.conv_shortcut(input_tensor)</span><br><span class="line"></span><br><span class="line">        output_tensor = input_tensor + hidden_states</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_tensor</span><br></pre></td></tr></table></figure>
<p>混合模块 <code>AlphaBlender</code> 其实就只是定义了一个可学习的混合比例 <code>mix_factor</code>，之后用这个比例来混合空间层输出和时序层输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlphaBlender</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        alpha: <span class="built_in">float</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.register_parameter(<span class="string">&quot;mix_factor&quot;</span>, torch.nn.Parameter(torch.Tensor([alpha])))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        x_spatial,</span></span></span><br><span class="line"><span class="params"><span class="function">        x_temporal,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># Get mix_factor</span></span><br><span class="line">        alpha = self.get_alpha(...)</span><br><span class="line">        alpha = alpha.to(x_spatial.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.switch_spatial_to_temporal_mix:</span><br><span class="line">            alpha = <span class="number">1.0</span> - alpha</span><br><span class="line"></span><br><span class="line">        x = alpha * x_spatial + (<span class="number">1.0</span> - alpha) * x_temporal</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>看完了3D 残差块 <code>SpatioTemporalResBlock</code> 的内容，我们接着来看 3D 注意力块 <code>TransformerSpatioTemporalModel</code> 的内容。<code>TransformerSpatioTemporalModel</code> 也主要由 2D Transformer 块 <code>BasicTransformerBlock</code>、时序 Transformer 块 <code>TemporalBasicTransformerBlock</code> 、混合模块组成 <code>AlphaBlender</code>。它们的连接方式和上面的残差块类似。时序 Transformer 块和普通 2D Transformer 块一样，都是有自注意力、交叉注意力、全连接层的标准 Transformer 模块，它们的区别只在于时序 Transformer 块对输入做形状变换的方式不同，会让数据在时序维度上做信息交互。这里我们就不去进一步深究它们的实现细节了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerSpatioTemporalModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_attention_heads: <span class="built_in">int</span> = <span class="number">16</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_head_dim: <span class="built_in">int</span> = <span class="number">88</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        in_channels: <span class="built_in">int</span> = <span class="number">320</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        out_channels: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_layers: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        cross_attention_dim: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                BasicTransformerBlock(...)</span><br><span class="line">                <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.temporal_transformer_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                TemporalBasicTransformerBlock(...)</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.time_pos_embed = TimestepEmbedding(in_channels, time_embed_dim, out_dim=in_channels)</span><br><span class="line">        self.time_proj = Timesteps(in_channels, <span class="literal">True</span>, <span class="number">0</span>)</span><br><span class="line">        self.time_mixer = AlphaBlender(alpha=<span class="number">0.5</span>, ...)</span><br></pre></td></tr></table></figure>
<p>这个时序 Transformer 模块类有一个地方值得注意。我们知道，Transformer 模型本身是不知道输入数据的顺序的。无论是注意力层还是全连接层，它们都与顺序无关。为了让模型知道数据的先后顺序，比如在 NLP 里我们希望模型知道一句话里每个单词的前后顺序，我们会给输入数据加上位置编码。而有些时候我们觉得模型不用知道数据的先后顺序。比如在 SD 的 2D 图像 Transformer 块里，我们把每个像素当成一个 token，每个像素在 Transformer 块的运算方式是相同的，与其所在位置无关。而在处理视频时序的 Transformer 块中，知道视频每一帧的先后顺序看起来还是很重要的。所以，和 SD 的 2D Transformer 块不同，SVD 的时序 Transformer 块根据视频的帧号设置了位置编码，用和 NLP 里处理文本类似的方式处理视频。SVD 的时序 Transformer 类在构造函数里定义了生成位置编码的模块 <code>TimestepEmbedding</code>, <code>Timesteps</code>。在前向传播时，<code>forward</code> 方法会用 <code>torch.arange(num_frames)</code> 根据总帧数生成帧号列表，并经过两个模块得到最终的位置编码嵌入 <code>emb</code>。嵌入 <code>emb</code> 会在数据过时序 Transformer 块前与输入 <code>hidden_states_mix</code> 相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerSpatioTemporalModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.time_pos_embed = TimestepEmbedding(in_channels, time_embed_dim, out_dim=in_channels)</span><br><span class="line">        self.time_proj = Timesteps(in_channels, <span class="literal">True</span>, <span class="number">0</span>)</span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        num_frames_emb = torch.arange(num_frames, device=hidden_states.device)</span><br><span class="line">        num_frames_emb = num_frames_emb.repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        num_frames_emb = num_frames_emb.reshape(-<span class="number">1</span>)</span><br><span class="line">        t_emb = self.time_proj(num_frames_emb)</span><br><span class="line">        emb = self.time_pos_embed(t_emb)</span><br><span class="line">        emb = emb[:, <span class="literal">None</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> block, temporal_block <span class="keyword">in</span> <span class="built_in">zip</span>(self.transformer_blocks, self.temporal_transformer_blocks):</span><br><span class="line">            hidden_states = block(</span><br><span class="line">                ...</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            hidden_states_mix = hidden_states</span><br><span class="line">            hidden_states_mix = hidden_states_mix + emb</span><br><span class="line"></span><br><span class="line">            hidden_states_mix = temporal_block(...)</span><br><span class="line">            hidden_states = self.time_mixer(...)</span><br><span class="line"></span><br><span class="line">        hidden_states = self.proj_out(hidden_states)</span><br><span class="line">        hidden_states = hidden_states.reshape(batch_frames, height, width, inner_dim).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        output = hidden_states + residual</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>到这里，我们就读完了 SVD U-Net 的主要代码。相比 SD U-Net，SVD U-Net 主要做了以下修改：</p>
<ul>
<li>由于输入多了一张约束图像，输入通道数变为原来的两倍。</li>
<li>多加了三个和视频相关的额外约束。它们是通过和扩散模型的时刻嵌入相加输入进模型的。它们的命名通常与 <code>add_time</code> 相关。</li>
<li>仿照 Video LDM 的结构设计，SVD 也在 2D 残差块后面加入了由 3D 卷积组成的时序残差块，在空间 Transformer 块后面加入了对时序维度做注意力的时序 Transformer 块。新旧模块的输出会以一个可学习的比例线性混合。</li>
</ul>
<h2 id="VAE-结构"><a href="#VAE-结构" class="headerlink" title="VAE 结构"></a>VAE 结构</h2><p>SVD 不仅微调了 SD 的 U-Net，还微调了 VAE 的解码器，让输出视频在时序上更加连贯。由于更新 VAE 和更新 U-Net 的方法几乎一致，我们就来快速看一下 SVD 的时序 VAE 的结构，而跳过每个模块的更新细节。</p>
<p>通过阅读 VAE 的配置文件，我们可以知道时序 VAE 的类名为  <code>AutoencoderKLTemporalDecoder</code>，它位于文件 <code>diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py</code> 中。从它的构造函数里我们可以知道，时序 VAE 的编码器类是 <code>Encoder</code>，和 SD 的一样，只是解码器类从 <code>Decoder</code> 变成了 <code>TemporalDecoder</code>。我们来看一下这个新解码器类的代码做了哪些改动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AutoencoderKLTemporalDecoder</span>(<span class="params">ModelMixin, ConfigMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        in_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        out_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(...)</span><br><span class="line"></span><br><span class="line">        self.decoder = TemporalDecoder(...)</span><br><span class="line"></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在 SD 中，VAE 和 U-Net 的组成模块是几乎一致的，二者的结构主要有三个区别：1）由于 VAE 的解码器和编码器是独立的，它们之间没有残差连接。而 U-Net 是一个整体，它的编码器（下采样块）和解码器（上采样块）之间有残差连接，以减少数据在下采样中的信息损失; 2）由于 VAE 中图像的尺寸较大，仅在 VAE 最深层图像尺寸为 <code>64x64</code> 时才有自注意力层。具体来说，这个自注意力层加到了 VAE 解码器的一开头，代码中相关模块称为 <code>mid_block</code>；3）VAE 仅有空间自注意力，而 SD U-Net 用了完整的 Transformer 块（包含自注意力层、交叉注意力层、全连接层）。由于 SD VAE 和 U-Net 结构上的相似性，SVD 的开发者直接把对 U-Net 的更新也搬到了 VAE 上来。</p>
<p>SVD VAE 解码器仅做了两项更新：1）将所有模块里的 2D 残差块都被换成了我们在上文中见过的 3D 残差块；2）在最终输出前加了一个 3D 卷积（时序维度上的 1D 卷积）。VAE 的自注意力层的结构并没有更新。更新 2D 残差块的方法和 U-Net 的是一致的。比如在新的上采样块类 <code>UpBlockTemporalDecoder</code> 中，我们就可以看到之前在新 U-Net 里看过的 3D 残差块类 <code>SpatioTemporalResBlock</code> 的身影。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> ..unet_3d_blocks <span class="keyword">import</span> MidBlockTemporalDecoder, UpBlockTemporalDecoder</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpBlockTemporalDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            ...</span><br><span class="line">            resnets.append(SpatioTemporalResBlock(...))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemporalDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers_per_block = layers_per_block</span><br><span class="line"></span><br><span class="line">        self.conv_in = nn.Conv2d(...)</span><br><span class="line">        self.mid_block = MidBlockTemporalDecoder(...)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(block_out_channels)):</span><br><span class="line">            ...</span><br><span class="line">            up_block = UpBlockTemporalDecoder(...)</span><br><span class="line">            self.up_blocks.append(up_block)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        conv_out_kernel_size = (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.time_conv_out = torch.nn.Conv3d(...)</span><br></pre></td></tr></table></figure>
<h2 id="采样流水线"><a href="#采样流水线" class="headerlink" title="采样流水线"></a>采样流水线</h2><p>看完了 U-Net 和 VAE 的代码后，我们来看整套 SVD 的采样代码。和其他方法一样，在 Diffusers 中，一套采样方法会用一个流水线类 (<code>xxxPipeline</code>)来表示。SVD 对应的流水线类叫做 <code>StableVideoDiffusionPipeline</code>。我们可以利用 IDE 的代码跳转功能，在本文开头的示例采样脚本中跳转至 <code>StableVideoDiffusionPipeline</code> 所在源文件 <code>diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py</code>。</p>
<p>如示例脚本所示，使用流水线类时，可以将类实例 <code>pipe</code> 当成一个函数来用。这种用法实际上会调用实例的 <code>__call__</code> 方法。所以，在阅读流水线类的代码时，我们可以先忽略其他部分，直接看 <code>__call__</code> 方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipe = StableVideoDiffusionPipeline.from_pretrained(...)</span><br><span class="line">frames = pipe(image, decode_chunk_size=<span class="number">8</span>, generator=generator).frames[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><code>__call__</code> 的参数定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    image: <span class="type">Union</span>[PIL.Image.Image, <span class="type">List</span>[PIL.Image.Image], torch.FloatTensor],</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="built_in">int</span> = <span class="number">576</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="built_in">int</span> = <span class="number">1024</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_frames: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">25</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    min_guidance_scale: <span class="built_in">float</span> = <span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_guidance_scale: <span class="built_in">float</span> = <span class="number">3.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    fps: <span class="built_in">int</span> = <span class="number">7</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    motion_bucket_id: <span class="built_in">int</span> = <span class="number">127</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    noise_aug_strength: <span class="built_in">float</span> = <span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    decode_chunk_size: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_videos_per_prompt: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    generator: <span class="type">Optional</span>[<span class="type">Union</span>[torch.Generator, <span class="type">List</span>[torch.Generator]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    latents: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    output_type: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;pil&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    callback_on_step_end: <span class="type">Optional</span>[<span class="type">Callable</span>[[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="type">Dict</span>], <span class="literal">None</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    callback_on_step_end_tensor_inputs: <span class="type">List</span>[<span class="built_in">str</span>] = [<span class="string">&quot;latents&quot;</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    return_dict: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br></pre></td></tr></table></figure>
<p><code>__call__</code> 的参数就是我们在使用 SVD 采样时能修改的参数，我们需要把其中的主要参数弄懂。各参数的解释如下：</p>
<ul>
<li><code>image</code>：SVD 会根据哪张图片生成视频。</li>
<li><code>height, width</code>: 生成视频的尺寸。如果输入图片与这个尺寸对不上，会将输入图片的尺寸调整为该尺寸。</li>
<li><code>num_frames</code>: 生成视频的帧数。SVD 1.0 版默认 14 帧，1.0-xt 版默认 25 帧。</li>
<li><code>min_guidance_scale</code>, <code>max_guidance_scale</code>: 使用 Classifiser-free Guidance (CFG) 的强度范围。SVD 用了一种特殊的设置 CFG 强度的机制，稍后我们会在采样代码里见到。</li>
<li><code>fps</code>：输出视频期望的帧率。SVD 的额外约束。实际上这个帧率肯定是不准的，只不过提高这个值可以让视频更平滑。</li>
<li><code>motion_bucket_id</code>: SVD 的额外约束。官方没有解释该值的原理，只说明了提高该值能让输出视频的运动更多。</li>
<li><code>noise_aug_strength</code>: 对输入图片添加的噪声强度。值越低输出视频越像原图。</li>
<li><code>decode_chunk_size</code>: 一次放几张图片进时序 VAE 做解码，用于在内存占用和效果之间取得一个平衡。按理说一次处理所有图片得到的视频连续性最好，但那样也会消耗过多的内存。</li>
<li><code>num_videos_per_prompt</code>: 对于每张输入图片 （prompt），输出几段视频。</li>
<li><code>generator</code>: PyTorch 的随机数生成器。如果想要手动控制生成中的随机种子，就手动设置这个变量。</li>
<li><code>latents</code>: 强制指定的扩散模型的初始高斯噪声。</li>
<li><code>output_type</code>: 输出图片格式，是 NumPy、PIL，还是 PyTorch。</li>
<li><code>callback_on_step_end</code>，<code>callback_on_step_end_tensor_inputs</code> 用于在不修改原流水线代码的情况下向采样过程中添加额外的处理逻辑。学习代码的时候可以忽略。</li>
<li><code>return_dict</code>: 流水线是返回一个词典，还是像普通 Python 函数一样返回用元组表示的多个返回值。</li>
</ul>
<p>大致搞清楚了输入参数的意义后，我们来看流水线的执行代码。一开始的代码都是在预处理输入，可以直接跳过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0. Default height and width to unet</span></span><br><span class="line">height = height <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">width = width <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line"></span><br><span class="line">num_frames = num_frames <span class="keyword">if</span> num_frames <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.unet.config.num_frames</span><br><span class="line">decode_chunk_size = decode_chunk_size <span class="keyword">if</span> decode_chunk_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> num_frames</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Check inputs. Raise error if not correct</span></span><br><span class="line">self.check_inputs(image, height, width)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define call parameters</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(image, PIL.Image.Image):</span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> <span class="built_in">isinstance</span>(image, <span class="built_in">list</span>):</span><br><span class="line">    batch_size = <span class="built_in">len</span>(image)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    batch_size = image.shape[<span class="number">0</span>]</span><br><span class="line">device = self._execution_device</span><br><span class="line">self._guidance_scale = max_guidance_scale</span><br></pre></td></tr></table></figure>
<p>之后，代码开始预处理交叉注意力层的约束信息。在 SD 里，约束信息是文本，所以这一步会用 CLIP 文本编码器得到约束文本的嵌入。而 SVD 是一个图生视频模型，所以这一步会用 CLIP 图像编码器得到约束图像的嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Encode input image</span></span><br><span class="line">image_embeddings = self._encode_image(image, device, num_videos_per_prompt, self.do_classifier_free_guidance)</span><br></pre></td></tr></table></figure>
<p>代码还把额外约束帧率 <code>fps</code> 减了个一，因为训练的时候模型实际上输入的额外约束是 <code>fps - 1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fps = fps - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>接着，代码开始处理与噪声拼接的约束图像。回顾一下，SVD 的约束图像以两种形式输入进模型：一种是过 CLIP 图像编码器，以交叉注意力 K，V 的形式输入，其预处理如上部分的代码所示；另一种形式是与原去噪 U-Net 的噪声输入拼接，其预处理如当前这部分代码所示。</p>
<p>在预处理要拼接的图像时，代码会先调用预处理器 <code>image_processor.preprocess</code>，把其他格式的图像转成 PyTorch 的 <code>Tensor</code> 类型。之后，代码会随机生成一点高斯噪声，并把噪声根据噪声增强强度 <code>noise_aug_strength</code> 加到这张约束图像上。这种做法来自于之前有约束图像的扩散模型 <em>Cascaded diffusion models</em>。<code>noise_aug_strength</code> 稍后会作为额外约束输入进 U-Net 里，与去噪时刻的编码相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image = self.image_processor.preprocess(image, height=height, width=width).to(device)</span><br><span class="line">noise = randn_tensor(image.shape, generator=generator, device=device, dtype=image.dtype)</span><br><span class="line">image = image + noise_aug_strength * noise</span><br></pre></td></tr></table></figure>
<p>加了这个噪声后，图像会过 VAE 的编码器，得到 <code>image_latents</code>。<code>image_latents</code> 会通过 <code>repeat</code> 操作复制成多份，并于稍后拼接到每一帧带噪图像上。注意，一般图像在过 VAE 的编码器后，要乘一个系数 <code>vae.config.scaling_factor</code>; 在过 VAE 的解码器前，要除以这个系数。然而，只有在这个地方，<code>image_latents</code> 没有乘系数。我个人觉得这是开发者的一个失误。当然，做不做这个操作对于模型来说区别不大，因为模型能很快学会这种系数上的差异。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Encode input image using VAE</span></span><br><span class="line">image_latents = self._encode_vae_image(</span><br><span class="line">    image,</span><br><span class="line">    device=device,</span><br><span class="line">    num_videos_per_prompt=num_videos_per_prompt,</span><br><span class="line">    do_classifier_free_guidance=self.do_classifier_free_guidance,</span><br><span class="line">)</span><br><span class="line">image_latents = image_latents.to(image_embeddings.dtype)</span><br><span class="line">image_latents = image_latents.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, num_frames, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>下一步，代码会把三个额外约束拼接在一起，得到 <code>added_time_ids</code>。它会接入到 U-Net 中，与时刻编码加到一起。在训练时，帧率 <code>fps</code> 和 运动程度 <code>motion_bucket_id</code> 完全来自于数据集标注，而 <code>noise_aug_strength</code> 是可以随机设置的。在采样时，这三个参数都可以手动设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5. Get Added Time IDs</span></span><br><span class="line">added_time_ids = self._get_add_time_ids(</span><br><span class="line">    fps,</span><br><span class="line">    motion_bucket_id,</span><br><span class="line">    noise_aug_strength,</span><br><span class="line">    image_embeddings.dtype,</span><br><span class="line">    batch_size,</span><br><span class="line">    num_videos_per_prompt,</span><br><span class="line">    self.do_classifier_free_guidance,</span><br><span class="line">)</span><br><span class="line">added_time_ids = added_time_ids.to(device)</span><br></pre></td></tr></table></figure>
<p>再下一步，代码会将采样的总步数 <code>num_inference_steps</code> 告知采样调度器 <code>scheduler</code>。这一步是 Diffusers API 的要求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6. Prepare timesteps</span></span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, <span class="literal">None</span>, sigmas)</span><br></pre></td></tr></table></figure>
<p>然后，代码会随机生成初始高斯噪声。不同的随机噪声即对应不同的输出视频。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 7. Prepare latent variables</span></span><br><span class="line">num_channels_latents = self.unet.config.in_channels</span><br><span class="line">latents = self.prepare_latents(</span><br><span class="line">    batch_size * num_videos_per_prompt,</span><br><span class="line">    num_frames,</span><br><span class="line">    num_channels_latents,</span><br><span class="line">    height,</span><br><span class="line">    width,</span><br><span class="line">    image_embeddings.dtype,</span><br><span class="line">    device,</span><br><span class="line">    generator,</span><br><span class="line">    latents,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>开始采样前，SVD 对约束图像的强度做了一种很特殊的设定。在看代码之前，我们先回顾一下约束强度的意义。现在的扩散模型普遍使用了 CFG (Classifier-free Guidance) 技术，它允许我们在采样时灵活地调整模型和约束信息的相符程度。这个强度默认取 1.0。我们可以通过增大强度来提升模型的生成效果，比如在 SD 中，这个强度一般取 7.5，这代表模型会更加贴近输入文本。</p>
<p>而 SVD 中，约束信息为图像。开发者对视频的不同帧采用了不同的约束强度：首帧为 <code>min_guidance_scale</code>, 末帧为 <code>max_guidance_scale</code>。强度从首帧到末帧线性增加。默认情况下，约束强度的范围是 [1, 3]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 8. Prepare guidance scale</span></span><br><span class="line">guidance_scale = torch.linspace(min_guidance_scale, max_guidance_scale, num_frames).unsqueeze(<span class="number">0</span>)</span><br><span class="line">guidance_scale = guidance_scale.to(device, latents.dtype)</span><br><span class="line">guidance_scale = guidance_scale.repeat(batch_size * num_videos_per_prompt, <span class="number">1</span>)</span><br><span class="line">guidance_scale = _append_dims(guidance_scale, latents.ndim)</span><br><span class="line"></span><br><span class="line">self._guidance_scale = guidance_scale</span><br></pre></td></tr></table></figure>
<p>最后，就来到了扩散模型的去噪循环了。根据之前采样调度器返回的采样时刻列表 <code>timesteps</code>，代码从中取出去噪时刻，对纯噪声输入迭代去噪。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_warmup_steps = <span class="built_in">len</span>(timesteps) - num_inference_steps * self.scheduler.order</span><br><span class="line">self._num_timesteps = <span class="built_in">len</span>(timesteps)</span><br><span class="line"><span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">    <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br></pre></td></tr></table></figure>
<p>去噪迭代的一开始，代码会根据是否要执行 CFG 来决定是否要把输入额外复制一份。这是因为做 CFG 时，我们需要把同一个输入过两次去噪模型，一次带约束，一次不带约束。为了简化这个流程，我们可以直接把输入复制一遍，这样只要过一次去噪模型就能得到两个输出了。下一行的 <code>scale_model_input</code> 是 Diffusers 的 API 要求，可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br></pre></td></tr></table></figure>
<p>接着，加了噪声、过了 VAE 解码器、没有乘系数的约束图像 <code>image_latents</code> 会与普通的噪声拼接到一起，作为模型的直接输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Concatenate image_latents over channels dimension</span></span><br><span class="line">latent_model_input = torch.cat([latent_model_input, image_latents], dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>准备好了所有输入后，代码调用 U-Net 对输入噪声图像去噪。输入包括直接输入 <code>latent_model_input</code>，去噪时刻 <code>t</code>，约束图像的 CLIP 嵌入 <code>image_embeddings</code>，三个额外约束的拼接 <code>added_time_ids</code>。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict the noise residual</span></span><br><span class="line">noise_pred = self.unet(</span><br><span class="line">    latent_model_input,</span><br><span class="line">    t,</span><br><span class="line">    encoder_hidden_states=image_embeddings,</span><br><span class="line">    added_time_ids=added_time_ids,</span><br><span class="line">    return_dict=<span class="literal">False</span>,</span><br><span class="line">)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>去噪结束后，代码根据公式做 CFG。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># perform guidance</span></span><br><span class="line"><span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_cond - noise_pred_uncond)</span><br></pre></td></tr></table></figure>
<p>有了去噪的输出 <code>noise_pred</code> 还不够，我们还需要用一些比较复杂的公式计算才能得到下一时刻的噪声图像。这一切都被 Diffusers 封装进调度器里了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">latents = self.scheduler.step(noise_pred, t, latents).prev_sample</span><br></pre></td></tr></table></figure>
<p>以上就是一步去噪迭代的主要内容。代码会反复执行去噪迭代。这后面除了下面这行会调用 VAE 解码器将隐空间的视频解码回真实视频外，没有其他重要代码了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frames = self.decode_latents(latents, num_frames, decode_chunk_size)</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们学习了图生视频模型 SVD 的模型结构和采样代码。整体上看，SVD 相较 SD 在模型上的修改不多，只是在原来的 2D 模块后面加了一些在时序维度上交互信息的卷积块和 Transformer 块。在学习时，我们应该着重关注 SVD 的采样流水线。SVD 使用拼接和交叉注意力两种方式添加了图像约束，并以与时刻编码相加的方式额外输入了三种约束信息。由于视频不同帧对于首帧的依赖情况不同，SVD 还使用了一种随帧号线性增长的 CFG 强度设置方式。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/en/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/en/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" rel="tag"># 扩散模型</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/en/2024/06/05/20240405-SVD/" rel="prev" title="Stable Video Diffusion 结构浅析与论文速览">
      <i class="fa fa-chevron-left"></i> Stable Video Diffusion 结构浅析与论文速览
    </a></div>
      <div class="post-nav-item">
    <a href="/en/2024/06/24/20240622-CVPR2024/" rel="next" title="顽抗生活中的厄运">
      顽抗生活中的厄运 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E9%87%87%E6%A0%B7%E5%AE%9E%E9%AA%8C"><span class="nav-number">1.</span> <span class="nav-text">简单采样实验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVD-%E6%A6%82%E8%A7%88"><span class="nav-number">2.</span> <span class="nav-text">SVD 概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%BB%E5%99%AA%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">去噪模型结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VAE-%E7%BB%93%E6%9E%84"><span class="nav-number">4.</span> <span class="nav-text">VAE 结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%87%E6%A0%B7%E6%B5%81%E6%B0%B4%E7%BA%BF"><span class="nav-number">5.</span> <span class="nav-text">采样流水线</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/en/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/en/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/en/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/en/lib/anime.min.js"></script>
  <script src="/en/lib/velocity/velocity.min.js"></script>
  <script src="/en/lib/velocity/velocity.ui.min.js"></script>

<script src="/en/js/utils.js"></script>

<script src="/en/js/motion.js"></script>


<script src="/en/js/schemes/muse.js"></script>


<script src="/en/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
