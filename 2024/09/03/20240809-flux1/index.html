<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/en/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/en/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/en/images/logo.svg" color="#222">

<link rel="stylesheet" href="/en/css/main.css">


<link rel="stylesheet" href="/en/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/en/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="前几个月，推出了著名文生图模型 Stable Diffusion 的 Stability AI 公司曝出了核心团队集体离职的消息。一时间，AI 从业者们议论纷纷，不知道这究竟是团队出现了矛盾，还是这些员工觉得文生图模型做下去没有前途了。而近期，该核心团队重新组建的创业团队 Black Forest Labs（黑森林实验室）带着名为 FLUX.1 的文生图模型「复仇归来」。FLUX.1 受到了用户的">
<meta property="og:type" content="article">
<meta property="og:title" content="Stable Diffusion 3「精神续作」FLUX.1 源码深度前瞻解读">
<meta property="og:url" content="https://zhouyifan.net/en/2024/09/03/20240809-flux1/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="前几个月，推出了著名文生图模型 Stable Diffusion 的 Stability AI 公司曝出了核心团队集体离职的消息。一时间，AI 从业者们议论纷纷，不知道这究竟是团队出现了矛盾，还是这些员工觉得文生图模型做下去没有前途了。而近期，该核心团队重新组建的创业团队 Black Forest Labs（黑森林实验室）带着名为 FLUX.1 的文生图模型「复仇归来」。FLUX.1 受到了用户的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/3.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/4.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/5.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/6.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/7.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/8.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/9.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/10.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/11.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/12.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/13.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/14.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/8.jpg">
<meta property="article:published_time" content="2024-09-03T06:41:19.000Z">
<meta property="article:modified_time" content="2024-09-03T06:41:19.445Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="编程">
<meta property="article:tag" content="扩散模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2024/09/03/20240809-flux1/1.jpg">

<link rel="canonical" href="https://zhouyifan.net/en/2024/09/03/20240809-flux1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Stable Diffusion 3「精神续作」FLUX.1 源码深度前瞻解读 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/en/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/en/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/en/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/en/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/en/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/en/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net" rel="section"><i class="fa fa-language fa-fw"></i>简体中文</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/09/03/20240809-flux1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Stable Diffusion 3「精神续作」FLUX.1 源码深度前瞻解读
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-09-03 14:41:19" itemprop="dateCreated datePublished" datetime="2024-09-03T14:41:19+08:00">2024-09-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>前几个月，推出了著名文生图模型 Stable Diffusion 的 Stability AI 公司曝出了核心团队集体离职的消息。一时间，AI 从业者们议论纷纷，不知道这究竟是团队出现了矛盾，还是这些员工觉得文生图模型做下去没有前途了。而近期，该核心团队重新组建的创业团队 Black Forest Labs（黑森林实验室）带着名为 FLUX.1 的文生图模型「复仇归来」。FLUX.1 受到了用户的广泛好评，让人们期盼更强开源文生图模型的热情得以延续。</p>
<p>Black Forest Labs 的成员基本上都是 Stable Diffusion 3 的作者，其中三名元老级成员还是 Stable Diffusion 论文的作者。同时，FLUX.1 也是一个在 Stable Diffusion 3 架构上做改进的模型。不管从哪个角度，FLUX.1 都称得上是Stable Diffusion 3 的「精神续作」。秉承着此前的开源精神，FLUX.1 也在上线之始就为社区开放了源代码和模型权重。不过，配套的技术论文并没能及时发布，想要了解 FLUX.1 技术细节的用户恐怕还得等上一阵子。为了尽快搞清楚 FLUX.1 相较 Stable Diffusion 3 做了哪些改进，我直接去细读了 FLUX.1 的源码。在这篇文章中，按照惯例，我将主要从源码层面上分享 FLUX.1 中已知的科研创新，做一个官方论文发布前的前瞻解读，而不会评测 FLUX.1 的图像生成效果。</p>
<p>具体来说，我会介绍 FLUX.1 中的以下改动：</p>
<ul>
<li>略微变动的图块化策略</li>
<li>不使用 Classifier-Free Guidance 的指引蒸馏</li>
<li>为不同分辨率图像调整流匹配噪声调度</li>
<li>用二维旋转式位置编码 (RoPE) 代替二维正弦位置编码</li>
<li>在原 Stable Diffusion 3 双流 Transformer 块后添加并行单流 Transformer 块</li>
</ul>
<p>我会先简单介绍 FLUX.1 的官方公告及 Diffusers 版使用示例，再按照我读代码的逻辑，从熟悉整个代码框架，到深究每一处创新的代码细节，最后分享我对于 FLUX.1 科研改进上的分析。对源码不感兴趣的读者，可以跳过通读代码框架章节，或者直接阅读感兴趣的那部分改动。想看省流版文章的读者，可以直接跳到结尾看总结。</p>
<p>建议读者在学习 Flux.1 前熟悉 Stable Diffusion 3。欢迎参考我之前写的文章：Stable Diffusion 3 论文及源码概览。</p>
<h2 id="模型简介与-Diffusers-示例脚本"><a href="#模型简介与-Diffusers-示例脚本" class="headerlink" title="模型简介与 Diffusers 示例脚本"></a>模型简介与 Diffusers 示例脚本</h2><p>在正式阅读源码前，我们先来看一下官方推文（<a target="_blank" rel="noopener" href="https://blackforestlabs.ai/announcing-black-forest-labs/">https://blackforestlabs.ai/announcing-black-forest-labs/</a> ）中有关 FLUX.1 的简介，并在 Diffusers 中跑通 FLUX.1 的图像生成示例脚本。</p>
<p>据官方介绍，FLUX.1 是一套文生图模型。它有三个变体（variant，可以理解成结构相似或相同，但权重不同的几个模型）：</p>
<ul>
<li>FLUX.1 [pro]: FLUX.1 系列的最强模型，只能通过付费的 API 或者在线平台使用。</li>
<li>FLUX.1 [dev]：FLUX.1 [pro] 的指引蒸馏（guidance-distilled）模型，质量与文本匹配度与原模型相近，运行时更高效。</li>
<li>FLUX.1 [schnell]：为本地开发和个人使用而裁剪过的本系列最快模型。据 Diffusers 中的文档介绍，这是一个 Timestep-distilled（时间戳蒸馏）的模型，因此仅需 1~4 步就可以完成生成。无法设置指引强度。</li>
</ul>
<p>官方对这些模型的详细介绍少之又少。FLUX.1 [dev] 用到的指引蒸馏技术似乎来自论文 <em>On Distillation of Guided Diffusion Models</em>，其目标是让模型直接学习 Classifier-Free Guidance (CFG) 的生成结果，使得模型一次输出之前要运行两次才能得到的指引生成结果，节约一半的运行时间。官方也没有讲 FLUX.1 [schnell] 的蒸馏细节，似乎它是从 FLUX.1 [dev] 中用扩散模型加速蒸馏手段得到的模型。因此，FLUX.1 [schnell] 不仅能一次输出有指引的生成结果，还能在极少的采样步数里完成生成。</p>
<p>官方推文中还说，FLUX.1 的生成神经网络基于 Stable Diffusion 3 的 MMDiT 架构和并行的 DiT 块，参数量扩大至 120 亿。生成模型是根据流匹配（flow matching）推导的扩散模型。为了提升性能与效率，模型新引入了旋转式位置编码 (RoPE) 和并行注意力层。</p>
<p>这段话这么长，还把并行注意力说了两遍，其实没有多少新东西。说白了，FLUX.1 就是在 Stable Diffusion 3 的基础上，加了 RoPE 和并行注意力层。官方推文到这里就没有其他有关模型细节的介绍了。FLUX.1 具体做了哪些改动，我们直接去源码里看。</p>
<p>FLUX.1 的官方仓库是 <a target="_blank" rel="noopener" href="https://github.com/black-forest-labs/flux">https://github.com/black-forest-labs/flux</a> 。相比 Stable Diffusion 那个臃肿杂乱的 generative-models 仓库，这个仓库的代码要简洁很多。不过，我还是推荐使用 Diffusers 框架来运行 FLUX.1。</p>
<p>Diffusers 中运行 FLUX.1 的官方文档为 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux">https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux</a> 。目前（2024 年 8 月 11 日），相关代码还在 Diffusers 的在线主分支里进行开发，并没有集成进 pip 版的 Diffusers 里。因此，要在 Diffusers 中使用 FLUX，必须要从源码安装 Diffusers：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/huggingface/diffusers.git</span><br><span class="line">cd diffusers</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure>
<p>安装完毕后，我们可以随便新建一个 python 脚本，填入以下的官方示例代码。在能够连通 Hugging Face 的环境中运行此脚本的话，脚本会自动下载模型并把生成结果保存在 <code>image.png</code> 中。注意，FLUX.1 的神经网络很大，显存占用极高，可能至少需要在 RTX 3090 同等级的显卡上运行。在示例代码中，我还改了一行，使用 <code>pipe.enable_sequential_cpu_offload()</code> 让模型把更多参数临时放到 CPU 上，避免显存不够。经测试，改了这一行后，FLUX.1 才勉强能在显存为 24G 的 RTX 3090 上运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> FluxPipeline</span><br><span class="line"></span><br><span class="line">pipe = FluxPipeline.from_pretrained(<span class="string">&quot;black-forest-labs/FLUX.1-schnell&quot;</span>, torch_dtype=torch.bfloat16)</span><br><span class="line"><span class="comment"># pipe.enable_model_cpu_offload()</span></span><br><span class="line">pipe.enable_sequential_cpu_offload()</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;A cat holding a sign that says hello world&quot;</span></span><br><span class="line">image = pipe(</span><br><span class="line">    prompt,</span><br><span class="line">    guidance_scale=<span class="number">0.0</span>,</span><br><span class="line">    num_inference_steps=<span class="number">4</span>,</span><br><span class="line">    max_sequence_length=<span class="number">256</span>,</span><br><span class="line">    height=<span class="number">1024</span>,</span><br><span class="line">    width=<span class="number">1024</span>,</span><br><span class="line">    generator=torch.Generator(<span class="string">&quot;cpu&quot;</span>).manual_seed(<span class="number">0</span>)</span><br><span class="line">).images[<span class="number">0</span>]</span><br><span class="line">image.save(<span class="string">&quot;image.png&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>由于随机数是固定的，运行后，我们应该总能得到这样的图片：</p>
<p><img src="/2024/09/03/20240809-flux1/1.jpg" alt></p>
<h2 id="通读代码框架"><a href="#通读代码框架" class="headerlink" title="通读代码框架"></a>通读代码框架</h2><p>由于开发还没有结束，在当前 Diffusers 的 FLUX.1 源码中，我们能看到各种潦草的写法及残缺不全的文档，这让读源码变成了一项颇具趣味的挑战性任务。让我们先看一下代码的整体框架，找出 FLUX.1 相较 Stable Diffusioni 3 在代码上的改动，再来详细分析这些创新。</p>
<p>和 Diffusers 中的其他生成模型一样，FLUX.1 的采样算法写在一个采样流水线类里。我们可以通过示例脚本里的 <code>FluxPipeline</code> 类跳转到定义该类的文件 <code>diffusers/pipelines/flux/pipeline_flux.py</code> 里。这个文件是从 Stable Diffusion 3 的采样流水线文件 <code>diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py</code> 改过来的，大部分文档都没有更新。我们可以用肉眼对比两份文件的区别。</p>
<p>先看构造函数。Stable Diffusion 3 用了三个文本编码器，<code>clip-vit-large-patch14</code>, <code>CLIP-ViT-bigG-14-laion2B-39B-b160k</code>, <code>t5-v1_1-xxl</code>，而 FLUX.1 没有用第二个 CLIP 编码器，只用了另外两个文本编码器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StableDiffusion3Pipeline</span>(<span class="params">DiffusionPipeline, SD3LoraLoaderMixin, FromSingleFileMixin</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        transformer: SD3Transformer2DModel,</span></span></span><br><span class="line"><span class="params"><span class="function">        scheduler: FlowMatchEulerDiscreteScheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">        vae: AutoencoderKL,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder: CLIPTextModelWithProjection,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder_2: CLIPTextModelWithProjection,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer_2: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder_3: T5EncoderModel,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer_3: T5TokenizerFast,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FluxPipeline</span>(<span class="params">DiffusionPipeline, FluxLoraLoaderMixin</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        scheduler: FlowMatchEulerDiscreteScheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">        vae: AutoencoderKL,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder: CLIPTextModel,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder_2: T5EncoderModel,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer_2: T5TokenizerFast,</span></span></span><br><span class="line"><span class="params"><span class="function">        transformer: FluxTransformer2DModel,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br></pre></td></tr></table></figure>
<p>再往下翻，我们能用火眼金睛发现 FLUX.1 的 VAE 压缩比是 16，是所有版本的 Stable Diffusion VAE 压缩比的两倍。这是为什么呢？不是增加压缩比会让 VAE 重建效果下降吗？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SD3</span></span><br><span class="line">self.vae_scale_factor = (</span><br><span class="line">    <span class="number">2</span> ** (<span class="built_in">len</span>(self.vae.config.block_out_channels) - <span class="number">1</span>) </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;vae&quot;</span>) <span class="keyword">and</span> self.vae <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">8</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FLUX.1</span></span><br><span class="line">self.vae_scale_factor = (</span><br><span class="line">    <span class="number">2</span> ** (<span class="built_in">len</span>(self.vae.config.block_out_channels)) </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;vae&quot;</span>) <span class="keyword">and</span> self.vae <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">16</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>查看周围其他代码，我们能找到 <code>_pack_latents</code>，<code>_unpack_latents</code> 这两个方法。<code>_pack_latents</code> 其实就是一个图块化操作，它能把 $2 \times 2$ 个像素在通道维度上拼接到一起，而 <code>_unpack_latents</code> 是该操作的逆操作。原来，代码把图块化的两倍压缩比也算进 VAE 里了。这里直接把 <code>vae_scale_factor</code> 乘个 2 是一种非常差，歧义性极强的写法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_pack_latents</span>(<span class="params">latents, batch_size, num_channels_latents, height, width</span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_unpack_latents</span>(<span class="params">latents, height, width, vae_scale_factor</span>):</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>相比 SD3, FLUX.1 将图块化操作写在了去噪网络外面。因此，SD3 的去噪网络的输入通道数是 16，和 VAE 的隐空间通道数相同；而 FLUX.1 由于把 $2 \times 2$ 个像素在通道上拼接到了一起，其去噪网络的输入通道数是 64。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;SD3Transformer2DModel&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;in_channels&quot;</span>: <span class="number">16</span>,</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;FluxTransformer2DModel&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;in_channels&quot;</span>: <span class="number">64</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>再来看采样主方法 <code>__call__</code>。先看一下它的主要参数。相比之下，FLUX.1 少了一组提示词，且没有负面提示词。少一组提示词是因为少用了一个文本编码器。而没有负面提示词是因为该模型是指引蒸馏过的，在文本指引上没那么灵活。我们稍后还会看到 FLUX.1 具体是怎么利用文本指引的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SD3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt_2: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt_3: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">28</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">List</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    guidance_scale: <span class="built_in">float</span> = <span class="number">7.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt_2: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt_3: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"># <span class="title">FLUX</span>.1</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt_2: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">28</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">List</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    guidance_scale: <span class="built_in">float</span> = <span class="number">7.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<p>之后的内容都与其他扩散模型流水线一样，代码会判断输入是否合法、给输入文本编码、随机生成初始化噪声。值得关注的是初始化噪声采样器前的一段新内容：代码会算一个 <code>mu</code>，并传进 <code>retrieve_timesteps</code> 里。这个变量最后会传到流匹配采样算法里。我们先把该改动记在心里，不看细节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mu = calculate_shift(</span><br><span class="line">    image_seq_len,</span><br><span class="line">    self.scheduler.config.base_image_seq_len,</span><br><span class="line">    self.scheduler.config.max_image_seq_len,</span><br><span class="line">    self.scheduler.config.base_shift,</span><br><span class="line">    self.scheduler.config.max_shift,</span><br><span class="line">)</span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(</span><br><span class="line">    self.scheduler,</span><br><span class="line">    num_inference_steps,</span><br><span class="line">    device,</span><br><span class="line">    timesteps,</span><br><span class="line">    sigmas,</span><br><span class="line">    mu=mu,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在去噪循环部分，FLUX.1 没有做 Classifier-Free Guidance (CFG)，而是把指引强度 <code>guidance</code> 当成了一个和时刻 <code>t</code> 一样的约束信息，传入去噪模型 <code>transformer</code> 中。CFG 的本意是过两遍去噪模型，一次输入为空文本，另一次输入为给定文本，让模型的输出远离空文本，靠近给定文本。而负面提示词只是一种基于 CFG 的技巧。把 CFG 里的空文本换成负面文本，就能让结果背离负面文本。但现在这个模型是一个指引蒸馏模型，指引强度会作为一个变量输入模型，固定地表示输入文本和空文本间的差距。因此，我们就不能在这个模型里把空文本换成负面文本了。</p>
<p>除了指引方式上的变动外，FLUX.1 的去噪网络还多了 <code>txt_ids</code> 和 <code>img_ids</code> 这两个输入。我们待会来看它们的细节。</p>
<p>FLUX.1 的去噪网络和 SD3 的一样，除了输入完整文本嵌入 <code>prompt_embeds</code> 外，依然会将池化过的短文本嵌入 <code>pooled_prompt_embeds</code> 输入进模型。我们现在可以猜测，FLUX.1 使用了和 SD3 类似的文本约束机制，输入了两类文本约束信息。</p>
<p>代码里的 <code>/1000</code> 是临时代码。之后所有涉及乘除 1000 的代码全可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br><span class="line">    timestep = t.expand(latents.shape[<span class="number">0</span>]).to(latents.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># handle guidance</span></span><br><span class="line">    <span class="keyword">if</span> self.transformer.config.guidance_embeds:</span><br><span class="line">        guidance = torch.tensor([guidance_scale], device=device)</span><br><span class="line">        guidance = guidance.expand(latents.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        guidance = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    noise_pred = self.transformer(</span><br><span class="line">        hidden_states=latents,</span><br><span class="line">        timestep=timestep / <span class="number">1000</span>,</span><br><span class="line">        guidance=guidance,</span><br><span class="line">        pooled_projections=pooled_prompt_embeds,</span><br><span class="line">        encoder_hidden_states=prompt_embeds,</span><br><span class="line">        txt_ids=text_ids,</span><br><span class="line">        img_ids=latent_image_ids,</span><br><span class="line">        joint_attention_kwargs=self.joint_attention_kwargs,</span><br><span class="line">        return_dict=<span class="literal">False</span>,</span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">    latents_dtype = latents.dtype</span><br><span class="line">    latents = self.scheduler.step(noise_pred, t, latents, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>采样流水线最后会将隐空间图片解码。如前所述，由于现在图块化和反图块化是在去噪网络外面做的，这里隐空间图片在过 VAE 解码之前做了一次反图块化操作 <code>_unpack_latents</code>。对应的图块化操作是在之前随机生成初始噪声的 <code>prepare_latents</code> 方法里做的，为了节约时间我们就不去看了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> output_type == <span class="string">&quot;latent&quot;</span>:</span><br><span class="line">    image = latents</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)</span><br><span class="line">    latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor</span><br><span class="line">    image = self.vae.decode(latents, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">    image = self.image_processor.postprocess(image, output_type=output_type)</span><br></pre></td></tr></table></figure>
<p>接下来，我们再简单看一下去噪网络的结构。在采样流水线里找到对应类 <code>FluxTransformer2DModel</code>，我们能用代码跳转功能定位到文件 <code>diffusers/models/transformers/transformer_flux.py</code>。SD3 去噪网络类是 <code>SD3Transformer2DModel</code>，它位于文件 <code>diffusers/models/transformers/transformer_sd3.py</code>。</p>
<p>同样，我们先对比类的构造函数。构造函数的新参数我们暂时读不懂，所以直接跳到构造函数内部。</p>
<p>在使用位置编码时，SD3 用了二维位置编码类 <code>PatchEmbed</code>。该类会先对图像做图块化，再设置位置编码。 而 FLUX.1 的位置编码类叫 <code>EmbedND</code>。从官方简介以及参数里的单词 <code>rope</code> 中，我们能猜出这是一个旋转式位置编码 (RoPE)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SD3</span></span><br><span class="line">self.pos_embed = PatchEmbed(</span><br><span class="line">    height=self.config.sample_size,</span><br><span class="line">    width=self.config.sample_size,</span><br><span class="line">    patch_size=self.config.patch_size,</span><br><span class="line">    in_channels=self.config.in_channels,</span><br><span class="line">    embed_dim=self.inner_dim,</span><br><span class="line">    pos_embed_max_size=pos_embed_max_size,  <span class="comment"># hard-code for now.</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FLUX.1</span></span><br><span class="line">self.pos_embed = EmbedND(dim=self.inner_dim, theta=<span class="number">10000</span>, axes_dim=axes_dims_rope)</span><br></pre></td></tr></table></figure>
<p>再往下看，FLUX.1 的文本嵌入类有两种选择。不设置 <code>guidance_embeds</code> 的话，这个类就是 <code>CombinedTimestepTextProjEmbeddings</code>，和 SD3 的一样。这说明正如我们前面猜想的，FLUX.1 用了和 SD3 一样的额外文本约束机制，将一个池化过的文本嵌入约束加到了文本嵌入上。</p>
<p>设置 <code>guidance_embeds</code> 的话，<code>CombinedTimestepGuidanceTextProjEmbeddings</code> 类应该就得额外处理指引强度了。我们待会来看这个类是怎么工作的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">text_time_guidance_cls = (</span><br><span class="line">    CombinedTimestepGuidanceTextProjEmbeddings <span class="keyword">if</span> guidance_embeds <span class="keyword">else</span> CombinedTimestepTextProjEmbeddings</span><br><span class="line">)</span><br><span class="line">self.time_text_embed = text_time_guidance_cls(</span><br><span class="line">    embedding_dim=self.inner_dim, pooled_projection_dim=self.config.pooled_projection_dim</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>之后函数定义了两个线性层。<code>context_embedder</code> 在 SD3 里也有，是用来处理文本嵌入的。但神秘的 <code>x_embedder</code> 又是什么呢？可能得在其他函数里才能知道了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.context_embedder = nn.Linear(self.config.joint_attention_dim, self.inner_dim)</span><br><span class="line">self.x_embedder = torch.nn.Linear(self.config.in_channels, self.inner_dim)</span><br></pre></td></tr></table></figure>
<p>函数的末尾定义了两个模块列表。相比只有一种 Transformer 块的 SD3，FLUX.1 用了两种结构不同的 Transformer 块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">self.transformer_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        FluxTransformerBlock(...)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.num_layers)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.single_transformer_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        FluxSingleTransformerBlock(...)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.num_single_layers)</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们再来看 <code>forward</code> 方法，看看之前看构造函数时留下的问题能不能得到解答。</p>
<p><code>forward</code> 里首先是用 <code>x_embedder</code> 处理了一下输入。原本在 SD3 中，输入图像会在 <code>pos_embed</code> 里过一个下采样两倍的卷积层，同时完成图块化和修改通道数两件事。而现在 FLUX.1 的图块化写在外面了，所以这里只需要用一个普通线性层 <code>x_embedder</code> 处理一下输入通道数就行了。这样说来，变量名有个 <code>x</code> 估计是因为神经网络的输入名通常叫做 <code>x</code>。既然这样，把它叫做 <code>input_embedder</code> 不好吗？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SD3</span></span><br><span class="line">hidden_states = self.pos_embed(hidden_states)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FLUX.1</span></span><br><span class="line">hidden_states = self.x_embedder(hidden_states)</span><br></pre></td></tr></table></figure>
<p>下一步是求时刻编码。这段逻辑是说，如果模型输入了指引强度，就把指引强度当成一个额外的实数约束，将其编码加到时刻编码上。具体细节都在 <code>time_text_embed</code> 的类里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">timestep = timestep.to(hidden_states.dtype) * <span class="number">1000</span></span><br><span class="line"><span class="keyword">if</span> guidance <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    guidance = guidance.to(hidden_states.dtype) * <span class="number">1000</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    guidance = <span class="literal">None</span></span><br><span class="line">temb = (</span><br><span class="line">    self.time_text_embed(timestep, pooled_projections)</span><br><span class="line">    <span class="keyword">if</span> guidance <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span> self.time_text_embed(timestep, guidance, pooled_projections)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下一行是常规的修改约束文本嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder_hidden_states = self.context_embedder(encoder_hidden_states)</span><br></pre></td></tr></table></figure>
<p>再之后的两行出现了一个新操作。输入的 <code>txt_ids</code> 和 <code>img_ids</code> 拼接到了一起，构成了 <code>ids</code>，作为旋转式位置编码的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">image_rotary_emb = self.pos_embed(ids)</span><br></pre></td></tr></table></figure>
<p>此后图像信息 <code>hidden_states</code> 和文本信息 <code>encoder_hidden_states</code> 会反复输入进第一类 Transformer 块里。和之前相比，模块多了一个旋转式位置编码输入 <code>image_rotary_emb</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoder_hidden_states, hidden_states = block(</span><br><span class="line">    hidden_states=hidden_states,</span><br><span class="line">    encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">    temb=temb,</span><br><span class="line">    image_rotary_emb=image_rotary_emb,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>本来过了这些块后，SD3 会直接会直接返回 <code>hidden_states</code> 经后处理后的信息。而 FLUX.1 在过完第一类 Transformer 块后，将图像和文本信息拼接，又输入了第二类 Transformer 块中。第二类 Transformer 块的输出才是最终输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span>:</span><br><span class="line">    hidden_states = block(</span><br><span class="line">        hidden_states=hidden_states,</span><br><span class="line">        temb=temb,</span><br><span class="line">        image_rotary_emb=image_rotary_emb,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">hidden_states = hidden_states[:, encoder_hidden_states.shape[<span class="number">1</span>] :, ...]</span><br></pre></td></tr></table></figure>
<p>到这里，我们就把 FLUX.1 的代码结构过了一遍。我们发现，FLUX.1 是一个基于 SD3 开发的模型。它在图块化策略、噪声调度器输入、位置编码类型、Transformer 块类型上略有改动。且由于开源的 FLUX.1 是指引蒸馏过的，该模型无法使用 CFG。[dev] 版可以以实数约束的方式设置指引强度，而 [schnell] 版无法设置指引强度。</p>
<p>在这次阅读中，我们已经弄懂了以下细节：</p>
<ul>
<li>采样流水线会在去噪网络外面以通道堆叠的方式实现图块化。</li>
<li>指引强度不是以 CFG 的形式写在流水线里，而是以约束的形式输入进了去噪网络。</li>
</ul>
<p>我们还留下了一些未解之谜：</p>
<ul>
<li>输入进噪声采样器的 <code>mu</code> 是什么？</li>
<li>决定旋转式位置编码的 <code>txt_ids</code> 和 <code>img_ids</code> 是什么？</li>
<li>旋转式位置编码在网络里的实现细节是什么？</li>
<li>新的那种 Transformer 块的结构是怎么样的？</li>
</ul>
<p>针对这些问题，我们来细读代码。</p>
<h2 id="调整流匹配标准差"><a href="#调整流匹配标准差" class="headerlink" title="调整流匹配标准差"></a>调整流匹配标准差</h2><p>在采样流水线里，我们见到了这样一个神秘变量 <code>mu</code>。从名字中，我们猜测这是一个表示正态分布均值的变量，用来平移 (shift) 某些量的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mu = calculate_shift(...)</span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(</span><br><span class="line">    ...</span><br><span class="line">    mu=mu,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们先看 <code>calculate_shift</code> 做了什么。第一个参数 <code>image_seq_len</code> 表示图像 token 数，可以认为是函数的自变量 <code>x</code>。后面四个参数其实定义了一条直线。我们可以认为 <code>base_seq_len</code> 是 <code>x1</code>, <code>max_seq_len</code> 是 <code>x2</code>，<code>base_shift</code> 是 <code>y1</code>，<code>max_shift</code> 是 <code>y2</code>。根据这两个点的坐标就可以解出一条直线方程出来。也就是说，<code>calculate_shift</code> 会根据模型允许的最大 token 数 4096 ($64 \times 64$) 和最小 token 数 256 ($16 \times 16$)，把当前的输入 token 数线性映射到 0.5 ~ 1.16 之间。但我们暂时不知道输出 <code>mu</code> 的意义是什么。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_shift</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    image_seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">    base_seq_len: <span class="built_in">int</span> = <span class="number">256</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_seq_len: <span class="built_in">int</span> = <span class="number">4096</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    base_shift: <span class="built_in">float</span> = <span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_shift: <span class="built_in">float</span> = <span class="number">1.16</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)</span><br><span class="line">    b = base_shift - m * base_seq_len</span><br><span class="line">    mu = image_seq_len * m + b</span><br><span class="line">    <span class="keyword">return</span> mu</span><br></pre></td></tr></table></figure>
<p>再追踪进调用了 <code>mu</code> 的 <code>retrieve_timesteps</code> 函数里，我们发现 <code>mu</code> 并不在参数表中，而是在 <code>kwargs</code> 里被传递给了噪声迭代器的 <code>set_timesteps</code> 方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieve_timesteps</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    scheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    device: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, torch.device]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">int</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    sigmas: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">float</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    **kwargs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">...</span><br><span class="line">scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)</span><br></pre></td></tr></table></figure></p>
<p>根据流水线构造函数里的类名，我们能找到位于 <code>diffusers/schedulers/scheduling_flow_match_euler_discrete.py</code> 调度器类 <code>FlowMatchEulerDiscreteScheduler</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    scheduler: FlowMatchEulerDiscreteScheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">    ...</span>)</span></span><br></pre></td></tr></table></figure></p>
<p>再找到类的 <code>set_timesteps</code> 方法。<code>set_timesteps</code> 一般是用来设置推理步数 <code>num_inference_steps</code> 的。有些调度器还会在总推理步数确定后，初始化一些其他变量。比如这里的流匹配调度器，会在这个方法里初始化变量 <code>sigmas</code>。我们可以忽略这背后的原理，仅从代码上看，输入 <code>mu</code> 会通过 <code>time_shift</code> 修改 <code>sigmas</code> 的值。</p>
<blockquote>
<p>这里的变量命名又乱七八糟，输入 <code>time_shift</code> 的 <code>sigmas</code> 是第三个参数，而在 <code>time_shift</code> 里的 <code>sigmas</code> 是除了 <code>self</code> 以外的第二个参数。这是因为 Diffusers 在移植官方代码时没有取好变量名。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_shift</span>(<span class="params">self, mu: <span class="built_in">float</span>, sigma: <span class="built_in">float</span>, t: torch.Tensor</span>):</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(mu) / (math.exp(mu) + (<span class="number">1</span> / t - <span class="number">1</span>) ** sigma)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_timesteps</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    device: <span class="type">Union</span>[<span class="built_in">str</span>, torch.device] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    sigmas: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">float</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    mu: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    <span class="keyword">if</span> self.config.use_dynamic_shifting:</span><br><span class="line">        sigmas = self.time_shift(mu, <span class="number">1.0</span>, sigmas)</span><br></pre></td></tr></table></figure>
<p>我们再跑出去看一下流水线里输入的 <code>sigmas</code> 是什么。假设总采样步数为 $T$，则 <code>sigmas</code> 是 $1$ 到 $\frac{1}{T}$ 间均匀采样的 $T$ 个实数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sigmas = np.linspace(<span class="number">1.0</span>, <span class="number">1</span> / num_inference_steps, num_inference_steps)</span><br></pre></td></tr></table></figure>
<p>现在要解读 <code>mu</code> 的作用就很容易了。假设 <code>sigmas</code> 是下标和值构成的点，我们可以测试 <code>mu</code> 不同的情况下， <code>sigmas</code> 经过 <code>time_shift</code> 函数形成的曲线图。 </p>
<p><img src="/2024/09/03/20240809-flux1/2.jpg" alt></p>
<p>可以看出，<code>mu=0</code>则不修改曲线。随着 <code>mu</code> 增大，曲线逐渐上凸。</p>
<p>我对流匹配的具体细节不是很懂，只能大概猜测 <code>mu</code> 的作用。流匹配中，图像沿着某条路线从纯噪声运动到训练集中，标准差 sigma 用于控制不同时刻图像的不确定性。时刻为 0 时，图像为纯噪声，标准差为 1； 时刻为 1 时，图像为生成集合中的图像，标准差要尽可能趋于 0。对于中间时刻，标准差默认按照时刻线性变化。而 <code>mu</code> 是一个 0.5 ~ 1.16 之间的数，可能控制的是中间时刻的噪声均值。图像分辨率越大，token 越多，<code>mu</code> 越大，要加的噪声越重。这也符合之前 Stable Diffusion 3 论文在 <em>Resolution-dependent shifting of timestep schedules</em> 小节里的设计，对于分辨率越高的图像，需要加更多噪声来摧毁原图像的信号。总之，这个 <code>mu</code> 可能是训练的时候加的，用于给高分辨率图像加更多噪声，推理时也不得不带上这个变量。</p>
<p>FLUX.1 官方仓库对应部分是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lin_function</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    x1: <span class="built_in">float</span> = <span class="number">256</span>, y1: <span class="built_in">float</span> = <span class="number">0.5</span>, x2: <span class="built_in">float</span> = <span class="number">4096</span>, y2: <span class="built_in">float</span> = <span class="number">1.15</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">Callable</span>[[<span class="built_in">float</span>], <span class="built_in">float</span>]:</span></span><br><span class="line">    m = (y2 - y1) / (x2 - x1)</span><br><span class="line">    b = y1 - m * x1</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> x: m * x + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_schedule</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    num_steps: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    image_seq_len: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    base_shift: <span class="built_in">float</span> = <span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_shift: <span class="built_in">float</span> = <span class="number">1.15</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    shift: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">float</span>]:</span></span><br><span class="line">    <span class="comment"># extra step for zero</span></span><br><span class="line">    timesteps = torch.linspace(<span class="number">1</span>, <span class="number">0</span>, num_steps + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shifting the schedule to favor high timesteps for higher signal images</span></span><br><span class="line">    <span class="keyword">if</span> shift:</span><br><span class="line">        <span class="comment"># eastimate mu based on linear estimation between two points</span></span><br><span class="line">        mu = get_lin_function(y1=base_shift, y2=max_shift)(image_seq_len)</span><br><span class="line">        timesteps = time_shift(mu, <span class="number">1.0</span>, timesteps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> timesteps.tolist()</span><br></pre></td></tr></table></figure>
<p><code>mu</code> 的作用确实和高信号图片有关。但他们的设计初衷是偏移时间戳，而不是根据某种公式修改 sigma。比如原来去噪迭代 0~500 步就表示 t=0 到 t=0.5，偏移时间戳后，0~500 步就变成了 t=0 到 t=0.3。偏移时间戳使得模型能够把更多精力学习对如何对高噪声的图像去噪。</p>
<h2 id="使用单流并行注意力层的-Transformer-架构"><a href="#使用单流并行注意力层的-Transformer-架构" class="headerlink" title="使用单流并行注意力层的 Transformer 架构"></a>使用单流并行注意力层的 Transformer 架构</h2><p>接下来的问题都和 FLUX.1 的新 Transformer 架构相关。我们先把整个网络架构弄懂，再去看旋转式位置编码的细节。</p>
<p>为了理清网络架构，我们来根据已知信息，逐步完善网络的模块图。首先，我们先粗略地画一个 Transformer 结构，定义好输入输出。相比 SD3，FLUX.1 多了指引强度和编号集 <code>txt_ids</code>, <code>img_ids</code>这两类输入。</p>
<p><img src="/2024/09/03/20240809-flux1/3.jpg" alt></p>
<p>接下来，我们把和 SD3 相似的结构画进来。所有 Transformer 块都是那种同时处理两类 token 的双流注意力块。输入文本的 T5 嵌入会作为文本支流进入主模型。输入文本的 CLIP 嵌入会经池化与MLP，与经过了位置编码和 MLP 的时刻编码加到一起。时刻编码会以 AdaLayerNorm 的方式修改所有层的数据规模，以及数据在输出前的尺寸与均值。</p>
<p><img src="/2024/09/03/20240809-flux1/4.jpg" alt></p>
<p>在 <code>CombinedTimestepGuidanceTextProjEmbeddings</code> 类中，我们能知道小文本嵌入、时刻嵌入、指引嵌入是怎么加到一起的。我们主要关心指引嵌入的有关操作。由于指引强度 <code>guidance</code> 和时刻 <code>timestep</code> 都是实数，所以 <code>guidance_emb</code> 的处理方式与 <code>timesteps_emb</code> 一模一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CombinedTimestepGuidanceTextProjEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim, pooled_projection_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.time_proj = Timesteps(num_channels=<span class="number">256</span>, flip_sin_to_cos=<span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">        self.timestep_embedder = TimestepEmbedding(in_channels=<span class="number">256</span>, time_embed_dim=embedding_dim)</span><br><span class="line">        self.guidance_embedder = TimestepEmbedding(in_channels=<span class="number">256</span>, time_embed_dim=embedding_dim)</span><br><span class="line">        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn=<span class="string">&quot;silu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, timestep, guidance, pooled_projection</span>):</span></span><br><span class="line">        timesteps_proj = self.time_proj(timestep)</span><br><span class="line">        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=pooled_projection.dtype))  <span class="comment"># (N, D)</span></span><br><span class="line"></span><br><span class="line">        guidance_proj = self.time_proj(guidance)</span><br><span class="line">        guidance_emb = self.guidance_embedder(guidance_proj.to(dtype=pooled_projection.dtype))  <span class="comment"># (N, D)</span></span><br><span class="line"></span><br><span class="line">        time_guidance_emb = timesteps_emb + guidance_emb</span><br><span class="line"></span><br><span class="line">        pooled_projections = self.text_embedder(pooled_projection)</span><br><span class="line">        conditioning = time_guidance_emb + pooled_projections</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> conditioning</span><br></pre></td></tr></table></figure>
<p><img src="/2024/09/03/20240809-flux1/5.jpg" alt></p>
<p>在去噪模型 <code>FluxTransformer2DModel</code> 的 <code>forward</code> 方法中，原先的图块化及二维位置编码模块被一个简单的线性层 <code>x_embedder</code> 取代了，现在的位置编码 <code>image_rotary_emb</code> 会输入进所有层中，而不是一开始和输入加在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">hidden_states, ...</span>):</span></span><br><span class="line">    hidden_states = self.x_embedder(hidden_states)</span><br><span class="line">    ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">    image_rotary_emb = self.pos_embed(ids)</span><br></pre></td></tr></table></figure>
<p><img src="/2024/09/03/20240809-flux1/6.jpg" alt></p>
<p>之后，除了过 MM-DiT 块以外，文本信息还会和图像信息融合在一起，过若干个单流 Transformer 块。过了这些模块后，原来文本 token 那部分会被丢弃。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.transformer_blocks):</span><br><span class="line">    encoder_hidden_states, hidden_states = block(</span><br><span class="line">        hidden_states=hidden_states,</span><br><span class="line">        encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">        temb=temb,</span><br><span class="line">        image_rotary_emb=image_rotary_emb,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.single_transformer_blocks):</span><br><span class="line">    hidden_states = block(</span><br><span class="line">        hidden_states=hidden_states,</span><br><span class="line">        temb=temb,</span><br><span class="line">        image_rotary_emb=image_rotary_emb,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">hidden_states = hidden_states[:, encoder_hidden_states.shape[<span class="number">1</span>] :, ...]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2024/09/03/20240809-flux1/7.jpg" alt></p>
<p>我们已经画完了去噪模型的结构，最后把 VAE 部分加上就比较完美了。</p>
<p><img src="/2024/09/03/20240809-flux1/8.jpg" alt></p>
<p>多数模块的细节都可以在 SD3 的论文里找到，除了 RoPE 和单流 DiT 块。我们在这一节里再仔细学习一下单流 DiT 块的结构。</p>
<p>根据官方介绍，FLUX.1 的 Transformer 里用到了并行 Transformer。准确来说，FLUX.1 仅在最后的单流 DiT 块里用到了并行注意力层。并行注意力层是在文章 <em>Scaling Vision Transformers to 22 Billion Parameters</em> 中提出的。如下图所示，这项技术很好理解，只不过是把注意力和线性层之间的串联结构变成并联结构。这样的好处是，由于数据在过注意力层前后本身就要各过一次线性层，在并联后，这些线性层和 MLP 可以融合。这样的话，由于计算的并行度更高，模型的运行效率会高上一些。</p>
<p>顺带一提，在 Q, K 后做归一化以提升训练稳定性也是在这篇文章里提出的。SD3 和 FLUX.1 同样用了这一设计，但用的是 RMSNorm 而不是 LayerNorm。</p>
<p><img src="/2024/09/03/20240809-flux1/9.jpg" alt></p>
<p>我们可以在 <code>FluxSingleTransformerBlock</code> 类里找到相关实现。代码不长，我们可以一次性读完。相比上面的示意图，Q, K, V 的投影操作被单独放进了 <code>Attention</code> 类里，并没有和第一个线性层融合。而做了注意力操作后，Att-out 和 MLP-out 确实是放在一起做的。<code>attn_output</code> 和 <code>mlp_hidden_states</code> 拼接了起来，一起过了 <code>proj_out</code>。此外，这里的归一化层还是 DiT 里的 AdaLN，模块能接收时刻编码的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FluxSingleTransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_attention_heads, attention_head_dim, mlp_ratio=<span class="number">4.0</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line"></span><br><span class="line">        self.norm = AdaLayerNormZeroSingle(dim)</span><br><span class="line">        self.proj_mlp = nn.Linear(dim, self.mlp_hidden_dim)</span><br><span class="line">        self.act_mlp = nn.GELU(approximate=<span class="string">&quot;tanh&quot;</span>)</span><br><span class="line">        self.proj_out = nn.Linear(dim + self.mlp_hidden_dim, dim)</span><br><span class="line"></span><br><span class="line">        processor = FluxSingleAttnProcessor2_0()</span><br><span class="line">        self.attn = Attention(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        temb: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        image_rotary_emb=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        residual = hidden_states</span><br><span class="line">        norm_hidden_states, gate = self.norm(hidden_states, emb=temb)</span><br><span class="line">        mlp_hidden_states = self.act_mlp(self.proj_mlp(norm_hidden_states))</span><br><span class="line"></span><br><span class="line">        attn_output = self.attn(</span><br><span class="line">            hidden_states=norm_hidden_states,</span><br><span class="line">            image_rotary_emb=image_rotary_emb,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        hidden_states = torch.cat([attn_output, mlp_hidden_states], dim=<span class="number">2</span>)</span><br><span class="line">        gate = gate.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        hidden_states = gate * self.proj_out(hidden_states)</span><br><span class="line">        hidden_states = residual + hidden_states</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>此处具体的注意力运算写在 <code>FluxSingleAttnProcessor2_0</code> 类里。跳过前面繁杂的形状变换操作，我们来看该注意力运算的关键部分。在做完了标准注意力运算 <code>scaled_dot_product_attention</code> 后，一般要调用 <code>attn.to_out[0](hidden_states)</code> 对数据做一次投影变换。但是，在这个注意力运算中，并没有对应的操作。这表明该模块确实是照着并行注意力层设计的，离开注意力的投影与 MLP 的第二个线性层融合到了一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    attn: Attention,</span></span></span><br><span class="line"><span class="params"><span class="function">    hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    image_rotary_emb: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; torch.Tensor:</span></span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> image_rotary_emb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        query, key = apply_rope(query, key, image_rotary_emb)</span><br><span class="line"></span><br><span class="line">    hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=<span class="number">0.0</span>, is_causal=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    hidden_states = hidden_states.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, attn.heads * head_dim)</span><br><span class="line">    hidden_states = hidden_states.to(query.dtype)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h2 id="旋转式位置编码思想及-FLUX-1-实现"><a href="#旋转式位置编码思想及-FLUX-1-实现" class="headerlink" title="旋转式位置编码思想及 FLUX.1 实现"></a>旋转式位置编码思想及 FLUX.1 实现</h2><p>旋转式位置编码是苏剑林在 <em>RoFormer: Enhanced Transformer with Rotary Position Embedding</em> 中提出的一种专门为注意力计算设计的位置编码。在这篇文章中，我们来简单地了解一下旋转式位置编码的设计思想，为学习 FLUX.1 的结构做准备。</p>
<blockquote>
<p>想深究旋转式位置编码的读者可以去阅读苏剑林的博文，先阅读《让研究人员绞尽脑汁的Transformer位置编码》（<a target="_blank" rel="noopener" href="https://kexue.fm/archives/8130）">https://kexue.fm/archives/8130）</a> 了解该怎么设计位置编码，再阅读《Transformer升级之路：2、博采众长的旋转式式位置编码》（<a target="_blank" rel="noopener" href="https://kexue.fm/archives/8265）">https://kexue.fm/archives/8265）</a> 了解旋转式位置编码的细节。</p>
</blockquote>
<p>Transformer 仅包括注意力和全连接两种运算，这两种运算都是和位置无关的。为了让 Transformer 知道词语的前后关系，或者像素间的空间关系，就要给 Transformer 中的 token 注入某种位置信息。然而，仅仅告诉每个 token 它的<strong>绝对位置</strong>是不够好的，这样做最明显的缺点是模型无法处理训练时没有见过的长序列。比如训练集里最长的句子是 512 个 token，如果输入 600 个 token，由于模型没有见过编号超过 512 的位置编码，就不能很好地处理 512 号以后的 token。因此，我们不仅希望每个 token 知道自己的绝对位置，还希望 token 能从位置编码里知道<strong>相对位置</strong>的信息。</p>
<p>在提出 Transfomer 的论文中，作者给出了如下的一套正弦位置编码方案。这也是多数工作默认使用的位置编码方式。为了简化表示，我们假设输入 token 是一个二维向量，这样，每个 token 需要的位置编码也是一个二维向量。</p>
<script type="math/tex; mode=display">
PE(k) = (cos(\frac{k}{10000}), sin(\frac{k}{10000}))</script><p>其中，$k$ 表示第 $k$ 个 token。这样做的好处是，根据三角函数和角公式，位置编码之间可以用线性组合来表示，这种编码蕴含了一定的相对位置信息。</p>
<script type="math/tex; mode=display">
\begin{aligned}
sin(a+b) = sina \cdot cosb + cosa \cdot sinb \\
cos(a+b) = cosa \cdot cosb - sina \cdot sinb
\end{aligned}</script><p>当我们要把二维向量拓展成 $d$ 维向量时，只需要把 $d$ 维两两打包成一组，每组用不同周期的正弦函数即可。因此，在后文中，我们也不讨论 $d$ 维的 token，只需要搞明白二维的 token 该怎么编码就行。</p>
<script type="math/tex; mode=display">
PE(k, 2i) = sin(\frac{k}{10000^{2i/d}}) \\
PE(k, 2i+1) = cos(\frac{k}{10000^{2i/d}})</script><p>尽管正弦编码能表示一定的相对信息，但是，由于位置编码之间是线性关系，经过了 Transformer 中最重要的操作——注意力操作后，这种相对位置信息几乎就消失了。有没有一种位置编码方式能够让注意力计算也能知道 token 间的相对位置关系呢？</p>
<p>经苏剑林设计，假设每个 token 的二维位置编码是一个复数，如果用以下的公式来定义绝对位置编码，那么经过注意力计算里的求内积操作后，结果里恰好会出现相对位置关系。设两个 token 分别位于位置 $m$ 和 $n$，令给位置为 $j$ 的注意力输入 Q, K $q_j, k_j$ 右乘上 $e^{ij/10000}$的位置编码，则求 Q, K 内积的结果为：</p>
<script type="math/tex; mode=display">
\langle q_me^{im/10000}, k_ne^{in/10000} \rangle=Re[q_mk_n^*e^{i(m-n)/10000}]</script><p>其中，$i$ 为虚数单位，$*$ 为共轭复数，$Re$ 为取复数实部。只是为了理解方法的思想的话，我们不需要仔细研究这个公式，只需要注意到输入的 Q, K 位置编码分别由位置 $m$, $n$ 决定，而输出的位置编码由相对位置 $m-n$ 决定。这种位置编码既能给输入提供绝对位置关系，又能让注意力输出有相对位置关系，非常巧妙。</p>
<p>根据欧拉公式，我们可以把 $e^i$ 用一个含 $sin$ 和 $cos$ 的向量表示。由于该变换对应向量的旋转，所以这种位置编码被称为「旋转式位置编码」。在实际实现时，我们不需要复数库，只需要用两个分别含 $sin$ 和 $cos$ 的数来表示一个位置编码。也就是说，原来正弦位置编码中每个位置的编码只有一个实数，现在需要两个实数，或者说要一个二维向量。</p>
<p>总结一下用旋转式位置编码替换正弦位置编码后，我们在实现时应该做的改动。现在，我们不是提前算好位置编码，再加到输入上，而是先预处理好位置编码，在每次注意力 Q，K 求内积前给输入乘上。和正弦编码一样，我们会把特征长度为 $d$ 的 token 向量的分量两两分组，分别维护位置关系。但是，现在每个分量的编码由两个而不是一个实数表示。所以，在之后的代码中，我们会看到生成位置编码时，会先把 token 特征向量长度除以二，再给每组 token 生成 $2 \times 2$ 个编码，对应每组两个编码，每个编码长度为二。</p>
<p>我们来看一下 FLUX.1 的 Transformer 是怎么处理位置编码的。在 <code>FluxTransformer2DModel</code> 的 <code>forward</code> 方法中，我们能看到输入的 <code>0, 1, 2, 3</code> 这样的整数位置编码 <code>ids</code> 被传入了位置编码层 <code>pos_embed</code> 中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">image_rotary_emb = self.pos_embed(ids)</span><br></pre></td></tr></table></figure>
<p>位置编码层类 <code>EmbedND</code> 定义了位置编码的具体计算方式。这个类的逻辑我们暂时跳过，直接看最后真正在算旋转式位置编码的 <code>rope</code> 函数。函数中，输入参数 <code>pos</code> 是一个 <code>0, 1, 2, 3</code> 这样的整数序号张量，<code>dim</code> 表示希望生成多长的位置编码，其值应该等于 token 的特征长度，<code>theta</code> 用来控制三角函数的周期，一般都是取常数 <code>10000</code>。我们能看到，<code>rope</code> 计算了输入的三角函数值，并把长度为 <code>dim</code> 的编码两两分组，每组有 <code>(2, 2)</code> 个位置编码值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rope</span>(<span class="params">pos: torch.Tensor, dim: <span class="built_in">int</span>, theta: <span class="built_in">int</span></span>) -&gt; torch.Tensor:</span></span><br><span class="line">    <span class="keyword">assert</span> dim % <span class="number">2</span> == <span class="number">0</span>, <span class="string">&quot;The dimension must be even.&quot;</span></span><br><span class="line"></span><br><span class="line">    scale = torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>, dtype=torch.float64, device=pos.device) / dim</span><br><span class="line">    omega = <span class="number">1.0</span> / (theta**scale)</span><br><span class="line"></span><br><span class="line">    batch_size, seq_length = pos.shape</span><br><span class="line">    out = torch.einsum(<span class="string">&quot;...n,d-&gt;...nd&quot;</span>, pos, omega)</span><br><span class="line">    cos_out = torch.cos(out)</span><br><span class="line">    sin_out = torch.sin(out)</span><br><span class="line"></span><br><span class="line">    stacked_out = torch.stack([cos_out, -sin_out, sin_out, cos_out], dim=-<span class="number">1</span>)</span><br><span class="line">    out = stacked_out.view(batch_size, -<span class="number">1</span>, dim // <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> out.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbedND</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, theta: <span class="built_in">int</span>, axes_dim: <span class="type">List</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.theta = theta</span><br><span class="line">        self.axes_dim = axes_dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, ids: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        n_axes = ids.shape[-<span class="number">1</span>]</span><br><span class="line">        emb = torch.cat(</span><br><span class="line">            [rope(ids[..., i], self.axes_dim[i], self.theta) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_axes)],</span><br><span class="line">            dim=-<span class="number">3</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> emb.unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>我们来看一下位置编码是怎么传入 Transformer 块的注意力计算的。在预处理完位置编码后，<code>image_rotary_emb</code> 会作为输入参数传入所有 Transformer 块，包括前面的双流块和后面的单流块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">...</span>):</span></span><br><span class="line">    ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">    image_rotary_emb = self.pos_embed(ids)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.transformer_blocks):</span><br><span class="line">        encoder_hidden_states, hidden_states = block(</span><br><span class="line">                        ...</span><br><span class="line">                        image_rotary_emb=image_rotary_emb,</span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.single_transformer_blocks):</span><br><span class="line">        hidden_states = block(</span><br><span class="line">            ...</span><br><span class="line">            image_rotary_emb=image_rotary_emb,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>位置编码 <code>image_rotary_emb</code> 最后会传入双流注意力计算类 <code>FluxAttnProcessor2_0</code> 和单流注意力计算类 <code>FluxSingleAttnProcessor2_0</code>。由于位置编码在这两个类中的用法都相同，我们就找 <code>FluxSingleAttnProcessor2_0</code> 的代码来看一看。在其 <code>__call__</code> 方法中，可以看到，在做完了 Q, K 的投影变换、形状变换、归一化后，方法调用了 <code>apply_rope</code> 来执行旋转式位置编码的计算。而 <code>apply_rope</code> 会把 Q, K 特征向量的分量两两分组，根据之前的公式，模拟与位置编码的复数乘法运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FluxSingleAttnProcessor2_0</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">        image_rotary_emb: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        query = attn.to_q(hidden_states)</span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_hidden_states = hidden_states</span><br><span class="line">        key = attn.to_k(encoder_hidden_states)</span><br><span class="line">        value = attn.to_v(encoder_hidden_states)</span><br><span class="line"></span><br><span class="line">        query = query.view(batch_size, -<span class="number">1</span>, attn.heads, head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        key = key.view(batch_size, -<span class="number">1</span>, attn.heads, head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        value = value.view(batch_size, -<span class="number">1</span>, attn.heads, head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn.norm_q <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            query = attn.norm_q(query)</span><br><span class="line">        <span class="keyword">if</span> attn.norm_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            key = attn.norm_k(key)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> image_rotary_emb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            query, key = apply_rope(query, key, image_rotary_emb)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_rope</span>(<span class="params">xq, xk, freqs_cis</span>):</span></span><br><span class="line">    xq_ = xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    xk_ = xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    xq_out = freqs_cis[..., <span class="number">0</span>] * xq_[..., <span class="number">0</span>] + freqs_cis[..., <span class="number">1</span>] * xq_[..., <span class="number">1</span>]</span><br><span class="line">    xk_out = freqs_cis[..., <span class="number">0</span>] * xk_[..., <span class="number">0</span>] + freqs_cis[..., <span class="number">1</span>] * xk_[..., <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> xq_out.reshape(*xq.shape).type_as(xq), xk_out.reshape(*xk.shape).type_as(xk)</span><br></pre></td></tr></table></figure>
<p>这样，我们就看完了旋转式位置编码在 FLUX.1 里的实现。但是，我们还遗留了一个重要问题：在 NLP 中，句子天然有前后关系，我们按照 <code>0, 1, 2, 3</code> 给 token 编号就行了。而在这个模型中，既有图像 token，又有文本 token，该怎么给 token 编号呢？</p>
<h2 id="图像及文本-token-的位置编号"><a href="#图像及文本-token-的位置编号" class="headerlink" title="图像及文本 token 的位置编号"></a>图像及文本 token 的位置编号</h2><p>现在，我们把目光倒回到流水线类。输入给去噪模型的序号变量有两个：<code>text_ids</code>，<code>latent_image_ids</code>。它们是怎么得到的？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">noise_pred = self.transformer(</span><br><span class="line">    ...</span><br><span class="line">    txt_ids=text_ids,</span><br><span class="line">    img_ids=latent_image_ids,</span><br><span class="line">    ...</span><br><span class="line">)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>在文本编码方法中，我们看到，<code>text_ids</code> 竟然只是一个全零张量。它的第一维表示 batch 大小，第二维序列长度等于文本编码 <code>prompt_embeds</code> 的长度，第三维序号长度为 3。也就是说，对于每一个文本 token 的每一个位置，都用 <code>(0, 0, 0)</code> 来表示它的位置编号。这也暗示在 FLUX.1 中，token 的位置是三维的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_prompt</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    ...</span><br><span class="line">    text_ids = torch.zeros(batch_size, prompt_embeds.shape[<span class="number">1</span>], <span class="number">3</span>).to(device=device, dtype=dtype)</span><br><span class="line">    text_ids = text_ids.repeat(num_images_per_prompt, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> prompt_embeds, pooled_prompt_embeds, text_ids</span><br></pre></td></tr></table></figure>
<p>而 <code>latent_image_ids</code> 主要是在 <code>_prepare_latent_image_ids</code> 函数里生成的。这个函数的主要输入参数是图像的高宽。根据高宽，函数会生成 <code>(0, 0) ~ (height, width)</code> 的二维位置坐标表格，作为位置坐标 <code>latent_image_ids</code> 的第二、第三维。而位置坐标的第一维全是 0。也就是说，位置为 <code>(i, j)</code> 的像素的位置编号为 <code>(0, i, j)</code>。代码里给高宽除以 2 是因为输入没有考虑 2 倍的图块化，这写得真够乱的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prepare_latent_image_ids</span>(<span class="params">batch_size, height, width, device, dtype</span>):</span></span><br><span class="line">    latent_image_ids = torch.zeros(height // <span class="number">2</span>, width // <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    latent_image_ids[..., <span class="number">1</span>] = latent_image_ids[..., <span class="number">1</span>] + torch.arange(height // <span class="number">2</span>)[:, <span class="literal">None</span>]</span><br><span class="line">    latent_image_ids[..., <span class="number">2</span>] = latent_image_ids[..., <span class="number">2</span>] + torch.arange(width // <span class="number">2</span>)[<span class="literal">None</span>, :]</span><br><span class="line"></span><br><span class="line">    latent_image_id_height, latent_image_id_width, latent_image_id_channels = latent_image_ids.shape</span><br><span class="line"></span><br><span class="line">    latent_image_ids = latent_image_ids[<span class="literal">None</span>, :].repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    latent_image_ids = latent_image_ids.reshape(</span><br><span class="line">        batch_size, latent_image_id_height * latent_image_id_width, latent_image_id_channels</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latent_image_ids.to(device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_latents</span>(<span class="params">...</span>):</span></span><br><span class="line">    height = <span class="number">2</span> * (<span class="built_in">int</span>(height) // self.vae_scale_factor)</span><br><span class="line">    width = <span class="number">2</span> * (<span class="built_in">int</span>(width) // self.vae_scale_factor)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    latent_image_ids = self._prepare_latent_image_ids(batch_size, height, width, device, dtype)</span><br><span class="line">    <span class="keyword">return</span> latents, latent_image_ids</span><br></pre></td></tr></table></figure>
<p>文本位置编号 <code>txt_ids</code> 和 <code>img_ids</code> 会在第二维，也就是序列长度那一维拼接成 <code>ids</code>。<code>ids</code> 会输入给 <code>EmbedND</code> 类的实例 <code>pos_embed</code>。<code>EmbedND</code> 的构造函数参数中，<code>dim</code> 完全没有被用到，<code>theta</code> 控制编码的三角函数周期，<code>axes_dim</code> 表示位置坐标每一维的编码长度。比如 FLUX.1 的位置坐标是三维的， <code>axes_dim</code> 是 <code>[16, 56, 56]</code>，那么它就表示第一个维度用长度 <code>16</code> 的位置编码，后两维用长度 <code>56</code> 的位置编码。位置编号经 <code>rope</code> 函数计算得到旋转式位置编码后，会拼接到一起，最后形成 <code>128</code> 维的位置编码。注意，所有 Transformer 块每个头的特征数 <code>attention_head_dim</code> 也是 <code>128</code>。这两个值必须相等。</p>
<blockquote>
<p>「头」指的是「多头注意力」里的「头」。头数乘上每次参与注意力运算的特征长度才等于总特征长度。由于位置编码是给 Q, K 准备的，所以位置编码的长度应该与参与注意力运算的特征长度相同。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FluxTransformer2DModel</span>():</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        patch_size: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        in_channels: <span class="built_in">int</span> = <span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_layers: <span class="built_in">int</span> = <span class="number">19</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_single_layers: <span class="built_in">int</span> = <span class="number">38</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_head_dim: <span class="built_in">int</span> = <span class="number">128</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_attention_heads: <span class="built_in">int</span> = <span class="number">24</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        joint_attention_dim: <span class="built_in">int</span> = <span class="number">4096</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        pooled_projection_dim: <span class="built_in">int</span> = <span class="number">768</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        guidance_embeds: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        axes_dims_rope: <span class="type">List</span>[<span class="built_in">int</span>] = [<span class="number">16</span>, <span class="number">56</span>, <span class="number">56</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">    self.pos_embed = EmbedND(dim=self.inner_dim, theta=<span class="number">10000</span>, axes_dim=axes_dims_rope)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">...</span>):</span></span><br><span class="line">    ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">    image_rotary_emb = self.pos_embed(ids)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbedND</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, theta: <span class="built_in">int</span>, axes_dim: <span class="type">List</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.theta = theta</span><br><span class="line">        self.axes_dim = axes_dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, ids: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        n_axes = ids.shape[-<span class="number">1</span>]</span><br><span class="line">        emb = torch.cat(</span><br><span class="line">            [rope(ids[..., i], self.axes_dim[i], self.theta) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_axes)],</span><br><span class="line">            dim=-<span class="number">3</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> emb.unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>我们来整理一下 FLUX.1 的位置编码机制。每个文本 token 的位置编号都是 <code>(0, 0, 0)</code>。位于 <code>(i, j)</code> 的像素的位置编号是 <code>(0, i, j)</code>。它们会生成 <code>128</code> 维的位置编码。编码前 <code>16</code> 个通道是第一维位置编号的位置编码，后面两组 <code>56</code> 个通道分别是第二维、第三位位置编号的位置编码。也就是说，在每个头做多头注意力运算时，特征的前 <code>16</code> 个通道不知道位置信息，中间 <code>56</code> 个通道知道垂直的位置信息，最后 <code>56</code> 个通道知道水平的位置信息。</p>
<p>乍看下来，这种位置编号方式还是非常奇怪的。所有 token 的第一维位置编号都是 0，这一维岂不是什么用都没有？</p>
<h2 id="FLUX-1-旋转式位置编码原理猜测与实验"><a href="#FLUX-1-旋转式位置编码原理猜测与实验" class="headerlink" title="FLUX.1 旋转式位置编码原理猜测与实验"></a>FLUX.1 旋转式位置编码原理猜测与实验</h2><p>在这一节中，我将主观分析 FLUX.1 的现有源码，猜测 FLUX.1 未开源的 [pro] 版本中旋转式位置编码是怎么设置的。此外，我还会分享一些简单的相关实验结果。</p>
<p>已开源的 FLUX.1 为什么会出现 <code>(0, 0, 0)</code>, <code>(0, i, j)</code> 这样奇怪的位置编号呢？由于现在已开源的两版模型是在 FLUX.1 [pro] 上指引蒸馏的结果，很可能原模型在指引机制，也就是和文本相关的处理机制上与现有模型不同。因此，我使用我独创的代码心理学，对现有源码进行了分析。</p>
<p>首先，令我感到疑惑的是采样流水线里生成位置编号的代码。<code>latent_image_ids</code> 一开始是一个全零张量，你写它加一个数，和直接赋值的结果不是一样的吗？为什么要浪费时间多写一个加法呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">latent_image_ids = torch.zeros(height // <span class="number">2</span>, width // <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">latent_image_ids[..., <span class="number">1</span>] = latent_image_ids[..., <span class="number">1</span>] + torch.arange(height // <span class="number">2</span>)[:, <span class="literal">None</span>]</span><br><span class="line">latent_image_ids[..., <span class="number">2</span>] = latent_image_ids[..., <span class="number">2</span>] + torch.arange(width // <span class="number">2</span>)[<span class="literal">None</span>, :]</span><br></pre></td></tr></table></figure>
<p>为了确认这段代码不是 Diffusers 的开发者写的，我去看了 FLUX.1 的官方代码，发现他们的写法是一样的。在看 Diffusers 源码时，我们还看到了其他一些写得很差的代码，这些代码其实也都是从官方仓库里搬过来的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare</span>(<span class="params">t5: HFEmbedder, clip: HFEmbedder, img: Tensor, prompt: <span class="built_in">str</span> | <span class="built_in">list</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="built_in">dict</span>[<span class="built_in">str</span>, Tensor]:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    img_ids = torch.zeros(h // <span class="number">2</span>, w // <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    img_ids[..., <span class="number">1</span>] = img_ids[..., <span class="number">1</span>] + torch.arange(h // <span class="number">2</span>)[:, <span class="literal">None</span>]</span><br><span class="line">    img_ids[..., <span class="number">2</span>] = img_ids[..., <span class="number">2</span>] + torch.arange(w // <span class="number">2</span>)[<span class="literal">None</span>, :]</span><br><span class="line">    img_ids = repeat(img_ids, <span class="string">&quot;h w c -&gt; b (h w) c&quot;</span>, b=bs)</span><br></pre></td></tr></table></figure>
<p>从这些代码中，我们不难猜出开发者的心理。FLUX.1 的开发者想，我们要赶快搞一个大新闻，论文也不写了，直接加班加点准备开源。Diffusers 的开发者一看，你们这么急，我们也得搞快一点。于是他们先把 SD3 的代码复制了一遍，然后又照搬了 FLUX.1 官方仓库里的一些逻辑，直接修改 SD3 的代码。</p>
<p>相信大家都有这样的代码重构经历：把自己写的个人开发代码，急忙删删改改，变成能给别人看的代码。能少改一点，就少改一点。上面的代码用加法而不是赋值，就是重构的时候代码没删干净的痕迹。这说明，一开始的 <code>img_ids</code> 很可能不是一个全零张量，而是写了一些东西在里面。</p>
<p>而另一边，设置文本位置编号的官方源码里，非常干脆地写着一个全零向量。我倾向于这部分代码没有在开源时改过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">txt_ids = torch.zeros(bs, txt.shape[<span class="number">1</span>], <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>那么，问题就来了，这个看似全零的图像位置编号一开始是什么？它对整个位置编码的设计有什么影响？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_ids = torch.zeros(h // <span class="number">2</span>, w // <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>我猜开发者设置这个变量的目的是为了区分文本和图像 token。目前，所有文本 token 的位置编号是 <code>(0, 0, 0)</code>，这其实不太合理，因为这种做法实际上是把所有文本 token 都默认当成位置为 <code>(0, 0)</code> 图像 token。为了区分文本和图像 token，应该还有其他设计。我猜最简单的方法是在第一维上做一些改动，比如令所有图像 token 的第一维都是 1。但看起来更合理的做法是对三个维度的编号都一些更改，比如给所有图像位置编号都加上一个常量 <code>(a, b, c)</code>。这样，图像 token 间的相对位置并不会变，而图像和文本 token 的相对位置就不同了，文本就不会默认在图像 <code>(0, 0)</code> 处了。从代码里的加法来看，我更倾向于认为 <code>img_ids</code> 原来是一个三个维度都有值的常量，且这个量或许是可以学习的。而在指引蒸馏时，位置编号的设计被简化了。</p>
<blockquote>
<p>网上有人说文本位置编码全零是因为 t5 编码器自带位置编码。而在我看来，过了一个文本编码器后，文本的每个 token 已经包含所有文本的全局信息，文本 token 之间的位置编码在这里已经不重要了。重要的是文本 token 和图像 token 之间的「位置」关系，这并不能通过 t5 的位置编码来反映。</p>
</blockquote>
<p>为了验证位置编码的作用，我尝试修改了图像位置编号的定义，还是跑本文开头那个测试示例。</p>
<p>如果把图像位置编号全置零，会得到下面的结果。这说明位置编码对结果的影响还是很大的，模型只能从位置编码处获取 token 间的相对关系。</p>
<p><img src="/2024/09/03/20240809-flux1/10.jpg" alt></p>
<p>如果把位置编号除以二，会得到下面的结果。我们能发现，图像好像变模糊了一点，且像素有锯齿化的倾向。这非常合理，因为位置编号除以二后，模型实际上被要求生成分辨率低一倍的结果。但突然又多了一些距离为 0.5 的像素，模型突然就不知道怎么处理了，最终勉强生成了这种略显模糊，锯齿现象明显的图片。注意哦，这里虽然像素间的关系不对，但图中的文字很努力地想要变得正常一点。</p>
<p><img src="/2024/09/03/20240809-flux1/11.jpg" alt></p>
<p>位置编号乘二的结果如下所示。可能模型并没有见过没有距离为 1 的图像 token 的情况，结果全乱套了。但尽管是这样，我们依然能看到图中的 “Hello World”。结合上面的结果，这说明文本指引对结果的影响还是很大的，正常的文本 token 在努力矫正图像生成结果。</p>
<p><img src="/2024/09/03/20240809-flux1/12.jpg" alt></p>
<p>位置编号乘 1.2 的结果如下所示。图像的结果还是比较正常的。这说明这套位置编码允许位置编号发生小的扰动，且模型能认识非整数的位置编号，即在模型看来，位置编号是连续的。</p>
<p><img src="/2024/09/03/20240809-flux1/13.jpg" alt></p>
<p>原图片和将位置编号第一维全置 1 的结果如下所示。如我所料，位置编号的第一维几乎没什么作用。图片只是某些地方发生了更改，整体的画面结构没有变化。</p>
<p><img src="/2024/09/03/20240809-flux1/14.jpg" alt></p>
<p>目前看下来，由于现在我们有了显式定义 token 相对位置关系的方法，要在 FLUX.1 上做一些图像编辑任务的科研，最容易想到地方就是位置编码这一块。我目前随便能想到的做法有两个：</p>
<ul>
<li>直接基于位置编号做超分辨率。想办法修改位置编码的机制，使得所有图像 token 距离 2 个单位时也能某种程度上正常输出图片。以此配置反演一张低分辨率图片，得到纯噪声，重新以图像 token 距离 1 单位的正常配置来生成图片，但旧像素不对新像素做注意力，再想一些办法控制文本那部分，尽量保持旧像素输出不变，最后就能得到两倍超分辨率的结果了。inpainting 似乎也能拿类似的思路来做。</li>
<li>目前所有文本 token 的位置默认是 <code>(0, 0)</code>，改变文本 token 的位置编号或许能让我们精确控制文本指定的生成区域。当然，这个任务在之前的 Stable Diffusion 里好像已经被做滥了。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们围绕 FLUX.1 相对 Stable Diffusion 3 的改动，仔细阅读了 FLUX.1 在 Diffusers 中的源码。这些改动具体总结如下：</p>
<ul>
<li>SD3 是在去噪网络里用下采样 2 倍的卷积实现图块化，而 FLUX.1 通过把 $2 \times 2$ 个图像 token 在通道上堆叠直接实现图块化。</li>
<li>FLUX.1 目前公布的两个模型都是指引蒸馏过的。我们无需使用 Classifier-Free Guidance，只要把指引强度当成一个约束条件输出进模型，就能在一次推理中得到带指定指引强度的输出。</li>
<li>FLUX.1 遵照 Stable Diffusion 3 的噪声调度机制，对于分辨率越高的图像，把越多的去噪迭代放在了高噪声的时刻上。但相较 Stable Diffusion 3，似乎不仅训练时有这种设计，采样时也需要用到这种设计。</li>
<li>FLUX.1 将文本的位置编号设为 <code>(0, 0, 0)</code>，图像的位置编号设为 <code>(0, i, j)</code>，之后用标准的旋转式位置编码对三个维度的编号编码，再把三组编码拼接。这种看似不太合理的位置编号设计方式或许是指引蒸馏导致的，目前从源代码中看不出原 FLUX.1 模型的位置编号设计方式。</li>
<li>在原 Stable Diffusion 的 MM-DiT 块之后，FLUX.1 将文本和图像 token 拼接，输入进了一个单流的 Transformer 块。该 Transformer 块遵照之前并行注意力层的设计，注意力层和 MLP 并联执行，在执行速度上有所提升。</li>
</ul>
<p>FLUX.1 的总模型结构图如下所示。</p>
<p><img src="/2024/09/03/20240809-flux1/8.jpg" alt></p>
<p>作为最强开源 DiT 文生图模型，FLUX.1 狠狠打脸了拖拖拉拉刚开源没多久的 Stable Diffusion 3。可以预见，之后大家会把开发图像编辑工作的基础模型从 U-Net 版 Stable Diffusion 逐渐换成 FLUX.1。这方面的研究目前还是蓝海，值得大家投入精力研究。</p>
<p>FLUX.1 还是在科研上能给我们一些启示的。RoPE 都是 NLP 那边已经出了很久的工作了，直到现在才搬到图像生成这边来。我们或许能够把 NLP 或者其他视觉任务中使用的神经网络技术搬到图像生成这边来，不费什么力气地改进现有的图像生成模型。</p>
<p>但是，在搬运 NLP 技术中，我们也要思考如何更合理地在视觉应用中使用这些技术。文本和图像存在本质上的区别：文本是离散的，而图像是连续的。这种连续性不仅体现在图像的颜色值上，还体现在图像像素间的位置关系上。就以这里的旋转式位置编码为例，NLP 中，token 间的距离就得是整数。而在 CV 中，如果我们认为图像是一种连续信号，那么非整数的 token 距离或许也是有意义的。从文本和图像的本质区别出发，我们或许能够把 NLP 的技术更好地适配到 CV 上，而不是把 Transformer 搬过来，然后加数据一把梭。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/en/tags/%E7%BC%96%E7%A8%8B/" rel="tag"># 编程</a>
              <a href="/en/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" rel="tag"># 扩散模型</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/en/2024/07/27/20240717-ar-wo-vq/" rel="prev" title="解读何恺明新作：不用向量离散化的自回归图像生成（Autoregressive Image Generation without Vector Quantization）">
      <i class="fa fa-chevron-left"></i> 解读何恺明新作：不用向量离散化的自回归图像生成（Autoregressive Image Generation without Vector Quantization）
    </a></div>
      <div class="post-nav-item">
    <a href="/en/2024/09/03/20240829-GameNGen/" rel="next" title="锐评能模拟射击游戏的扩散模型 GameNGen">
      锐评能模拟射击游戏的扩散模型 GameNGen <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B%E4%B8%8E-Diffusers-%E7%A4%BA%E4%BE%8B%E8%84%9A%E6%9C%AC"><span class="nav-number">1.</span> <span class="nav-text">模型简介与 Diffusers 示例脚本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%9A%E8%AF%BB%E4%BB%A3%E7%A0%81%E6%A1%86%E6%9E%B6"><span class="nav-number">2.</span> <span class="nav-text">通读代码框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E6%95%B4%E6%B5%81%E5%8C%B9%E9%85%8D%E6%A0%87%E5%87%86%E5%B7%AE"><span class="nav-number">3.</span> <span class="nav-text">调整流匹配标准差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%8D%95%E6%B5%81%E5%B9%B6%E8%A1%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E7%9A%84-Transformer-%E6%9E%B6%E6%9E%84"><span class="nav-number">4.</span> <span class="nav-text">使用单流并行注意力层的 Transformer 架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%8B%E8%BD%AC%E5%BC%8F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%80%9D%E6%83%B3%E5%8F%8A-FLUX-1-%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">旋转式位置编码思想及 FLUX.1 实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%8F%8A%E6%96%87%E6%9C%AC-token-%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E5%8F%B7"><span class="nav-number">6.</span> <span class="nav-text">图像及文本 token 的位置编号</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FLUX-1-%E6%97%8B%E8%BD%AC%E5%BC%8F%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%8E%9F%E7%90%86%E7%8C%9C%E6%B5%8B%E4%B8%8E%E5%AE%9E%E9%AA%8C"><span class="nav-number">7.</span> <span class="nav-text">FLUX.1 旋转式位置编码原理猜测与实验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">8.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/en/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/en/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/en/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/en/lib/anime.min.js"></script>
  <script src="/en/lib/velocity/velocity.min.js"></script>
  <script src="/en/lib/velocity/velocity.ui.min.js"></script>

<script src="/en/js/utils.js"></script>

<script src="/en/js/motion.js"></script>


<script src="/en/js/schemes/muse.js"></script>


<script src="/en/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
