<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/en/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/en/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/en/images/logo.svg" color="#222">

<link rel="stylesheet" href="/en/css/main.css">


<link rel="stylesheet" href="/en/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/en/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Diffusers 库为社区用户提供了多种扩散模型任务的训练脚本。每个脚本都平铺直叙，没有多余的封装，把训练的绝大多数细节都写在了一个脚本里。这种设计既能让入门用户在不阅读源码的前提下直接用脚本训练，又方便高级用户直接修改脚本。 可是，这种设计就是最好的吗？关于训练脚本的最佳设计风格，社区用户们往往各执一词。有人更喜欢更贴近 PyTorch 官方示例的写法，而有人会喜欢用 PyTorch Ligh">
<meta property="og:type" content="article">
<meta property="og:title" content="定制适合自己的 Diffusers 扩散模型训练脚本">
<meta property="og:url" content="https://zhouyifan.net/en/2024/07/27/20240605-diffusers-training/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="Diffusers 库为社区用户提供了多种扩散模型任务的训练脚本。每个脚本都平铺直叙，没有多余的封装，把训练的绝大多数细节都写在了一个脚本里。这种设计既能让入门用户在不阅读源码的前提下直接用脚本训练，又方便高级用户直接修改脚本。 可是，这种设计就是最好的吗？关于训练脚本的最佳设计风格，社区用户们往往各执一词。有人更喜欢更贴近 PyTorch 官方示例的写法，而有人会喜欢用 PyTorch Ligh">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2024/07/27/20240605-diffusers-training/1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/07/27/20240605-diffusers-training/2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/07/27/20240605-diffusers-training/3.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/07/27/20240605-diffusers-training/4.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/07/27/20240605-diffusers-training/5.jpg">
<meta property="article:published_time" content="2024-07-27T12:31:57.000Z">
<meta property="article:modified_time" content="2024-08-19T13:00:18.179Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="编程">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="扩散模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2024/07/27/20240605-diffusers-training/1.jpg">

<link rel="canonical" href="https://zhouyifan.net/en/2024/07/27/20240605-diffusers-training/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>定制适合自己的 Diffusers 扩散模型训练脚本 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/en/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/en/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/en/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/en/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/en/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/en/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net" rel="section"><i class="fa fa-language fa-fw"></i>简体中文</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/07/27/20240605-diffusers-training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          定制适合自己的 Diffusers 扩散模型训练脚本
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-27 20:31:57" itemprop="dateCreated datePublished" datetime="2024-07-27T20:31:57+08:00">2024-07-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%88%9B%E4%BD%9C/" itemprop="url" rel="index"><span itemprop="name">创作</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%88%9B%E4%BD%9C/%E7%BC%96%E7%A8%8B%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">编程项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Diffusers 库为社区用户提供了多种扩散模型任务的训练脚本。每个脚本都平铺直叙，没有多余的封装，把训练的绝大多数细节都写在了一个脚本里。这种设计既能让入门用户在不阅读源码的前提下直接用脚本训练，又方便高级用户直接修改脚本。</p>
<p>可是，这种设计就是最好的吗？关于训练脚本的最佳设计风格，社区用户们往往各执一词。有人更喜欢更贴近 PyTorch 官方示例的写法，而有人会喜欢用 PyTorch Lightning 等封装度高、重复代码少的库。而在我看来，选择哪种风格的训练脚本，确实是个人喜好问题。但是，在开始使用训练脚本之前，我们要从细节入手，理解训练脚本到底要做哪些事。学懂了之后，不管是用别人的训练库，还是定制适合自己的训练脚本，都是很轻松的。不管怎么说，Diffusers 的这种训练脚本是一份很好的学习素材。</p>
<p>当然，我在用 Diffusers 的训练脚本时，发现一旦涉及多类任务的训练，比如既要能训练 Stable Diffusion，又要能训练 VAE，那么这份脚本就会用起来比较困难，而写两份训练脚本又会有很大的冗余。Diffusers 的训练脚本依然有改进的空间。</p>
<p>在这篇文章中，我会主要面向想系统性学习扩散模型训练框架的读者，先详细介绍 Diffusers 官方训练脚本，再分享我重构训练脚本的过程，使得脚本能够更好地兼容多类模型的训练。文章的末尾，我会展示几个简单的扩散模型训练实例。</p>
<p>在阅读本文时，建议大家用电脑端，一边看源代码一边读文章。「官方训练脚本细读」一节细节较多，初次阅读时可以快速浏览，看完「训练脚本内容总结」中的流程图，再回头仔细看一遍。</p>
<h2 id="准备源代码"><a href="#准备源代码" class="headerlink" title="准备源代码"></a>准备源代码</h2><p>我们将以最简单的 DDPM 官方训练脚本 <code>examples/unconditional_image_generation/train_unconditional.py</code> 为例，学习训练脚本的通用写法。<code>examples</code> 文件夹在位于 Diffusers 官方 GitHub 仓库中，用 pip 安装的 Diffusers 可能没有这个文件夹，最好是手动 clone 官方仓库，再在本地查看这个文件夹。使用 Diffusers 训练时，可能还要安装其他库。官方在不同的训练教程里给了不同的安装指令，建议大家都安装上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd examples/text_to_image</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install diffusers[training]</span><br></pre></td></tr></table></figure>
<p>我为本教程准备的脚本在仓库 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample">https://github.com/SingleZombie/DiffusersExample</a> 中。请 clone 这个仓库，再切换到 <code>TrainingScript</code> 目录下。<code>train_official.py</code> 是原官方训练脚本 <code>train_unconditional.py</code>，<code>train_0.py</code> 是第一次修改后的训练脚本<br>，<code>train_1.py</code> 是第二次修改后的训练脚本。</p>
<h2 id="官方训练脚本细读"><a href="#官方训练脚本细读" class="headerlink" title="官方训练脚本细读"></a>官方训练脚本细读</h2><p>先拉到文件的最底部，我们能在这找到程序的入口。在 <code>parse_args</code> 函数中，脚本会用 <code>argparse</code> 库解析命令行参数，并将所有参数保存在 <code>args</code> 里。<code>args</code> 会传进 <code>main</code> 函数里。稍后我们看到所有 <code>args.</code> 打头的变量调用，都表明该变量来自于命令行参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    args = parse_args()</span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure>
<p>接着，我们正式开始学习训练主函数。一开始，函数会配置 accelerate 库及日志记录器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">logging_dir = os.path.join(args.output_dir, args.logging_dir)</span><br><span class="line">accelerator_project_config = ProjectConfiguration(</span><br><span class="line">    project_dir=args.output_dir, logging_dir=logging_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># a big number for high resolution or big dataset</span></span><br><span class="line">kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=<span class="number">7200</span>))</span><br><span class="line">accelerator = Accelerator(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_tensorboard_available():</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">elif</span> args.logger == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_wandb_available():</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">import</span> wandb</span><br></pre></td></tr></table></figure>
<p>在配置日志的中途，函数插入了一段修改模型存取逻辑的代码。为了让我们阅读代码的顺序与实际运行顺序一致，我们等待会用到了这段代码时再回头来读。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `accelerate` 0.16.0 will have better support for customized saving</span></span><br><span class="line"><span class="keyword">if</span> version.parse(accelerate.__version__) &gt;= version.parse(<span class="string">&quot;0.16.0&quot;</span>):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">models, weights, output_dir</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">models, input_dir</span>):</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>跳过上面的代码，还是日志配置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make one log on every process with the configuration for debugging.</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">)</span><br><span class="line">logger.info(accelerator.state, main_process_only=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">if</span> accelerator.is_local_main_process:</span><br><span class="line">    datasets.utils.logging.set_verbosity_warning()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_info()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    datasets.utils.logging.set_verbosity_error()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_error()</span><br></pre></td></tr></table></figure>
<p>之后其他版本的训练脚本会有一段设置随机种子的代码，我们给这份脚本补上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If passed along, set the training seed now.</span></span><br><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    set_seed(args.seed)</span><br></pre></td></tr></table></figure>
<p>接着，函数会创建输出文件夹。如果我们想把模型推送到在线仓库上，函数还会创建一个仓库。</p>
<p>这段代码还出现了一行比较重要的判断语句：<code>if accelerator.is_main_process:</code>。在多卡训练时，只有主进程会执行这个条件语句块里的内容。该判断在并行编程中十分重要。很多时候，比如在输出、存取模型时，我们只需要让一个进程执行操作就行了。这个时候就要用到这行判断语句。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Handle the repository creation</span></span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        os.makedirs(args.output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        repo_id = create_repo(...).repo_id</span><br></pre></td></tr></table></figure>
<p>准备完辅助工具后，函数开始准备模型。输入参数里的 <code>model_config_name_or_path</code> 表示预定义的模型配置文件。如果该配置文件不存在，则函数会用默认的配置创建一个 DDPM 的 U-Net 模型。在写我们自己的训练脚本时，我们需要在这个地方初始化我们需要的所有模型。比如训练 Stable Diffusion 时，除了 U-Net，需要在此处准备 VAE、CLIP 文本编码器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the model</span></span><br><span class="line"><span class="keyword">if</span> args.model_config_name_or_path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    model = UNet2DModel(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    config = UNet2DModel.load_config(args.model_config_name_or_path)</span><br><span class="line">    model = UNet2DModel.from_config(config)</span><br></pre></td></tr></table></figure>
<p>这份脚本还帮我们写好了维护 EMA（指数移动平均）模型的功能。EMA 模型用于存储模型可学习的参数的局部平均值。有时 EMA 模型的效果会比原模型要好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create EMA for the model.</span></span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    ema_model = EMAModel(</span><br><span class="line">        model.parameters(),</span><br><span class="line">        model_cls=UNet2DModel,</span><br><span class="line">        model_config=model.config,</span><br><span class="line">        ...)</span><br></pre></td></tr></table></figure>
<p>此处函数还会根据 accelerate 配置自动设置模型的精度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">weight_dtype = torch.float32</span><br><span class="line"><span class="keyword">if</span> accelerator.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.float16</span><br><span class="line">    args.mixed_precision = accelerator.mixed_precision</span><br><span class="line"><span class="keyword">elif</span> accelerator.mixed_precision == <span class="string">&quot;bf16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.bfloat16</span><br><span class="line">    args.mixed_precision = accelerator.mixed_precision</span><br></pre></td></tr></table></figure>
<p>函数还会尝试启用 <code>xformers</code> 来提升 Attention 的效率。PyTorch 在 2.0 版本也加入了类似的 Attention 优化技术。如果你的显卡性能有限，且 PyTorch 版本小于 2.0，可以考虑使用 <code>xformers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.enable_xformers_memory_efficient_attention:</span><br><span class="line">    <span class="keyword">if</span> is_xformers_available():</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>准备了 U-Net 后，函数会准备噪声调度器，即定义扩散模型的细节。</p>
<blockquote>
<p>注意，扩散模型不是一个神经网络，而是一套定义了加噪、去噪公式的模型。扩散模型中需要一个去噪模型来去噪，去噪模型一般是一个神经网络。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the scheduler</span></span><br><span class="line">accepts_prediction_type = <span class="string">&quot;prediction_type&quot;</span> <span class="keyword">in</span> <span class="built_in">set</span>(</span><br><span class="line">    inspect.signature(DDPMScheduler.__init__).parameters.keys())</span><br><span class="line"><span class="keyword">if</span> accepts_prediction_type:</span><br><span class="line">    noise_scheduler = DDPMScheduler(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    noise_scheduler = DDPMScheduler(...)</span><br></pre></td></tr></table></figure>
<p>准备完所有扩散模型组件后，函数开始准备其他和训练相关的模块。其他版本的训练脚本会在这个地方加一段缓存梯度和自动放缩学习率的代码，我们给这份脚本补上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.gradient_checkpointing:</span><br><span class="line">    unet.enable_gradient_checkpointing()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.scale_lr:</span><br><span class="line">    args.learning_rate = (</span><br><span class="line">        args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>函数先准备的训练模块是优化器。这里默认使用的优化器是 <code>AdamW</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=args.learning_rate,</span><br><span class="line">    betas=(args.adam_beta1, args.adam_beta2),</span><br><span class="line">    weight_decay=args.adam_weight_decay,</span><br><span class="line">    eps=args.adam_epsilon,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>函数随后会准备训练集。这个脚本用 HuggingFace 的 datasets 库来管理数据集。我们既可以读取在线数据集，也可以读取本地的图片文件夹数据集。自定义数据集的方法可以参考 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder">https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</a> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.dataset_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        args.dataset_name,</span><br><span class="line">        args.dataset_config_name,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">        split=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        <span class="string">&quot;imagefolder&quot;</span>, data_dir=args.train_data_dir, cache_dir=args.cache_dir, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    <span class="comment"># See more about loading custom images at</span></span><br><span class="line">    <span class="comment"># https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</span></span><br></pre></td></tr></table></figure>
<p>有了数据集后，函数会继续准备 PyTorch 的 DataLoader。在这一步中，除了定义 DataLoader 外，我们还要编写数据预处理的方法。下面这段代码的编写顺序和执行顺序不同，我们按执行顺序来整理一遍下面的代码：</p>
<ol>
<li>将预定义的预处理函数传给数据集对象 <code>dataset.set_transform(transform_images)</code>。在使用数据集里的数据时，才会调用这个函数预处理图像。</li>
<li>使用 PyTorch API 定义 DataLoader。<code>train_dataloader = ...</code></li>
<li>每次用 DataLoader 获取数据时，一个数据词典 <code>examples</code> 会被传入预处理函数 <code>transform_images</code>。<code>examples</code> 里既包含了图像数据，也包含了数据的各种标签。而对于无约束图像生成任务，我们只需要图像数据，因此可以直接通过词典的 <code>&quot;image&quot;</code> 键得到 PIL 格式的图像数据。用 <code>convert(&quot;RGB&quot;)</code> 把图像转成三通道后，该 PIL 图像会被传入预处理流水线。</li>
<li>图像预处理流水线 <code>augmentations</code> 是用 Torchvision 里的 <code>transform</code> API 定义的。默认的流水线包括短边缩放至指定分辨率、按分辨率裁剪、随机反转、归一化。</li>
<li>处理过的数据会被存到词典的 <code>&quot;input&quot;</code> 键里。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets and DataLoaders creation.</span></span><br><span class="line">augmentations = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize(</span><br><span class="line">            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),</span><br><span class="line">        transforms.CenterCrop(</span><br><span class="line">            args.resolution) <span class="keyword">if</span> args.center_crop <span class="keyword">else</span> transforms.RandomCrop(args.resolution),</span><br><span class="line">        transforms.RandomHorizontalFlip() <span class="keyword">if</span> args.random_flip <span class="keyword">else</span> transforms.Lambda(<span class="keyword">lambda</span> x: x),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_images</span>(<span class="params">examples</span>):</span></span><br><span class="line">    images = [augmentations(image.convert(<span class="string">&quot;RGB&quot;</span>))</span><br><span class="line">                <span class="keyword">for</span> image <span class="keyword">in</span> examples[<span class="string">&quot;image&quot;</span>]]</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;input&quot;</span>: images&#125;</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">f&quot;Dataset size: <span class="subst">&#123;<span class="built_in">len</span>(dataset)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">dataset.set_transform(transform_images)</span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=args.train_batch_size, shuffle=<span class="literal">True</span>, num_workers=args.dataloader_num_workers</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在准备工作的最后，函数会准备学习率调度器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the learning rate scheduler</span></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    args.lr_scheduler,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,</span><br><span class="line">    num_training_steps=(<span class="built_in">len</span>(train_dataloader) * args.num_epochs),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>准备完了所有模块，函数会调用 accelerate 库来把所有模块变成适合并行训练的模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(</span><br><span class="line">    model, optimizer, train_dataloader, lr_scheduler</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    ema_model.to(accelerator.device)</span><br></pre></td></tr></table></figure>
<p>之后函数还会用 accelerate 库配置训练日志。默认情况下日志名 <code>run</code> 由当前脚本名决定。如果不想让之前的日志被覆盖的话，可以让日志名 <code>run</code> 由当前的时间决定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    run = os.path.split(__file__)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    accelerator.init_trackers(run)</span><br></pre></td></tr></table></figure>
<p>马上就要开始训练了。在此之前，函数会准备全局变量并记录日志。注意，这里函数会算一次总的 batch 数，它由输入 batch 数、进程数（显卡数）、梯度累计步数共同决定。梯度累计是一种用较少的显存实现大 batch 训练的技术。使用这项技术时，训练梯度不会每步优化，而是累计了若干步后再优化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">total_batch_size = args.train_batch_size * \</span><br><span class="line">    accelerator.num_processes * args.gradient_accumulation_steps</span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line">max_train_steps = args.num_epochs * num_update_steps_per_epoch</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;***** Running training *****&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Num examples = <span class="subst">&#123;<span class="built_in">len</span>(dataset)&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Num Epochs = <span class="subst">&#123;args.num_epochs&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Instantaneous batch size per device = <span class="subst">&#123;args.train_batch_size&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Total train batch size (w. parallel, distributed &amp; accumulation) = <span class="subst">&#123;total_batch_size&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Gradient Accumulation steps = <span class="subst">&#123;args.gradient_accumulation_steps&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Total optimization steps = <span class="subst">&#123;max_train_steps&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line">first_epoch = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>在开始训练前，如果设置了 <code>args.resume_from_checkpoint</code>，则函数会读取之前训练过的权重。负责读取训练权重的函数是 <code>load_state</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint:</span><br><span class="line">    <span class="keyword">if</span> args.resume_from_checkpoint != <span class="string">&quot;latest&quot;</span>:</span><br><span class="line">        path = ..</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the most recent checkpoint</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        accelerator.load_state(os.path.join(args.output_dir, path))</span><br><span class="line">        accelerator.<span class="built_in">print</span>(<span class="string">f&quot;Resuming from checkpoint <span class="subst">&#123;path&#125;</span>&quot;</span>)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在每个 epoch 中，函数会重置进度条。接着，函数会进入每一个 batch 的训练迭代。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train!</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(first_epoch, args.num_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    progress_bar = tqdm(total=num_update_steps_per_epoch,</span><br><span class="line">                        disable=<span class="keyword">not</span> accelerator.is_local_main_process)</span><br><span class="line">    progress_bar.set_description(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br></pre></td></tr></table></figure>
<p>如果是继续训练的话，训练开始之前会更新当前的步数 <code>step</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Skip steps until we reach the resumed step</span></span><br><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint <span class="keyword">and</span> epoch == first_epoch <span class="keyword">and</span> step &lt; resume_step:</span><br><span class="line">    <span class="keyword">if</span> step % args.gradient_accumulation_steps == <span class="number">0</span>:</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<p>训练的一开始，函数会从数据的 <code>&quot;input&quot;</code> 键里取出图像数据。此处的键名是我们之前在数据预处理函数 <code>transform_images</code> 里写的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clean_images = batch[<span class="string">&quot;input&quot;</span>].to(weight_dtype)</span><br></pre></td></tr></table></figure>
<p>之后函数会设置扩散模型训练中的其他变量，包含随机噪声、时刻。由于本文的重点并不是介绍扩散模型的原理，这段代码我们就快速略过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">noise = torch.randn(...)</span><br><span class="line">timesteps =...</span><br><span class="line">noisy_images = noise_scheduler.add_noise(</span><br><span class="line">    clean_images, noise, timesteps)</span><br></pre></td></tr></table></figure>
<p>接下来，函数会用去噪网络做前向传播。为了让模型能正确累计梯度，我们要用 <code>with accelerator.accumulate(model):</code> 把模型调用与反向传播的逻辑包起来。在这段代码中，我们会先得到模型的输出 <code>model_output</code>，再根据扩散模型得到损失函数 <code>loss</code>，最后用 accelerate 库的 API <code>accelerator</code> 代替原来 PyTorch API 来完成反向传播、梯度裁剪，并完成参数更新、学习率调度器更新、优化器更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> accelerator.accumulate(model):</span><br><span class="line">    <span class="comment"># Predict the noise residual</span></span><br><span class="line">    model_output = model(noisy_images, timesteps).sample</span><br><span class="line"></span><br><span class="line">    loss = ...</span><br><span class="line"></span><br><span class="line">    accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">        accelerator.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    lr_scheduler.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>确保一步训练结束后，函数会更新和步数相关的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.step(model.parameters())</span><br><span class="line">    progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    global_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>在这个地方，函数还会尝试保存模型。默认情况下，每 <code>args.checkpointing_steps</code> 步保存一次中间结果。确认要保存后，函数会算出当前的保存点名称，并根据最大保存点数 <code>checkpoints_total_limit</code> 决定是否要删除以前的保存点。做完准备后，函数会调用 <code>save_state</code> 保存当前训练时的所有中间变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">f accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> global_step % args.checkpointing_steps == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> args.checkpoints_total_limit <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            checkpoints = os.listdir(args.output_dir)</span><br><span class="line">            checkpoints = [</span><br><span class="line">                d <span class="keyword">for</span> d <span class="keyword">in</span> checkpoints <span class="keyword">if</span> d.startswith(<span class="string">&quot;checkpoint&quot;</span>)]</span><br><span class="line">            checkpoints = <span class="built_in">sorted</span>(</span><br><span class="line">                checkpoints, key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x.split(<span class="string">&quot;-&quot;</span>)[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(checkpoints) &gt;= args.checkpoints_total_limit:</span><br><span class="line">                ...</span><br><span class="line"></span><br><span class="line">            save_path = os.path.join(</span><br><span class="line">            args.output_dir, <span class="string">f&quot;checkpoint-<span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">            accelerator.save_state(save_path)</span><br><span class="line">            logger.info(<span class="string">f&quot;Saved state to <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个地方，主函数开头设置的存取模型回调函数终于派上用场了。在调用 <code>save_state</code> 时，会自动触发下面的回调函数来保存模型。如果不加下面的代码，所有模型默认会以 <code>.safetensor</code> 的形式存下来。而用了下面的代码后，模型能够被 <code>save_pretrained</code> 存进一个文件夹里，就像其他标准 Diffusers 模型一样。</p>
<blockquote>
<p>这里的输入参数 <code>models</code> 来自于之前的 <code>accelerator.prepare</code>，感兴趣可以去阅读文档或源码。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">models, weights, output_dir</span>):</span></span><br><span class="line">    <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">        <span class="keyword">if</span> args.use_ema:</span><br><span class="line">            ema_model.save_pretrained(</span><br><span class="line">                os.path.join(output_dir, <span class="string">&quot;unet_ema&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, model <span class="keyword">in</span> <span class="built_in">enumerate</span>(models):</span><br><span class="line">            model.save_pretrained(os.path.join(output_dir, <span class="string">&quot;unet&quot;</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># make sure to pop weight so that corresponding model is not saved again</span></span><br><span class="line">            weights.pop()</span><br></pre></td></tr></table></figure>
<p>与上面的这段代码对应，脚本还提供了读取文件的回调函数。它会在继续中断的训练后调用 <code>load_state</code> 时被调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">models, input_dir</span>):</span></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        load_model = EMAModel.from_pretrained(</span><br><span class="line">            os.path.join(input_dir, <span class="string">&quot;unet_ema&quot;</span>), UNet2DModel)</span><br><span class="line">        ema_model.load_state_dict(load_model.state_dict())</span><br><span class="line">        ema_model.to(accelerator.device)</span><br><span class="line">        <span class="keyword">del</span> load_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(models)):</span><br><span class="line">        <span class="comment"># pop models so that they are not loaded again</span></span><br><span class="line">        model = models.pop()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># load diffusers style into model</span></span><br><span class="line">        load_model = UNet2DModel.from_pretrained(</span><br><span class="line">            input_dir, subfolder=<span class="string">&quot;unet&quot;</span>)</span><br><span class="line">        model.register_to_config(**load_model.config)</span><br><span class="line"></span><br><span class="line">        model.load_state_dict(load_model.state_dict())</span><br><span class="line">        <span class="keyword">del</span> load_model</span><br></pre></td></tr></table></figure>
<p>两个回调函数需要用下面的代码来设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accelerator.register_save_state_pre_hook(save_model_hook)</span><br><span class="line">accelerator.register_load_state_pre_hook(load_model_hook)</span><br></pre></td></tr></table></figure>
<p>回到最新的代码处。训练迭代的末尾，脚本会记录当前步的日志。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">logs = &#123;<span class="string">&quot;loss&quot;</span>: loss.detach().item(), <span class="string">&quot;lr&quot;</span>: lr_scheduler.get_last_lr()[<span class="number">0</span>], <span class="string">&quot;step&quot;</span>: global_step&#125;</span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    logs[<span class="string">&quot;ema_decay&quot;</span>] = ema_model.cur_decay_value</span><br><span class="line">progress_bar.set_postfix(**logs)</span><br><span class="line">accelerator.log(logs, step=global_step)</span><br></pre></td></tr></table></figure>
<p>执行完了一个 epoch 后，脚本调用 accelerate API 保证所有进程均训练完毕。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">progress_bar.close()</span><br><span class="line">accelerator.wait_for_everyone()</span><br></pre></td></tr></table></figure>
<p>此处脚本可能会在主进程中验证模型或保存模型。如果当前是最后一个 epoch，或者达到了配置指定的验证/保存时刻，脚本就会执行验证/保存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> epoch % args.save_images_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % args.save_model_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>脚本默认的验证方法是随机生成图片，并用日志库保存图片。生成图片的方法是使用标准 Diffusers 采样流水线 <code>DDPMPipeline</code>。由于此时模型 <code>model</code> 可能被包裹成了一个用于多卡训练的 PyTorch 模块，需要用相关 API 把 <code>model</code> 解包成普通 PyTorch 模块 <code>unet</code>。如果使用了 EMA 模型，为了避免对 EMA 模型的干扰，此处需要先保存 EMA 模型参数，采样结束再还原参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % args.save_images_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">    unet = accelerator.unwrap_model(model)</span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.store(unet.parameters())</span><br><span class="line">        ema_model.copy_to(unet.parameters())</span><br><span class="line"></span><br><span class="line">    pipeline = DDPMPipeline(</span><br><span class="line">        unet=unet,</span><br><span class="line">        scheduler=noise_scheduler,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    generator = torch.Generator(device=pipeline.device).manual_seed(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># run pipeline in inference (sample random noise and denoise)</span></span><br><span class="line">    images = pipeline(...).images</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.restore(unet.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># denormalize the images and save to tensorboard</span></span><br><span class="line">    images_processed = (images * <span class="number">255</span>).<span class="built_in">round</span>().astype(<span class="string">&quot;uint8&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">elif</span> args.logger == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在保存模型时，脚本同样会先用去噪模型 <code>model</code> 构建一个流水线，再调用流水线的保存方法 <code>save_pretrained</code> 将扩散模型的所有组件（去噪模型、噪声调度器）保存下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % args.save_model_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">    <span class="comment"># save the model</span></span><br><span class="line">    unet = accelerator.unwrap_model(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.store(unet.parameters())</span><br><span class="line">        ema_model.copy_to(unet.parameters())</span><br><span class="line"></span><br><span class="line">    pipeline = DDPMPipeline(</span><br><span class="line">        unet=unet,</span><br><span class="line">        scheduler=noise_scheduler,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    pipeline.save_pretrained(args.output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.restore(unet.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        upload_folder(...)</span><br></pre></td></tr></table></figure>
<p>一个 epoch 训练的代码就到此结束了。所有 epoch 的训练结束后，脚本调用 API 结束训练。这个 API 会自动关闭所有的日志库。训练代码到这里也就结束了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerator.end_training()</span><br></pre></td></tr></table></figure>
<h2 id="训练脚本内容总结"><a href="#训练脚本内容总结" class="headerlink" title="训练脚本内容总结"></a>训练脚本内容总结</h2><p>大概熟悉了一遍这份训练脚本后，我们可以用下面的流程图概括训练脚本的执行顺序和主要内容。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/1.jpg" alt></p>
<h2 id="去掉命令行参数"><a href="#去掉命令行参数" class="headerlink" title="去掉命令行参数"></a>去掉命令行参数</h2><p>我不喜欢用命令行参数传训练参数，而喜欢把训练参数写进配置文件里，理由有：</p>
<ul>
<li>我一般会直接在命令行里手敲命令。如果命令行参数过多，我则会把要运行的命令及其参数保存在某文件里。这样还不如把参数写在另外的文件里。</li>
<li>将大量参数藏在一个词典 <code>args</code> 里，而不是把所有需用的参数在某处定义好，是一种很差的编程方式。各个参数将难以追踪。</li>
</ul>
<p>在正式重构脚本之前，我做的第一步是去掉脚本中原来的命令行参数，将所有参数先塞进一个数据类里面。脚本将只留一个命令行参数，表示参数配置文件的路径。具体做法如下：</p>
<p>先编写一个存命令行参数的数据类。这个类是一个 Python 的 <code>dataclass</code>。Python 中 <code>dataclass</code> 是一种专门用来放数据的类。定义数据类时，我们只需要定义类中所有数据的类型及默认值，不需要编写任何方法。初始化数据类时，我们只需要传一个词典或列表。一个示例如下（示例来源 <a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/understanding-python-dataclasses/）：">https://www.geeksforgeeks.org/understanding-python-dataclasses/）：</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"> </span><br><span class="line"><span class="comment"># A class for holding an employees content</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">employee</span>:</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Attributes Declaration</span></span><br><span class="line">    <span class="comment"># using Type Hints</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    emp_id: <span class="built_in">str</span></span><br><span class="line">    age: <span class="built_in">int</span></span><br><span class="line">    city: <span class="built_in">str</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">emp1 = employee(<span class="string">&quot;Satyam&quot;</span>, <span class="string">&quot;ksatyam858&quot;</span>, <span class="number">21</span>, <span class="string">&#x27;Patna&#x27;</span>)</span><br><span class="line">emp2 = employee(<span class="string">&quot;Anurag&quot;</span>, <span class="string">&quot;au23&quot;</span>, <span class="number">28</span>, <span class="string">&#x27;Delhi&#x27;</span>)</span><br><span class="line">emp3 = employee(&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;Satyam&quot;</span>, </span><br><span class="line">   <span class="string">&quot;emp_id&quot;</span>: <span class="string">&quot;ksatyam858&quot;</span>, </span><br><span class="line">   <span class="string">&quot;age&quot;</span>: <span class="number">21</span>, </span><br><span class="line">   <span class="string">&quot;city&quot;</span>: <span class="string">&#x27;Patna&#x27;</span>&#125;)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;employee object are :&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(emp1)</span><br><span class="line"><span class="built_in">print</span>(emp2)</span><br><span class="line"><span class="built_in">print</span>(emp3)</span><br></pre></td></tr></table></figure></p>
<p>我们可以用 <code>dataclass</code> 编写一个存储所有命令行参数的数据类，该类开头内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseTrainingConfig</span>:</span></span><br><span class="line">    <span class="comment"># Dir</span></span><br><span class="line">    logging_dir: <span class="built_in">str</span></span><br><span class="line">    output_dir: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Logger and checkpoint</span></span><br><span class="line">    logger: <span class="built_in">str</span> = <span class="string">&#x27;tensorboard&#x27;</span></span><br><span class="line">    checkpointing_steps: <span class="built_in">int</span> = <span class="number">500</span></span><br><span class="line">    checkpoints_total_limit: <span class="built_in">int</span> = <span class="number">20</span></span><br><span class="line">    valid_epochs: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    valid_batch_size: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">    save_model_epochs: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    resume_from_checkpoint: <span class="built_in">str</span> = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>之后在训练脚本里，我们可以把旧的命令行参数全删了，再加一个命令行参数 <code>cfg</code>，表示训练配置文件的路径。我们可以用 <code>omegaconf</code> 打开这个配置文件，得到一个词典 <code>data_dict</code>，再用这个词典构建配置文件 <code>cfg</code>。接下来，只需要把原来代码里所有 <code>args.</code> 改成 <code>cfg.</code> 就行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> omegaconf <span class="keyword">import</span> OmegaConf</span><br><span class="line"><span class="keyword">from</span> training_cfg_0 <span class="keyword">import</span> BaseTrainingConfig</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;cfg&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">data_dict = OmegaConf.load(args.cfg)</span><br><span class="line">cfg = BaseTrainingConfig(**data_dict)</span><br></pre></td></tr></table></figure>
<p>第一次修改过的训练脚本为 <code>train_0.py</code>，配置文件类在 <code>training_cfg_0.py</code> 里，示例配置文件为 <code>cfg_0.json</code>，一个简单 DDPM 模型配置写在 <code>unet_cfg</code> 目录里。可以直接运行下面的命令测试此训练脚本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_0.py cfg_0.json</span><br></pre></td></tr></table></figure>
<p>在配置文件里，我们只需要改少量的训练参数就行了。如果想知道还有哪些参数可以改，可以去查看 <code>training_cfg_0.py</code> 文件。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;logging_dir&quot;</span>: <span class="string">&quot;logs&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;models/ddpm_0&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;model_config&quot;</span>: <span class="string">&quot;unet_cfg&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;num_epochs&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">64</span>,</span><br><span class="line">    <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">5000</span>,</span><br><span class="line">    <span class="attr">&quot;valid_epochs&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;valid_batch_size&quot;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="attr">&quot;dataset_name&quot;</span>: <span class="string">&quot;ylecun/mnist&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;resolution&quot;</span>: <span class="number">32</span>,</span><br><span class="line">    <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>读者感兴趣的话也可以尝试这样改一遍代码。这样做会强迫自己读一遍训练脚本，让自己更熟悉这份代码。</p>
<h2 id="适配多种任务的训练脚本"><a href="#适配多种任务的训练脚本" class="headerlink" title="适配多种任务的训练脚本"></a>适配多种任务的训练脚本</h2><p>如果只是训练一种任务，Diffusers 的这种训练脚本还算好用。但如果我们想用完全相同的训练流程训练多种任务，这种脚本的弊端就暴露出来了：</p>
<ul>
<li>各任务的官方示例脚本本身就不完全统一。比如有的训练脚本支持设置随机种子，有的不支持。</li>
<li>一旦想修改训练过程，就得同时修改所有任务的脚本。这不符合编程中「代码复用」的思想。</li>
</ul>
<p>为此，我想重构一下官方训练脚本，将训练流程和每种任务的具体训练过程解耦开，让一份训练脚本能够被多种任务使用。于是，我又从头过了一遍训练脚本，将代码分成两类：所有任务都会用到的代码、仅 DDPM 训练会用到的代码。如下图所示，我用红字表示了训练脚本中应该由具体任务决定的部分。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/2.jpg" alt></p>
<p>根据这个划分规则，我将仅和 DDPM 相关的代码剥离出来，并用一个描述某具体任务的训练器接口类的方法调用代替原有代码。这样，每次换一个训练任务，只需要重新实现一个训练器类就行了。如下图所示，原流程图中所有红字的内容都可以由接口类的方法代替。对于不同任务，我们需要实现不同的训练器类。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/3.jpg" alt></p>
<p>具体在代码中，我写了一个接口类 <code>Trainer</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>(<span class="params">metaclass=ABCMeta</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, logger, cfg</span>):</span></span><br><span class="line">        self.weight_dtype = weight_dtype</span><br><span class="line">        self.accelerator = accelerator</span><br><span class="line">        self.logger = logger</span><br><span class="line">        self.cfg = cfg</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_modules</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                     enable_xformer: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                     gradient_checkpointing: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_optimizers</span>(<span class="params">self, train_batch_size</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_lr_schedulers</span>(<span class="params">self, gradient_accumulation_steps, num_epochs</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_dataset</span>(<span class="params">self, dataset, train_dataloader</span>):</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.train_dataloader = train_dataloader</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_modules</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">models_to_train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, global_step, batch</span>) -&gt; <span class="built_in">dict</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, global_step</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_pipeline</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">self, models, weights, output_dir</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">self, models, input_dir</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>根据类型名和初始化参数可以创建具体的训练器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_trainer</span>(<span class="params"><span class="built_in">type</span>, weight_dtype, accelerator, logger, cfg_dict</span>) -&gt; Trainer:</span></span><br><span class="line">    <span class="keyword">from</span> ddpm_trainer <span class="keyword">import</span> DDPMTrainer</span><br><span class="line">    <span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainer</span><br><span class="line"></span><br><span class="line">    __TYPE_CLS_DICT = &#123;</span><br><span class="line">        <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainer,</span><br><span class="line">        <span class="string">&#x27;lora&#x27;</span>: LoraTrainer</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> __TYPE_CLS_DICT[<span class="built_in">type</span>](weight_dtype, accelerator, logger, cfg_dict)</span><br></pre></td></tr></table></figure>
<p>原来训练脚本里的具体训练逻辑被接口类方法调用代替。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># old</span></span><br><span class="line"><span class="keyword">if</span> cfg.model_config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    model = UNet2DModel(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    config = UNet2DModel.load_config(cfg.model_config)</span><br><span class="line">    model = UNet2DModel.from_config(config)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create EMA for the model.</span></span><br><span class="line"><span class="keyword">if</span> cfg.use_ema:</span><br><span class="line">    ema_model = EMAModel(...)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># new</span></span><br><span class="line">trainer.init_modules(enable_xformers, cfg.gradient_checkpointing)</span><br></pre></td></tr></table></figure>
<p>原来仅和 DDPM 训练相关的代码全被我搬到了 <code>DDPMTrainer</code> 类中。与之对应，除了代码需要搬走外，原配置文件里的数据也需要搬走。我在 <code>DDPMTrainer</code> 类里加了一个 <code>DDPMTrainingConfig</code> 数据类，用来存对应的配置数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPMTrainingConfig</span>:</span></span><br><span class="line">    <span class="comment"># Diffuion Models</span></span><br><span class="line">    model_config: <span class="built_in">str</span></span><br><span class="line">    ddpm_num_steps: <span class="built_in">int</span> = <span class="number">1000</span></span><br><span class="line">    ddpm_beta_schedule: <span class="built_in">str</span> = <span class="string">&#x27;linear&#x27;</span></span><br><span class="line">    prediction_type: <span class="built_in">str</span> = <span class="string">&#x27;epsilon&#x27;</span></span><br><span class="line">    ddpm_num_inference_steps: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>因此，我们需要用稍微复杂一点的方式来创建配置文件。现在全局训练配置和任务配置放在两组配置里。配置文件最外层除 <code>&quot;base&quot;</code> 外的那个键表明了训练器的类型。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;base&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;logging_dir&quot;</span>: <span class="string">&quot;logs&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;models/ddpm_1&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">5000</span>,</span><br><span class="line">        <span class="attr">&quot;valid_epochs&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;dataset_name&quot;</span>: <span class="string">&quot;ylecun/mnist&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;resolution&quot;</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">64</span>,</span><br><span class="line">        <span class="attr">&quot;num_epochs&quot;</span>: <span class="number">10</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;ddpm&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;model_config&quot;</span>: <span class="string">&quot;unet_cfg&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span>,</span><br><span class="line">        <span class="attr">&quot;valid_batch_size&quot;</span>: <span class="number">4</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">__TYPE_CLS_DICT = &#123;</span><br><span class="line">    <span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;lora&#x27;</span>: LoraTrainingConfig</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_training_config</span>(<span class="params">config_path: <span class="built_in">str</span></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, BaseTrainingConfig]:</span></span><br><span class="line">    data_dict = OmegaConf.load(config_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The config must have a &quot;base&quot; key</span></span><br><span class="line">    base_cfg_dict = data_dict.pop(<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The config must have one another model config</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(data_dict) == <span class="number">1</span></span><br><span class="line">    model_key = <span class="built_in">next</span>(<span class="built_in">iter</span>(data_dict))</span><br><span class="line">    model_cfg_dict = data_dict[model_key]</span><br><span class="line">    model_cfg_cls = __TYPE_CLS_DICT[model_key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig(**base_cfg_dict),</span><br><span class="line">            model_key: model_cfg_cls(**model_cfg_dict)&#125;</span><br></pre></td></tr></table></figure>
<p>这样改完过后，训练脚本开头也需要稍作更改，其他地方保持不变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> training_cfg_1 <span class="keyword">import</span> BaseTrainingConfig, load_training_config</span><br><span class="line"><span class="keyword">from</span> trainer <span class="keyword">import</span> Trainer, create_trainer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;cfg&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    cfgs = load_training_config(args.cfg)</span><br><span class="line">    cfg: BaseTrainingConfig = cfgs.pop(<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">    trainer_type = <span class="built_in">next</span>(<span class="built_in">iter</span>(cfgs))</span><br><span class="line">    trainer_cfg_dict = cfgs[trainer_type]</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    trainer: Trainer = create_trainer(</span><br><span class="line">        trainer_type, weight_dtype, accelerator, cfg.logger, trainer_cfg_dict)</span><br></pre></td></tr></table></figure>
<p>这次修改过的训练脚本为 <code>train_1.py</code>，配置文件类在 <code>training_cfg_1.py</code> 里，DDPM 训练器在 <code>TrainingScript/ddpm_trainer.py</code> 里,示例配置文件为 <code>cfg_1.json</code>。可以直接运行下面的命令测试此训练脚本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_1.py cfg_1.json</span><br></pre></td></tr></table></figure>
<p>运行这一版或者上一版的训练脚本后，我们都能很快训练完一个 MNIST 上的 DDPM 模型。从训练可视化结果可以看出，代码重构大概是没有出错，模型能正确生成图片。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/4.jpg" alt></p>
<blockquote>
<p>对训练器类的程序设计思路感兴趣的话，欢迎阅读附录。</p>
</blockquote>
<h2 id="添加新的训练任务"><a href="#添加新的训练任务" class="headerlink" title="添加新的训练任务"></a>添加新的训练任务</h2><p>为了验证这套新代码的可拓展性，我仿照 Diffusers 官方 SD LoRA 训练脚本 <code>examples/text_to_image/train_text_to_image_lora.py</code>，快速实现了一个 SD LoRA 训练器类。这个类在 <code>sd_lora_trainer.py</code> 文件里。</p>
<p>我来简单介绍添加新训练任务的过程。要添加新训练任务，要修改三处：</p>
<ol>
<li>创建新文件，在文件里定义配置数据类及实现训练器类。</li>
<li>在 <code>trainer.py</code> 里导入新训练器类。</li>
<li>在 <code>training_cfg_1.py</code> 里导入新配置数据类。</li>
</ol>
<p>先来看较简单的第二处和第三处修改。导入新训练器类只需要加一行 import 和一条词典项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_trainer</span>(<span class="params"><span class="built_in">type</span>, weight_dtype, accelerator, logger, cfg_dict</span>) -&gt; Trainer:</span></span><br><span class="line">    <span class="keyword">from</span> ddpm_trainer <span class="keyword">import</span> DDPMTrainer</span><br><span class="line">    <span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainer</span><br><span class="line"></span><br><span class="line">    __TYPE_CLS_DICT = &#123;</span><br><span class="line">        <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainer,</span><br><span class="line">        <span class="string">&#x27;lora&#x27;</span>: LoraTrainer</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> __TYPE_CLS_DICT[<span class="built_in">type</span>](weight_dtype, accelerator, logger, cfg_dict)</span><br></pre></td></tr></table></figure>
<p>导入新配置数据类也一样，一行 import 和一项词典项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainingConfig</span><br><span class="line"></span><br><span class="line">__TYPE_CLS_DICT = &#123;</span><br><span class="line">    <span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;lora&#x27;</span>: LoraTrainingConfig</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而实现一个训练器类会比较繁琐。我是先把 DDPM 训练器类复制了过来，在此基础上进行修改。由于 SD LoRA 训练器有官方训练脚本作为参考，我还是和之前实现 DDPM 训练器一样，从官方训练脚本里抠出对应代码，将其填入训练器类方法里。比如在初始化模块时，我们不仅需要初始化 U-Net，还有 VAE 等模块。在初始化优化器时，应该只优化 LoRA 参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoraTrainer</span>(<span class="params">Trainer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_modules</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                     enable_xformer=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                     gradient_checkpointing=<span class="literal">False</span></span>):</span></span><br><span class="line">        cfg = self.cfg</span><br><span class="line">        <span class="comment"># Load scheduler, tokenizer and models.</span></span><br><span class="line">        self.noise_scheduler = DDPMScheduler...</span><br><span class="line">        self.tokenizer = CLIPTokenizer...</span><br><span class="line">        self.text_encoder = CLIPTextModel... </span><br><span class="line">        self.vae = AutoencoderKL...</span><br><span class="line">        self.unet = UNet2DConditionModel...</span><br><span class="line">        <span class="comment"># freeze parameters of models to save more memory</span></span><br><span class="line">        self.unet.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        self.vae.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        self.text_encoder.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.unet.parameters():</span><br><span class="line">            param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        unet_lora_config = LoraConfig(...)</span><br><span class="line">        self.lora_layers = <span class="built_in">filter</span>(</span><br><span class="line">                <span class="keyword">lambda</span> p: p.requires_grad, self.unet.parameters())</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_optimizers</span>(<span class="params">self, train_batch_size</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.optimizer = torch.optim.AdamW(</span><br><span class="line">            self.lora_layers,</span><br><span class="line">            ...)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>SD LoRA 训练器类在 <code>sd_lora_trainer.py</code> 文件里，对应配置文件为 <code>cfg_lora.json</code>。用下面的代码即可尝试 LoRA 训练。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_1.py cfg_lora.json</span><br></pre></td></tr></table></figure>
<p>可能是 MNIST 数据集的图片太小了，而 SD 又是为较大的图片设计的，又或是 LoRA 的拟合能力有限，生成的效果不是很好。但可以看出，SD LoRA 学到了 MNIST 的图片风格。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/5.jpg" alt></p>
<p>就我自己使用下来，添加一个新的训练任务还是非常轻松的。我可以只关心初始化模型、训练、验证等实现细节，而不用关心那些通用的训练代码。当然，这份通用训练脚本还不够强大，还不能处理更复杂的数据集。SD LoRA 其实需要一个带文本标注的数据集，但由于我只是想测试添加新训练器的难度，就没有去改数据集，只是默认用了空文本来训练 LoRA。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我自己在使用 Diffusers 训练脚本时，发现这种训练脚本难以适配多任务训练，于是重构了一份拓展性更强的训练脚本。在这篇文章中，我先是介绍了 Diffusers 训练脚本的通用框架，再分享了我改写脚本的过程。相信读者在读完本文后，不仅能够熟悉 Diffusers 训练脚本的具体原理，还能够动手修改它，或者基于我的这一版改进脚本，编写一份适合自己的训练脚本。</p>
<p>我重构的这套训练器也没有太多封装，在维持 Diffusers 那种平铺直叙风格的同时，将每种训练任务独有的代码、数据搬了出来，让开发者专注于编写新的逻辑。我没怎么用过别的训练框架，不太好直接对比。但至少相比于 PyTorch Lightning 那种模型和训练逻辑写在同一个类里的写法，我更认可 Diffusers 这种将模型结构和训练、采样分离的设计。这套框架的训练器也只有训练的逻辑，不会掺杂其他逻辑。</p>
<p>本文的代码链接为 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/TrainingScript">https://github.com/SingleZombie/DiffusersExample/tree/main/TrainingScript</a></p>
<p><strong>注意</strong>，这份代码是我随手写的，只测试了简单的训练命令。如果发现 bug，欢迎提 issue。这份代码仅供本文教学使用，功能有限，以后我会在其他地方更新这份代码。另外，以后我写其他训练教程时也会复用这套代码。</p>
<h2 id="附录：训练器程序设计思路"><a href="#附录：训练器程序设计思路" class="headerlink" title="附录：训练器程序设计思路"></a>附录：训练器程序设计思路</h2><p>在设计训练器接口类的接口时，其实我没有做多少主观设计，基本上都是按照一些设计原则，机械地将原来的训练脚本进行重构。我也不知道这些原则是怎么想出来的，只是根据我多年写代码的经验，我感觉按照这些规则做可以保证训练脚本和训练器之间耦合度更低，易于拓展。这些原则有：</p>
<ol>
<li>如果在另一项任务里这行代码会变动，则这项代码应写入训练器类。</li>
<li>如果某一数据的调用<strong>全部</strong>都被放入了训练器类里，那么这个数据应该是训练器类的成员变量。如果该数据来自配置文件，则将该数据的定义从全局配置移入训练器配置。</li>
<li>如果某数据既要在训练脚本中使用，又要在训练器类里使用，则在训练脚本中初始化该数据，并以<strong>初始化参数</strong>或者<strong>接口参数</strong>两种方式将数据传入训练器。传入方式由数据被确定的时刻决定。比如脚本一开始就初始化好的日志对象应该作为初始化参数，而一些中途计算的当前 batch 数等参数应该作为接口参数。</li>
<li>原则上，训练脚本不从数据类里获取数据。</li>
</ol>
<p>根据这些原则，在设计训练器接口类时，我并没有一开始就定下有哪些接口、接口的参数分别是什么，而是一边搬运代码，一边根据代码的实际内容动态地编写接口类。比如一开始，我的接口类构造函数并没有加入日志库类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>(<span class="params">metaclass=ABCMeta</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, cfg</span>):</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>后来写训练器验证方法时，我发现这里必须要获取日志类的类型，不得已在构造函数里多加了一个参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, global_step</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> self.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, logger, cfg</span>):</span></span><br></pre></td></tr></table></figure></p>
<p>原则 3 和原则 4 本质上是将训练脚本也看成一个对象。所有数据要么属于训练脚本，要么属于训练类。原则 4 不从训练器里获取信息，某种程度上体现了面向对象中的封装性，不让训练器去改训练脚本里的数据。我尽可能地遵守了原则 4，但只有一处例外。在调用 <code>accelerate.prapare</code> 后，<code>train_dataloader</code> 在训练器里发生了更改。而 <code>train_dataloader</code> 其实是属于训练脚本的。没办法，这里只能去训练器里获取一次数据。我没来得及仔细研究，说不定 <code>accelerate.prapare</code> 可以多次调用，这样我就能让训练脚本自己维护 <code>train_dataloader</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer.prepare_modules()</span><br><span class="line">train_dataloader = trainer.train_dataloader</span><br></pre></td></tr></table></figure>
<p>这样看下来，这份代码框架在各种角度上都有很大的改进空间。以后我会来慢慢改进这份代码。就目前的设计，训练中整体逻辑、数据集、训练器三部分应该是相互独立的。数据集我还没有单独拿出来写。应该至少实现纯图像、带文本标注图像这两种数据集。</p>
<p>这次重构之后，我也有一些程序设计上的体会。重构代码比从头做程序设计要简单很多。重构只需要根据已有代码，设计出一套更合理的逻辑，像我这样按照某些原则，无脑地修改代码就行了。而程序设计需要考虑未知的情况，为未来可能加入的功能铺路。也正因为从头设计更难，有时会出现设计过度或者设计不足的情况。感觉更合理的开发方式是从头设计与重构交替进行。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/en/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/en/tags/%E7%BC%96%E7%A8%8B/" rel="tag"># 编程</a>
              <a href="/en/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/en/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" rel="tag"># 扩散模型</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/en/2024/07/14/20240703-SD3/" rel="prev" title="Stable Diffusion 3 论文及源码概览">
      <i class="fa fa-chevron-left"></i> Stable Diffusion 3 论文及源码概览
    </a></div>
      <div class="post-nav-item">
    <a href="/en/2024/07/27/20240717-ar-wo-vq/" rel="next" title="解读何恺明新作：不用向量离散化的自回归图像生成（Autoregressive Image Generation without Vector Quantization）">
      解读何恺明新作：不用向量离散化的自回归图像生成（Autoregressive Image Generation without Vector Quantization） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E6%BA%90%E4%BB%A3%E7%A0%81"><span class="nav-number">1.</span> <span class="nav-text">准备源代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%98%E6%96%B9%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E7%BB%86%E8%AF%BB"><span class="nav-number">2.</span> <span class="nav-text">官方训练脚本细读</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E5%86%85%E5%AE%B9%E6%80%BB%E7%BB%93"><span class="nav-number">3.</span> <span class="nav-text">训练脚本内容总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%BB%E6%8E%89%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0"><span class="nav-number">4.</span> <span class="nav-text">去掉命令行参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%82%E9%85%8D%E5%A4%9A%E7%A7%8D%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC"><span class="nav-number">5.</span> <span class="nav-text">适配多种任务的训练脚本</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E6%96%B0%E7%9A%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="nav-number">6.</span> <span class="nav-text">添加新的训练任务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">7.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A%E8%AE%AD%E7%BB%83%E5%99%A8%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF"><span class="nav-number">8.</span> <span class="nav-text">附录：训练器程序设计思路</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/en/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/en/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/en/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/en/lib/anime.min.js"></script>
  <script src="/en/lib/velocity/velocity.min.js"></script>
  <script src="/en/lib/velocity/velocity.ui.min.js"></script>

<script src="/en/js/utils.js"></script>

<script src="/en/js/motion.js"></script>


<script src="/en/js/schemes/muse.js"></script>


<script src="/en/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
