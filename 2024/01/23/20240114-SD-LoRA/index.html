<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="如果你一直关注 Stable Diffusion (SD) 社区，那你一定不会对 “LoRA” 这个名词感到陌生。社区用户分享的 SD LoRA 模型能够修改 SD 的画风，使之画出动漫、水墨或像素等风格的图片。但实际上，LoRA 不仅仅能改变 SD 的画风，还有其他的妙用。在这篇文章中，我们会先简单学习 LoRA 的原理，再认识科研中 LoRA 的三种常见应用：1） 还原单幅图像；2）风格调整；">
<meta property="og:type" content="article">
<meta property="og:title" content="LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例">
<meta property="og:url" content="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="如果你一直关注 Stable Diffusion (SD) 社区，那你一定不会对 “LoRA” 这个名词感到陌生。社区用户分享的 SD LoRA 模型能够修改 SD 的画风，使之画出动漫、水墨或像素等风格的图片。但实际上，LoRA 不仅仅能改变 SD 的画风，还有其他的妙用。在这篇文章中，我们会先简单学习 LoRA 的原理，再认识科研中 LoRA 的三种常见应用：1） 还原单幅图像；2）风格调整；">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/20240114-SD-LORA/1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/output.gif">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/2.png">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/3.png">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/4.png">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/5.png">
<meta property="article:published_time" content="2024-01-23T11:42:10.000Z">
<meta property="article:modified_time" content="2024-01-23T11:42:10.576Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="编程">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="扩散模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/20240114-SD-LORA/1.jpg">

<link rel="canonical" href="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-23 19:42:10" itemprop="dateCreated datePublished" datetime="2024-01-23T19:42:10+08:00">2024-01-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>如果你一直关注 Stable Diffusion (SD) 社区，那你一定不会对 “LoRA” 这个名词感到陌生。社区用户分享的 SD LoRA 模型能够修改 SD 的画风，使之画出动漫、水墨或像素等风格的图片。但实际上，LoRA 不仅仅能改变 SD 的画风，还有其他的妙用。在这篇文章中，我们会先简单学习 LoRA 的原理，再认识科研中 LoRA 的三种常见应用：1） 还原单幅图像；2）风格调整；3）训练目标调整，最后阅读两个基于 Diffusers 的 SD LoRA 代码实现示例。</p>
<h2 id="LoRA-的原理"><a href="#LoRA-的原理" class="headerlink" title="LoRA 的原理"></a>LoRA 的原理</h2><p>在认识 LoRA 之前，我们先来回顾一下迁移学习的有关概念。迁移学习指在一次新的训练中，复用之前已经训练过的模型的知识。如果你自己动手训练过深度学习模型，那你应该不经意间地使用到了迁移学习：比如你一个模型训练了 500 步，测试后发现效果不太理想，于是重新读取该模型的参数，又继续训练了 100 步。之前那个被训练过的模型叫做预训练模型（pre-trained model），继续训练预训练模型的过程叫做微调（fine-tune）。</p>
<p>知道了微调的概念，我们就能来认识 LoRA 了。LoRA 的全称是 Low-Rank Adaptation (低秩适配)，它是一种 Parameter-Efficient Fine-Tuning (参数高效微调，PEFT) 方法，即在微调时只训练原模型中的部分参数，以加速微调的过程。相比其他的 PEFT 方法，LoRA 之所以能脱颖而出，是因为它有几个明显的优点：</p>
<ul>
<li>从性能上来看，使用 LoRA 时，只需要存储少量被微调过的参数，而不需要把整个新模型都保存下来。同时，LoRA 的新参数可以和原模型的参数合并到一起，不会增加模型的运算时间。</li>
<li>从功能上来看，LoRA 维护了模型在微调中的「变化量」。通过用一个介于 0~1 之间的混合比例乘变化量，我们可以控制模型的修改程度。此外，基于同一个原模型独立训练的多个 LoRA 可以同时使用。</li>
</ul>
<p>这些优点在 SD LoRA 中的体现为：</p>
<ul>
<li>SD LoRA 模型一般都很小，一般只有几十 MB。</li>
<li>SD LoRA 模型的参数可以合并到 SD 基础模型里，得到一个新的 SD 模型。</li>
<li>可以用一个 0~1 之间的比例来控制 SD LoRA 新画风的程度。</li>
<li>可以把不同画风的 SD LoRA 模型以不同比例混合。</li>
</ul>
<p>为什么 LoRA 能有这些优点呢？LoRA 名字中的 「低秩」又是什么意思呢？让我们从 LoRA 的优点入手，逐步揭示它原理。</p>
<p>上文提到过，LoRA 之所以那么灵活，是因为它维护了模型在微调过程中的变化量。那么，假设我们正在修改模型中的一个参数 $W \in \mathbb{R}^{d \times d}$，我们就应该维护它的变化量 $\Delta W \in \mathbb{R}^{d \times d}$，训练时的参数用 $W + \Delta W$ 表示。这样，想要在推理时控制模型的修改程度，只要添加一个 $\alpha \in [0, 1]$，令使用的参数为 $W + \alpha \Delta W$即可。</p>
<p>可是，这样做我们还是要记录一个和原参数矩阵一样大的参数矩阵 $\Delta W$，这就算不上是参数<strong>高效</strong>微调了。为此，LoRA 的作者提出假设：模型参数在微调时的变化量中蕴含的信息没有那么多。为了用更少的信息来表示参数的变化量$\Delta W$，我们可以把$\Delta W$拆解成两个低秩矩阵的乘积：</p>
<script type="math/tex; mode=display">
\Delta W = BA</script><p>其中，$A \in \mathbb{R}^{r \times d}$, $B \in \mathbb{R}^{d \times r}$，$d$ 是一个比 $r$ 小得多的数。这样，通过用两个参数量少得多的矩阵 $A, B$ 来维护变化量，我们不仅提高了微调的效率，还保持了使用变化量来描述微调过程的灵活性。这就是 LoRA 的全部原理，它十分简单，用 $\Delta W = BA$ 这一行公式足以表示。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/20240114-SD-LORA/1.jpg" alt></p>
<p>了解了 LoRA 的原理，我们再回头看前文提及的 LoRA 的四项优点。LoRA 模型由许多参数量较少的矩阵 $A, B$ 来表示，它可以被单独存储，且占用空间不大。由于 $\Delta W = BA$ 维护的其实是参数的变化量，我们既可以把它与预训练模型的参数加起来得到一个新模型以提高推理速度，也可以在线地用一个混合比例来灵活地组合新旧模型。LoRA 的最后一个优点是各个基于同一个原模型独立训练出来的 LoRA 模型可以混合使用。LoRA 甚至可以作用于被其他方式修改过的原模型，比如 SD LoRA 支持带 ControlNet 的 SD。这一点其实来自于社区用户的实践。一个可能的解释是，LoRA 用低秩矩阵来表示变化量，这种低秩的变化量恰好与其他方法的变化量「错开」，使得 LoRA 能向着一个不干扰其他方法的方向修改模型。</p>
<p>我们最后来学习一下 LoRA 的实现细节。LoRA 有两个超参数，除了上文中提到的$r$，还有一个叫$\alpha$的参数。LoRA 的作者在实现 LoRA 模块时，给修改量乘了一个 $\frac{\alpha}{r}$ 的系数，即对于输入$x$，带了 LoRA 模块后的输出为 $Wx + \frac{\alpha}{r}BAx$。作者解释说，调这个参数几乎等于调学习率，一开始令$\alpha=r$即可。在我们要反复调超参数$r$时，只要保持$\alpha$不变，就不用改其他超参数了（因为不加$\alpha$的话，改了$r$后，学习率等参数也得做相应调整以维持同样的训练条件）。当然，实际运用中，LoRA 的超参数很好调。一般令$r=4, 8, 16$即可。由于我们不怎么会变$r$，总是令$\alpha=r$就够了。</p>
<p>为了使用 LoRA，除了确定超参数外，我们还得指定需要被微调的参数矩阵。在 SD 中使用 LoRA 时，大家一般会对 SD 的 U-Net 的所有多头注意力模块的所有参数矩阵做微调。即对于多头注意力模块的四个矩阵 $W_Q, W_K, W_V, W_{out}$ 进行微调。</p>
<h2 id="LoRA-在-SD-中的三种运用"><a href="#LoRA-在-SD-中的三种运用" class="headerlink" title="LoRA 在 SD 中的三种运用"></a>LoRA 在 SD 中的三种运用</h2><p>LoRA 在 SD 的科研中有着广泛的应用。按照使用 LoRA 的动机，我们可以把 LoRA 的应用分成：1） 还原单幅图像；2）风格调整；3）训练目标调整。通过学习这些应用，我们能更好地理解 LoRA 的本质。</p>
<h3 id="还原单幅图像"><a href="#还原单幅图像" class="headerlink" title="还原单幅图像"></a>还原单幅图像</h3><p>SD 只是一个生成任意图片的模型。为了用 SD 来编辑一张给定的图片，我们一般要让 SD 先学会生成一张一模一样的图片，再在此基础上做修改。可是，由于训练集和输入图片的差异，SD 或许不能生成完全一样的图片。解决这个问题的思路很简单粗暴：我们只用这一张图片来微调 SD，让 SD 在这张图片上过拟合。这样，SD 的输出就会和这张图片非常相似了。</p>
<p>较早介绍这种提高输入图片保真度方法的工作是 Imagic，只不过它采取的是完全微调策略。后续的 DragDiffusion 也用了相同的方法，并使用 LoRA 来代替完全微调。近期的 DiffMorpher 为了实现两幅图像间的插值，不仅对两幅图像单独训练了 LoRA，还通过两个 LoRA 间的插值来平滑图像插值的过程。</p>
<h3 id="风格调整"><a href="#风格调整" class="headerlink" title="风格调整"></a>风格调整</h3><p>LoRA 在 SD 社区中最受欢迎的应用就是风格调整了。我们希望 SD 只生成某一画风，或者某一人物的图片。为此，我们只需要在一个符合我们要求的训练集上直接训练 SD LoRA 即可。</p>
<p>由于这种调整 SD 风格的方法非常直接，没有特别介绍这种方法的论文。稍微值得一提的是基于 SD 的视频模型 AnimateDiff，它用 LoRA 来控制输出视频的视角变换，而不是控制画风。</p>
<p>由于 SD 风格化 LoRA 已经被广泛使用，能否兼容 SD 风格化 LoRA 决定了一个工作是否易于在社区中传播。</p>
<h3 id="训练目标调整"><a href="#训练目标调整" class="headerlink" title="训练目标调整"></a>训练目标调整</h3><p>最后一个应用就有一点返璞归真了。LoRA 最初的应用就是把一个预训练模型适配到另一任务上。比如 GPT 一开始在大量语料中训练，随后在问答任务上微调。对于 SD 来说，我们也可以修改 U-Net 的训练目标，以提升 SD 的能力。</p>
<p>有不少相关工作用 LoRA 来改进 SD。比如 Smooth Diffusion 通过在训练目标中添加一个约束项并进行 LoRA 微调来使得 SD 的隐空间更加平滑。近期比较火的高速图像生成方法 LCM-LoRA 也是把原本作用于 SD 全参数上的一个模型蒸馏过程用 LoRA 来实现。</p>
<h3 id="SD-LoRA-应用总结"><a href="#SD-LoRA-应用总结" class="headerlink" title="SD LoRA 应用总结"></a>SD LoRA 应用总结</h3><p>尽管上述三种 SD LoRA 应用的设计出发点不同，它们本质上还是在利用微调这一迁移学习技术来调整模型的数据分布或者训练目标。LoRA 只是众多高效微调方法中的一种，只要是微调能实现的功能，LoRA 基本都能实现，只不过 LoRA 更轻便而已。如果你想微调 SD 又担心计算资源不够，那么用 LoRA 准没错。反过来说，你想用 LoRA 在 SD 上设计出一个新应用，就要去思考微调 SD 能够做到哪些事。</p>
<h2 id="Diffusers-SD-LoRA-代码实战"><a href="#Diffusers-SD-LoRA-代码实战" class="headerlink" title="Diffusers SD LoRA 代码实战"></a>Diffusers SD LoRA 代码实战</h2><p>看完了原理，我们来尝试用 Diffusers 自己训一训 LoRA。我们会先学习 Diffusers 训练 LoRA 的脚本，再学习两个简单的 LoRA 示例： SD 图像插值与 SD 图像风格迁移。</p>
<p>项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA">https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA</a></p>
<h3 id="Diffusers-脚本"><a href="#Diffusers-脚本" class="headerlink" title="Diffusers 脚本"></a>Diffusers 脚本</h3><p>我们将参考 Diffusers 中的 SD LoRA 文档 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/training/lora">https://huggingface.co/docs/diffusers/training/lora</a> ，使用官方脚本 <code>examples/text_to_image/train_text_to_image_lora.py</code> 训练 LoRA。为了使用这个脚本，建议直接克隆官方仓库，并安装根目录和 <code>text_to_image</code> 目录下的依赖文件。本文使用的 Diffusers 版本是 0.26.0，过旧的 Diffusers 的代码可能和本文展示的有所出入。目前，官方文档也描述的是旧版的代码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/huggingface/diffusers</span><br><span class="line">cd diffusers</span><br><span class="line">pip install .</span><br><span class="line">cd examples/text_to_image</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>这份代码使用 accelerate 库管理 PyTorch 的训练。对同一份代码，只需要修改 accelerate 的配置，就能实现单卡训练或者多卡训练。默认情况下，用 <code>accelerate launch</code> 命令运行 Python 脚本会使用所有显卡。如果你需要修改训练配置，请参考相关文档使用 <code>accelerate config</code> 命令配置环境。</p>
<p>做好准备后，我们来开始阅读 <code>examples/text_to_image/train_text_to_image_lora.py</code> 的代码。这份代码写得十分易懂，复杂的地方都有注释。我们跳过命令行参数部分，直接从 <code>main</code> 函数开始读。</p>
<p>一开始，函数会配置 accelerate 库及日志记录器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">args = parse_args()</span><br><span class="line">logging_dir = Path(args.output_dir, args.logging_dir)</span><br><span class="line"></span><br><span class="line">accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator(</span><br><span class="line">    gradient_accumulation_steps=args.gradient_accumulation_steps,</span><br><span class="line">    mixed_precision=args.mixed_precision,</span><br><span class="line">    log_with=args.report_to,</span><br><span class="line">    project_config=accelerator_project_config,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> args.report_to == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_wandb_available():</span><br><span class="line">        <span class="keyword">raise</span> ImportError(<span class="string">&quot;Make sure to install wandb if you want to use it for logging during training.&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> wandb</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make one log on every process with the configuration for debugging.</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">)</span><br><span class="line">logger.info(accelerator.state, main_process_only=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">if</span> accelerator.is_local_main_process:</span><br><span class="line">    datasets.utils.logging.set_verbosity_warning()</span><br><span class="line">    transformers.utils.logging.set_verbosity_warning()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_info()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    datasets.utils.logging.set_verbosity_error()</span><br><span class="line">    transformers.utils.logging.set_verbosity_error()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_error()</span><br></pre></td></tr></table></figure>
<p>随后的代码决定是否手动设置随机种子。保持默认即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If passed along, set the training seed now.</span></span><br><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    set_seed(args.seed)</span><br></pre></td></tr></table></figure>
<p>接着，函数会创建输出文件夹。如果我们想把模型推送到在线仓库上，函数还会创建一个仓库。我们的项目不必上传，忽略所有 <code>args.push_to_hub</code> 即可。另外，<code>if accelerator.is_main_process:</code> 表示多卡训练时只有主进程会执行这段代码块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Handle the repository creation</span></span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        os.makedirs(args.output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        repo_id = create_repo(</span><br><span class="line">            repo_id=args.hub_model_id <span class="keyword">or</span> Path(args.output_dir).name, exist_ok=<span class="literal">True</span>, token=args.hub_token</span><br><span class="line">        ).repo_id</span><br></pre></td></tr></table></figure>
<p>准备完辅助工具后，函数正式开始着手训练。训练前，函数会先实例化好一切处理类，包括用于维护扩散模型中间变量的 <code>DDPMScheduler</code>，负责编码输入文本的 <code>CLIPTokenizer, CLIPTextModel</code>，压缩图像的VAE <code>AutoencoderKL</code>，预测噪声的 U-Net <code>UNet2DConditionModel</code>。参数 <code>args.pretrained_model_name_or_path</code> 是 Diffusers 在线仓库的地址（如<code>runwayml/stable-diffusion-v1-5</code>），或者本地的 Diffusers 模型文件夹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load scheduler, tokenizer and models.</span></span><br><span class="line">noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;scheduler&quot;</span>)</span><br><span class="line">tokenizer = CLIPTokenizer.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;tokenizer&quot;</span>, revision=args.revision</span><br><span class="line">)</span><br><span class="line">text_encoder = CLIPTextModel.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;text_encoder&quot;</span>, revision=args.revision</span><br><span class="line">)</span><br><span class="line">vae = AutoencoderKL.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;vae&quot;</span>, revision=args.revision, variant=args.variant</span><br><span class="line">)</span><br><span class="line">unet = UNet2DConditionModel.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;unet&quot;</span>, revision=args.revision, variant=args.variant</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>函数还会设置各个带参数模型是否需要计算梯度。由于我们待会要优化的是新加入的 LoRA 模型，所有预训练模型都不需要计算梯度。另外，函数还会根据 accelerate 配置自动设置这些模型的精度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># freeze parameters of models to save more memory</span></span><br><span class="line">unet.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">vae.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">text_encoder.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze the unet parameters before adding adapters</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> unet.parameters():</span><br><span class="line">    param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to half-precision</span></span><br><span class="line"><span class="comment"># as these weights are only used for inference, keeping weights in full precision is not required.</span></span><br><span class="line">weight_dtype = torch.float32</span><br><span class="line"><span class="keyword">if</span> accelerator.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.float16</span><br><span class="line"><span class="keyword">elif</span> accelerator.mixed_precision == <span class="string">&quot;bf16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.bfloat16</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move unet, vae and text_encoder to device and cast to weight_dtype</span></span><br><span class="line">unet.to(accelerator.device, dtype=weight_dtype)</span><br><span class="line">vae.to(accelerator.device, dtype=weight_dtype)</span><br><span class="line">text_encoder.to(accelerator.device, dtype=weight_dtype)</span><br></pre></td></tr></table></figure></p>
<p>把预训练模型都调好了后，函数会配置 LoRA 模块并将其加入 U-Net 模型中。最近，Diffusers 更新了添加 LoRA 的方式。Diffusers 用 Attention 处理器来描述 Attention 的计算。为了把 LoRA 加入到 Attention 模块中，早期的 Diffusers 直接在 Attention 处理器里加入可训练参数。现在，为了和其他 Hugging Face 库统一，Diffusers 使用 PEFT 库来管理 LoRA。我们不需要关注 LoRA 的实现细节，只需要写一个 <code>LoraConfig</code> 就行了。</p>
<blockquote>
<p>PEFT 中的 LoRA 文档参见 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/peft/conceptual_guides/lora">https://huggingface.co/docs/peft/conceptual_guides/lora</a></p>
</blockquote>
<p><code>LoraConfig</code> 中有四个主要参数: <code>r, lora_alpha, init_lora_weights, target_modules</code>。 <code>r, lora_alpha</code> 的意义我们已经在前文中见过了，前者决定了 LoRA 矩阵的大小，后者决定了训练速度。默认配置下，它们都等于同一个值 <code>args.rank</code>。<code>init_lora_weights</code> 表示如何初始化训练参数，<code>gaussian</code>是论文中使用的方法。<code>target_modules</code> 表示 Attention 模块的哪些层需要添加 LoRA。按照通常的做法，会给所有层，即三个输入变换矩阵 <code>to_k, to_q, to_v</code> 和一个输出变换矩阵 <code>to_out.0</code> 加 LoRA。</p>
<p>创建了配置后，用 <code>unet.add_adapter(unet_lora_config)</code> 就可以创建 LoRA 模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unet_lora_config = LoraConfig(</span><br><span class="line">    r=args.rank,</span><br><span class="line">    lora_alpha=args.rank,</span><br><span class="line">    init_lora_weights=<span class="string">&quot;gaussian&quot;</span>,</span><br><span class="line">    target_modules=[<span class="string">&quot;to_k&quot;</span>, <span class="string">&quot;to_q&quot;</span>, <span class="string">&quot;to_v&quot;</span>, <span class="string">&quot;to_out.0&quot;</span>],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">unet.add_adapter(unet_lora_config)</span><br><span class="line"><span class="keyword">if</span> args.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> unet.parameters():</span><br><span class="line">        <span class="comment"># only upcast trainable parameters (LoRA) into fp32</span></span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            param.data = param.to(torch.float32)</span><br></pre></td></tr></table></figure>
<p>更新完了 U-Net 的结构，函数会尝试启用 <code>xformers</code> 来提升 Attention 的效率。PyTorch 在 2.0 版本也加入了类似的 Attention 优化技术。如果你的显卡性能有限，且 PyTorch 版本小于 2.0，可以考虑使用 <code>xformers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.enable_xformers_memory_efficient_attention:</span><br><span class="line">  <span class="keyword">if</span> is_xformers_available():</span><br><span class="line">      <span class="keyword">import</span> xformers</span><br><span class="line"></span><br><span class="line">      xformers_version = version.parse(xformers.__version__)</span><br><span class="line">      <span class="keyword">if</span> xformers_version == version.parse(<span class="string">&quot;0.0.16&quot;</span>):</span><br><span class="line">          logger.warn(</span><br><span class="line">              ...</span><br><span class="line">          )</span><br><span class="line">      unet.enable_xformers_memory_efficient_attention()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;xformers is not available. Make sure it is installed correctly&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>做完了 U-Net 的处理后，函数会过滤出要优化的模型参数，这些参数稍后会传递给优化器。过滤的原则很简单，如果参数要求梯度，就是待优化参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lora_layers = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, unet.parameters())</span><br></pre></td></tr></table></figure>
<p>之后是优化器的配置。函数先是配置了一些细枝末节的训练选项，一般可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.gradient_checkpointing:</span><br><span class="line">    unet.enable_gradient_checkpointing()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable TF32 for faster training on Ampere GPUs,</span></span><br><span class="line"><span class="comment"># cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices</span></span><br><span class="line"><span class="keyword">if</span> args.allow_tf32:</span><br><span class="line">    torch.backends.cuda.matmul.allow_tf32 = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>然后是优化器的选择。我们可以忽略其他逻辑，直接用 <code>AdamW</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the optimizer</span></span><br><span class="line"><span class="keyword">if</span> args.use_8bit_adam:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">import</span> bitsandbytes <span class="keyword">as</span> bnb</span><br><span class="line">    <span class="keyword">except</span> ImportError:</span><br><span class="line">        <span class="keyword">raise</span> ImportError(</span><br><span class="line">            <span class="string">&quot;...&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    optimizer_cls = bnb.optim.AdamW8bit</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    optimizer_cls = torch.optim.AdamW</span><br></pre></td></tr></table></figure>
<p>选择了优化器类，就可以实例化优化器了。优化器的第一个参数是之前准备好的待优化 LoRA 参数，其他参数是 Adam 优化器本身的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizer_cls(</span><br><span class="line">    lora_layers,</span><br><span class="line">    lr=args.learning_rate,</span><br><span class="line">    betas=(args.adam_beta1, args.adam_beta2),</span><br><span class="line">    weight_decay=args.adam_weight_decay,</span><br><span class="line">    eps=args.adam_epsilon,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>准备了优化器，之后需要准备训练集。这个脚本用 Hugging Face 的 datasets 库来管理数据集。我们既可以读取在线数据集，也可以读取本地的图片文件夹数据集。在本文的示例项目中，我们将使用图片文件夹数据集。稍后我们再详细学习这样的数据集文件夹该怎么构建。相关的文档可以参考 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder">https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</a> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.dataset_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># Downloading and loading a dataset from the hub.</span></span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        args.dataset_name,</span><br><span class="line">        args.dataset_config_name,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">        data_dir=args.train_data_dir,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_files = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> args.train_data_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        data_files[<span class="string">&quot;train&quot;</span>] = os.path.join(args.train_data_dir, <span class="string">&quot;**&quot;</span>)</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        <span class="string">&quot;imagefolder&quot;</span>,</span><br><span class="line">        data_files=data_files,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># See more about loading custom images at</span></span><br><span class="line">    <span class="comment"># https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</span></span><br></pre></td></tr></table></figure>
<p>训练 SD 时，每一个数据样本需要包含两项信息：图像数据与对应的文本描述。在数据集 <code>dataset</code> 中，每个数据样本包含了多项属性。下面的代码用于从这些属性中取出图像与文本描述。默认情况下，第一个属性会被当做图像数据，第二个属性会被当做文本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets.</span></span><br><span class="line"><span class="comment"># We need to tokenize inputs and targets.</span></span><br><span class="line">column_names = dataset[<span class="string">&quot;train&quot;</span>].column_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Get the column names for input/target.</span></span><br><span class="line">dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, <span class="literal">None</span>)</span><br><span class="line"><span class="keyword">if</span> args.image_column <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    image_column = dataset_columns[<span class="number">0</span>] <span class="keyword">if</span> dataset_columns <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> column_names[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    image_column = args.image_column</span><br><span class="line">    <span class="keyword">if</span> image_column <span class="keyword">not</span> <span class="keyword">in</span> column_names:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;--image_column&#x27; value &#x27;<span class="subst">&#123;args.image_column&#125;</span>&#x27; needs to be one of: <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(column_names)&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line"><span class="keyword">if</span> args.caption_column <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    caption_column = dataset_columns[<span class="number">1</span>] <span class="keyword">if</span> dataset_columns <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> column_names[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    caption_column = args.caption_column</span><br><span class="line">    <span class="keyword">if</span> caption_column <span class="keyword">not</span> <span class="keyword">in</span> column_names:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;--caption_column&#x27; value &#x27;<span class="subst">&#123;args.caption_column&#125;</span>&#x27; needs to be one of: <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(column_names)&#125;</span>&quot;</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>准备好了数据集，接下来要定义数据预处理流程以创建 <code>DataLoader</code>。函数先定义了一个把文本标签预处理成 token ID 的 token 化函数。我们不需要修改它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_captions</span>(<span class="params">examples, is_train=<span class="literal">True</span></span>):</span></span><br><span class="line">    captions = []</span><br><span class="line">    <span class="keyword">for</span> caption <span class="keyword">in</span> examples[caption_column]:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(caption, <span class="built_in">str</span>):</span><br><span class="line">            captions.append(caption)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(caption, (<span class="built_in">list</span>, np.ndarray)):</span><br><span class="line">            <span class="comment"># take a random caption if there are multiple</span></span><br><span class="line">            captions.append(random.choice(caption) <span class="keyword">if</span> is_train <span class="keyword">else</span> caption[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Caption column `<span class="subst">&#123;caption_column&#125;</span>` should contain either strings or lists of strings.&quot;</span></span><br><span class="line">            )</span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        captions, max_length=tokenizer.model_max_length, padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> inputs.input_ids</span><br></pre></td></tr></table></figure>
<p>接着，函数定义了图像数据的预处理流程。该流程是用 <code>torchvision</code> 中的 <code>transforms</code> 实现的。如代码所示，处理流程中包括了 resize 至指定分辨率 <code>args.resolution</code>、将图像长宽均裁剪至指定分辨率、随机翻转、转换至 tensor 和归一化。</p>
<p>经过这一套预处理后，所有图像的长宽都会被设置为 <code>args.resolution</code> 。统一图像的尺寸，主要的目的是对齐数据，以使多个数据样本能拼接成一个 batch。注意，数据预处理流程中包括了随机裁剪。如果数据集里的多数图片都长宽不一致，模型会倾向于生成被裁剪过的图片。为了解决这一问题，要么自己手动预处理图片，使训练图片都是分辨率至少为 <code>args.resolution</code> 的正方形图片，要么令 batch size 为 1 并取消掉随机裁剪。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets.</span></span><br><span class="line">train_transforms = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize(</span><br><span class="line">            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),</span><br><span class="line">        transforms.CenterCrop(</span><br><span class="line">            args.resolution) <span class="keyword">if</span> args.center_crop <span class="keyword">else</span> transforms.RandomCrop(args.resolution),</span><br><span class="line">        transforms.RandomHorizontalFlip() <span class="keyword">if</span> args.random_flip <span class="keyword">else</span> transforms.Lambda(<span class="keyword">lambda</span> x: x),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>定义了预处理流程后，函数对所有数据进行预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_train</span>(<span class="params">examples</span>):</span></span><br><span class="line">    images = [image.convert(<span class="string">&quot;RGB&quot;</span>) <span class="keyword">for</span> image <span class="keyword">in</span> examples[image_column]]</span><br><span class="line">    examples[<span class="string">&quot;pixel_values&quot;</span>] = [</span><br><span class="line">        train_transforms(image) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br><span class="line">    examples[<span class="string">&quot;input_ids&quot;</span>] = tokenize_captions(examples)</span><br><span class="line">    <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> accelerator.main_process_first():</span><br><span class="line">    <span class="keyword">if</span> args.max_train_samples <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        dataset[<span class="string">&quot;train&quot;</span>] = dataset[<span class="string">&quot;train&quot;</span>].shuffle(</span><br><span class="line">            seed=args.seed).select(<span class="built_in">range</span>(args.max_train_samples))</span><br><span class="line">    <span class="comment"># Set the training transforms</span></span><br><span class="line">    train_dataset = dataset[<span class="string">&quot;train&quot;</span>].with_transform(preprocess_train)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>之后函数用预处理过的数据集创建 <code>DataLoader</code>。这里要注意的参数是 batch size <code>args.train_batch_size</code> 和读取数据的进程数 <code>args.dataloader_num_workers</code> 。这两个参数的用法和一般的 PyTorch 项目一样。<code>args.train_batch_size</code> 决定了训练速度，一般设置到不爆显存的最大值。如果要读取的数据过多，导致数据读取成为了模型训练的速度瓶颈，则应该提高 <code>args.dataloader_num_workers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">examples</span>):</span></span><br><span class="line">    pixel_values = torch.stack([example[<span class="string">&quot;pixel_values&quot;</span>]</span><br><span class="line">                                <span class="keyword">for</span> example <span class="keyword">in</span> examples])</span><br><span class="line">    pixel_values = pixel_values.to(</span><br><span class="line">        memory_format=torch.contiguous_format).<span class="built_in">float</span>()</span><br><span class="line">    input_ids = torch.stack([example[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> example <span class="keyword">in</span> examples])</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;pixel_values&quot;</span>: pixel_values, <span class="string">&quot;input_ids&quot;</span>: input_ids&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataLoaders creation:</span></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(</span><br><span class="line">    train_dataset,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=collate_fn,</span><br><span class="line">    batch_size=args.train_batch_size,</span><br><span class="line">    num_workers=args.dataloader_num_workers,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>如果想用更大的 batch size，显存又不够，则可以使用梯度累计技术。使用这项技术时，训练梯度不会每步优化，而是累计了若干步后再优化。<code>args.gradient_accumulation_steps</code> 表示要累计几步再优化模型。实际的 batch size 等于输入 batch size 乘 GPU 数乘梯度累计步数。下面的代码维护了训练步数有关的信息，并创建了学习率调度器。我们按照默认设置使用一个常量学习率即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scheduler and math around the number of training steps.</span></span><br><span class="line">overrode_max_train_steps = <span class="literal">False</span></span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line"><span class="keyword">if</span> args.max_train_steps <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch</span><br><span class="line">    overrode_max_train_steps = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    args.lr_scheduler,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,</span><br><span class="line">    num_training_steps=args.max_train_steps * accelerator.num_processes,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare everything with our `accelerator`.</span></span><br><span class="line">unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(</span><br><span class="line">    unet, optimizer, train_dataloader, lr_scheduler</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We need to recalculate our total training steps as the size of the training dataloader may have changed.</span></span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line"><span class="keyword">if</span> overrode_max_train_steps:</span><br><span class="line">    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch</span><br><span class="line"><span class="comment"># Afterwards we recalculate our number of training epochs</span></span><br><span class="line">args.num_train_epochs = math.ceil(</span><br><span class="line">    args.max_train_steps / num_update_steps_per_epoch)</span><br></pre></td></tr></table></figure>
<p>在准备工作的最后，函数会用 accelerate 库记录配置信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    accelerator.init_trackers(<span class="string">&quot;text2image-fine-tune&quot;</span>, config=<span class="built_in">vars</span>(args))</span><br></pre></td></tr></table></figure>
<p>终于，要开始训练了。训练开始前，函数会准备全局变量并记录日志。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train!</span></span><br><span class="line">total_batch_size = args.train_batch_size * \</span><br><span class="line">    accelerator.num_processes * args.gradient_accumulation_steps</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;***** Running training *****&quot;</span>)</span><br><span class="line">...</span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line">first_epoch = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>此时，如果设置了 <code>args.resume_from_checkpoint</code>，则函数会读取之前训练过的权重。一般继续训练时可以把该参数设为 <code>latest</code>，程序会自动找最新的权重。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Potentially load in the weights and states from a previous save</span></span><br><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint:</span><br><span class="line">    <span class="keyword">if</span> args.resume_from_checkpoint != <span class="string">&quot;latest&quot;</span>:</span><br><span class="line">        path = ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the most recent checkpoint</span></span><br><span class="line">        path = ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        args.resume_from_checkpoint = <span class="literal">None</span></span><br><span class="line">        initial_global_step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        accelerator.load_state(os.path.join(args.output_dir, path))</span><br><span class="line">        global_step = <span class="built_in">int</span>(path.split(<span class="string">&quot;-&quot;</span>)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        initial_global_step = global_step</span><br><span class="line">        first_epoch = global_step // num_update_steps_per_epoch</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    initial_global_step = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>随后，函数根据总步数和已经训练过的步数设置迭代器，正式进入训练循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">progress_bar = tqdm(</span><br><span class="line">    <span class="built_in">range</span>(<span class="number">0</span>, args.max_train_steps),</span><br><span class="line">    initial=initial_global_step,</span><br><span class="line">    desc=<span class="string">&quot;Steps&quot;</span>,</span><br><span class="line">    <span class="comment"># Only show the progress bar once on each machine.</span></span><br><span class="line">    disable=<span class="keyword">not</span> accelerator.is_local_main_process,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(first_epoch, args.num_train_epochs):</span><br><span class="line">    unet.train()</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        <span class="keyword">with</span> accelerator.accumulate(unet):</span><br></pre></td></tr></table></figure>
<p>训练的过程基本和 LDM 论文中展示的一致。一开始，要取出图像<code>batch[&quot;pixel_values&quot;]</code> 并用 VAE 把它压缩进隐空间。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert images to latent space</span></span><br><span class="line">latents = vae.encode(batch[<span class="string">&quot;pixel_values&quot;</span>].to(</span><br><span class="line">    dtype=weight_dtype)).latent_dist.sample()</span><br><span class="line">latents = latents * vae.config.scaling_factor</span><br></pre></td></tr></table></figure><br>再随机生成一个噪声。该噪声会套入扩散模型前向过程的公式，和输入图像一起得到 <code>t</code> 时刻的带噪图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sample noise that we&#x27;ll add to the latents</span></span><br><span class="line">noise = torch.randn_like(latents)</span><br></pre></td></tr></table></figure>
<p>下一步，这里插入了一个提升扩散模型训练质量的小技巧，用上它后输出图像的颜色分布会更合理。原理见注释中的链接。<code>args.noise_offset</code> 默认为 0。如果要启用这个特性，一般令 <code>args.noise_offset = 0.1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.noise_offset:</span><br><span class="line">    <span class="comment"># https://www.crosslabs.org//blog/diffusion-with-offset-noise</span></span><br><span class="line">    noise += args.noise_offset * torch.randn(</span><br><span class="line">        (latents.shape[<span class="number">0</span>], latents.shape[<span class="number">1</span>], <span class="number">1</span>, <span class="number">1</span>), device=latents.device</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>然后是时间戳的随机生成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bsz = latents.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># Sample a random timestep for each image</span></span><br><span class="line">timesteps = torch.randint(</span><br><span class="line">    <span class="number">0</span>, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)</span><br><span class="line">timesteps = timesteps.long()</span><br></pre></td></tr></table></figure></p>
<p>时间戳和前面随机生成的噪声一起经 DDPM 的前向过程得到带噪图片 <code>noisy_latents</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add noise to the latents according to the noise magnitude at each timestep</span></span><br><span class="line"><span class="comment"># (this is the forward diffusion process)</span></span><br><span class="line">noisy_latents = noise_scheduler.add_noise(</span><br><span class="line">    latents, noise, timesteps)</span><br></pre></td></tr></table></figure>
<p>再把文本 <code>batch[&quot;input_ids&quot;]</code> 编码，为之后的 U-Net 前向传播做准备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the text embedding for conditioning</span></span><br><span class="line">encoder_hidden_states = text_encoder(batch[<span class="string">&quot;input_ids&quot;</span>])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>在 U-Net 推理开始前，函数这里做了一个关于 U-Net 输出类型的判断。一般 U-Net 都是输出预测的噪声 <code>epsilon</code>，可以忽略这段代码。当 U-Net 是想预测噪声时，要拟合的目标是之前随机生成的噪声 <code>noise</code> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the target for loss depending on the prediction type</span></span><br><span class="line"><span class="keyword">if</span> args.prediction_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># set prediction_type of scheduler if defined</span></span><br><span class="line">    noise_scheduler.register_to_config(</span><br><span class="line">        prediction_type=args.prediction_type)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> noise_scheduler.config.prediction_type == <span class="string">&quot;epsilon&quot;</span>:</span><br><span class="line">    target = noise</span><br><span class="line"><span class="keyword">elif</span> noise_scheduler.config.prediction_type == <span class="string">&quot;v_prediction&quot;</span>:</span><br><span class="line">    target = noise_scheduler.get_velocity(</span><br><span class="line">        latents, noise, timesteps)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">f&quot;Unknown prediction type <span class="subst">&#123;noise_scheduler.config.prediction_type&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>之后把带噪图像、时间戳、文本编码输入进 U-Net，U-Net 输出预测的噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict the noise residual and compute loss</span></span><br><span class="line">model_pred = unet(noisy_latents, timesteps,</span><br><span class="line">                  encoder_hidden_states).sample</span><br></pre></td></tr></table></figure>
<p>有了预测值，下一步是算 loss。这里又可以选择是否使用一种加速训练的技术。如果使用，则 <code>args.snr_gamma</code> 推荐设置为 5.0。原 DDPM 的做法是直接算预测噪声和真实噪声的均方误差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.snr_gamma <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    loss = F.mse_loss(model_pred.<span class="built_in">float</span>(),</span><br><span class="line">                      target.<span class="built_in">float</span>(), reduction=<span class="string">&quot;mean&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>训练迭代的最后，要用 accelerate 库来完成梯度计算和反向传播。在更新梯度前，可以通过设置 <code>args.max_grad_norm</code> 来裁剪梯度，以防梯度过大。<code>args.max_grad_norm</code> 默认为 1.0。代码中的 <code>if accelerator.sync_gradients:</code> 可以保证所有 GPU 都同步了梯度再执行后续代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Backpropagate</span></span><br><span class="line">accelerator.backward(loss)</span><br><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    params_to_clip = lora_layers</span><br><span class="line">    accelerator.clip_grad_norm_(</span><br><span class="line">        params_to_clip, args.max_grad_norm)</span><br><span class="line">optimizer.step()</span><br><span class="line">lr_scheduler.step()</span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>一步训练结束后，更新和步数相关的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    global_step += <span class="number">1</span></span><br><span class="line">    accelerator.log(&#123;<span class="string">&quot;train_loss&quot;</span>: train_loss&#125;, step=global_step)</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>脚本默认每 <code>args.checkpointing_steps</code> 步保存一次中间结果。当需要保存时，函数会清理多余的 checkpoint，再把模型状态和 LoRA 模型分别保存下来。<code>accelerator.save_state(save_path)</code> 负责把模型及优化器等训练用到的所有状态存下来，后面的 <code>StableDiffusionPipeline.save_lora_weights</code> 负责存储 LoRA 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> global_step % args.checkpointing_steps == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">        <span class="comment"># _before_ saving state, check if this save would set us over the `checkpoints_total_limit`</span></span><br><span class="line">        <span class="keyword">if</span> args.checkpoints_total_limit <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            checkpoints = ...</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(checkpoints) &gt;= args.checkpoints_total_limit:</span><br><span class="line">                <span class="comment"># remove ckpt</span></span><br><span class="line">                ...</span><br><span class="line"></span><br><span class="line">        save_path = os.path.join(</span><br><span class="line">            args.output_dir, <span class="string">f&quot;checkpoint-<span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">        accelerator.save_state(save_path)</span><br><span class="line"></span><br><span class="line">        unwrapped_unet = accelerator.unwrap_model(unet)</span><br><span class="line">        unet_lora_state_dict = convert_state_dict_to_diffusers(</span><br><span class="line">            get_peft_model_state_dict(unwrapped_unet)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        StableDiffusionPipeline.save_lora_weights(</span><br><span class="line">            save_directory=save_path,</span><br><span class="line">            unet_lora_layers=unet_lora_state_dict,</span><br><span class="line">            safe_serialization=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        logger.info(<span class="string">f&quot;Saved state to <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>训练循环的最后，函数会更新进度条上的信息，并根据当前的训练步数决定是否停止训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logs = &#123;<span class="string">&quot;step_loss&quot;</span>: loss.detach().item(</span><br><span class="line">), <span class="string">&quot;lr&quot;</span>: lr_scheduler.get_last_lr()[<span class="number">0</span>]&#125;</span><br><span class="line">progress_bar.set_postfix(**logs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> global_step &gt;= args.max_train_steps:</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>训完每一个 epoch 后，函数会进行验证。默认的验证方法是新建一个图像生成 pipeline，生成一些图片并保存。如果有其他验证方法，如计算某一指标，可以自行编写这部分的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">   <span class="keyword">if</span> args.validation_prompt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> epoch % args.validation_epochs == <span class="number">0</span>:</span><br><span class="line">       logger.info(</span><br><span class="line">           <span class="string">f&quot;Running validation... \n Generating <span class="subst">&#123;args.num_validation_images&#125;</span> images with prompt:&quot;</span></span><br><span class="line">           <span class="string">f&quot; <span class="subst">&#123;args.validation_prompt&#125;</span>.&quot;</span></span><br><span class="line">       )</span><br><span class="line">       pipeline = DiffusionPipeline.from_pretrained(...)</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure>
<p>所有训练结束后，函数会再存一次最终的 LoRA 模型权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save the lora layers</span></span><br><span class="line">accelerator.wait_for_everyone()</span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    unet = unet.to(torch.float32)</span><br><span class="line"></span><br><span class="line">    unwrapped_unet = accelerator.unwrap_model(unet)</span><br><span class="line">    unet_lora_state_dict = convert_state_dict_to_diffusers(</span><br><span class="line">        get_peft_model_state_dict(unwrapped_unet))</span><br><span class="line">    StableDiffusionPipeline.save_lora_weights(</span><br><span class="line">        save_directory=args.output_dir,</span><br><span class="line">        unet_lora_layers=unet_lora_state_dict,</span><br><span class="line">        safe_serialization=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>函数还会再测试一次模型。具体方法和之前的验证是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Final inference</span></span><br><span class="line"><span class="comment"># Load previous pipeline</span></span><br><span class="line"><span class="keyword">if</span> args.validation_prompt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>运行完了这里，函数也就结束了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerator.end_training()</span><br></pre></td></tr></table></figure>
<p>为了方便使用，我把这个脚本改写了一下：删除了部分不常用的功能，并且配置参数能通过配置文件而不是命令行参数传入。新的脚本为项目根目录下的 <code>train_lora.py</code>，示例配置文件在 <code>cfg</code> 目录下。</p>
<p>以 <code>cfg</code> 中的某个配置文件为例，我们来回顾一下训练脚本主要用到的参数：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;log_dir&quot;</span>: <span class="string">&quot;log&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;ckpt&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;data_dir&quot;</span>: <span class="string">&quot;dataset/mountain&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;ckpt_name&quot;</span>: <span class="string">&quot;mountain&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;pretrained_model_name_or_path&quot;</span>: <span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;rank&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="attr">&quot;enable_xformers_memory_efficient_attention&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span>,</span><br><span class="line">    <span class="attr">&quot;adam_beta1&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="attr">&quot;adam_beta2&quot;</span>: <span class="number">0.999</span>,</span><br><span class="line">    <span class="attr">&quot;adam_weight_decay&quot;</span>: <span class="number">1e-2</span>,</span><br><span class="line">    <span class="attr">&quot;adam_epsilon&quot;</span>: <span class="number">1e-08</span>,</span><br><span class="line">    <span class="attr">&quot;resolution&quot;</span>: <span class="number">512</span>,</span><br><span class="line">    <span class="attr">&quot;n_epochs&quot;</span>: <span class="number">200</span>,</span><br><span class="line">    <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;dataloader_num_workers&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;lr_scheduler_name&quot;</span>: <span class="string">&quot;constant&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;resume_from_checkpoint&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">&quot;noise_offset&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="attr">&quot;max_grad_norm&quot;</span>: <span class="number">1.0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要关注的参数：<code>output_dir</code> 为输出 checkpoint 的文件夹，<code>ckpt_name</code> 为输出 checkpoint 的文件名。<code>data_dir</code> 是训练数据集所在文件夹。<code>pretrained_model_name_or_path</code> 为 SD 模型文件夹。<code>rank</code> 是决定 LoRA 大小的参数。<code>learning_rate</code> 是学习率。<code>adam</code> 打头的是 AdamW 优化器的参数。<code>resolution</code> 是训练图片的统一分辨率。<code>n_epochs</code> 是训练的轮数。<code>checkpointing_steps</code> 指每过多久存一次 checkpoint。<code>train_batch_size</code> 是 batch size。<code>gradient_accumulation_steps</code> 是梯度累计步数。</p>
<p>要修改这个配置文件，要先把文件夹的路径改对，填上训练时的分辨率，再通过 <code>gradient_accumulation_steps</code> 和 <code>train_batch_size</code> 决定 batch size，接着填 <code>n_epochs</code> (一般训 10~20 轮就会过拟合)。最后就可以一边改 LoRA 的主要超参数 <code>rank</code> 一边反复训练了。</p>
<h3 id="SD-图像插值"><a href="#SD-图像插值" class="headerlink" title="SD 图像插值"></a>SD 图像插值</h3><p>在这个示例中，我们来实现 DiffMorpher 工作的一小部分，完成一个简单的图像插值工具。在此过程中，我们将学会怎么在单张图片上训练 SD LoRA，以验证我们的训练环境。</p>
<p>这个工具的原理很简单：我们对两张图片分别训练一个 LoRA。之后，为了获取两张图片的插值，我们可以对两张图片 DDIM Inversion 的初始隐变量及两个 LoRA 分别插值，用插值过的隐变量在插值过的 SD LoRA 上生成图片就能得到插值图片。</p>
<p>该示例的所有数据和代码都已经在项目文件夹中给出。首先，我们看一下该怎么在单张图片上训 LoRA。训练之前，我们要准备一个数据集文件夹。数据集文件夹及包含所有图片及一个描述文件 <code>metadata.jsonl</code>。比如单图片的数据集文件夹的结构应如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">├── mountain</span><br><span class="line">│       ├── metadata.jsonl</span><br><span class="line">│       └── mountain.jpg</span><br></pre></td></tr></table></figure>
<p><code>metadata.jsonl</code> 元数据文件的每一行都是一个 json 结构，包含该图片的路径及文本描述。单图片的元数据文件如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>如果是多图片，就应该是：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain_up.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>我们可以运行项目目录下的数据集测试文件 <code>test_dataset.py</code> 来看看 datasets 库的数据集对象包含哪些信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;imagefolder&quot;</span>, data_dir=<span class="string">&quot;dataset/mountain&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>].column_names)</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>][<span class="string">&#x27;image&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>][<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>其输出大致为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Generating train split: 1 examples [00:00, 66.12 examples/s]</span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [&#x27;image&#x27;, &#x27;text&#x27;],</span><br><span class="line">        num_rows: 1</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br><span class="line">[&#x27;image&#x27;, &#x27;text&#x27;]</span><br><span class="line">[&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F0400246670&gt;]</span><br><span class="line">[&#x27;mountain&#x27;]</span><br></pre></td></tr></table></figure></p>
<p>这说明数据集对象实际上是一个词典。默认情况下，数据集放在词典的 <code>train</code> 键下。数据集的 <code>column_names</code> 属性可以返回每项数据有哪些属性。在我们的数据集里，数据的 <code>image</code> 是图像数据，<code>text</code> 是文本标签。训练脚本默认情况下会把每项数据的第一项属性作为图像，第二项属性作为文本标签。我们的这个数据集定义与训练脚本相符。</p>
<p>认识了数据集，我们可以来训练模型了。用下面的两行命令就可以分别在两张图片上训练 LoRA。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python train_lora.py cfg/mountain.json</span><br><span class="line">python train_lora.py cfg/mountain_up.json</span><br></pre></td></tr></table></figure>
<p>如果要用所有显卡训练，则应该用 accelerate。当然，对于这个简单的单图片训练，不需要用那么多显卡。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch train_lora.py cfg/mountain.json</span><br><span class="line">accelerate launch train_lora.py cfg/mountain_up.json</span><br></pre></td></tr></table></figure>
<p>这两个 LoRA 模型的配置文件我们已经在前文见过了。相比普通的风格化 LoRA，这两个 LoRA 的训练轮数非常多，有 200 轮。设置较大的训练轮数能保证模型在单张图片上过拟合。</p>
<p>训练结束后，项目的 <code>ckpt</code> 文件夹下会多出两个 LoRA 权重文件: <code>mountain.safetensor</code>, <code>mountain_up.safetensor</code>。我们可以用它们来做图像插值了。</p>
<p>图像插值的脚本为 <code>morph.py</code>，它的主要内容为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> inversion_pipeline <span class="keyword">import</span> InversionPipeline</span><br><span class="line"></span><br><span class="line">lora_path = <span class="string">&#x27;ckpt/mountain.safetensor&#x27;</span></span><br><span class="line">lora_path2 = <span class="string">&#x27;ckpt/mountain_up.safetensor&#x27;</span></span><br><span class="line">sd_path = <span class="string">&#x27;runwayml/stable-diffusion-v1-5&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pipeline: InversionPipeline = InversionPipeline.from_pretrained(</span><br><span class="line">    sd_path).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path, adapter_name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path2, adapter_name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img1_path = <span class="string">&#x27;dataset/mountain/mountain.jpg&#x27;</span></span><br><span class="line">img2_path = <span class="string">&#x27;dataset/mountain_up/mountain_up.jpg&#x27;</span></span><br><span class="line">prompt = <span class="string">&#x27;mountain&#x27;</span></span><br><span class="line">latent1 = pipeline.inverse(img1_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">latent2 = pipeline.inverse(img2_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">n_frames = <span class="number">10</span></span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_frames + <span class="number">1</span>):</span><br><span class="line">    alpha = i / n_frames</span><br><span class="line">    pipeline.set_adapters([<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>], adapter_weights=[<span class="number">1</span> - alpha, alpha])</span><br><span class="line">    latent = slerp(latent1, latent2, alpha)</span><br><span class="line">    output = pipeline(prompt=prompt, latents=latent,</span><br><span class="line">                      guidance_scale=<span class="number">1.0</span>).images[<span class="number">0</span>]</span><br><span class="line">    images.append(output)</span><br></pre></td></tr></table></figure>
<p>对于每一个 Diffusers 的 Pipeline 类实例，都可以用 <code>pipeline.load_lora_weights</code> 来读取 LoRA 权重。如果我们在同一个模型上使用了多个 LoRA，为了区分它们，我们要加上 <code>adapter_name</code> 参数为每个 LoRA 命名。稍后我们会用到这些名称。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipeline.load_lora_weights(lora_path, adapter_name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path2, adapter_name=<span class="string">&#x27;b&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>读好了文件，使用已经写好的 DDIM Inversion 方法来得到两张图片的初始隐变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img1_path = <span class="string">&#x27;dataset/mountain/mountain.jpg&#x27;</span></span><br><span class="line">img2_path = <span class="string">&#x27;dataset/mountain_up/mountain_up.jpg&#x27;</span></span><br><span class="line">prompt = <span class="string">&#x27;mountain&#x27;</span></span><br><span class="line">latent1 = pipeline.inverse(img1_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">latent2 = pipeline.inverse(img2_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>最后开始生成不同插值比例的图片。根据混合比例 <code>alpha</code>，我们可以用 <code>pipeline.set_adapters([&quot;a&quot;, &quot;b&quot;], adapter_weights=[1 - alpha, alpha])</code> 来融合 LoRA 模型的比例。随后，我们再根据 <code>alpha</code> 对隐变量插值。用插值隐变量在插值 SD LoRA 上生成图片即可得到最终的插值图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_frames = <span class="number">10</span></span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_frames + <span class="number">1</span>):</span><br><span class="line">    alpha = i / n_frames</span><br><span class="line">    pipeline.set_adapters([<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>], adapter_weights=[<span class="number">1</span> - alpha, alpha])</span><br><span class="line">    latent = slerp(latent1, latent2, alpha)</span><br><span class="line">    output = pipeline(prompt=prompt, latents=latent,</span><br><span class="line">                      guidance_scale=<span class="number">1.0</span>).images[<span class="number">0</span>]</span><br><span class="line">    images.append(output)</span><br></pre></td></tr></table></figure>
<p>下面两段动图中，左图和右图分别是无 LoRA 和有 LoRA 的插值结果。可见，通过 LoRA 权重上的插值，图像插值的过度会更加自然。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/output.gif" alt></p>
<h3 id="图片风格迁移"><a href="#图片风格迁移" class="headerlink" title="图片风格迁移"></a>图片风格迁移</h3><p>接下来，我们来实现最流行的 LoRA 应用——风格化 LoRA。当然，训练一个每张随机输出图片都质量很高的模型是很困难的。我们退而求其次，来实现一个能对输入图片做风格迁移的 LoRA 模型。</p>
<p>训练风格化 LoRA 对技术要求不高，其主要难点其实是在数据收集上。大家可以根据自己的需求，准备自己的数据集。我在本文中会分享我的实验结果。我希望把《弹丸论破》的画风——一种颜色渐变较多的动漫画风——应用到一张普通动漫画风的图片上。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/2.png" alt></p>
<p>由于我的目标是拟合画风而不是某一种特定的物体，我直接选取了 50 张左右的游戏 CG 构成训练数据集，且没有对图片做任何处理。训风格化 LoRA 时，文本标签几乎没用，我把所有数据的文本都设置成了游戏名 <code>danganronpa</code>。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;1.png&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;danganronpa&quot;</span>&#125;</span><br><span class="line">...</span><br><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;59.png&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;danganronpa&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>我的配置文件依然和前文的相同，LoRA rank 设置为 8。我一共训了 100 轮，但发现训练后期模型的过拟合很严重，其实令 <code>n_epochs</code> 为 10 到 20 就能有不错的结果。50 张图片训 10 轮最多几十分钟就训完。</p>
<p>由于训练图片的内容不够多样，且图片预处理时加入了随机裁剪，我的 LoRA 模型随机生成的图片质量较低。于是我决定在图像风格迁移任务上测试该模型。具体来说，我使用了 ControlNet Canny 加上图生图 （SDEdit）技术。相关的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableDiffusionControlNetImg2ImgPipeline, ControlNetModel</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lora_path = <span class="string">&#x27;...&#x27;</span></span><br><span class="line">sd_path = <span class="string">&#x27;runwayml/stable-diffusion-v1-5&#x27;</span></span><br><span class="line">controlnet_canny_path = <span class="string">&#x27;lllyasviel/sd-controlnet-canny&#x27;</span></span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&#x27;1 man, look at right, side face, Ace Attorney, Phoenix Wright, best quality, danganronpa&#x27;</span></span><br><span class="line">neg_prompt = <span class="string">&#x27;longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, &#123;multiple people&#125;&#x27;</span></span><br><span class="line">img_path = <span class="string">&#x27;...&#x27;</span></span><br><span class="line">init_image = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">init_image = init_image.resize((<span class="number">768</span>, <span class="number">512</span>))</span><br><span class="line">np_image = np.array(init_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get canny image</span></span><br><span class="line">np_image = cv2.Canny(np_image, <span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line">np_image = np_image[:, :, <span class="literal">None</span>]</span><br><span class="line">np_image = np.concatenate([np_image, np_image, np_image], axis=<span class="number">2</span>)</span><br><span class="line">canny_image = Image.fromarray(np_image)</span><br><span class="line">canny_image.save(<span class="string">&#x27;tmp_edge.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">controlnet = ControlNetModel.from_pretrained(controlnet_canny_path)</span><br><span class="line">pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(</span><br><span class="line">    sd_path, controlnet=controlnet</span><br><span class="line">)</span><br><span class="line">pipe.load_lora_weights(lora_path)</span><br><span class="line"></span><br><span class="line">output = pipe(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    negative_prompt=neg_prompt,</span><br><span class="line">    strength=<span class="number">0.5</span>,</span><br><span class="line">    guidance_scale=<span class="number">7.5</span>,</span><br><span class="line">    controlnet_conditioning_scale=<span class="number">0.5</span>,</span><br><span class="line">    num_inference_steps=<span class="number">50</span>,</span><br><span class="line">    image=init_image,</span><br><span class="line">    cross_attention_kwargs=&#123;<span class="string">&quot;scale&quot;</span>: <span class="number">1.0</span>&#125;,</span><br><span class="line">    control_image=canny_image,</span><br><span class="line">).images[<span class="number">0</span>]</span><br><span class="line">output.save(<span class="string">&quot;tmp.png&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><code>StableDiffusionControlNetImg2ImgPipeline</code> 是 Diffusers 中 ControlNet 加图生图的 Pipeline。使用它生成图片的重要参数有：</p>
<ul>
<li><code>strength</code>：0~1 之间重绘比例。越低越接近输入图片。</li>
<li><code>controlnet_conditioning_scale</code>： 0~1 之间的 ControlNet 约束比例。越高越贴近约束。</li>
<li><code>cross_attention_kwargs=&#123;&quot;scale&quot;: scale&#125;</code>：此处的 <code>scale</code> 是 0~1 之间的 LoRA 混合比例。越高越贴近 LoRA 模型的输出。</li>
</ul>
<p>这里贴一下输入图片和两张编辑后的图片。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/3.png" alt><br><img src="/2024/01/23/20240114-SD-LoRA/4.png" alt><br><img src="/2024/01/23/20240114-SD-LoRA/5.png" alt></p>
<p>可以看出，输出图片中人物的画风确实得到了修改，颜色渐变更加丰富。我在几乎没有调试 LoRA 参数的情况下得到了这样的结果，可见虽然训练一个高质量的随机生成新画风的 LoRA 难度较高，但只是做风格迁移还是比较容易的。</p>
<p>尽管实验的经历不多，我还是基本上了解了 SD LoRA 风格化的能力边界。LoRA 风格化的本质还是修改输出图片的分布，数据集的质量基本上决定了生成的质量，其他参数的影响不会很大（包括训练图片的文本标签）。数据集最好手动裁剪至 512x512。如果想要生成丰富的风格化内容而不是只生成人物，就要丰富训练数据，减少人物数据的占比。训练时，最容易碰到的机器学习上的问题是过拟合问题。解决此问题的最简单的方式是早停，即不用最终的训练结果而用中间某一步的结果。如果你想实现改变输出数据分布以外的功能，比如精确生成某类物体、向模型中加入一些改变画风的关键词，那你应该使用更加先进的技术，而不仅仅是用最基本的 LoRA 微调。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LoRA 是当今深度学习领域中常见的技术。对于 SD，LoRA 则是能够编辑单幅图片、调整整体画风，或者是通过修改训练目标来实现更强大的功能。LoRA 的原理非常简单，它其实就是用两个参数量较少的矩阵来描述一个大参数矩阵在微调中的变化量。Diffusers 库提供了非常便利的 SD LoRA 训练脚本。相信读完了本文后，我们能知道如何用 Diffusers 训练 LoRA，修改训练中的主要参数，并在简单的单图片 LoRA 编辑任务上验证训练的正确性。利用这些知识，我们也能把 LoRA 拓展到风格化生成及其他应用上。</p>
<p>本文的项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA">https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E7%BC%96%E7%A8%8B/" rel="tag"># 编程</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" rel="tag"># 扩散模型</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/01/23/20230713-SD3/" rel="prev" title="Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读">
      <i class="fa fa-chevron-left"></i> Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/01/27/20240123-SD-Attn/" rel="next" title="Stable Diffusion 中的自注意力替换技术与 Diffusers 实现">
      Stable Diffusion 中的自注意力替换技术与 Diffusers 实现 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#LoRA-%E7%9A%84%E5%8E%9F%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">LoRA 的原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LoRA-%E5%9C%A8-SD-%E4%B8%AD%E7%9A%84%E4%B8%89%E7%A7%8D%E8%BF%90%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">LoRA 在 SD 中的三种运用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%98%E5%8E%9F%E5%8D%95%E5%B9%85%E5%9B%BE%E5%83%8F"><span class="nav-number">2.1.</span> <span class="nav-text">还原单幅图像</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A3%8E%E6%A0%BC%E8%B0%83%E6%95%B4"><span class="nav-number">2.2.</span> <span class="nav-text">风格调整</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87%E8%B0%83%E6%95%B4"><span class="nav-number">2.3.</span> <span class="nav-text">训练目标调整</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SD-LoRA-%E5%BA%94%E7%94%A8%E6%80%BB%E7%BB%93"><span class="nav-number">2.4.</span> <span class="nav-text">SD LoRA 应用总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Diffusers-SD-LoRA-%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98"><span class="nav-number">3.</span> <span class="nav-text">Diffusers SD LoRA 代码实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Diffusers-%E8%84%9A%E6%9C%AC"><span class="nav-number">3.1.</span> <span class="nav-text">Diffusers 脚本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SD-%E5%9B%BE%E5%83%8F%E6%8F%92%E5%80%BC"><span class="nav-number">3.2.</span> <span class="nav-text">SD 图像插值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E7%89%87%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB"><span class="nav-number">3.3.</span> <span class="nav-text">图片风格迁移</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">124</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
