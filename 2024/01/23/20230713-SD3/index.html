<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="看完了Stable Diffusion的论文，在最后这篇文章里，我们来学习Stable Diffusion的代码实现。具体来说，我们会学习Stable Diffusion官方仓库及Diffusers开源库中有关采样算法和U-Net的代码，而不会学习有关训练、VAE、text encoder (CLIP) 的代码。如今大多数工作都只会用到预训练的Stable Diffusion，只学采样算法和U-N">
<meta property="og:type" content="article">
<meta property="og:title" content="Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读">
<meta property="og:url" content="https://zhouyifan.net/2024/01/23/20230713-SD3/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="看完了Stable Diffusion的论文，在最后这篇文章里，我们来学习Stable Diffusion的代码实现。具体来说，我们会学习Stable Diffusion官方仓库及Diffusers开源库中有关采样算法和U-Net的代码，而不会学习有关训练、VAE、text encoder (CLIP) 的代码。如今大多数工作都只会用到预训练的Stable Diffusion，只学采样算法和U-N">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/0-1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/0-2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/0-3.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/1-2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/1-3.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/1-4.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/1-5.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/0-3.jpg">
<meta property="og:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/2-1.jpg">
<meta property="article:published_time" content="2024-01-23T11:42:00.000Z">
<meta property="article:modified_time" content="2024-01-23T11:46:08.259Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="编程">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="扩散模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2024/01/23/20230713-SD3/0-1.jpg">

<link rel="canonical" href="https://zhouyifan.net/2024/01/23/20230713-SD3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/23/20230713-SD3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-23 19:42:00" itemprop="dateCreated datePublished" datetime="2024-01-23T19:42:00+08:00">2024-01-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>看完了Stable Diffusion的论文，在最后这篇文章里，我们来学习Stable Diffusion的代码实现。具体来说，我们会学习Stable Diffusion官方仓库及Diffusers开源库中有关采样算法和U-Net的代码，而不会学习有关训练、VAE、text encoder (CLIP) 的代码。如今大多数工作都只会用到预训练的Stable Diffusion，只学采样算法和U-Net代码就能理解大多数工作了。</p>
<blockquote>
<p>建议读者在阅读本文之前了解DDPM、ResNet、U-Net、Transformer。</p>
<p>本文用到的Stable Diffusion版本是v1.5。Diffusers版本是0.25.0。为了提升可读性，本文对源代码做了一定的精简，部分不会运行到的分支会被略过。</p>
</blockquote>
<h2 id="算法梳理"><a href="#算法梳理" class="headerlink" title="算法梳理"></a>算法梳理</h2><p>在正式读代码之前，我们先用伪代码梳理一下Stable Diffusion的采样过程，并回顾一下U-Net架构的组成。实现Stable Diffusion的代码库有很多，各个库之间的API差异很大。但是，它们实际上都是在描述同一个算法，同一个模型。如果我们理解了算法和模型本身，就可以在学习时主动去找一个算法对应哪一段代码，而不是被动地去理解每一行代码在干什么。</p>
<h3 id="LDM-采样算法"><a href="#LDM-采样算法" class="headerlink" title="LDM 采样算法"></a>LDM 采样算法</h3><p>让我们从最早的DDPM开始，一步一步还原Latent Diffusion Model (LDM)的采样算法。DDPM的采样算法如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddpm_sample</span>(<span class="params">image_shape</span>):</span></span><br><span class="line">  ddpm_scheduler = DDPMScheduler()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  xt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> T ... <span class="number">1</span>:</span><br><span class="line">    eps = unet(xt, t)</span><br><span class="line">    std = ddpm_scheduler.get_std(t)</span><br><span class="line">    xt = ddpm_scheduler.get_xt_prev(xt, t, eps, std)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>在DDPM的实现中，一般会有一个类专门维护扩散模型的$\alpha, \beta$等变量。我们这里把这个类称为<code>DDPMScheduler</code>。此外，DDPM会用到一个U-Net神经网络<code>unet</code>，用于计算去噪过程中图像应该去除的噪声<code>eps</code>。准备好这两个变量后，就可以用<code>randn()</code>从标准正态分布中采样一个纯噪声图像<code>xt</code>。它会被逐渐去噪，最终变成一幅图片。去噪过程中，时刻<code>t</code>会从总时刻<code>T</code>遍历至<code>1</code>(总时刻<code>T</code>一般取<code>1000</code>)。在每一轮去噪步骤中，U-Net会根据这一时刻的图像<code>xt</code>和当前时间戳<code>t</code>估计出此刻应去除的噪声<code>eps</code>，根据<code>xt</code>和<code>eps</code>就能知道下一步图像的均值。除了均值，我们还要获取下一步图像的方差，这一般可以从DDPM调度类中直接获取。有了下一步图像的均值和方差，我们根据DDPM的公式，就能采样出下一步的图像。反复执行去噪循环，<code>xt</code>会从纯噪声图像变成一幅有意义的图像。</p>
<p>DDIM对DDPM的采样过程做了两点改进：1) 去噪的有效步数可以少于<code>T</code>步，由另一个变量<code>ddim_steps</code>决定；2) 采样的方差大小可以由<code>eta</code>决定。因此，改进后的DDIM算法可以写成这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddim_sample</span>(<span class="params">image_shape, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  xt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> timesteps:</span><br><span class="line">    eps = unet(xt, t)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    xt = ddim_scheduler.get_xt_prev(xt, t, eps, std)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>其中，<code>ddim_steps</code>是去噪循环的执行次数。根据<code>ddim_steps</code>，DDIM调度器可以生成所有被使用到的<code>t</code>。比如对于<code>T=1000, ddim_steps=20</code>，被使用到的就只有<code>[1000, 950, 900, ..., 50]</code>这20个时间戳，其他时间戳就可以跳过不算了。<code>eta</code>会被用来计算方差，一般这个值都会设成<code>0</code>。</p>
<blockquote>
<p>DDIM是早期的加速扩散模型采样的算法。如今有许多比DDIM更好的采样方法，但它们多数都保留了<code>steps</code>和<code>eta</code>这两个参数。因此，在使用所有采样方法时，我们可以不用关心实现细节，只关注多出来的这两个参数。</p>
</blockquote>
<p>在DDIM的基础上，LDM从生成像素空间上的图像变为生成隐空间上的图像。隐空间图像需要再做一次解码才能变回真实图像。从代码上来看，使用LDM后，只需要多准备一个VAE，并对最后的隐空间图像<code>zt</code>解码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_ddim_sample</span>(<span class="params">image_shape, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  vae = VAE()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  zt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> timesteps:</span><br><span class="line">    eps = unet(zt, t)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure></p>
<p>而想用LDM实现文生图，则需要给一个额外的文本输入<code>text</code>。文本编码器会把文本编码成张量<code>c</code>，输入进<code>unet</code>。其他地方的实现都和之前的LDM一样。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  vae = VAE()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  zt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line"></span><br><span class="line">  text_encoder = CLIP()</span><br><span class="line">  c = text_encoder.encode(text)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> t = timesteps:</span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>最后这个能实现文生图的LDM就是我们熟悉的Stable Diffusion。Stable Diffusion的采样算法看上去比较复杂，但如果能够从DDPM开始把各个功能都拆开来看，理解起来就不是那么困难了。</p>
<h3 id="U-Net-结构组成"><a href="#U-Net-结构组成" class="headerlink" title="U-Net 结构组成"></a>U-Net 结构组成</h3><p>Stable Diffusion代码实现中的另一个重点是去噪网络U-Net的实现。仿照上一节的学习方法，我们来逐步学习Stable Diffusion中的U-Net是怎么从最经典的纯卷积U-Net逐渐发展而来的。</p>
<p>最早的U-Net的结构如下图所示：</p>
<p><img src="/2024/01/23/20230713-SD3/0-1.jpg" alt></p>
<p>可以看出，U-Net的结构有以下特点：</p>
<ul>
<li>整体上看，U-Net由若干个大层组成。特征在每一大层会被下采样成尺寸更小的特征，再被上采样回原尺寸的特征。整个网络构成一个U形结构。</li>
<li>下采样后，特征的通道数会变多。一般情况下，每次下采样后图像尺寸减半，通道数翻倍。上采样过程则反之。</li>
<li>为了防止信息在下采样的过程中丢失，U-Net每一大层在下采样前的输出会作为额外输入拼接到每一大层上采样前的输入上。这种数据连接方式类似于ResNet中的「短路连接」。</li>
</ul>
<p>DDPM则使用了一种改进版的U-Net。改进主要有两点：</p>
<ul>
<li>原来的卷积层被替换成了ResNet中的残差卷积模块。每一大层有若干个这样的子模块。对于较深的大层，残差卷积模块后面还会接一个自注意力模块。</li>
<li>原来模型每一大层只有一个短路连接。现在每个大层下采样部分的每个子模块的输出都会额外输入到其对称的上采样部分的子模块上。直观上来看，就是短路连接更多了一点，输入信息更不容易在下采样过程中丢失。</li>
</ul>
<p><img src="/2024/01/23/20230713-SD3/0-2.jpg" alt></p>
<p>最后，LDM提出了一种给U-Net添加额外约束信息的方法：把U-Net中的自注意力模块换成交叉注意力模块。具体来说，DDPM的U-Net的自注意力模块被换成了标准的Transformer模块。约束信息$C$可以作为Cross Attention的K, V输入进模块中。</p>
<p>Stable Diffusion的U-Net还在结构上有少许修改，该U-Net的每一大层都有Transformer块，而不是只有较深的大层有。</p>
<p><img src="/2024/01/23/20230713-SD3/0-3.jpg" alt></p>
<p>至此，我们已经学完了Stable Diffusion的采样原理和U-Net结构。接下来我们来看一看它们在不同框架下的代码实现。</p>
<h2 id="Stable-Diffusion-官方-GitHub-仓库"><a href="#Stable-Diffusion-官方-GitHub-仓库" class="headerlink" title="Stable Diffusion 官方 GitHub 仓库"></a>Stable Diffusion 官方 GitHub 仓库</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>克隆仓库后，照着官方Markdown文档安装即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:CompVis/stable-diffusion.git</span><br></pre></td></tr></table></figure>
<p>先用下面的命令创建conda环境，此后<code>ldm</code>环境就是运行Stable Diffusiion的conda环境。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yaml</span><br><span class="line">conda activate ldm</span><br></pre></td></tr></table></figure>
<p>之后去网上下一个Stable Diffusion的模型文件。比较常见一个版本是v1.5，该模型在Hugging Face上：<a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5">https://huggingface.co/runwayml/stable-diffusion-v1-5</a> （推荐下载<code>v1-5-pruned.ckpt</code>）。下载完毕后，把模型软链接到指定位置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p models/ldm/stable-diffusion-v1/</span><br><span class="line">ln -s &lt;path/to/model.ckpt&gt; models/ldm/stable-diffusion-v1/model.ckpt </span><br></pre></td></tr></table></figure>
<p>准备完毕后，只要输入下面的命令，就可以生成实现文生图了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/txt2img.py --prompt &quot;a photograph of an astronaut riding a horse&quot; </span><br></pre></td></tr></table></figure>
<p>在默认的参数下，“一幅骑着马的飞行员的照片”的绘制结果会被保存在<code>outputs/txt2img-samples</code>中。你也可以通过<code>--outdir &lt;dir&gt;</code>参数来指定输出到的文件夹。我得到的一些绘制结果为：</p>
<p><img src="/2024/01/23/20230713-SD3/1.jpg" alt></p>
<blockquote>
<p>如果你在安装时碰到了错误，可以在搜索引擎上或者GitHub的issue里搜索，一般都能搜到其他人遇到的相同错误。</p>
</blockquote>
<h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><p>接下来，我们来探究一下<code>scripts/txt2img.py</code>的执行过程。为了方便阅读，我们可以简化代码中的命令行处理，得到下面这份精简代码。（你可以把这份代码复制到仓库根目录下的一个新Python脚本里并直接运行。别忘了修改代码中的模型路径）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> omegaconf <span class="keyword">import</span> OmegaConf</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm, trange</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> seed_everything</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> autocast</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ldm.util <span class="keyword">import</span> instantiate_from_config</span><br><span class="line"><span class="keyword">from</span> ldm.models.diffusion.ddim <span class="keyword">import</span> DDIMSampler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model_from_config</span>(<span class="params">config, ckpt, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loading model from <span class="subst">&#123;ckpt&#125;</span>&quot;</span>)</span><br><span class="line">    pl_sd = torch.load(ckpt, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;global_step&quot;</span> <span class="keyword">in</span> pl_sd:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Global Step: <span class="subst">&#123;pl_sd[<span class="string">&#x27;global_step&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    sd = pl_sd[<span class="string">&quot;state_dict&quot;</span>]</span><br><span class="line">    model = instantiate_from_config(config.model)</span><br><span class="line">    m, u = model.load_state_dict(sd, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(m) &gt; <span class="number">0</span> <span class="keyword">and</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;missing keys:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(m)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(u) &gt; <span class="number">0</span> <span class="keyword">and</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;unexpected keys:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(u)</span><br><span class="line"></span><br><span class="line">    model.cuda()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    seed = <span class="number">42</span></span><br><span class="line">    config = <span class="string">&#x27;configs/stable-diffusion/v1-inference.yaml&#x27;</span></span><br><span class="line">    ckpt = <span class="string">&#x27;ckpt/v1-5-pruned.ckpt&#x27;</span></span><br><span class="line">    outdir = <span class="string">&#x27;tmp&#x27;</span></span><br><span class="line">    n_samples = batch_size = <span class="number">3</span></span><br><span class="line">    n_rows = batch_size</span><br><span class="line">    n_iter = <span class="number">2</span></span><br><span class="line">    prompt = <span class="string">&#x27;a photograph of an astronaut riding a horse&#x27;</span></span><br><span class="line">    data = [batch_size * [prompt]]</span><br><span class="line">    scale = <span class="number">7.5</span></span><br><span class="line">    C = <span class="number">4</span></span><br><span class="line">    f = <span class="number">8</span></span><br><span class="line">    H = W = <span class="number">512</span></span><br><span class="line">    ddim_steps = <span class="number">50</span></span><br><span class="line">    ddim_eta = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    seed_everything(seed)</span><br><span class="line"></span><br><span class="line">    config = OmegaConf.load(config)</span><br><span class="line">    model = load_model_from_config(config, ckpt)</span><br><span class="line"></span><br><span class="line">    device = torch.device(</span><br><span class="line">        <span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    sampler = DDIMSampler(model)</span><br><span class="line"></span><br><span class="line">    os.makedirs(outdir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    outpath = outdir</span><br><span class="line"></span><br><span class="line">    sample_path = os.path.join(outpath, <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    os.makedirs(sample_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    grid_count = <span class="built_in">len</span>(os.listdir(outpath)) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    start_code = <span class="literal">None</span></span><br><span class="line">    precision_scope = autocast</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">with</span> precision_scope(<span class="string">&quot;cuda&quot;</span>):</span><br><span class="line">            <span class="keyword">with</span> model.ema_scope():</span><br><span class="line">                all_samples = <span class="built_in">list</span>()</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> trange(n_iter, desc=<span class="string">&quot;Sampling&quot;</span>):</span><br><span class="line">                    <span class="keyword">for</span> prompts <span class="keyword">in</span> tqdm(data, desc=<span class="string">&quot;data&quot;</span>):</span><br><span class="line">                        uc = <span class="literal">None</span></span><br><span class="line">                        <span class="keyword">if</span> scale != <span class="number">1.0</span>:</span><br><span class="line">                            uc = model.get_learned_conditioning(</span><br><span class="line">                                batch_size * [<span class="string">&quot;&quot;</span>])</span><br><span class="line">                        <span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">                            prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">                        c = model.get_learned_conditioning(prompts)</span><br><span class="line">                        shape = [C, H // f, W // f]</span><br><span class="line">                        samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                                         conditioning=c,</span><br><span class="line">                                                         batch_size=n_samples,</span><br><span class="line">                                                         shape=shape,</span><br><span class="line">                                                         verbose=<span class="literal">False</span>,</span><br><span class="line">                                                         unconditional_guidance_scale=scale,</span><br><span class="line">                                                         unconditional_conditioning=uc,</span><br><span class="line">                                                         eta=ddim_eta,</span><br><span class="line">                                                         x_T=start_code)</span><br><span class="line"></span><br><span class="line">                        x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br><span class="line">                        x_samples_ddim = torch.clamp(</span><br><span class="line">                            (x_samples_ddim + <span class="number">1.0</span>) / <span class="number">2.0</span>, <span class="built_in">min</span>=<span class="number">0.0</span>, <span class="built_in">max</span>=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">                        all_samples.append(x_samples_ddim)</span><br><span class="line">                grid = torch.stack(all_samples, <span class="number">0</span>)</span><br><span class="line">                grid = rearrange(grid, <span class="string">&#x27;n b c h w -&gt; (n b) c h w&#x27;</span>)</span><br><span class="line">                grid = make_grid(grid, nrow=n_rows)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># to image</span></span><br><span class="line">                grid = <span class="number">255.</span> * rearrange(grid, <span class="string">&#x27;c h w -&gt; h w c&#x27;</span>).cpu().numpy()</span><br><span class="line">                img = Image.fromarray(grid.astype(np.uint8))</span><br><span class="line">                img.save(os.path.join(outpath, <span class="string">f&#x27;grid-<span class="subst">&#123;grid_count:04&#125;</span>.png&#x27;</span>))</span><br><span class="line">                grid_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Your samples are ready and waiting for you here: \n<span class="subst">&#123;outpath&#125;</span> \n&quot;</span></span><br><span class="line">          <span class="string">f&quot; \nEnjoy.&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>抛开前面一大堆初始化操作，代码的核心部分只有下面几行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">uc = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> scale != <span class="number">1.0</span>:</span><br><span class="line">    uc = model.get_learned_conditioning(</span><br><span class="line">        batch_size * [<span class="string">&quot;&quot;</span>])</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">    prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">c = model.get_learned_conditioning(prompts)</span><br><span class="line">shape = [C, H // f, W // f]</span><br><span class="line">samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                  conditioning=c,</span><br><span class="line">                                  batch_size=n_samples,</span><br><span class="line">                                  shape=shape,</span><br><span class="line">                                  verbose=<span class="literal">False</span>,</span><br><span class="line">                                  unconditional_guidance_scale=scale,</span><br><span class="line">                                  unconditional_conditioning=uc,</span><br><span class="line">                                  eta=ddim_eta,</span><br><span class="line">                                  x_T=start_code)</span><br><span class="line"></span><br><span class="line">x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br></pre></td></tr></table></figure>
<p>我们来逐行分析一下这段代码。一开始的几行是执行Classifier-Free Guidance (CFG)。<code>uc</code>表示的是CFG中的无约束下的约束张量。<code>scale</code>表示的是执行CFG的程度，<code>scale</code>不等于<code>1.0</code>即表示启用CFG。<code>model.get_learned_conditioning</code>表示用CLIP把文本编码成张量。对于文本约束的模型，无约束其实就是输入文本为空字符串(<code>&quot;&quot;</code>)。因此，在代码中，若启用了CFG，则会用CLIP编码空字符串，编码结果为<code>uc</code>。</p>
<blockquote>
<p>如果你没学过CFG，也不用担心。你可以暂时不要去理解上面这段话。等读完了后文中有关CFG的代码后，你差不多就能理解CFG的用法了。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uc = None</span><br><span class="line">if scale != 1.0:</span><br><span class="line">    uc = model.get_learned_conditioning(</span><br><span class="line">        batch_size * [&quot;&quot;])</span><br></pre></td></tr></table></figure>
<p>之后的几行是在把用户输入的文本编码成张量。同样，<code>model.get_learned_conditioning</code>表示用CLIP把输入文本编码成张量<code>c</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">    prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">c = model.get_learned_conditioning(prompts)</span><br></pre></td></tr></table></figure></p>
<p>接着是用扩散模型的采样器生成图片。在这份代码中，<code>sampler</code>是DDIM采样器，<code>sampler.sample</code>函数直接完成了图像生成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">shape = [C, H // f, W // f]</span><br><span class="line">samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                  conditioning=c,</span><br><span class="line">                                  batch_size=n_samples,</span><br><span class="line">                                  shape=shape,</span><br><span class="line">                                  verbose=<span class="literal">False</span>,</span><br><span class="line">                                  unconditional_guidance_scale=scale,</span><br><span class="line">                                  unconditional_conditioning=uc,</span><br><span class="line">                                  eta=ddim_eta,</span><br><span class="line">                                  x_T=start_code)</span><br></pre></td></tr></table></figure>
<p>最后，LDM生成的隐空间图片被VAE解码成真实图片。函数<code>model.decode_first_stage</code>负责图片解码。<code>x_samples_ddim</code>在后续的代码中会被后处理成正确格式的RGB图片，并输出至文件里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br></pre></td></tr></table></figure>
<p>Stable Diffusion 官方实现的主函数主要就做了这些事情。这份实现还是有一些凌乱的。采样算法的一部分内容被扔到了主函数里，另一部分放到了DDIM采样器里。在阅读官方实现的源码时，既要去读主函数里的内容，也要去读采样器里的内容。</p>
<p>接下来，我们来看一看DDIM采样器的部分代码，学完采样算法的剩余部分的实现。</p>
<h3 id="DDIM-采样器"><a href="#DDIM-采样器" class="headerlink" title="DDIM 采样器"></a>DDIM 采样器</h3><p>回头看主函数的前半部分，DDIM采样器是在下面的代码里导入的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ldm.models.diffusion.ddim <span class="keyword">import</span> DDIMSampler</span><br></pre></td></tr></table></figure><br>跳转到<code>ldm/models/diffusion/ddim.py</code>文件，我们可以找到<code>DDIMSampler</code>类的实现。</p>
<p>先看一下这个类的构造函数。构造函数主要是把U-Net <code>model</code>给存了下来。后文中的<code>self.model</code>都指的是U-Net。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model, schedule=<span class="string">&quot;linear&quot;</span>, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.model = model</span><br><span class="line">    self.ddpm_num_timesteps = model.num_timesteps</span><br><span class="line">    self.schedule = schedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># in main</span></span><br><span class="line"></span><br><span class="line">config = OmegaConf.load(config)</span><br><span class="line">model = load_model_from_config(config, ckpt)</span><br><span class="line">model = model.to(device)</span><br><span class="line">sampler = DDIMSampler(model)</span><br></pre></td></tr></table></figure></p>
<p>再沿着类的<code>self.sample</code>方法，看一下DDIM采样的实现代码。以下是<code>self.sample</code>方法的主要内容。这个方法其实就执行了一个<code>self.make_schedule</code>，之后把所有参数原封不动地传到了<code>self.ddim_sampling</code>里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">            S,</span></span></span><br><span class="line"><span class="params"><span class="function">            batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">            shape,</span></span></span><br><span class="line"><span class="params"><span class="function">            conditioning=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            ...</span></span></span><br><span class="line"><span class="params"><span class="function">            </span>):</span></span><br><span class="line">    <span class="keyword">if</span> conditioning <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)</span><br><span class="line">    <span class="comment"># sampling</span></span><br><span class="line">    C, H, W = shape</span><br><span class="line">    size = (batch_size, C, H, W)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Data shape for DDIM sampling is <span class="subst">&#123;size&#125;</span>, eta <span class="subst">&#123;eta&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    samples, intermediates = self.ddim_sampling(...)</span><br></pre></td></tr></table></figure>
<p><code>self.make_schedule</code>用于预处理扩散模型的中间计算参数。它的大部分实现细节可以略过。DDIM用到的有效时间戳列表就是在这个函数里设置的，该列表通过<code>make_ddim_timesteps</code>获取，并保存在<code>self.ddim_timesteps</code>中。此外，由<code>ddim_eta</code>决定的扩散模型的方差也是在这个方法里设置的。大致扫完这个方法后，我们可以直接跳到<code>self.ddim_sampling</code>的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span></span><br><span class="line">    self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,</span><br><span class="line">                                              num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>穿越重重的嵌套，我们总算能看到DDIM采样的实现方法<code>self.ddim_sampling</code>了。它的主要内容如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddim_sampling</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">    device = self.model.betas.device</span><br><span class="line">    b = shape[<span class="number">0</span>]</span><br><span class="line">    img = torch.randn(shape, device=device)</span><br><span class="line">    timesteps = self.ddim_timesteps</span><br><span class="line">    intermediates = ...</span><br><span class="line">    time_range = np.flip(timesteps)</span><br><span class="line">    total_steps = timesteps.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    iterator = tqdm(time_range, desc=<span class="string">&#x27;DDIM Sampler&#x27;</span>, total=total_steps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, step <span class="keyword">in</span> <span class="built_in">enumerate</span>(iterator):</span><br><span class="line">        index = total_steps - i - <span class="number">1</span></span><br><span class="line">        ts = torch.full((b,), step, device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        outs = self.p_sample_ddim(img, cond, ts, ...)</span><br><span class="line">        img, pred_x0 = outs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img, intermediates</span><br></pre></td></tr></table></figure>
<p>这段代码和我们之前自己写的伪代码非常相似。一开始，方法获取了在<code>make_schedule</code>里初始化的DDIM有效时间戳列表<code>self.ddim_timesteps</code>，并预处理成一个<code>iterator</code>。该迭代器用于控制DDIM去噪循环。每一轮循环会根据当前时刻的图像<code>img</code>和时间戳<code>ts</code>计算下一步的图像<code>img</code>。具体来说，代码每次用当前的时间戳<code>step</code>创建一个内容全部为<code>step</code>，形状为<code>(b,)</code>的张量<code>ts</code>。该张量会和当前的隐空间图像<code>img</code>，约束信息张量<code>cond</code>一起传给执行一轮DDIM去噪的<code>p_sample_ddim</code>方法。<code>p_sample_ddim</code>方法会返回下一步的图像<code>img</code>。最后，经过多次去噪后，<code>ddim_sampling</code>方法将去噪后的隐空间图像<code>img</code>返回。</p>
<blockquote>
<p><code>p_sample_ddim</code>里的<code>p_sample</code>看上去似乎意义不明，实际上这个叫法来自于DDPM论文。在DDPM论文中，扩散模型的前向过程用字母$q$表示，反向过程用字母$p$表示。因此，反向过程的一轮去噪在代码里被叫做<code>p_sample</code>。</p>
</blockquote>
<p>最后来看一下<code>p_sample_ddim</code>这个方法，它的主体部分如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">p_sample_ddim</span>(<span class="params">self, x, c, t, ...</span>):</span></span><br><span class="line">    b, *_, device = *x.shape, x.device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">        e_t = self.model.apply_model(x, t, c)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">        t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">        c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">        e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">        e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare variables</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># current prediction for x_0</span></span><br><span class="line">    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()</span><br><span class="line">    <span class="keyword">if</span> quantize_denoised:</span><br><span class="line">        pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)</span><br><span class="line">    <span class="comment"># direction pointing to x_t</span></span><br><span class="line">    dir_xt = (<span class="number">1.</span> - a_prev - sigma_t**<span class="number">2</span>).sqrt() * e_t</span><br><span class="line">    noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature</span><br><span class="line">    <span class="keyword">if</span> noise_dropout &gt; <span class="number">0.</span>:</span><br><span class="line">        noise = torch.nn.functional.dropout(noise, p=noise_dropout)</span><br><span class="line">    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise</span><br><span class="line">    <span class="keyword">return</span> x_prev, pred_x0</span><br></pre></td></tr></table></figure>
<p>方法的内容大致可以拆成三段：首先，方法调用U-Net <code>self.model</code>，使用CFG来计算除这一轮该去掉的噪声<code>e_t</code>。然后，方法预处理出DDIM的中间变量。最后，方法根据DDIM的公式，计算出这一轮去噪后的图片<code>x_prev</code>。我们着重看第一部分的代码。</p>
<p>不启用CFG时，方法直接通过<code>self.model.apply_model(x, t, c)</code>调用U-Net，算出这一轮的噪声<code>e_t</code>。而想启用CFG，需要输入空字符串的约束张量<code>unconditional_conditioning</code>，且CFG的强度<code>unconditional_guidance_scale</code>不为1。CFG的执行过程是：对U-Net输入不同的约束<code>c</code>，先用空字符串约束得到一个预测噪声<code>e_t_uncond</code>，再用输入的文本约束得到一个预测噪声<code>e_t</code>。之后令<code>e_t = et_uncond + scale * (e_t - e_t_uncond)</code>。<code>scale</code>大于1，即表明我们希望预测噪声更加靠近有输入文本的那一个。直观上来看，<code>scale</code>越大，最后生成的图片越符合输入文本，越偏离空文本。下面这段代码正是实现了上述这段逻辑，只不过代码使用了一些数据拼接技巧，让空字符串约束下和输入文本约束下的结果在一次U-Net推理中获得。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">    e_t = self.model.apply_model(x, t, c)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">    t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">    c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">    e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">    e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br></pre></td></tr></table></figure></p>
<p><code>p_sample_ddim</code> 方法的后续代码都是在实现下面这个DDIM采样公式。代码工工整整地计算了公式中的<code>predicted_x0</code>, <code>dir_xt</code>, <code>noise</code>，非常易懂，没有需要特别注意的地方。</p>
<p><img src="/2024/01/23/20230713-SD3/1-2.jpg" alt></p>
<p>我们已经看完了<code>p_sample_ddim</code>的代码。该方法可以实现一步去噪操作。多次调用该方法去噪后，我们就能得到生成的隐空间图片。该图片会被返回到main函数里，被VAE的解码器解码成普通图片。至此，我们就学完了Stable Diffusion官方仓库的采样代码。</p>
<p>对照下面这份我们之前写的伪代码，我们再来梳理一下Stable Diffusion官方仓库的代码逻辑。官方仓库的采样代码一部分在main函数里，另一部分在<code>ldm/models/diffusion/ddim.py</code>里。main函数主要完成了编码约束文字、解码隐空间图像这两件事。剩下的DDIM采样以及各种Diffusion图像编辑功能都是在<code>ldm/models/diffusion/ddim.py</code>文件中实现的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>)</span></span><br><span class="line"><span class="function">  <span class="title">ddim_scheduler</span> = <span class="title">DDIMScheduler</span>()</span></span><br><span class="line"><span class="function">  <span class="title">vae</span> = <span class="title">VAE</span>()</span></span><br><span class="line"><span class="function">  <span class="title">unet</span> = <span class="title">UNet</span>()</span></span><br><span class="line"><span class="function">  <span class="title">zt</span> = <span class="title">randn</span>(<span class="params">image_shape</span>)</span></span><br><span class="line"><span class="function">  <span class="title">eta</span> = <span class="title">input</span>()</span></span><br><span class="line"><span class="function">  <span class="title">T</span> = 1000</span></span><br><span class="line"><span class="function">  <span class="title">timesteps</span> = <span class="title">ddim_scheduler</span>.<span class="title">get_timesteps</span>(<span class="params">T, ddim_steps</span>) # [1000, 950, 900, ...]</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">text_encoder</span> = <span class="title">CLIP</span>()</span></span><br><span class="line"><span class="function">  <span class="title">c</span> = <span class="title">text_encoder</span>.<span class="title">encode</span>(<span class="params">text</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">for</span> <span class="title">t</span> = <span class="title">timesteps</span>:</span></span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure>
<p>在学习代码时，要着重学习DDIM采样器部分的代码。大部分基于Diffusion的图像编辑技术都是在DDIM采样的中间步骤中做文章，只要学懂了DDIM采样的代码，学相关图像编辑技术就会非常轻松。除此之外，和LDM相关的文字约束编码、隐空间图像编码解码的接口函数也需要熟悉，不少技术会调用到这几项功能。</p>
<p>还有一些Diffusion相关工作会涉及U-Net的修改。接下来，我们就来看Stable Diffusion官方仓库中U-Net的实现。</p>
<h3 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h3><p>我们来回头看一下main函数和DDIM采样中U-Net的调用逻辑。和U-Net有关的代码如下所示。LDM模型类 <code>model</code>在主函数中通过<code>load_model_from_config</code>从配置文件里创建，随后成为了<code>sampler</code>的成员变量。在DDIM去噪循环中，LDM模型里的U-Net会在<code>self.model.apply_model</code>方法里被调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># main.py</span></span><br><span class="line">config = <span class="string">&#x27;configs/stable-diffusion/v1-inference.yaml&#x27;</span></span><br><span class="line">config = OmegaConf.load(config)</span><br><span class="line">model = load_model_from_config(config, ckpt)</span><br><span class="line">sampler = DDIMSampler(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ldm/models/diffusion/ddim.py</span></span><br><span class="line">e_t = self.model.apply_model(x, t, c)</span><br></pre></td></tr></table></figure>
<p>为了知道U-Net是在哪个类里定义的，我们需要打开配置文件 <code>configs/stable-diffusion/v1-inference.yaml</code>。该配置文件有这样一段话：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">target:</span> <span class="string">ldm.models.diffusion.ddpm.LatentDiffusion</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">conditioning_key:</span> <span class="string">crossattn</span></span><br><span class="line">    <span class="attr">unet_config:</span></span><br><span class="line">        <span class="attr">target:</span> <span class="string">ldm.modules.diffusionmodules.openaimodel.UNetModel</span></span><br></pre></td></tr></table></figure></p>
<p>根据这段话，我们知道LDM类定义在<code>ldm/models/diffusion/ddpm.py</code>的<code>LatentDiffusion</code>里，U-Net类定义在<code>ldm/modules/diffusionmodules/openaimodel.py</code>的<code>UNetModel</code>里。一个LDM类有一个U-Net类的实例。我们先简单看一看<code>LatentDiffusion</code>类的实现。</p>
<p><code>ldm/models/diffusion/ddpm.py</code>原本来自DDPM论文的官方仓库，内含<code>DDPM</code>类的实现。<code>DDPM</code>类维护了扩散模型公式里的一些变量，同时维护了U-Net类的实例。LDM的作者基于之前DDPM的代码进行开发，定义了一个继承自<code>DDPM</code>的<code>LatentDiffusion</code>类。除了DDPM本身的功能外，<code>LatentDiffusion</code>还维护了VAE(<code>self.first_stage_model</code>)，CLIP（<code>self.cond_stage_model</code>）。也就是说，<code>LatentDiffusion</code>主要维护了扩散模型中间变量、U-Net、VAE、CLIP这四类信息。这样，所有带参数的模型都在<code>LatentDiffusion</code>里，我们可以从一个checkpoint文件中读取所有的模型的参数。相关代码定义代码如下：</p>
<blockquote>
<p>把所有模型定义在一起有好处也有坏处。好处在于，用户想使用Stable Diffusion时，只需要下载一个checkpoint文件就行了。坏处在于，哪怕用户只改了某个子模型（如U-Net），为了保存整个模型，他还是得把其他子模型一起存下来。这其中存在着信息冗余，十分不灵活。Diffusers框架没有把模型全存在一个文件里，而是放到了一个文件夹里。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPM</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line">    <span class="comment"># classic DDPM with Gaussian diffusion, in image space</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 unet_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ...</span>):</span></span><br><span class="line">        self.model = DiffusionWrapper(unet_config, conditioning_key)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LatentDiffusion</span>(<span class="params">DDPM</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;main class&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cond_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.instantiate_first_stage(first_stage_config)</span><br><span class="line">        self.instantiate_cond_stage(cond_stage_config)</span><br></pre></td></tr></table></figure>
<p>我们主要关注<code>LatentDiffusion</code>类的<code>apply_model</code>方法，它用于调用U-Net <code>self.model</code>。<code>apply_model</code>看上去有很长，但略过了我们用不到的一些代码后，整个方法其实非常短。一开始，方法对输入的约束信息编码<code>cond</code>做了一个前处理，判断约束是哪种类型。如论文里所描述的，LDM支持两种约束：将约束与输入拼接、将约束注入到交叉注意力层中。方法会根据<code>self.model.conditioning_key</code>是<code>concat</code>还是<code>crossattn</code>，使用不同的约束方式。Stable Diffusion使用的是后者，即<code>self.model.conditioning_key == crossattn</code>。做完前处理后，方法执行了<code>x_recon = self.model(x_noisy, t, **cond)</code>。接下来的处理交给U-Net <code>self.model</code>来完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_model</span>(<span class="params">self, x_noisy, t, cond, return_ids=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(cond, <span class="built_in">dict</span>):</span><br><span class="line">        <span class="comment"># hybrid case, cond is exptected to be a dict</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(cond, <span class="built_in">list</span>):</span><br><span class="line">            cond = [cond]</span><br><span class="line">        key = <span class="string">&#x27;c_concat&#x27;</span> <span class="keyword">if</span> self.model.conditioning_key == <span class="string">&#x27;concat&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;c_crossattn&#x27;</span></span><br><span class="line">        cond = &#123;key: cond&#125;</span><br><span class="line"></span><br><span class="line">    x_recon = self.model(x_noisy, t, **cond)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(x_recon, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="keyword">not</span> return_ids:</span><br><span class="line">        <span class="keyword">return</span> x_recon[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x_recon</span><br></pre></td></tr></table></figure>
<p>现在，我们跳转到<code>ldm/modules/diffusionmodules/openaimodel.py</code>的<code>UNetModel</code>类里。<code>UNetModel</code>只定义了神经网络层的运算，没有多余的功能。我们只需要看它的<code>__init__</code>方法和<code>forward</code>方法。我们先来看较为简短的<code>forward</code>方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = self.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">    h = x.<span class="built_in">type</span>(self.dtype)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> self.input_blocks:</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">        hs.append(h)</span><br><span class="line">    h = self.middle_block(h, emb, context)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> self.output_blocks:</span><br><span class="line">        h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">    h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">    <span class="keyword">return</span> self.out(h)</span><br></pre></td></tr></table></figure>
<p><code>forward</code>方法的输入是<code>x, timesteps, context</code>，分别表示当前去噪时刻的图片、当前时间戳、文本约束编码。根据这些输入，<code>forward</code>会输出当前时刻应去除的噪声<code>eps</code>。一开始，方法会先对<code>timesteps</code>使用Transformer论文中介绍的位置编码<code>timestep_embedding</code>，得到时间戳的编码<code>t_emb</code>。<code>t_emb</code>再经过几个线性层，得到最终的时间戳编码<code>emb</code>。而<code>context</code>已经是CLIP处理过的编码，它不需要做额外的预处理。时间戳编码<code>emb</code>和文本约束编码<code>context</code>随后会注入到U-Net的所有中间模块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = self.time_embed(t_emb)</span><br></pre></td></tr></table></figure>
<p>经过预处理后，方法开始处理U-Net的计算。中间结果<code>h</code>会经过U-Net的下采样模块<code>input_blocks</code>，每一个子模块的临时输出都会被保存进一个栈<code>hs</code>里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> h = x.<span class="built_in">type</span>(self.dtype)</span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> self.input_blocks:</span><br><span class="line">    h = module(h, emb, context)</span><br><span class="line">    hs.append(h)</span><br></pre></td></tr></table></figure>
<p>接着，<code>h</code>会经过U-Net的中间模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h = self.middle_block(h, emb, context)</span><br></pre></td></tr></table></figure>
<p>随后，<code>h</code>开始经过U-Net的上采样模块<code>output_blocks</code>。此时每一个编码器子模块的临时输出会从栈<code>hs</code>里弹出，作为对应解码器子模块的额外输入。额外输入<code>hs.pop()</code>会与中间结果<code>h</code>拼接到一起输入进子模块里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> self.output_blocks:</span><br><span class="line">    h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">    h = module(h, emb, context)</span><br><span class="line">h = h.<span class="built_in">type</span>(x.dtype)</span><br></pre></td></tr></table></figure>
<p>最后，<code>h</code>会被输出层转换成一个通道数正确的<code>eps</code>张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> self.out(h)</span><br></pre></td></tr></table></figure>
<p>这段代码的数据连接图如下所示：</p>
<p><img src="/2024/01/23/20230713-SD3/1-3.jpg" alt></p>
<p>在阅读<code>__init__</code>前，我们先看一下待会会用到的另一个模块类<code>TimestepEmbedSequential</code>的定义。在PyTorch中，一系列输入和输出都只有一个变量的模块在串行连接时，可以用串行模块类<code>nn.Sequential</code>来把多个模块合并简化成一个模块。而在扩散模型中，多数模块的输入是<code>x, t, c</code>三个变量，输出是一个变量。为了也能用类似的串行模块类把扩散模型的模块合并在一起，代码中包含了一个<code>TimestepEmbedSequential</code>类。它的行为类似于<code>nn.Sequential</code>，只不过它支持<code>x, t, c</code>的输入。<code>forward</code>中用到的多数模块都是通过<code>TimestepEmbedSequential</code>创建的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimestepEmbedSequential</span>(<span class="params">nn.Sequential, TimestepBlock</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, TimestepBlock):</span><br><span class="line">                x = layer(x, emb)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(layer, SpatialTransformer):</span><br><span class="line">                x = layer(x, context)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>看完了数据的计算过程，我们回头来看各个子模块在<code>__init__</code>方法中是怎么被详细定义的。<code>__init__</code>的主要内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNetModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.time_embed = nn.Sequential(</span><br><span class="line">            linear(model_channels, time_embed_dim),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(time_embed_dim, time_embed_dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.input_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                TimestepEmbedSequential(</span><br><span class="line">                    conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                )</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">                layers = [</span><br><span class="line">                    ResBlock(...)]</span><br><span class="line">                ch = mult * model_channels</span><br><span class="line">                <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                     layers.append(</span><br><span class="line">                        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...))</span><br><span class="line"></span><br><span class="line">                self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">            <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">                out_ch = ch</span><br><span class="line">                self.input_blocks.append(</span><br><span class="line">                    TimestepEmbedSequential(</span><br><span class="line">                        ResBlock(...)</span><br><span class="line">                        <span class="keyword">if</span> resblock_updown</span><br><span class="line">                        <span class="keyword">else</span> Downsample(...)</span><br><span class="line">                    )</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">        self.middle_block = TimestepEmbedSequential(</span><br><span class="line">            ResBlock(...),</span><br><span class="line">            AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...),</span><br><span class="line">            ResBlock(...),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.output_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">enumerate</span>(channel_mult))[::-<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks + <span class="number">1</span>):</span><br><span class="line">                ich = input_block_chans.pop()</span><br><span class="line">                layers = [</span><br><span class="line">                    ResBlock(...)</span><br><span class="line">                ]</span><br><span class="line">                ch = model_channels * mult</span><br><span class="line">                <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                    layers.append(</span><br><span class="line">                        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...)</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">if</span> level <span class="keyword">and</span> i == num_res_blocks:</span><br><span class="line">                    out_ch = ch</span><br><span class="line">                    layers.append(</span><br><span class="line">                        ResBlock(...)</span><br><span class="line">                        <span class="keyword">if</span> resblock_updown</span><br><span class="line">                        <span class="keyword">else</span> Upsample(...)</span><br><span class="line">                    )</span><br><span class="line">                    ds //= <span class="number">2</span></span><br><span class="line">                self.output_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">    self.out = nn.Sequential(</span><br><span class="line">            normalization(ch),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            zero_module(conv_nd(dims, model_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p><code>__init__</code>方法的代码很长。在阅读这样的代码时，我们不需要每一行都去细读，只需要理解代码能拆成几块，每一块在做什么即可。<code>__init__</code>方法其实就是定义了<code>forward</code>中用到的5个模块，我们一个一个看过去即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNetModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.time_embed = ...</span><br><span class="line"></span><br><span class="line">        self.input_blocks = nn.ModuleList(...)</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">        self.middle_block = ...</span><br><span class="line"></span><br><span class="line">        self.output_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">enumerate</span>(channel_mult))[::-<span class="number">1</span>]:</span><br><span class="line">            ...</span><br><span class="line">    self.out = ...</span><br></pre></td></tr></table></figure>
<p>先来看<code>time_embed</code>。回忆一下，在<code>forward</code>里，输入的整数时间戳会被正弦编码<code>timestep_embedding</code>（即Transformer中的位置编码）编码成一个张量。之后，时间戳编码处理模块<code>time_embed</code>用于进一步提取时间戳编码的特征。从下面的代码中可知，它本质上就是一个由两个普通线性层构成的模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.time_embed = nn.Sequential(</span><br><span class="line">            linear(model_channels, time_embed_dim),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(time_embed_dim, time_embed_dim),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>再来看U-Net最后面的输出模块<code>out</code>。输出模块的结构也很简单，它主要包含了一个卷积层，用于把中间变量的通道数从<code>dims</code>变成<code>model_channels</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.out = nn.Sequential(</span><br><span class="line">            normalization(ch),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            zero_module(conv_nd(dims, model_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>接下来，我们把目光聚焦在U-Net的三个核心模块上：<code>input_blocks</code>, <code>middle_block</code>, <code>output_blocks</code>。这三个模块的组成都很类似，都用到了残差块<code>ResBlock</code>和注意力块。稍有不同的是，<code>input_blocks</code>的每一大层后面都有一个下采样模块，<code>output_blocks</code>的每一大层后面都有一个上采样模块。上下采样模块的结构都很常规，与经典的U-Net无异。我们把学习的重点放在残差块和注意力块上。我们先看这两个模块的内部实现细节，再来看它们是怎么拼接起来的。</p>
<p>Stable Diffusion的U-Net中的<code>ResBlock</code>和原DDPM的U-Net的<code>ResBlock</code>功能完全一样，都是在普通残差块的基础上，支持时间戳编码的额外输入。具体来说，普通的残差块是由两个卷积模块和一条短路连接构成的，即<code>y = x + conv(conv(x))</code>。如果经过两个卷积块后数据的通道数发生了变化，则要在短路连接上加一个转换通道数的卷积，即<code>y = conv(x) + conv(conv(x))</code>。</p>
<p>在这种普通残差块的基础上，扩散模型中的残差块还支持时间戳编码<code>t</code>的输入。为了把<code>t</code>和输入<code>x</code>的信息融合在一起，<code>t</code>会和经过第一个卷积后的中间结果<code>conv(x)</code>加在一起。可是，<code>t</code>的通道数和<code>conv(x)</code>的通道数很可能会不一样。通道数不一样的数据是不能直接加起来的。为此，每一个残差块中都有一个用于转换<code>t</code>通道数的线性层。这样，<code>t</code>和<code>conv(x)</code>就能相加了。整个模块的计算可以表示成<code>y=conv(x) + conv(conv(x) + linear(t))</code>。残差块的示意图和源代码如下：</p>
<p><img src="/2024/01/23/20230713-SD3/1-4.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span>(<span class="params">TimestepBlock</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        self.in_layers = nn.Sequential(</span><br><span class="line">            normalization(channels),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            conv_nd(dims, channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.emb_layers = nn.Sequential(</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(</span><br><span class="line">                emb_channels,</span><br><span class="line">                <span class="number">2</span> * self.out_channels <span class="keyword">if</span> use_scale_shift_norm <span class="keyword">else</span> self.out_channels,</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        self.out_layers = nn.Sequential(</span><br><span class="line">            normalization(self.out_channels),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            zero_module(</span><br><span class="line">                conv_nd(dims, self.out_channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.out_channels == channels:</span><br><span class="line">            self.skip_connection = nn.Identity()</span><br><span class="line">        <span class="keyword">elif</span> use_conv:</span><br><span class="line">            self.skip_connection = conv_nd(</span><br><span class="line">                dims, channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.skip_connection = conv_nd(dims, channels, self.out_channels, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb</span>):</span></span><br><span class="line">        h = self.in_layers(x)</span><br><span class="line">        emb_out = self.emb_layers(emb).<span class="built_in">type</span>(h.dtype)</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(emb_out.shape) &lt; <span class="built_in">len</span>(h.shape):</span><br><span class="line">            emb_out = emb_out[..., <span class="literal">None</span>]</span><br><span class="line">        h = h + emb_out</span><br><span class="line">        h = self.out_layers(h)</span><br><span class="line">        <span class="keyword">return</span> self.skip_connection(x) + h</span><br></pre></td></tr></table></figure>
<p>代码中的<code>in_layers</code>是第一个卷积模块，<code>out_layers</code>是第二个卷积模块。<code>skip_connection</code>是用于调整短路连接通道数的模块。若输入输出的通道数相同，则该模块是一个恒等函数，不对数据做任何修改。<code>emb_layers</code>是调整时间戳编码通道数的线性层模块。这些模块的定义都在<code>ResBlock</code>的<code>__init__</code>里。它们的结构都很常规，没有值得注意的地方。我们可以着重阅读模型的<code>forward</code>方法。</p>
<p>如前文所述，在<code>forward</code>中，输入<code>x</code>会先经过第一个卷积模块<code>in_layers</code>，再与经过了<code>emb_layers</code>调整的时间戳编码<code>emb</code>相加后，输入进第二个卷积模块<code>out_layers</code>。最后，做完计算的数据会和经过了短路连接的原输入<code>skip_connection(x)</code>加在一起，作为整个残差块的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb</span>):</span></span><br><span class="line">    h = self.in_layers(x)</span><br><span class="line">    emb_out = self.emb_layers(emb).<span class="built_in">type</span>(h.dtype)</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(emb_out.shape) &lt; <span class="built_in">len</span>(h.shape):</span><br><span class="line">        emb_out = emb_out[..., <span class="literal">None</span>]</span><br><span class="line">    h = h + emb_out</span><br><span class="line">    h = self.out_layers(h)</span><br><span class="line">    <span class="keyword">return</span> self.skip_connection(x) + h</span><br></pre></td></tr></table></figure>
<p>这里有一点实现细节需要注意。时间戳编码<code>emb_out</code>的形状是<code>[n, c]</code>。为了把它和形状为<code>[n, c, h, w]</code>的图片加在一起，需要把它的形状变成<code>[n, c, 1, 1]</code>后再相加（形状为<code>[n, c, 1, 1]</code>的数据在与形状为<code>[n, c, h, w]</code>的数据做加法时形状会被自动广播成<code>[n, c, h, w]</code>）。在PyTorch中，<code>x=x[..., None]</code>可以在一个数据最后加一个长度为1的维度。比如对于形状为<code>[n, c]</code>的<code>t</code>，<code>t[..., None]</code>的形状就会是<code>[n, c, 1]</code>。</p>
<p>残差块的内容到此结束。我们接着来看注意力模块。在看模块的具体实现之前，我们先看一下源代码中有哪几种注意力模块。在U-Net的代码中，注意力模型是用以下代码创建的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">    layers.append(</span><br><span class="line">        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>第一行<code>if ds in attention_resolutions:</code>用于控制在U-Net的哪几个大层。Stable Diffusion每一大层都用了注意力模块，可以忽略这一行。随后，代码根据是否设置<code>use_spatial_transformer</code>来创建<code>AttentionBlock</code>或是<code>SpatialTransformer</code>。<code>AttentionBlock</code>是DDPM中采样的普通自注意力模块，而<code>SpatialTransformer</code>是LDM中提出的支持额外约束的标准Transfomer块。Stable Diffusion使用的是<code>SpatialTransformer</code>。我们就来看一看这个模块的实现细节。</p>
<p>如前所述，<code>SpatialTransformer</code>使用的是<strong>标准</strong>的Transformer块，它和Transformer中的Transformer块完全一致。输入<code>x</code>先经过一个自注意力层，再过一个交叉注意力层。在此期间，约束编码<code>c</code>会作为交叉注意力层的<code>K, V</code>输入进模块。最后，数据经过一个全连接层。每一层的输入都会和输出做一个残差连接。</p>
<p><img src="/2024/01/23/20230713-SD3/1-5.jpg" alt></p>
<p>当然，标准Transformer是针对一维序列数据的。要把Transformer用到图像上，则需要把图像的宽高拼接到同一维，即对张量做形状变换<code>n c h w -&gt; n c (h * w)</code>。做完这个变换后，就可以把数据直接输入进Transformer模块了。<br>这些图像数据与序列数据的适配都是在<code>SpatialTransformer</code>类里完成的。<code>SpatialTransformer</code>类并没有直接实现Transformer块的细节，仅仅是U-Net和Transformer块之间的一个过渡。Transformer块的实现在它的一个子模块里。我们来看它的实现代码。</p>
<p><code>SpatialTransformer</code>有两个卷积层<code>proj_in</code>, <code>proj_out</code>，负责图像通道数与Transformer模块通道数之间的转换。<code>SpatialTransformer</code>的<code>transformer_blocks</code>才是真正的Transformer模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpatialTransformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, n_heads, d_head,</span></span></span><br><span class="line"><span class="params"><span class="function">                 depth=<span class="number">1</span>, dropout=<span class="number">0.</span>, context_dim=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        inner_dim = n_heads * d_head</span><br><span class="line">        self.norm = Normalize(in_channels)</span><br><span class="line"></span><br><span class="line">        self.proj_in = nn.Conv2d(in_channels,</span><br><span class="line">                                 inner_dim,</span><br><span class="line">                                 kernel_size=<span class="number">1</span>,</span><br><span class="line">                                 stride=<span class="number">1</span>,</span><br><span class="line">                                 padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim)</span><br><span class="line">                <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(depth)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.proj_out = zero_module(nn.Conv2d(inner_dim,</span><br><span class="line">                                              in_channels,</span><br><span class="line">                                              kernel_size=<span class="number">1</span>,</span><br><span class="line">                                              stride=<span class="number">1</span>,</span><br><span class="line">                                              padding=<span class="number">0</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在<code>forward</code>中，图像数据在进出Transformer模块前后都会做形状和通道数上的适配。运算结束后，结果和输入之间还会做一个残差连接。<code>context</code>就是约束信息编码，它会接入到交叉注意力层上。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">    b, c, h, w = x.shape</span><br><span class="line">    x_in = x</span><br><span class="line">    x = self.norm(x)</span><br><span class="line">    x = self.proj_in(x)</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b c h w -&gt; b (h w) c&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> self.transformer_blocks:</span><br><span class="line">        x = block(x, context=context)</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b (h w) c -&gt; b c h w&#x27;</span>, h=h, w=w)</span><br><span class="line">    x = self.proj_out(x)</span><br><span class="line">    <span class="keyword">return</span> x + x_in</span><br></pre></td></tr></table></figure></p>
<p>每一个Transformer模块的结构完全符合上文的示意图。如果你之前学过Transformer，那这些代码你会十分熟悉。我们快速把这部分代码浏览一遍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, n_heads, d_head, dropout=<span class="number">0.</span>, context_dim=<span class="literal">None</span>, gated_ff=<span class="literal">True</span>, checkpoint=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)  <span class="comment"># is a self-attention</span></span><br><span class="line">        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)</span><br><span class="line">        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,</span><br><span class="line">                                    heads=n_heads, dim_head=d_head, dropout=dropout)  <span class="comment"># is self-attn if context is none</span></span><br><span class="line">        self.norm1 = nn.LayerNorm(dim)</span><br><span class="line">        self.norm2 = nn.LayerNorm(dim)</span><br><span class="line">        self.norm3 = nn.LayerNorm(dim)</span><br><span class="line">        self.checkpoint = checkpoint</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.attn1(self.norm1(x)) + x</span><br><span class="line">        x = self.attn2(self.norm2(x), context=context) + x</span><br><span class="line">        x = self.ff(self.norm3(x)) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>自注意力层和交叉注意力层都是用<code>CrossAttention</code>类实现的。该模块与Transformer论文中的多头注意力机制完全相同。当<code>forward</code>的参数<code>context=None</code>时，模块其实只是一个提取特征的自注意力模块；而当<code>context</code>为约束文本的编码时，模块就是一个根据文本约束进行运算的交叉注意力模块。该模块用不到<code>mask</code>，相关的代码可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, query_dim, context_dim=<span class="literal">None</span>, heads=<span class="number">8</span>, dim_head=<span class="number">64</span>, dropout=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head * heads</span><br><span class="line">        context_dim = default(context_dim, query_dim)</span><br><span class="line"></span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span></span><br><span class="line">        self.heads = heads</span><br><span class="line"></span><br><span class="line">        self.to_q = nn.Linear(query_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_k = nn.Linear(context_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_v = nn.Linear(context_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, query_dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        h = self.heads</span><br><span class="line"></span><br><span class="line">        q = self.to_q(x)</span><br><span class="line">        context = default(context, x)</span><br><span class="line">        k = self.to_k(context)</span><br><span class="line">        v = self.to_v(context)</span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; (b h) n d&#x27;</span>, h=h), (q, k, v))</span><br><span class="line"></span><br><span class="line">        sim = einsum(<span class="string">&#x27;b i d, b j d -&gt; b i j&#x27;</span>, q, k) * self.scale</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> exists(mask):</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention, what we cannot get enough of</span></span><br><span class="line">        attn = sim.softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        out = einsum(<span class="string">&#x27;b i j, b j d -&gt; b i d&#x27;</span>, attn, v)</span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;(b h) n d -&gt; b n (h d)&#x27;</span>, h=h)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br></pre></td></tr></table></figure>
<p>Transformer块的内容到此结束。看完了<code>SpatialTransformer</code>和<code>ResBlock</code>，我们可以回头去看模块之间是怎么拼接的了。先来看U-Net的中间块。它其实就是一个<code>ResBlock</code>接一个<code>SpatialTransformer</code>再接一个<code>ResBlock</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.middle_block = TimestepEmbedSequential(</span><br><span class="line">    ResBlock(...),</span><br><span class="line">    SpatialTransformer(...),</span><br><span class="line">    ResBlock(...),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下采样块<code>input_blocks</code>和上采样块<code>output_blocks</code>的结构几乎一模一样，区别只在于每一大层最后是做下采样还是上采样。这里我们以下采样块为例来学习一下这两个块的结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">self.input_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        TimestepEmbedSequential(</span><br><span class="line">            conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        layers = [</span><br><span class="line">            ResBlock(...)]</span><br><span class="line">        ch = mult * model_channels</span><br><span class="line">        <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                layers.append(</span><br><span class="line">                AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...))</span><br><span class="line"></span><br><span class="line">        self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">    <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">        out_ch = ch</span><br><span class="line">        self.input_blocks.append(</span><br><span class="line">            TimestepEmbedSequential(</span><br><span class="line">                ResBlock(...)</span><br><span class="line">                <span class="keyword">if</span> resblock_updown</span><br><span class="line">                <span class="keyword">else</span> Downsample(...)</span><br><span class="line">            )</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>上采样块一开始是一个调整输入图片通道数的卷积层，它的作用和<code>self.out</code>输出层一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.input_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        TimestepEmbedSequential(</span><br><span class="line">            conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>之后正式进行上采样块的构造。此处代码有两层循环，外层循环表示正在构造哪一个大层，内层循环表示正在构造该大层的哪一组模块。也就是说，共有<code>len(channel_mult)</code>个大层，每一大层都有<code>num_res_blocks</code>组相同的模块。在Stable Diffusion中，<code>channel_mult=[1, 2, 4, 4]</code>, <code>num_res_blocks=2</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>每一组模块由一个<code>ResBlock</code>和一个<code>SpatialTransformer</code>构成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">layers = [</span><br><span class="line">    ResBlock(...)</span><br><span class="line">]</span><br><span class="line">ch = mult * model_channels</span><br><span class="line"><span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">    ...</span><br><span class="line">    layers.append(</span><br><span class="line">        SpatialTransformer(...)</span><br><span class="line">    )</span><br><span class="line">self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">...</span><br></pre></td></tr></table></figure><br>构造完每一组模块后，若现在还没到最后一个大层，则添加一个下采样模块。Stable Diffusion有4个大层，只有运行到前3个大层时才会添加下采样模块。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">        out_ch = ch</span><br><span class="line">        self.input_blocks.append(</span><br><span class="line">            TimestepEmbedSequential(</span><br><span class="line">                ResBlock(...)</span><br><span class="line">                <span class="keyword">if</span> resblock_updown</span><br><span class="line">                <span class="keyword">else</span> Downsample(...)</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        ch = out_ch</span><br><span class="line">        input_block_chans.append(ch)</span><br><span class="line">        ds *= <span class="number">2</span></span><br></pre></td></tr></table></figure><br>至此，我们已经学完了Stable Diffusion的U-Net的主要实现代码。让我们来总结一下。U-Net是一种先对数据做下采样，再做上采样的网络结构。为了防止信息丢失，下采样模块和对应的上采样模块之间有残差连接。下采样块、中间块、上采样块都包含了<code>ResBlock</code>和<code>SpatialTransformer</code>两种模块。<code>ResBlock</code>是图像网络中常使用的残差块，而<code>SpatialTransformer</code>是能够融合图像全局信息并融合不同模态信息的Transformer块。Stable Diffusion的U-Net的输入除了有图像外，还有时间戳<code>t</code>和约束编码<code>c</code>。<code>t</code>会先过几个嵌入层和线性层，再输入进每一个<code>ResBlock</code>中。<code>c</code>会直接输入到所有Transformer块的交叉注意力块中。</p>
<p><img src="/2024/01/23/20230713-SD3/0-3.jpg" alt></p>
<h2 id="Diffusers"><a href="#Diffusers" class="headerlink" title="Diffusers"></a>Diffusers</h2><p>Diffusers是由Hugging Face维护的一套Diffusion框架。这个库的代码被封装进了一个Python模块里，我们可以在安装了Diffusers的Python环境中用<code>import diffusers</code>随时调用该库。相比之下，Diffusers的代码架构更加清楚，且各类Stable Diffusion的新技术都会及时集成进Diffusers库中。</p>
<p>由于我们已经在上文中学过了Stable Diffusion官方源码，在学习Diffusers代码时，我们只会大致过一过每一段代码是在做什么，而不会赘述Stable Diffusion的原理。</p>
<h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>安装该库时，不需要克隆仓库，只需要直接用pip即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade diffusers[torch]</span><br></pre></td></tr></table></figure>
<p>之后，随便在某个地方创建一个Python脚本文件，输入官方的示例项目代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DiffusionPipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">pipeline = DiffusionPipeline.from_pretrained(<span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>, torch_dtype=torch.float16)</span><br><span class="line">pipeline.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline(<span class="string">&quot;An image of a squirrel in Picasso style&quot;</span>).images[<span class="number">0</span>].save(<span class="string">&#x27;output.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>运行代码后，”一幅毕加索风格的松鼠图片”的绘制结果会保存在<code>output.jpg</code>中。我得到的结果如下：</p>
<p><img src="/2024/01/23/20230713-SD3/2-1.jpg" alt></p>
<p>在Diffusers中，<code>from_pretrained</code>函数可以直接从Hugging Face的模型仓库中下载预训练模型。比如，示例代码中<code>from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, ...)</code>指的就是从模型仓库<code>https://huggingface.co/runwayml/stable-diffusion-v1-5</code>中获取模型。</p>
<p>如果在当前网络下无法从命令行中访问Hugging Face，可以先想办法在网页上访问上面的模型仓库，手动下载<code>v1-5-pruned.ckpt</code>。之后，克隆Diffusers的GitHub仓库，再用Diffusers的工具把Stable Diffusion模型文件转换成Diffusers支持的模型格式。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:huggingface/diffusers.git</span><br><span class="line">cd diffusers</span><br><span class="line">python scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path &lt;src&gt; --dump_path &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<p>比如，假设你的模型文件存在<code>ckpt/v1-5-pruned.ckpt</code>，你想把输出的Diffusers的模型文件存在<code>ckpt/sd15</code>，则应该输入：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path ckpt/v1-5-pruned.ckpt --dump_path ckpt/sd15 </span><br></pre></td></tr></table></figure><br>之后修改示例脚本中的路径，就可以成功运行了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DiffusionPipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">pipeline = DiffusionPipeline.from_pretrained(<span class="string">&quot;ckpt/sd15&quot;</span>, torch_dtype=torch.float16)</span><br><span class="line">pipeline.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline(<span class="string">&quot;An image of a squirrel in Picasso style&quot;</span>).images[<span class="number">0</span>].save(<span class="string">&#x27;output.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure><br>对于其他的原版SD checkpoint（比如在civitai上下载的），也可以用同样的方式把它们转换成Diffusers兼容的版本。</p>
<h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>Diffusers使用<code>Pipeline</code>来管理一类图像生成算法。和图像生成相关的模块（如U-Net，DDIM采样器）都是<code>Pipeline</code>的成员变量。打开Diffusers版Stable Diffusion模型的配置文件<code>model_index.json</code>（在 <a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json">https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json</a> 网页上直接访问或者在本地的模型文件夹中找到），我们能看到该模型使用的<code>Pipeline</code>:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;StableDiffusionPipeline&quot;</span>,</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py</code>中，我们能找到<code>StableDiffusionPipeline</code>类的定义。所有<code>Pipeline</code>类的代码都非常长，一般我们可以忽略其他部分，只看运行方法<code>__call__</code>里的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">50</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">List</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    guidance_scale: <span class="built_in">float</span> = <span class="number">7.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_images_per_prompt: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    eta: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 0. Default height and width to unet</span></span><br><span class="line">    height = height <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">    width = width <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">    <span class="comment"># to deal with lora scaling and other possible forward hooks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Check inputs. Raise error if not correct</span></span><br><span class="line">    self.check_inputs(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Define call parameters</span></span><br><span class="line">    batch_size = ...</span><br><span class="line"></span><br><span class="line">    device = self._execution_device</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Encode input prompt</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    prompt_embeds, negative_prompt_embeds = self.encode_prompt(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For classifier free guidance, we need to do two forward passes.</span></span><br><span class="line">    <span class="comment"># Here we concatenate the unconditional and text embeddings into a single batch</span></span><br><span class="line">    <span class="comment"># to avoid doing two forward passes</span></span><br><span class="line">    <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">        prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Prepare timesteps</span></span><br><span class="line">    timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Prepare latent variables</span></span><br><span class="line">    num_channels_latents = self.unet.config.in_channels</span><br><span class="line">    latents = self.prepare_latents(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. Prepare extra step kwargs. <span class="doctag">TODO:</span> Logic should ideally just be moved out of the pipeline</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 7. Denoising loop</span></span><br><span class="line">    num_warmup_steps = <span class="built_in">len</span>(timesteps) - num_inference_steps * self.scheduler.order</span><br><span class="line">    self._num_timesteps = <span class="built_in">len</span>(timesteps)</span><br><span class="line">    <span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">        <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br><span class="line">            <span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">            latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># predict the noise residual</span></span><br><span class="line">            noise_pred = self.unet(</span><br><span class="line">                latent_model_input,</span><br><span class="line">                t,</span><br><span class="line">                encoder_hidden_states=prompt_embeds,</span><br><span class="line">                ...</span><br><span class="line">            )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform guidance</span></span><br><span class="line">            <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">                noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">                noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">and</span> self.guidance_rescale &gt; <span class="number">0.0</span>:</span><br><span class="line">                <span class="comment"># Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf</span></span><br><span class="line">                noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># call the callback, if provided</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(timesteps) - <span class="number">1</span> <span class="keyword">or</span> ((i + <span class="number">1</span>) &gt; num_warmup_steps <span class="keyword">and</span> (i + <span class="number">1</span>) % self.scheduler.order == <span class="number">0</span>):</span><br><span class="line">                progress_bar.update()</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> output_type == <span class="string">&quot;latent&quot;</span>:</span><br><span class="line">        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>, generator=generator)[</span><br><span class="line">            <span class="number">0</span></span><br><span class="line">        ]</span><br><span class="line">        image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        image = latents</span><br><span class="line">        has_nsfw_concept = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)</span><br></pre></td></tr></table></figure>
<p>虽然这段代码很长，但代码中的关键内容和我们在本文开头写的伪代码完全一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>)</span></span><br><span class="line"><span class="function">  <span class="title">ddim_scheduler</span> = <span class="title">DDIMScheduler</span>()</span></span><br><span class="line"><span class="function">  <span class="title">vae</span> = <span class="title">VAE</span>()</span></span><br><span class="line"><span class="function">  <span class="title">unet</span> = <span class="title">UNet</span>()</span></span><br><span class="line"><span class="function">  <span class="title">zt</span> = <span class="title">randn</span>(<span class="params">image_shape</span>)</span></span><br><span class="line"><span class="function">  <span class="title">eta</span> = <span class="title">input</span>()</span></span><br><span class="line"><span class="function">  <span class="title">T</span> = 1000</span></span><br><span class="line"><span class="function">  <span class="title">timesteps</span> = <span class="title">ddim_scheduler</span>.<span class="title">get_timesteps</span>(<span class="params">T, ddim_steps</span>) # [1000, 950, 900, ...]</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">text_encoder</span> = <span class="title">CLIP</span>()</span></span><br><span class="line"><span class="function">  <span class="title">c</span> = <span class="title">text_encoder</span>.<span class="title">encode</span>(<span class="params">text</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">for</span> <span class="title">t</span> = <span class="title">timesteps</span>:</span></span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure>
<p>我们可以对照着上面的伪代码来阅读这个方法。经过Diffusers框架本身的一些前处理后，方法先获取了约束文本的编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Encode input prompt</span></span><br><span class="line"><span class="comment"># c = text_encoder.encode(text)</span></span><br><span class="line">prompt_embeds, negative_prompt_embeds = self.encode_prompt(...)</span><br></pre></td></tr></table></figure>
<p>方法再从采样器里获取了要用到的时间戳，并随机生成了一个初始噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Prepare timesteps</span></span><br><span class="line"><span class="comment"># timesteps = ddim_scheduler.get_timesteps(T, ddim_steps)</span></span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Prepare latent variables</span></span><br><span class="line"><span class="comment"># zt = randn(image_shape)</span></span><br><span class="line">num_channels_latents = self.unet.config.in_channels</span><br><span class="line">latents = self.prepare_latents(</span><br><span class="line">    ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>做完准备后，方法进入去噪循环。循环一开始是用U-Net算出当前应去除的噪声<code>noise_pred</code>。由于加入了CFG，U-Net计算的前后有一些对数据形状处理的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">    <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br><span class="line">        <span class="comment"># eps = unet(zt, t, c)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">        latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># predict the noise residual</span></span><br><span class="line">        noise_pred = self.unet(</span><br><span class="line">            latent_model_input,</span><br><span class="line">            t,</span><br><span class="line">            encoder_hidden_states=prompt_embeds,</span><br><span class="line">            ...</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># perform guidance</span></span><br><span class="line">        <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">            noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">            noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">and</span> self.guidance_rescale &gt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="comment"># Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf</span></span><br><span class="line">            noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)</span><br></pre></td></tr></table></figure>
<p>有了应去除的噪声，方法会调用扩散模型采样器对当前的噪声图片进行更新。Diffusers把采样的逻辑全部封装进了采样器的<code>step</code>方法里。对于包括DDIM在内的所有采样器，都可以调用这个通用的接口，完成一步采样。<code>eta</code>等采样器参数会通过<code>**extra_step_kwargs</code>传入采样器的<code>step</code>方法里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># std = ddim_scheduler.get_std(t, eta)</span></span><br><span class="line"><span class="comment"># zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>经过若干次循环后，我们得到了隐空间下的生成图片。我们还需要调用VAE把隐空间图片解码成普通图片。代码中的<code>self.vae.decode(latents / self.vae.config.scaling_factor, ...)</code>用于解码图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> output_type == <span class="string">&quot;latent&quot;</span>:</span><br><span class="line">    image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>, generator=generator)[</span><br><span class="line">        <span class="number">0</span></span><br><span class="line">    ]</span><br><span class="line">    image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    image = latents</span><br><span class="line">    has_nsfw_concept = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)</span><br></pre></td></tr></table></figure>
<p>就这样，我们很快就看完了Diffusers的采样代码。相比之下，Diffusers的封装确实更合理，主要的图像生成逻辑都写在<code>Pipeline</code>类的<code>__call__</code>里，剩余逻辑都封装在VAE、U-Net、采样器等各自的类里。</p>
<h3 id="U-Net-1"><a href="#U-Net-1" class="headerlink" title="U-Net"></a>U-Net</h3><p>接下来我们来看Diffusers中的U-Net实现。还是打开模型配置文件<code>model_index.json</code>，我们可以找到U-Net的类名。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;unet&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;diffusers&quot;</span>,</span><br><span class="line">    <span class="string">&quot;UNet2DConditionModel&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/models/unet_2d_condition.py</code>文件中，我们可以找到类<code>UNet2DConditionModel</code>。由于Diffusers集成了非常多新特性，整个文件就像一锅大杂烩一样，掺杂着各种功能的实现代码。不过，这份U-Net的实现还是基于原版Stable Diffusion的U-Net进行开发的，原版代码的每一部分都能在这份代码里找到对应。在阅读代码时，我们可以跳过无关的功能，只看我们在Stable Diffusion官方仓库中见过的部分。</p>
<p>先看初始化函数的主要内容。初始化函数依然主要包括<code>time_proj, time_embedding</code>, <code>down_blocks</code>, <code>mid_block</code>, <code>up_blocks</code>, <code>conv_in</code>, <code>conv_out</code>这几个模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet2DConditionModel</span>(<span class="params">ModelMixin, ConfigMixin, UNet2DConditionLoadersMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.conv_in = nn.Conv2d(</span><br><span class="line">            in_channels, block_out_channels[<span class="number">0</span>], kernel_size=conv_in_kernel, padding=conv_in_padding</span><br><span class="line">        )</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">elif</span> time_embedding_type == <span class="string">&quot;positional&quot;</span>:</span><br><span class="line">            self.time_proj = Timesteps(block_out_channels[<span class="number">0</span>], flip_sin_to_cos, freq_shift)</span><br><span class="line">        ...</span><br><span class="line">        self.time_embedding = TimestepEmbedding(...)</span><br><span class="line">        self.down_blocks = nn.ModuleList([])</span><br><span class="line">        self.up_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> i, down_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(down_block_types):</span><br><span class="line">            ...</span><br><span class="line">            down_block = get_down_block(...)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> mid_block_type == ...</span><br><span class="line">            self.mid_block = ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, up_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(up_block_types):</span><br><span class="line">            up_block = get_up_block(...)</span><br><span class="line"></span><br><span class="line">        self.conv_out = nn.Conv2d(...)</span><br></pre></td></tr></table></figure>
<p>其中，较为重要的<code>down_blocks</code>, <code>mid_block</code>, <code>up_blocks</code>都是根据模块类名称来创建的。我们可以在Diffusers的Stable Diffusion模型文件夹的U-Net的配置文件<code>unet/config.json</code>中找到对应的模块类名称。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">&quot;down_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;DownBlock2D&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;mid_block_type&quot;</span>: <span class="string">&quot;UNetMidBlock2DCrossAttn&quot;</span>,</span><br><span class="line">  <span class="string">&quot;up_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;UpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/models/unet_2d_blocks.py</code>中，我们可以找到这几个模块类的定义。和原版代码一样，这几个模块的核心组件都是残差块和Transformer块。在Diffusers中，残差块叫做<code>ResnetBlock2D</code>，Transformer块叫做<code>Transformer2DModel</code>。这几个类的执行逻辑和原版仓库的也几乎一样。比如<code>CrossAttnDownBlock2D</code>的定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttnDownBlock2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            resnets.append(ResnetBlock2D(...))</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> dual_cross_attention:</span><br><span class="line">                attentions.append(Transformer2DModel(...))</span><br></pre></td></tr></table></figure>
<p>接着我们来看U-Net的<code>forward</code>方法。忽略掉其他功能的实现，该方法的主要内容如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        sample: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        timestep: <span class="type">Union</span>[torch.Tensor, <span class="built_in">float</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 0. center input if necessary</span></span><br><span class="line">    <span class="keyword">if</span> self.config.center_input_sample:</span><br><span class="line">        sample = <span class="number">2</span> * sample - <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. time</span></span><br><span class="line">    timesteps = timestep</span><br><span class="line">    t_emb = self.time_proj(timesteps)</span><br><span class="line">    emb = self.time_embedding(t_emb, timestep_cond)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. pre-process</span></span><br><span class="line">    sample = self.conv_in(sample)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. down</span></span><br><span class="line">    down_block_res_samples = (sample,)</span><br><span class="line">    <span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">        sample, res_samples = downsample_block(</span><br><span class="line">            hidden_states=sample,</span><br><span class="line">            temb=emb,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line">        down_block_res_samples += res_samples</span><br><span class="line">    <span class="comment"># 4. mid</span></span><br><span class="line">    sample = self.mid_block(</span><br><span class="line">            sample,</span><br><span class="line">            emb,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. up</span></span><br><span class="line">    <span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">        res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">        down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">        sample = upsample_block(</span><br><span class="line">            hidden_states=sample,</span><br><span class="line">            temb=emb,</span><br><span class="line">            res_hidden_states_tuple=res_samples,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 6. post-process</span></span><br><span class="line">    sample = self.conv_out(sample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> UNet2DConditionOutput(sample=sample)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>该方法和原版仓库的实现差不多，唯一要注意的是栈相关的实现。在方法的下采样计算中，每个<code>downsample_block</code>会返回多个残差输出的元组<code>res_samples</code>，该元组会拼接到栈<code>down_block_res_samples</code>的栈顶。在上采样计算中，代码会根据当前的模块个数，从栈顶一次取出<code>len(upsample_block.resnets)</code>个残差输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">down_block_res_samples = (sample,)</span><br><span class="line"><span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">    sample, res_samples = downsample_block(...)</span><br><span class="line">    down_block_res_samples += res_samples</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">    res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">    down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">    sample = upsample_block(...)</span><br></pre></td></tr></table></figure>
<p>现在，我们已经看完了Diffusers中U-Net的主要内容。可以看出，Diffusers的U-Net包含了很多功能，一般情况下是难以自己更改这些代码的。有没有什么办法能方便地修改U-Net的实现呢？由于很多工作都需要修改U-Net的Attention，Diffusers给U-Net添加了几个方法，用于精确地修改每一个Attention模块的实现。我们来学习一个修改Attention模块的示例。</p>
<p>U-Net类的<code>attn_processors</code>属性会返回一个词典，它的key是每个Attention运算类所在位置，比如<code>down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor</code>，它的value是每个Attention运算类的实例。默认情况下，每个Attention运算类都是<code>AttnProcessor</code>，它的实现在<code>diffusers/models/attention_processor.py</code>文件中。</p>
<p>为了修改Attention运算的实现，我们需要构建一个格式一样的词典<code>attn_processor_dict</code>，再调用<code>unet.set_attn_processor(attn_processor_dict)</code>，取代原来的<code>attn_processors</code>。假如我们自己实现了另一个Attention运算类<code>MyAttnProcessor</code>，我们可以编写下面的代码来修改Attention的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">attn_processor_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">    <span class="keyword">if</span> we_want_to_modify(k):</span><br><span class="line">        attn_processor_dict[k] = MyAttnProcessor()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">unet.set_attn_processor(attn_processor_dict)</span><br></pre></td></tr></table></figure>
<p><code>MyAttnProcessor</code>的唯一要求是，它需要实现一个<code>__call__</code>方法，且方法参数与<code>AttnProcessor</code>的一致。除此之外，我们可以自由地实现Attention处理的细节。一般来说，我们可以先把原来<code>AttnProcessor</code>的实现代码复制过去，再对某些细节做修改。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们学习了Stable Diffusion的原版实现和Diffusers实现的主要内容：采样算法和U-Net。具体来说，在原版仓库中，采样的实现一部分在主函数中，一部分在DDIM采样器类中。U-Net由一个简明的PyTorch模块类实现，其中比较重要的子模块是残差块和Transformer块。相比之下，Diffusers实现的封装更好，功能更多。Diffusers用一个Pipeline类来维护采样过程。Diffusers的U-Net实现与原版完全相同，且支持更复杂的功能。此外，Diffusers还给U-Net提供了精确修改Attention计算的接口。</p>
<p>不管是哪个Stable Diffusion的框架，都会提供一些相同的原子操作。各种基于Stable Diffusion的应用都应该基于这些原子操作开发，而无需修改这些操作的细节。在学习时，我们应该注意这些操作在不同的框架下的写法是怎么样的。常用的原子操作包括：</p>
<ul>
<li>VAE的解码和编码</li>
<li>文本编码器（CLIP）的编码</li>
<li>用U-Net预测当前图像应去除的噪声</li>
<li>用采样器计算下一去噪迭代的图像</li>
</ul>
<p>在原版仓库中，相关的实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VAE的解码和编码</span></span><br><span class="line">model.decode_first_stage(...)</span><br><span class="line">model.encode_first_stage(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本编码器（CLIP）的编码</span></span><br><span class="line">model.get_learned_conditioning(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用U-Net预测当前图像应去除的噪声</span></span><br><span class="line">model.apply_model(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用采样器计算下一去噪迭代的图像</span></span><br><span class="line">p_sample_ddim(...)</span><br></pre></td></tr></table></figure>
<p>在Diffusers中，相关的实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VAE的解码和编码</span></span><br><span class="line">image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">latents = self.vae.encode(image).latent_dist.sample(generator) * self.vae.config.scaling_factor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本编码器（CLIP）的编码</span></span><br><span class="line">self.encode_prompt(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用U-Net预测当前图像应去除的噪声</span></span><br><span class="line">self.unet(..., return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用采样器计算下一去噪迭代的图像</span></span><br><span class="line">self.scheduler.step(..., return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<p>如今zero-shot（无需训练）的Stable Diffusion编辑技术一般只会修改采样算法和Attention计算，需训练的编辑技术有时会在U-Net里加几个模块。只要我们熟悉了普通的Stable Diffusion是怎么样生成图像的，知道原来U-Net的结构是怎么样的，我们在阅读新论文的源码时就可以把这份代码与原来的代码进行对比，只看那些有修改的部分。相信读完了本文后，我们不仅加深了对Stable Diffusion本身的理解，以后学习各种新出的Stable Diffusion编辑技术时也会更加轻松。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E7%BC%96%E7%A8%8B/" rel="tag"># 编程</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" rel="tag"># 扩散模型</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/01/23/20230709-SD2/" rel="prev" title="Stable Diffusion 解读（二）：论文精读">
      <i class="fa fa-chevron-left"></i> Stable Diffusion 解读（二）：论文精读
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/01/23/20240114-SD-LoRA/" rel="next" title="LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例">
      LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%A2%B3%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">算法梳理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LDM-%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95"><span class="nav-number">1.1.</span> <span class="nav-text">LDM 采样算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#U-Net-%E7%BB%93%E6%9E%84%E7%BB%84%E6%88%90"><span class="nav-number">1.2.</span> <span class="nav-text">U-Net 结构组成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stable-Diffusion-%E5%AE%98%E6%96%B9-GitHub-%E4%BB%93%E5%BA%93"><span class="nav-number">2.</span> <span class="nav-text">Stable Diffusion 官方 GitHub 仓库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">2.1.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">主函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DDIM-%E9%87%87%E6%A0%B7%E5%99%A8"><span class="nav-number">2.3.</span> <span class="nav-text">DDIM 采样器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#U-Net"><span class="nav-number">2.4.</span> <span class="nav-text">U-Net</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Diffusers"><span class="nav-number">3.</span> <span class="nav-text">Diffusers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85-1"><span class="nav-number">3.1.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E6%A0%B7"><span class="nav-number">3.2.</span> <span class="nav-text">采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#U-Net-1"><span class="nav-number">3.3.</span> <span class="nav-text">U-Net</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">125</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
