<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="在上一门课中，我们学习了如何用CNN处理网格状的数据。由于最常见的网格状数据是图像，我们主要学习了如何用CNN完成和图像相关的任务。而在这一门课中，我们要学习如何用循环神经网络（RNN）等序列模型处理序列数据。序列数据的种类就比较丰富多彩了：  如上图所示，对于语音识别、音乐生成、情绪分类、DNA序列分析、机器翻译、视频动作识别、命名实体识别这些任务，它们的输入和输出至少有一个是某类序列数据，它们">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达《深度学习专项》笔记（十四）：循环神经网络基础">
<meta property="og:url" content="https://zhouyifan.net/2022/09/21/DLS-note-14/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="在上一门课中，我们学习了如何用CNN处理网格状的数据。由于最常见的网格状数据是图像，我们主要学习了如何用CNN完成和图像相关的任务。而在这一门课中，我们要学习如何用循环神经网络（RNN）等序列模型处理序列数据。序列数据的种类就比较丰富多彩了：  如上图所示，对于语音识别、音乐生成、情绪分类、DNA序列分析、机器翻译、视频动作识别、命名实体识别这些任务，它们的输入和输出至少有一个是某类序列数据，它们">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/1.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/1.gif">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/2.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/3.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/4.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/5.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/6.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/7.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/8.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/9.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/10.jpg">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/11.jpg">
<meta property="article:published_time" content="2022-09-21T09:47:24.000Z">
<meta property="article:modified_time" content="2022-09-21T09:47:24.733Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14/1.jpg">

<link rel="canonical" href="https://zhouyifan.net/2022/09/21/DLS-note-14/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>吴恩达《深度学习专项》笔记（十四）：循环神经网络基础 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/09/21/DLS-note-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达《深度学习专项》笔记（十四）：循环神经网络基础
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-21 17:47:24" itemprop="dateCreated datePublished" datetime="2022-09-21T17:47:24+08:00">2022-09-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在上一门课中，我们学习了如何用CNN处理网格状的数据。由于最常见的网格状数据是图像，我们主要学习了如何用CNN完成和图像相关的任务。而在这一门课中，我们要学习如何用循环神经网络（RNN）等序列模型处理序列数据。序列数据的种类就比较丰富多彩了：</p>
<p><img src="/2022/09/21/DLS-note-14/1.jpg" alt></p>
<p>如上图所示，对于语音识别、音乐生成、情绪分类、DNA序列分析、机器翻译、视频动作识别、命名实体识别这些任务，它们的输入和输出至少有一个是某类序列数据，它们都可以用序列模型来建模。</p>
<p>计算机科学中的自然语言处理（NLP）任务常常需要使用序列模型。我们在学这门课时，主要会围绕NLP问题进行讨论。</p>
<h2 id="符号标记"><a href="#符号标记" class="headerlink" title="符号标记"></a>符号标记</h2><p>序列数据需要用到一些新的符号标记。在开始正式学习之前，我们先以命名实体识别任务为例，认识一下这些新的符号标记。</p>
<p>命名实体识别任务要求找出句子中的有意义的人名、地名等特殊名词。以这个任务的一个训练样本为例，我们来看一看序列数据的符号标记。</p>
<p>设输入是 Harry Potter and Hermione Granger invented a new spell。</p>
<p>命名实体识别任务的输出$y$也是一个序列。序列的每一个元素是1或0，表示输入中对应的单词是否为命名实体。刚刚那个输入对应的输出应该是1 1 0 1 1 0 0 0 0。</p>
<p>输入输出都由单词或数字组成的序列。设输入为$x$，序列中的第$i$个单词记作$x^{&lt;  i  &gt;}$。比如$x^{&lt; 2 &gt;}$ = Potter。整个序列的长度$T_x=9$。同理，输出序列中的第$i$个元素记作$y^{&lt; i &gt;}$，整个序列的长度$T_y=9$。</p>
<p>对于第$i$个样本而言，其输入的长度为$T_x^{(i)}$，第$t$个单词为$x^{(i)&lt; t &gt;}$。值得注意的是，每个样本的长度可能不一致。</p>
<p>在表示这些数据时，还有一个问题：怎么表示一个单词？计算机可不认识一大堆字母。为了让只懂数字的计算机能够分清不同的单词，我们要先准备一个词汇表，并用单词在词汇表里的one-hot编码作为单词的表示。比如Harry是长度为10000的词汇表里第4075个单词，则Harry的表示是$[0, 0, 0…,0, 1, 0, …, 0]$，这个向量的长度是10000，只有第4075个元素是1，其他地方都是0。</p>
<blockquote>
<p>在大型的模型中，词汇表的大小会是30000至50000，甚至有100000的。</p>
</blockquote>
<p>有了这些符号标记，和序列数据有关的任务就可以完全用数学语言表示了。比如对于命名实体识别任务，每一条样本输入是一个向量序列，输出是一个01的数字序列。</p>
<h2 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h2><p>在序列问题上使用标准神经网络（全连接网络）会有几个问题：</p>
<ul>
<li>每个样本的长度可能不一致。尽管我们可以找到一个最大的长度，并对长度不足的样本进行零填充，但这种做法看上去就不太好。</li>
<li>同一个单词在不同的位置时应该算出类似的特征。而标准神经网络会把每个位置的输入区别对待，无法共享各个位置的知识。</li>
<li>和图像数据类似，序列数据的输入长度也很大。假如一个句子有10个单词，词汇表的大小是10000，则输入向量的大小就是100000。这样，神经网络第一层的参数量会很大，网络会过拟合。</li>
</ul>
<p>因此，我们要用一些其他的架构来处理序列数据以规避这些问题。其中一种可行的架构就是循环神经网络(Recurrent Neural Network, RNN)。</p>
<p>RNN运算过程如下图所示。在RNN中，对于一个样本，我们每次只输入一个单词$x^{&lt; t &gt;}$，得到一个输出$y^{&lt; t &gt;}$。除了输出$y^{&lt; t &gt;}$外，神经网络还会把中间激活输出$a^{&lt; t &gt;}$传递给下一轮计算，这个$a^{&lt; t &gt;}$记录了之前单词的某些信息。所有的输出按照这种方法依次计算。当然，第一轮计算时也会用到激活输出$a^{&lt; 0 &gt;}$，简单地令$a^{&lt; 0 &gt;}$为零张量即可。注意，所有的计算都是用同一个权重一样的神经网络。</p>
<p><img src="/2022/09/21/DLS-note-14/1.gif" alt></p>
<blockquote>
<p>有些文章会把一行RNN计算折叠成一个带循环箭头的神经网络，这只是另一种画图的方法：</p>
</blockquote>
<p><img src="/2022/09/21/DLS-note-14/2.jpg" alt></p>
<p>看完了示意图，让我们看看怎样用数学语言表示RNN。在第$t$轮计算中：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{ax} x^{< t >} + W_{aa} a^{< t - 1 >} + b_a) \\
\hat{y}^{< t >} &=g_2(W_{ya} a^{< t >} + b_y)
\end{aligned}</script><p>其中，$W_{ij}$表示用于计算$i$的，乘在$j$上的矩阵。$g$是激活函数。中间层激活函数多数情况下用tanh，也可以用ReLU。输出的激活函数视任务而定，在二分类的命名实体识别中，输出激活函数是sigmoid。</p>
<p>为了简化表示，可以把$W<em>{ax}, W</em>{aa}$拼一下，$x^{&lt; t &gt;}, a^{&lt; t - 1 &gt;}$拼一下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
W_a &= [W_{aa} | W_{ax}] \\
[a^{< t - 1 >}, x^{< t >}] &= \left[
  \begin{matrix}
  a^{< t - 1 >} \\
  --- \\
   x^{< t >}
  \end{matrix}
\right]
\end{aligned}</script><p>这样，原来的式子就可以化简了：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{a} [a^{< t - 1 >}, x^{< t >}] + b_a) \\
\hat{y}^{< t >} &=g_2(W_{y} a^{< t >} + b_y)
\end{aligned}</script><p>RNN也有一些问题。首先，显而易见，每个RNN的输出只能看到它之前的单词。这样是不太好的。比如一句话第一个单词是Teddy，你不知道这是一个人名，还是Teddy bear这样一个普通的物体。稍后我们会学习双向的RNN以解决此问题。</p>
<p>另外，RNN也会面临标准神经网络的梯度爆炸问题。后几节会介绍一些更高级的RNN架构。</p>
<h2 id="「穿越时空之反向传播」"><a href="#「穿越时空之反向传播」" class="headerlink" title="「穿越时空之反向传播」"></a>「穿越时空之反向传播」</h2><p>看完了正向传播，我们稍微看一下RNN反向传播的过程。</p>
<p>反向传播前，先要定义一个误差函数。对于命名实体识别这种结果是01的问题，可以继续采用交叉熵误差，即对于序列中每一个元素：</p>
<script type="math/tex; mode=display">
L^{< t >}(y^{< t >}, \hat{y}^{< t >}) = -(y^{< t >}log\hat{y}^{< t >}+(1 - y^{< t >})log(1 - \hat{y}^{< t >}))</script><p>对于一个样本：</p>
<script type="math/tex; mode=display">
L(y, \hat{y}) = -\Sigma_{t=1}^{T_y}(y^{< t >}log\hat{y}^{< t >}+(1 - y^{< t >})log(1 - \hat{y}^{< t >}))</script><p>接下来是反向传播的过程。RNN的计算虽然复杂了一些，但它本质上还是一个计算图。如下图所示，按照红色箭头的方向对变量反向求导即可：</p>
<p><img src="/2022/09/21/DLS-note-14/3.jpg" alt></p>
<p>使用了编程框架后，反向传播可以自动由框架完成，可以不需要关心里面的细节了。</p>
<p>由于序列数据有先后的概念，而RNN的反向传播又是从后面的数据向前面的数据进行，因此这样的反向传播有着「穿越时空之反向传播」的称呼。</p>
<h2 id="不同输入输出格式的RNN"><a href="#不同输入输出格式的RNN" class="headerlink" title="不同输入输出格式的RNN"></a>不同输入输出格式的RNN</h2><p>刚刚学习的那种RNN只能描述输入输出长度一致的任务。在那种架构的基础上稍作修改，我们就能得到描述各种输入输出格式的任务。</p>
<p><img src="/2022/09/21/DLS-note-14/4.jpg" alt></p>
<ul>
<li>一对一：其实一对一问题就是标准神经网络，可以不要那个激活输入$a$。</li>
<li>一对多：只把输入放入第一轮计算中，后续计算的输入是上一轮的输出。</li>
<li>多对一：只输出最后一轮计算的结果。</li>
<li>等长多对多：输入一个元素就输出一个元素。</li>
<li>不等长多对多：先做几轮不产生输出的计算（编码器），再做几轮只产生输出的计算（解码器）。解码器的输入也可以和一对多一样，来自于上一轮的输出（图中没有画出）。</li>
</ul>
<h2 id="RNN应用：语言模型"><a href="#RNN应用：语言模型" class="headerlink" title="RNN应用：语言模型"></a>RNN应用：语言模型</h2><p>为了加深对RNN的理解，我们来看一个基于RNN的应用——语言模型。</p>
<p>语言模型是NLP中的一个基础任务。一个语言模型能够输出某种语言某句话的出现概率。通过比较不同句子的出现概率，我们能够开发出很多应用。比如在英语里，同音的”apple and pear”比”apple and pair”的出现概率高（更可能是一个合理的句子）。当一个语音识别软件听到这句话时，可以分别写下这两句发音相近的句子，再根据语言模型断定这句话应该写成前者。</p>
<p>也就是说，对于一句话$x^{&lt; 1 &gt;}…x^{&lt; T_x &gt;}$，语言模型的输出是$P(x^{&lt; 1 &gt;},…, x^{&lt; T_x &gt;})$。这个式子也可以写成$P(x^{&lt; 1 &gt;}) \times P(x^{&lt; 2 &gt;} |x^{&lt; 1 &gt;}) \times  P(x^{&lt; 3 &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}) … \times  P(x^{&lt; T_x &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}, …, x^{&lt; T_x-1 &gt;})$，即一句话的出现概率，等于第一个单词出现在句首的概率，乘上第二个单词在第一个单词之后的概率，乘上第三个单词再第一、二个单词之后的概率，这样一直乘下去。</p>
<p>在训练语言模型时，我们一般要用到语料库(corpus)。语料库包含了某种语言大量的通顺的句子。我们希望一个模型能够学习到这些句子中的规律，知道一句话在这种语言中的出现概率是多少。</p>
<p>语言模型可以用RNN巧妙地实现。整个实现分两步：数据预处理和模型训练。</p>
<p>由于语料库中包含的是自然语言，而RNN的输入是one-hot编码，所以这中间要经过一个预处理的步骤。在NLP中，这一步骤叫做符号化(tokenize)。如我们在「符号标记」一节所学的，我们可以找来一个大小为10000的词汇表，根据每个单词在词汇表中的位置，生成一个one-hot编码。除了普通的词汇外，NLP中还有一些特殊的符号，比如表示句尾的<code>&lt;EOS&gt;</code> (End Of Sentence)，表示词汇表里没有的词的<code>&lt;UNK&gt;</code> (Unknown)。</p>
<p>经过预处理后，语料库里的每一句自然语言就变成了训练样本$x^{&lt; 1 &gt;} … x^{&lt; T_x &gt;}$。我们可以把每一句话输入RNN，巧妙地训练一个语言模型：</p>
<p><img src="/2022/09/21/DLS-note-14/5.jpg" alt></p>
<p>这个计算过程初次接触时有些令人费解，我们慢慢来看懂它。先竖着看一轮计算是怎么完成的。对于每一轮计算，都会给定一个单词编码$x^{&lt; i &gt;}$，输出一个softmax后的概率分布$\hat{y}^{&lt; i &gt;}$，它要对齐的训练标签是训练集某一句话的某个单词$y^{&lt; i &gt;}$。$\hat{y}$表示接收之前所有的输入单词后，此时刻应该输出某单词的概率分布，这个输出的含义和多分类中的类似。</p>
<p>算出这样的$\hat{y}$有什么用呢？别急，再横着看一遍。回忆一下，语言模型要求的概率可以写成$P(y^{&lt; 1 &gt;}) \times P(y^{&lt; 2 &gt;} |y^{&lt; 1 &gt;}) \times  P(y^{&lt; 3 &gt;} |y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;}) …$。RNN每一轮的输出，其实就是要拟合$P(y^{&lt; 1 &gt;})$, $P(y^{&lt; 2 &gt;} |y^{&lt; 1 &gt;})$, $P(y^{&lt; 3 &gt;} |y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;})$, …。每一个条件概率的条件，就是每一轮RNN的输入；每一个条件概率的待求事件，就是每一轮RNN的训练标签。比如$P(y^{&lt; 3 &gt;} |y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;})$这个条件概率，它的条件是$y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;}$，待求事件是$y^{&lt; 3 &gt;}$，所以第三轮RNN的标签是$y^{&lt; 3 &gt;}$，输入是$y^{&lt; 1 &gt;}, y^{&lt; 2 &gt;}$（别忘了，在RNN中，前几轮的输入其实也影响了后续的计算）。当然，第一个概率$P(y^{&lt; 1 &gt;})$没有条件，所以第一轮的输入$x^{&lt; 1 &gt;}=0$。对softmax的结果依然使用的是交叉熵误差，一个序列的误差等于所有元素的误差之和。</p>
<p>刚刚介绍的是训练过程。在用这个模型计算某句子的概率时，只要把一个句子输入进这个RNN，再去softmax的概率分布里取出需要的概率，一乘，就能算出语言模型要求的整句话的概率了。比如<code>Cats average 15 hours of sleep a day. &lt; EOS &gt;</code>这个句子，我们要令$x^{&lt; 1 &gt;}=0$, $x^{&lt; 2 &gt;}=one_ hot(cats)$, $x^{&lt; 3 &gt;}=one_ hot(average)$，……。然后，从第一个输出概率分布$\hat{y}^{&lt; 1 &gt;}$里找出cats对应的概率，去$\hat{y}^{&lt; 2 &gt;}$里找到average对应的概率，去$\hat{y}^{&lt; 3 &gt;}$里找到15对应的概率，以此类推。最后把所有的概率乘起来。</p>
<p>通过这一节，我们学到了RNN的一种应用。是否真正理解语言模型这一任务，并不重要。重要的是，我们学到了RNN是怎么巧妙去完成一项任务的。在完成和序列数据有关的任务时，我们要精心定义RNN的输入序列和输出序列。一旦这两个序列定义好了，训练模型并解决任务就是很轻松的事情。</p>
<h2 id="用语言模型采样出全新的序列"><a href="#用语言模型采样出全新的序列" class="headerlink" title="用语言模型采样出全新的序列"></a>用语言模型采样出全新的序列</h2><p>给定一个别人训练好的RNN语言模型，我们可以弄出一个很好玩的应用：生成一个训练集里没有的句子。</p>
<p>我们刚刚学过，在计算一句话的概率时，RNN会把句子里的每一个单词输入，输出单词出现在前几个单词之后的概率分布$\hat{y}$。反过来想，我们可以根据RNN输出的概率分布，随机采样出某一个单词的下一个单词出来。具体来说，我们先随机生成句子里的第一个单词，把它输入RNN。再用RNN生成概率分布，对概率分布采样出下一个单词，采样出一个单词就输入一个单词，直到采样出<code>&lt; EOS &gt;</code>。这个过程就好像是在让AI生成句子一样。</p>
<blockquote>
<p>对概率分布采样，其实就是以某种概率随机挑选。比如我有两个骰子，我要计算两个骰子点数之和。这个点数之和就是一个概率分布，掷一轮骰子就是去分布里采样。我们可以快速地算出，点数之和为2的概率是$\frac{1}{36}$, 点数之和为3的概率是$\frac{2}{36}$。也就是说，我们在采样时，有$\frac{1}{36}$的概率取到2，$\frac{2}{36}$的概率取到3。</p>
</blockquote>
<p>如果把语料库的最小单元从单词换成字母，句子生成就变成了单词生成，我们可以让AI生成出从没出现过却看上去很合理的单词。</p>
<h2 id="RNN-的梯度问题"><a href="#RNN-的梯度问题" class="headerlink" title="RNN 的梯度问题"></a>RNN 的梯度问题</h2><p>在前几门课中，我们曾学过，过深的神经网络会有梯度过大/过小的问题。这些问题在RNN中也存在，毕竟RNN一般都是用来处理很长的序列数据的。</p>
<p>梯度过大的问题倒是有办法解决：设置一个梯度最大值，让所有梯度都不能超过这个值。</p>
<p>梯度过小的问题比较麻烦。想象一个很长的句子:The <strong>cat</strong>, which ate apples, pears, …., <strong>was</strong> full.这个cat和was存在着依赖关系。一旦梯度过小，一个句子前后的依赖关系就不是那么好传递了。</p>
<p>下面几节我们会学一些解决梯度问题的架构。</p>
<h2 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU (Gated Recurrent Unit)"></a>GRU (Gated Recurrent Unit)</h2><p>我们可以用GRU (Gated Recurrent Unit)来替代标准RNN中的计算单元，以解决梯度问题。</p>
<p>为了明确标准RNN的哪些模块被替换了，我们先回顾一下RNN原来的计算单元。</p>
<p><img src="/2022/09/21/DLS-note-14/6.jpg" alt></p>
<p>在标准的RNN单元中，$x^{&lt; t &gt;}$和$a^{&lt; t-1 &gt;}$一起决定了$a^{&lt; t &gt;}$，$a^{&lt; t &gt;}$又决定了$\hat{y}^{&lt; t &gt;}$。</p>
<p>先在脑中对这张图有个印象，稍后我们会看到GRU是怎样改进这个计算单元的。</p>
<p>上一节里，我们分析过，梯度消失会导致一句话后面的单词忘掉了前面的单词。那么，可不可以让网络的“记性”更好一点呢？我们可以参考一下人类的记忆行为。比如，当我们在接到了验证码短信后，要把验证码在脑子里记一段时间：读完了短信，要记住验证码；关闭短信应用，要记住验证码；打开需要验证码的应用，要记住验证码；输入验证吗时，要记住验证码；输完了验证码，总算可以忘记验证码了。这个过程中，我们一直在维护“验证码”这个信息，决定是记住它还是忘记它。我们可以让神经网络模型也按照这种思路记忆之前的信息。</p>
<p>在每轮计算更新中间变量$a$时，GRU还要使用到一个新的变量，表示中间变量$a$该不该忘记。这个变量越靠近0，就说明越应该保持之前的中间变量；越靠近1，就越靠近新的$a$。GRU的这个变量起到了电路中逻辑门的效果（GRU的第一个单词是gated）。</p>
<p>具体来说，一个简化版GRU的计算过程用数学符号表示如下：</p>
<p>我们把中间变量$a$临时更名为$c$，表示记忆单元memory cell。和之前的$a$一样，我们每轮要算一个新的$\tilde{c}$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{c}^{< t >} &= tanh(W_{c} [c^{< t - 1 >}, x^{< t >}] + b_c)
\end{aligned}</script><p>而同时，我们还要算一个决定是不是要用$\tilde{c}$去更新过去的$c$的“逻辑门” $\Gamma_u$:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\Gamma_u &= sigmoid(W_{u} [c^{< t - 1 >}, x^{< t >}] + b_u)
\end{aligned}</script><p>注意，$\Gamma_u$和逻辑门不同，不是真的只能取0或1，而是取0~1中一个中间的值，表示更新的程度。</p>
<p>之后，每一轮的$c^{&lt; t &gt;}$是这样更新的：</p>
<script type="math/tex; mode=display">
c^{< t >} = \Gamma_u \ast \tilde{c}^{< t >} + (1 - \Gamma_u) \ast c^{< t - 1 >}</script><p>它的图示如下，其中紫色部分是更新操作。</p>
<p><img src="/2022/09/21/DLS-note-14/7.jpg" alt></p>
<p>在完整的GRU中，计算公式稍微复杂一点：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{c}^{< t >} &= tanh(W_{c} [\Gamma_r \ast c^{< t - 1 >}, x^{< t >}] + b_c) \\
\Gamma_u &= sigmoid(W_{u} [c^{< t - 1 >}, x^{< t >}] + b_u) \\
\Gamma_r &= sigmoid(W_{r} [c^{< t - 1 >}, x^{< t >}] + b_r) \\
c^{< t >} &= \Gamma_u \ast \tilde{c}^{< t >} + (1 - \Gamma_u) \ast c^{< t - 1 >}
\end{aligned}</script><p>唯一的区别是$c^{&lt; t - 1 &gt;}$多过了一道$\Gamma_r$。这种设计的好处很难从理论上解释。当时的研究者试了很多类似的GRU架构，最后发现这样的GRU是效果最好的。</p>
<h2 id="LSTM-long-short-term-memory-单元"><a href="#LSTM-long-short-term-memory-单元" class="headerlink" title="LSTM (long short term memory) 单元"></a>LSTM (long short term memory) 单元</h2><p>LSTM单元是另一种改进版的RNN单元。LSTM的核心思想和GRU一模一样，也是使用门来控制记忆变量的更新幅度，只是公式更复杂了一点。在LSTM中，要传递的中间变量有两个：$c$和$a$，使用的输出门也从2个增加到了3个。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{c}^{< t >} &= tanh(W_{c} [\Gamma_r \ast a^{< t - 1 >}, x^{< t >}] + b_c) \\
\Gamma_u &= sigmoid(W_{u} [a^{< t - 1 >}, x^{< t >}] + b_u) \\
\Gamma_f &= sigmoid(W_{f} [a^{< t - 1 >}, x^{< t >}] + b_f) \\
\Gamma_o &= sigmoid(W_{o} [a^{< t - 1 >}, x^{< t >}] + b_o) \\
c^{< t >} &= \Gamma_u \ast \tilde{c}^{< t >} + \Gamma_f \ast c^{< t - 1 >} \\
a^{< t >} &= \Gamma_o 
\ast tanh (c^{< t >})
\end{aligned}</script><p><img src="/2022/09/21/DLS-note-14/8.jpg" alt></p>
<blockquote>
<p>我觉得看图不如看公式看得清楚。</p>
</blockquote>
<p>我们不需要刻意去记LSTM的结构，也不要纠结为什么要在哪个地方用哪个门，只需要知道LSTM和GRU的区别，会用它们就行了。</p>
<p>虽然LSTM比GRU更复杂，但实际上LSTM很早（1997年）就有了，GRU是近几年才有的。二者的效果并没有显著的差别，一般认为LSTM功能更强大，GRU计算速度更快。碰到新任务无脑用LSTM即可，而如果要构建较大的网络则可以考虑使用性能更好的GRU。</p>
<h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p>之前提过，标准的RNN有一个问题：先出现的单词无法获取后续单词的信息。比如句首单词是Teddy，你不知道这是“泰迪熊”还是“泰迪”这个人名。为了解决这个问题，我们可以升级一下RNN的基础架构，使用双向RNN（BRNN）。在这个新架构下，GRU和LSTM的单元可以照用，不受影响。</p>
<p>BRNN的示意图如下：</p>
<p><img src="/2022/09/21/DLS-note-14/9.jpg" alt></p>
<p>假设一句话有4个单词。在BRNN中，除了会先从1-4正着输入一遍序列外，还会从4-1倒着输入一遍序列。正着传的中间变量叫$\overrightarrow{a}$，倒着传的中间变量叫做$\overleftarrow{a}$。每一轮输出满足$\hat{y}=g(W_y[\ \overrightarrow{a}, \overleftarrow{a} \ ] + b_y)$。</p>
<p>BRNN+LSTM通常是一个新序列任务的标配。当然，BRNN也有一个缺点：必须等一个序列输入完了才能返回结果，而不能实时返回结果。在语音识别等实时性较强的任务里，可能普通RNN更合适一点。</p>
<h2 id="深层RNN"><a href="#深层RNN" class="headerlink" title="深层RNN"></a>深层RNN</h2><p>到目前为止，我们学的RNN都是由几个简单的矩阵运算构成的，似乎和这套课的标题“深度学习”不沾边。实际上，也可以给基础的RNN多加一些参数，变成一个深层的RNN。</p>
<p>正如堆叠标准神经网络的隐藏层一样，我们可以堆叠RNN的基础模块，并传递多个中间变量。由于时序计算的计算成本很高，堆3层的计算量就已经很大了。</p>
<p><img src="/2022/09/21/DLS-note-14/10.jpg" alt></p>
<p>如果想进一步提升网络的拟合能力，可以修改计算输出$y$的结构，堆叠一些非时序的神经网络隐藏层。</p>
<p><img src="/2022/09/21/DLS-note-14/11.jpg" alt></p>
<blockquote>
<p>时序模块之所以计算缓慢，一大原因是无法并行。靠后的变量必须等之前的变量算好了才能计算。而在输出$y$的路径中添加一些隐藏层的运算代价没有那么大，因为这些运算是可以并行的。</p>
</blockquote>
<p>同样，使用深层RNN时，双向RNN，还有LSTM, GRU都是可以用的。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这一课里，我们初次认识了序列数据，并学习了处理序列数据的RNN。利用RNN，我们可以开发出许多和序列数据相关的应用。基础的RNN存在不少问题，所以RNN存在着许多改进方法。让我们看一看这一课的具体知识点：</p>
<ul>
<li><p>序列数据及相关任务的示例</p>
<ul>
<li>语音识别：输入语音，输出文字</li>
<li>音乐生成：无输入，输出语音</li>
<li>机器翻译：输入某种文字，输出另一种文字</li>
<li>情绪分类：输入文字，输出1-5的分数</li>
<li>命名实体识别：输入文字，输出每个单词是否是命名实体</li>
</ul>
</li>
<li><p>单词的表示</p>
<ul>
<li>先准备好一个词汇表。比如大小为10000的词汇表（要包括<code>&lt;UNK&gt;, &lt;EOS&gt;</code>）。</li>
<li>每一个单词是一个长度10000的one-hot向量。单词在词汇表中的序号，就是one-hot向量中值为1的下标。</li>
</ul>
</li>
<li><p>循环神经网络（RNN）</p>
<ul>
<li>基本思想：用$a$表示上文信息</li>
<li>计算流程：循环输入序列元素，维护$a$</li>
<li>计算公式</li>
<li>反向传播的大概流程</li>
</ul>
</li>
<li><p>防止RNN梯度消失：GRU, LSTM</p>
<ul>
<li>基本思想：选择性更新$a$</li>
<li>大概了解GRU, LSTM的公式</li>
<li>GRU, LSTM的使用场景</li>
</ul>
</li>
<li><p>获取下文信息：BRNN</p>
<ul>
<li>基本思想与结构</li>
<li>使用场景</li>
</ul>
</li>
<li><p>增强表达能力：深层RNN</p>
<ul>
<li>怎么添加更多层RNN</li>
<li>怎么更好地拟合输出</li>
</ul>
</li>
<li><p>RNN应用：语言模型</p>
<ul>
<li>语言模型的定义</li>
<li>语言模型的训练与推理</li>
<li>对语言模型采样</li>
</ul>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/09/21/DLS-note-14-2/" rel="prev" title="你的第一个PyTorch RNN模型——字母级语言模型">
      <i class="fa fa-chevron-left"></i> 你的第一个PyTorch RNN模型——字母级语言模型
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/09/21/DLS-note-15-2/" rel="next" title="在 PyTorch 中借助 GloVe 词嵌入完成情感分析">
      在 PyTorch 中借助 GloVe 词嵌入完成情感分析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%A6%E5%8F%B7%E6%A0%87%E8%AE%B0"><span class="nav-number">1.</span> <span class="nav-text">符号标记</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">循环神经网络模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E3%80%8C%E7%A9%BF%E8%B6%8A%E6%97%B6%E7%A9%BA%E4%B9%8B%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E3%80%8D"><span class="nav-number">3.</span> <span class="nav-text">「穿越时空之反向传播」</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E7%9A%84RNN"><span class="nav-number">4.</span> <span class="nav-text">不同输入输出格式的RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN%E5%BA%94%E7%94%A8%EF%BC%9A%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.</span> <span class="nav-text">RNN应用：语言模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%87%87%E6%A0%B7%E5%87%BA%E5%85%A8%E6%96%B0%E7%9A%84%E5%BA%8F%E5%88%97"><span class="nav-number">6.</span> <span class="nav-text">用语言模型采样出全新的序列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-%E7%9A%84%E6%A2%AF%E5%BA%A6%E9%97%AE%E9%A2%98"><span class="nav-number">7.</span> <span class="nav-text">RNN 的梯度问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GRU-Gated-Recurrent-Unit"><span class="nav-number">8.</span> <span class="nav-text">GRU (Gated Recurrent Unit)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM-long-short-term-memory-%E5%8D%95%E5%85%83"><span class="nav-number">9.</span> <span class="nav-text">LSTM (long short term memory) 单元</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8C%E5%90%91RNN"><span class="nav-number">10.</span> <span class="nav-text">双向RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82RNN"><span class="nav-number">11.</span> <span class="nav-text">深层RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">12.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">100</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
