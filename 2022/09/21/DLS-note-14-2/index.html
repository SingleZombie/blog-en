<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="想要入门一项新技术，最快的方法就是写一个”Hello World”程序。入门CNN，大家一般会写一个简单的图片分类项目。可是，RNN的入门项目就比较少见了。自然语言处理任务要求的数据量都比较大，不是那么好设计一个入门项目。 在这篇文章中，我将展示一个入门级的RNN项目——字母级语言模型。这个项目的逻辑比较简单，要求的数据量不大，几分钟就可以训练完，非常适合新手入门。 这个项目使用的框架是PyTor">
<meta property="og:type" content="article">
<meta property="og:title" content="你的第一个PyTorch RNN模型——字母级语言模型">
<meta property="og:url" content="https://zhouyifan.net/2022/09/21/DLS-note-14-2/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="想要入门一项新技术，最快的方法就是写一个”Hello World”程序。入门CNN，大家一般会写一个简单的图片分类项目。可是，RNN的入门项目就比较少见了。自然语言处理任务要求的数据量都比较大，不是那么好设计一个入门项目。 在这篇文章中，我将展示一个入门级的RNN项目——字母级语言模型。这个项目的逻辑比较简单，要求的数据量不大，几分钟就可以训练完，非常适合新手入门。 这个项目使用的框架是PyTor">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14-2/DLS-note-14/5.jpg">
<meta property="article:published_time" content="2022-09-21T09:46:13.000Z">
<meta property="article:modified_time" content="2022-09-21T09:46:13.900Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="编程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhouyifan.net/2022/09/21/DLS-note-14-2/DLS-note-14/5.jpg">

<link rel="canonical" href="https://zhouyifan.net/2022/09/21/DLS-note-14-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>你的第一个PyTorch RNN模型——字母级语言模型 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/09/21/DLS-note-14-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          你的第一个PyTorch RNN模型——字母级语言模型
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-21 17:46:13" itemprop="dateCreated datePublished" datetime="2022-09-21T17:46:13+08:00">2022-09-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>想要入门一项新技术，最快的方法就是写一个”Hello World”程序。入门CNN，大家一般会写一个简单的图片分类项目。可是，RNN的入门项目就比较少见了。自然语言处理任务要求的数据量都比较大，不是那么好设计一个入门项目。</p>
<p>在这篇文章中，我将展示一个入门级的RNN项目——字母级语言模型。这个项目的逻辑比较简单，要求的数据量不大，几分钟就可以训练完，非常适合新手入门。</p>
<p>这个项目使用的框架是PyTorch。首先，我会抛弃PyTorch的高级组件，仅使用线性层、自动求导机制来从头实现一个简单的RNN。之后，我还会用PyTorch的高级组件搭一个更通用的RNN。相信通过阅读这篇教程，大家不仅能够理解RNN的底层原理，还能够学到PyTorch中RNN组件的用法，能够自己搭建出各种各样的NLP任务模型。</p>
<h2 id="知识背景"><a href="#知识背景" class="headerlink" title="知识背景"></a>知识背景</h2><p>详细的知识介绍可以参考我的上篇文章：<a href>循环神经网络基础</a>。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>RNN 适用于处理序列数据。令$x^{&lt; i &gt;}$是序列的第$i$个元素，那么$x^{&lt; 1 &gt;} x^{&lt; 2 &gt;}…x^{&lt; T_x &gt;}$就是一个长度为$T_x$的序列。NLP中最常见的元素是单词，对应的序列是句子。</p>
<p>RNN使用同一个神经网络处理序列中的每一个元素。同时，为了表示序列的先后关系，RNN还有表示记忆的隐变量$a$，它记录了前几个元素的信息。对第$t$个元素的运算如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{ax} x^{< t >} + W_{aa} a^{< t - 1 >} + b_a) \\
\hat{y}^{< t >} &=g_2(W_{ya} a^{< t >} + b_y)
\end{aligned}</script><p>其中，$W, b$都是线性运算的参数，$g$是激活函数。隐藏层的激活函数一般用tanh，输出层的激活函数根据实际情况选用。另外，$a$得有一个初始值$a^{&lt; 1 &gt;}$，一般令$a^{&lt; 1 &gt;}=\vec0$。</p>
<h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>语言模型是NLP中的一个基础任务。假设我们以单词为基本元素，句子为序列，那么一个语言模型能够输出某句话的出现概率。通过比较不同句子的出现概率，我们能够开发出很多应用。比如在英语里，同音的”apple and pear”比”apple and pair”的出现概率高（更可能是一个合理的句子）。当一个语音识别软件听到这句话时，可以分别写下这两句发音相近的句子，再根据语言模型断定这句话应该写成前者。</p>
<p>规范地说，对于序列$x^{&lt; 1 &gt;}…x^{&lt; T_x &gt;}$，语言模型的输出是$P(x^{&lt; 1 &gt;},…, x^{&lt; T_x &gt;})$。这个式子也可以写成$P(x^{&lt; 1 &gt;}) \times P(x^{&lt; 2 &gt;} |x^{&lt; 1 &gt;}) \times  P(x^{&lt; 3 &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}) … \times  P(x^{&lt; T_x &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}, …, x^{&lt; T_x-1 &gt;})$，即一句话的出现概率，等于第一个单词出现在句首的概率，乘上第二个单词在第一个单词之后的概率，乘上第三个单词再第一、二个单词之后的概率，这样一直乘下去。</p>
<p>单词级的语言模型需要的数据量比较大，在这个项目中，我们将搭建一个字母级语言模型。即我们以字母为基本元素，单词为序列。语言模型会输出每个单词的概率。比如我们输入”apple”和”appll”，语言模型会告诉我们单词”apple”的概率更高，这个单词更可能是一个正确的英文单词。</p>
<h3 id="RNN-语言模型"><a href="#RNN-语言模型" class="headerlink" title="RNN 语言模型"></a>RNN 语言模型</h3><p>为了计算语言模型的概率，我们可以用RNN分别输出$P(x^{&lt; 1 &gt;})$, $P(x^{&lt; 2 &gt;} |x^{&lt; 1 &gt;})$, …，最后把这些概率乘起来。</p>
<p>$P(x^{&lt; t &gt;} |x^{&lt; 1 &gt;}, x^{&lt; 2 &gt;}, …, x^{&lt; t-1 &gt;})$这个式子，说白了就是给定前$t-1$个字母，猜一猜第$t$个字母最可能是哪个。比如给定了前四个字母”appl”，第五个单词构成”apply”, “apple”的概率比较大，构成”appll”, “appla”的概率较小。</p>
<p>为了让神经网络学会这个概率，我们可以令RNN的输入为<code>&lt;sos&gt; x_1, x_2, ..., x_T</code>，RNN的标签为<code>x_1, x_2, ..., x_T, &lt;eos&gt;</code>（<code>&lt;sos&gt;</code>和<code>&lt;eos&gt;</code>是句子开始和结束的特殊字符，实际实现中可以都用空格<code>&#39; &#39;</code>表示。<code>&lt;sos&gt;</code>也可以粗暴地用全零向量表示），即输入和标签都是同一个单词，只是它们的位置差了一格。模型每次要输出一个softmax的多分类概率，预测给定前几个字母时下一个字母的概率。这样，这个模型就能学习到前面那个条件概率了。</p>
<p><img src="/2022/09/21/DLS-note-14-2/DLS-note-14/5.jpg" alt></p>
<h2 id="代码讲解"><a href="#代码讲解" class="headerlink" title="代码讲解"></a>代码讲解</h2><p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN。">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN。</a></p>
<h3 id="数据集获取"><a href="#数据集获取" class="headerlink" title="数据集获取"></a>数据集获取</h3><p>为了搭建字母级语言模型，我们只需要随便找一个有很多单词的数据集。这里我选择了斯坦福大学的<a target="_blank" rel="noopener" href="https://ai.stanford.edu/~amaas/data/sentiment/">大型电影数据集</a>，它收录了IMDb上的电影评论，正面评论和负面评论各25000条。这个数据集本来是用于情感分类这一比较简单的NLP任务，拿来搭字母级语言模型肯定是没问题的。</p>
<p>这个数据集的文件结构大致如下：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">├─test</span><br><span class="line">│  ├─neg</span><br><span class="line">│  │  ├ 0_2.txt</span><br><span class="line">│  │  ├ 1_3.txt</span><br><span class="line">│  │  └ ...</span><br><span class="line">│  └─pos</span><br><span class="line">├─train</span><br><span class="line">│   ├─neg</span><br><span class="line">│   └─pos</span><br><span class="line">└─imdb.vocab</span><br></pre></td></tr></table></figure>
<p>其中，<code>imdb.vocab</code>记录了数据集中的所有单词，一行一个。<code>test</code>和<code>train</code>是测试集和训练集，它们的<code>neg</code>和<code>pos</code>子文件夹分别记录了负面评论和正面评论。每一条评论都是一句话，存在一个txt文件里。</p>
<p>训练字母级语言模型时，直接拿词汇表来训练也行，从评论中截取一个个单词也行。我已经写好了这些读取数据集的代码，在<code>dldemos/BasicRNN/read_imdb.py</code>文件中。</p>
<p>在读取单词时，我们只需要26个字母和空格这一共27个字符。其他的字符全可以过滤掉。为了方便，我使用了正则表达式过滤出这27个字符：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words = re.sub(<span class="string">u&#x27;([^\u0020\u0061-\u007a])&#x27;</span>, <span class="string">&#x27;&#x27;</span>, words)</span><br></pre></td></tr></table></figure>
<p>这样，一个读取词汇表文件的函数就长这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_imdb_vocab</span>(<span class="params"><span class="built_in">dir</span>=<span class="string">&#x27;data/aclImdb&#x27;</span></span>):</span></span><br><span class="line">    fn = os.path.join(<span class="built_in">dir</span>, <span class="string">&#x27;imdb.vocab&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fn, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        word = f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>).replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        words = re.sub(<span class="string">u&#x27;([^\u0020\u0061-\u007a])&#x27;</span>, <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">                       word.lower()).split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        filtered_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> <span class="built_in">len</span>(w) &gt; <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> filtered_words</span><br></pre></td></tr></table></figure>
<p>我写好了读取词汇表的函数<code>read_imdb_vocab</code>和<code>read_imdb_words</code>，它们都会返回一个单词的列表。我还写了一个读数据集整个句子的函数<code>read_imdb</code>。它们的用法和输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    vocab = read_imdb_vocab()</span><br><span class="line">    <span class="built_in">print</span>(vocab[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(vocab[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    lines = read_imdb()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Length of the file:&#x27;</span>, <span class="built_in">len</span>(lines))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lines[0]:&#x27;</span>, lines[<span class="number">0</span>])</span><br><span class="line">    words = read_imdb_words(n_files=<span class="number">100</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Length of the words:&#x27;</span>, <span class="built_in">len</span>(words))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(words[i])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">the</span><br><span class="line">and</span><br><span class="line">Length of the file: 12500</span><br><span class="line">lines[0]: Bromwell High is a cartoon ...</span><br><span class="line">Length of the words: 23425</span><br><span class="line">bromwell</span><br><span class="line">high</span><br><span class="line">is</span><br><span class="line">a</span><br><span class="line">cartoon</span><br></pre></td></tr></table></figure>
<h3 id="数据集读取"><a href="#数据集读取" class="headerlink" title="数据集读取"></a>数据集读取</h3><p>RNN的输入不是字母，而是表示字母的向量。最简单的字母表示方式是one-hot编码，每一个字母用一个某一维度为1，其他维度为0的向量表示。比如我有a, b, c三个字母，它们的one-hot编码分别为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">b: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">c: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>现在，我们只有单词数组。我们要把每个单词转换成这种one-hot编码的形式。</p>
<p>在转换之前，我准备了一些常量（<code>dldemos/BasicRNN/constant.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">EMBEDDING_LENGTH = <span class="number">27</span></span><br><span class="line">LETTER_MAP = &#123;<span class="string">&#x27; &#x27;</span>: <span class="number">0</span>&#125;</span><br><span class="line">ENCODING_MAP = [<span class="string">&#x27; &#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">26</span>):</span><br><span class="line">    LETTER_MAP[<span class="built_in">chr</span>(<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>) + i)] = i + <span class="number">1</span></span><br><span class="line">    ENCODING_MAP.append(<span class="built_in">chr</span>(<span class="built_in">ord</span>(<span class="string">&#x27;a&#x27;</span>) + i))</span><br><span class="line">LETTER_LIST = <span class="built_in">list</span>(LETTER_MAP.keys())</span><br></pre></td></tr></table></figure>
<p>我们一共有27个字符，0号字符是空格，剩余字母按照字母表顺序排列。<code>LETTER_MAP</code>和<code>ENCODING_MAP</code>分别完成了字母到数字的正向和反向映射。<code>LETTER_LIST</code>是所有字母的列表。</p>
<p>PyTorch提供了用于管理数据集读取的Dataset类。Dataset一般只会存储获取数据的信息，而非原始数据，比如存储图片路径。而每次读取时，Dataset才会去实际读取数据。在这个项目里，我们用Dataset存储原始的单词数组，实际读取时，每次返回一个one-hot编码的向量。</p>
<p>使用Dataset时，要继承这个类，实现<code>__len__</code>和<code>__getitem__</code>方法。前者表示获取数据集的长度，后者表示获取某项数据。我们的单词数据集<code>WordDataset</code>应该这样写（<code>dldemos/BasicRNN/main.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.constant <span class="keyword">import</span> EMBEDDING_LENGTH, LETTER_MAP</span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.read_imdb <span class="keyword">import</span> read_imdb_vocab, read_imdb_words</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, words, max_length, is_onehot=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function">        <span class="title">super</span>().<span class="title">__init__</span>()</span></span><br><span class="line"><span class="function">        <span class="title">n_words</span> = <span class="title">len</span>(<span class="params">words</span>)</span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">words</span> = <span class="title">words</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">n_words</span> = <span class="title">n_words</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">max_length</span> = <span class="title">max_length</span></span></span><br><span class="line"><span class="function">        <span class="title">self</span>.<span class="title">is_onehot</span> = <span class="title">is_onehot</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.n_words</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;return the (one-hot) encoding vector of a word&quot;&quot;&quot;</span></span><br><span class="line">        word = self.words[index] + <span class="string">&#x27; &#x27;</span></span><br><span class="line">        word_length = <span class="built_in">len</span>(word)</span><br><span class="line">        <span class="keyword">if</span> self.is_onehot:</span><br><span class="line">            tensor = torch.zeros(self.max_length, EMBEDDING_LENGTH)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">                <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">                    tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tensor = torch.zeros(self.max_length, dtype=torch.long)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(word_length):</span><br><span class="line">                tensor[i] = LETTER_MAP[word[i]]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>
<p>构造数据集的参数是<code>words, max_length, is_onehot</code>。<code>words</code>是单词数组。<code>max_length</code>表示单词的最大长度。在训练时，我们一般要传入一个batch的单词。可是，单词有长有短，我们不可能拿一个动态长度的数组去表示单词。为了统一地表达所有单词，我们可以记录单词的最大长度，把较短的单词填充空字符，直到最大长度。<code>is_onehot</code>表示是不是one-hot编码，我设计的这个数据集既能输出用数字标签表示的单词（比如abc表示成<code>[0, 1, 2]</code>），也能输出one-hoe编码表示的单词（比如abc表示成<code>[[1, 0, 0], [0, 1, 0], [0, 0, 1]]</code>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, words, max_length, is_onehot=<span class="literal">True</span></span>)</span></span><br><span class="line"><span class="function">    <span class="title">super</span>().<span class="title">__init__</span>()</span></span><br><span class="line"><span class="function">    <span class="title">n_words</span> = <span class="title">len</span>(<span class="params">words</span>)</span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">words</span> = <span class="title">words</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">n_words</span> = <span class="title">n_words</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">max_length</span> = <span class="title">max_length</span></span></span><br><span class="line"><span class="function">    <span class="title">self</span>.<span class="title">is_onehot</span> = <span class="title">is_onehot</span></span></span><br></pre></td></tr></table></figure>
<p>在获取数据集时，我们要根据是不是one-hot编码，先准备好一个全是0的输出张量。如果存的是one-hot编码，张量的形状是<code>[MAX_LENGTH, EMBEDDING_LENGTH]</code>，第一维是单词的最大长度，第二维是one-hot编码的长度。而如果是普通的标签数组，则张量的形状是<code>[MAX_LENGTH]</code>。准备好张量后，遍历每一个位置，令one-hot编码的对应位为1，或者填入数字标签。</p>
<p>另外，我们用空格表示单词的结束。要在处理前给单词加一个<code>&#39; &#39;</code>，保证哪怕最长的单词也会至少有一个空格。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;return the (one-hot) encoding vector of a word&quot;&quot;&quot;</span></span><br><span class="line">    word = self.words[index] + <span class="string">&#x27; &#x27;</span></span><br><span class="line">    word_length = <span class="built_in">len</span>(word)</span><br><span class="line">    <span class="keyword">if</span> self.is_onehot:</span><br><span class="line">        tensor = torch.zeros(self.max_length, EMBEDDING_LENGTH)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">            <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">                tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tensor = torch.zeros(self.max_length, dtype=torch.long)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(word_length):</span><br><span class="line">            tensor[i] = LETTER_MAP[word[i]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure>
<p>注意！短单词的填充部分应该全是空字符。千万不要忘记给空字符的one-hot编码赋值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_length):</span><br><span class="line">    <span class="keyword">if</span> i &lt; word_length:</span><br><span class="line">        tensor[i][LETTER_MAP[word[i]]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tensor[i][<span class="number">0</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>有了数据集类，结合之前写好的数据集获取函数，可以搭建一个DataLoader。DataLoader是PyTorch提供的数据读取类，它可以方便地从Dataset的子类里读取一个batch的数据，或者以更高级的方式取数据（比如随机取数据）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader_and_max_length</span>(<span class="params">limit_length=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  is_onehot=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  is_vocab=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> is_vocab:</span><br><span class="line">        words = read_imdb_vocab()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        words = read_imdb_words(n_files=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    max_length = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        max_length = <span class="built_in">max</span>(max_length, <span class="built_in">len</span>(word))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> limit_length <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> max_length &gt; limit_length:</span><br><span class="line">        words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> <span class="built_in">len</span>(w) &lt;= limit_length]</span><br><span class="line">        max_length = limit_length</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for &lt;EOS&gt; (space)</span></span><br><span class="line">    max_length += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    dataset = WordDataset(words, max_length, is_onehot)</span><br><span class="line">    <span class="keyword">return</span> DataLoader(dataset, batch_size=<span class="number">256</span>), max_length</span><br></pre></td></tr></table></figure>
<p>这个函数会先调用之前编写的数据读取API获取单词数组。之后，函数会计算最长的单词长度。这里，我用<code>limit_length</code>过滤了过长的单词。据实验，这个数据集里最长的单词竟然有60多个字母，把短单词填充至60需要浪费大量的计算资源。因此，我设置了<code>limit_length</code>这个参数，不去读取那些过长的单词。</p>
<p>计算完最大长度后，别忘了+1，保证每个单词后面都有一个表示单词结束的空格。</p>
<p>最后，用<code>DataLoader(dataset, batch_size=256)</code>就可以得到一个DataLoader。<code>batch_size</code>就是指定batch size的参数。我们这个神经网络很小，输入数据也很小，可以选一个很大的batch size加速训练。</p>
<h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><p>模型的初始化函数和训练函数定义如下（<code>dldemos/BasicRNN/models.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dldemos.BasicRNN.constant <span class="keyword">import</span> EMBEDDING_LENGTH, LETTER_LIST, LETTER_MAP</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN1</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">32</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line">        self.linear_a = nn.Linear(hidden_units + EMBEDDING_LENGTH,</span><br><span class="line">                                  hidden_units)</span><br><span class="line">        self.linear_y = nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">        self.tanh = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">        batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">        word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">        output = torch.empty_like(word)</span><br><span class="line"></span><br><span class="line">        a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">        x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">            next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">            hat_y = self.linear_y(next_a)</span><br><span class="line">            output[i] = hat_y</span><br><span class="line">            x = word[i]</span><br><span class="line">            a = next_a</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">        <span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>我们来一点一点地看看这个模型是怎么搭起来的。</p>
<p>回忆一下RNN的公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{< t >} &= g_1(W_{ax} x^{< t >} + W_{aa} a^{< t - 1 >} + b_a) \\
\hat{y}^{< t >} &=g_2(W_{ya} a^{< t >} + b_y)
\end{aligned}</script><p>我们可以把第一行公式里的两个$W$合并一下，$x, a$拼接一下。这样，只需要两个线性层就可以描述RNN了。</p>
<p>因此，在初始化函数中，我们定义两个线性层<code>linear_a</code>，<code>linear_y</code>。另外，<code>hidden_units</code>表示隐藏层<code>linear_a</code>的神经元数目。<code>tanh</code>就是普通的tanh函数，它用作第一层的激活函数。</p>
<p><code>linear_a</code>就是公式的第一行，由于我们把输入<code>x</code>和状态<code>a</code>拼接起来了，这一层的输入通道数是<code>hidden_units + EMBEDDING_LENGTH</code>，输出通道数是<code>hidden_units</code>。第二层<code>linear_y</code>表示公式的第二行。我们希望RNN能预测下一个字母的出现概率，因此这一层的输出通道数是<code>EMBEDDING_LENGTH=27</code>，即字符个数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">32</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.hidden_units = hidden_units</span><br><span class="line">    self.linear_a = nn.Linear(hidden_units + EMBEDDING_LENGTH,</span><br><span class="line">                              hidden_units)</span><br><span class="line">    self.linear_y = nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">    self.tanh = nn.Tanh()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在描述模型运行的<code>forward</code>函数中，我们先准备好输出张量，再初始化好隐变量<code>a</code>和第一轮的输入<code>x</code>。根据公式，循环遍历序列的每一个字母，用<code>a, x</code>计算<code>hat_y</code>，并维护每一轮的<code>a, x</code>。最后，所有<code>hat_y</code>拼接成的<code>output</code>就是返回结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    output = torch.empty_like(word)</span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        hat_y = self.linear_y(next_a)</span><br><span class="line">        output[i] = hat_y</span><br><span class="line">        x = word[i]</span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    <span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><br>我们来看一看这个函数的细节。一开始，输入张量<code>word</code>的形状是<code>[batch数，最大单词长度，字符数=27]</code>。我们提前获取好形状信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>我们循环遍历的其实是单词长度那一维。为了方便理解代码，我们可以把单词长度那一维转置成第一维。根据这个新的形状，我们准备好同形状的输出张量。输出张量<code>output[i][j]</code>表示第j个batch的序列的第i个元素的27个字符预测结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">output = torch.empty_like(word)</span><br></pre></td></tr></table></figure>
<p>按照前文知识准备的描述，第一轮的输入是空字符，期待的输出是句子里的第一个字母；第二轮的输入的第一个字母，期待的输出是第二个字母……。因此，我们要把输入<code>x</code>初始化为空。理论上<code>x</code>应该是一个空字符，其one-hot编码是<code>[1, 0, 0, ...]</code>，但这里我们拿一个全0的向量表示句首也是可行的。除了初始化<code>x</code>，还要初始化一个全零隐变量<code>a</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br></pre></td></tr></table></figure>
<p>之后，按照顺序遍历每一个元素，计算<code>y_hat</code>并维护<code>a, x</code>。最后输出结果前别忘了把转置过的维度复原回去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">    next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">    hat_y = self.linear_y(next_a)</span><br><span class="line">    output[i] = hat_y</span><br><span class="line">    x = word[i]</span><br><span class="line">    a = next_a</span><br><span class="line"></span><br><span class="line"><span class="comment"># output shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line"><span class="keyword">return</span> torch.transpose(output, <span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>从逻辑上讲，模型应该输出softmax的结果。但是，PyTorch的<code>CrossEntropyLoss</code>已经包含了softmax的计算，我们不用在模型里加softmax。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>main函数中完整的训练代码如下（<code>dldemos/BasicRNN/models.py</code>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_rnn1</span>():</span></span><br><span class="line">    device = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">    dataloader, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line"></span><br><span class="line">    model = RNN1().to(device)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">    citerion = torch.nn.CrossEntropyLoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line"></span><br><span class="line">        loss_sum = <span class="number">0</span></span><br><span class="line">        dataset_len = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> dataloader:</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            hat_y = model(y)</span><br><span class="line">            n, Tx, _ = hat_y.shape</span><br><span class="line">            hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">            y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">            label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">            loss = citerion(hat_y, label_y)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            loss_sum += loss</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;dldemos/BasicRNN/rnn1.pth&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>首先，调用之前编写的函数，准备好<code>dataloader</code>和<code>model</code>。同时，准备好优化器<code>optimizer</code>和损失函数<code>citerion</code>。优化器和损失函数按照常见配置选择即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">dataloader, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line"></span><br><span class="line">model = RNN1().to(device)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">citerion = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<p>这个语言模型一下就能训练完，做5个epoch就差不多了。每一代训练中，<br>先调用模型求出<code>hat_y</code>，再调用损失函数<code>citerion</code>，最后反向传播并优化模型参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    dataset_len = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> dataloader:</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        hat_y = model(y)</span><br><span class="line"></span><br><span class="line">        n, Tx, _ = hat_y.shape</span><br><span class="line">        hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">        y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">        label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">        loss = citerion(hat_y, label_y)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        loss_sum += loss</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p>算损失函数前需要预处理一下数据，交叉熵损失函数默认<code>hat_y</code>的维度是<code>[batch数，类型数]</code>，<code>label_y</code>是一个一维整形标签数组。而模型的输出形状是<code>[batch数，最大单词长度，字符数]</code>，我们要把前两个维度融合在一起。另外，我们并没有提前准备好<code>label_y</code>，需要调用<code>argmax</code>把one-hot编码转换回标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hat_y = model(y)</span><br><span class="line">n, Tx, _ = hat_y.shape</span><br><span class="line">hat_y = torch.reshape(hat_y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">y = torch.reshape(y, (n * Tx, -<span class="number">1</span>))</span><br><span class="line">label_y = torch.argmax(y, <span class="number">1</span>)</span><br><span class="line">loss = citerion(hat_y, label_y)</span><br></pre></td></tr></table></figure>
<p>之后就是调用PyTorch的自动求导功能。注意，为了防止RNN梯度过大，我们可以用<code>clip_grad_norm_</code>截取梯度的最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">0.5</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>我还顺带输出了每一代的loss。当然这里我偷了个懒，这个loss并不能表示每一个样本的平均loss。不过，我们能通过这个loss得到模型的训练进度，这就够了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>. loss: <span class="subst">&#123;loss_sum / dataset_len&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>我们可以手动为字母级语言模型写几个测试用例，看看每一个单词的概率是否和期望的一样。我的测试单词列表是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test_words = [</span><br><span class="line">    <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;appll&#x27;</span>, <span class="string">&#x27;appla&#x27;</span>, <span class="string">&#x27;apply&#x27;</span>, <span class="string">&#x27;bear&#x27;</span>, <span class="string">&#x27;beer&#x27;</span>, <span class="string">&#x27;berr&#x27;</span>, <span class="string">&#x27;beee&#x27;</span>, <span class="string">&#x27;car&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;cae&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;cac&#x27;</span>, <span class="string">&#x27;caq&#x27;</span>, <span class="string">&#x27;query&#x27;</span>, <span class="string">&#x27;queee&#x27;</span>, <span class="string">&#x27;queue&#x27;</span>, <span class="string">&#x27;queen&#x27;</span>, <span class="string">&#x27;quest&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;quess&#x27;</span>, <span class="string">&#x27;quees&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>我构筑了几组长度一样，但是最后几个字母不太一样的“单词”。通过观察这些词的概率，我们能够验证语言模型的正确性。理论上来说，英文里的正确单词的概率会更高。</p>
<p>我们的模型只能输出每一个单词的softmax前结果。我们还要为模型另写一个求语言模型概率的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">language_model</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length, embedding_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># word shape: [max_word_length, batch,  embedding_length]</span></span><br><span class="line">    <span class="comment"># word_label shape: [max_word_length, batch]</span></span><br><span class="line">    word = torch.transpose(word, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    word_label = torch.argmax(word, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># output shape: [batch]</span></span><br><span class="line">    output = torch.ones(batch, device=word.device)</span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=word.device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=word.device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        tmp = self.linear_y(next_a)</span><br><span class="line">        hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br><span class="line">        probs = hat_y[torch.arange(batch), word_label[i]]</span><br><span class="line">        output *= probs</span><br><span class="line">        x = word[i]</span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>这个函数和<code>forward</code>大致相同。只不过，这次我们的输出<code>output</code>要表示每一个单词的概率。因此，它被初始化成一个全1的向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output shape: [batch]</span></span><br><span class="line">output = torch.ones(batch, device=word.device)</span><br></pre></td></tr></table></figure>
<p>每轮算完最后一层的输出后，我们手动调用<code>F.softmax</code>得到softmax的概率值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmp = self.linear_y(next_a)</span><br><span class="line">hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们要根据每一个batch当前位置的单词，去<code>hat_y</code>里取出需要的概率。比如第2个batch当前的字母是<code>b</code>，我们就要取出<code>hat_y[2][2]</code>。</p>
<p>第<code>i</code>轮所有batch的字母可以用<code>word_label[i]</code>表示。根据这个信息，我们可以用<code>probs = hat_y[torch.arange(batch), word_label[i]]</code>神奇地从<code>hat_y</code>里取出每一个batch里<code>word_label[i]</code>处的概率。把这个概率乘到<code>output</code>上就算完成了一轮计算。</p>
<p>有了语言模型函数，我们可以测试一下开始那些单词的概率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_language_model</span>(<span class="params">model, is_onehot=<span class="literal">True</span>, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    _, max_length = get_dataloader_and_max_length(<span class="number">19</span>)</span><br><span class="line">    <span class="keyword">if</span> is_onehot:</span><br><span class="line">        test_word = words_to_onehot(test_words, max_length)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        test_word = words_to_label_array(test_words, max_length)</span><br><span class="line">    test_word = test_word.to(device)</span><br><span class="line">    probs = model.language_model(test_word)</span><br><span class="line">    <span class="keyword">for</span> word, prob <span class="keyword">in</span> <span class="built_in">zip</span>(test_words, probs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;word&#125;</span>: <span class="subst">&#123;prob&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apple: <span class="number">9.39846032110836e-08</span></span><br><span class="line">appll: <span class="number">6.516307937687316e-09</span></span><br><span class="line">appla: <span class="number">3.6599331565412285e-08</span></span><br><span class="line">apply: <span class="number">1.2422759709806996e-07</span></span><br><span class="line">bear: <span class="number">1.6009346381906653e-06</span></span><br><span class="line">beer: <span class="number">1.6936465954131563e-06</span></span><br><span class="line">berr: <span class="number">9.99331746243115e-07</span></span><br><span class="line">beee: <span class="number">1.5601625591443735e-07</span></span><br><span class="line">car: <span class="number">1.8536804418545216e-05</span></span><br><span class="line">cae: <span class="number">1.8946409454656532e-06</span></span><br><span class="line">cat: <span class="number">1.875695670605637e-05</span></span><br><span class="line">cac: <span class="number">6.04180786467623e-06</span></span><br><span class="line">caq: <span class="number">3.6483314147517376e-08</span></span><br><span class="line">query: <span class="number">1.6811516161396867e-06</span></span><br><span class="line">queee: <span class="number">5.9459132728534314e-08</span></span><br><span class="line">queue: <span class="number">9.488831942405795e-09</span></span><br><span class="line">queen: <span class="number">5.990783051856852e-07</span></span><br><span class="line">quest: <span class="number">2.737341446845676e-06</span></span><br><span class="line">quess: <span class="number">4.7091912165342364e-06</span></span><br><span class="line">quees: <span class="number">1.3468336419464322e-06</span></span><br></pre></td></tr></table></figure>
<p>通过观察每一组用例，我们能发现，<code>apple, apply, bear, beer</code>这些正确的单词的概率确实会高一些。这个语言模型训练得不错。有趣的是，<code>caq</code>这种英语里几乎不存在的字母组合的概率也偏低。当然，语言模型对难一点的单词的判断就不太准了。<code>queen</code>和<code>queue</code>的出现概率就比较低。</p>
<h3 id="采样单词"><a href="#采样单词" class="headerlink" title="采样单词"></a>采样单词</h3><p>语言模型有一个很好玩的应用：我们可以根据语言模型输出的概率分布，采样出下一个单词；输入这一个单词，再采样下一个单词。这样一直采样，直到采样出空格为止。使用这种采样算法，我们能够让模型自动生成单词，甚至是英文里不存在，却看上去很像那么回事的单词。</p>
<p>我们要为模型编写一个新的方法<code>sample_word</code>，采样出一个最大长度为10的单词。这段代码的运行逻辑和之前的<code>forward</code>也很相似。只不过，这一次我们没有输入张量，每一轮的<code>x</code>要靠采样获得。<code>np.random.choice(LETTER_LIST, p=np_prob)</code>可以根据概率分布<code>np_prob</code>对列表<code>LETTER_LIST</code>进行采样。根据每一轮采样出的单词<code>letter</code>，我们重新生成一个<code>x</code>，给one-hot编码的对应位置赋值1。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_word</span>(<span class="params">self, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    batch = <span class="number">1</span></span><br><span class="line">    output = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    a = torch.zeros(batch, self.hidden_units, device=device)</span><br><span class="line">    x = torch.zeros(batch, EMBEDDING_LENGTH, device=device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        next_a = self.tanh(self.linear_a(torch.cat((a, x), <span class="number">1</span>)))</span><br><span class="line">        tmp = self.linear_y(next_a)</span><br><span class="line">        hat_y = F.softmax(tmp, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        np_prob = hat_y[<span class="number">0</span>].detach().cpu().numpy()</span><br><span class="line">        letter = np.random.choice(LETTER_LIST, p=np_prob)</span><br><span class="line">        output += letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> letter == <span class="string">&#x27; &#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        x = torch.zeros(batch, EMBEDDING_LENGTH, device=device)</span><br><span class="line">        x[<span class="number">0</span>][LETTER_MAP[letter]] = <span class="number">1</span></span><br><span class="line">        a = next_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>使用这个方法，我们可以写一个采样20次的脚本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">model</span>):</span></span><br><span class="line">    words = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        word = model.sample_word()</span><br><span class="line">        words.append(word)</span><br><span class="line">    <span class="built_in">print</span>(*words)</span><br></pre></td></tr></table></figure>
<p>我的一次输出是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movine  oaceniefke xumedfasss tinkly  cerawedaus meblilesen douteni  ttingieftu sinsceered inelid  tniblicl  krouthyych mochonalos memp  dendusmani sttywima  dosmmek  dring  diummitt  pormoxthin</span><br></pre></td></tr></table></figure>
<p>采样出来的单词几乎不会是英文里的正确单词。不过，这些单词的词缀很符合英文的造词规则，非常好玩。如果为采样函数加一些限制，比如只考虑概率前3的字母，那么算法应该能够采样出更正确的单词。</p>
<h2 id="PyTorch里的RNN函数"><a href="#PyTorch里的RNN函数" class="headerlink" title="PyTorch里的RNN函数"></a>PyTorch里的RNN函数</h2><p>刚刚我们手动编写了RNN的实现细节。实际上，PyTorch提供了更高级的函数，我们能够更加轻松地实现RNN。其他部分的代码逻辑都不怎么要改，我这里只展示一下要改动的关键部分。</p>
<blockquote>
<p>写这份代码时我参考了 <a target="_blank" rel="noopener" href="https://github.com/floydhub/word-language-model">https://github.com/floydhub/word-language-model</a></p>
</blockquote>
<p>新的模型的主要函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN2</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">64</span>, embeding_dim=<span class="number">64</span>, dropout_rate=<span class="number">0.2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.drop = nn.Dropout(dropout_rate)</span><br><span class="line">        self.encoder = nn.Embedding(EMBEDDING_LENGTH, embeding_dim)</span><br><span class="line">        self.rnn = nn.GRU(embeding_dim, hidden_units, <span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.decoder = torch.nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">        self.hidden_units = hidden_units</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        nn.init.uniform_(self.encoder.weight, -initrange, initrange)</span><br><span class="line">        nn.init.zeros_(self.decoder.bias)</span><br><span class="line">        nn.init.uniform_(self.decoder.weight, -initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">        <span class="comment"># word shape: [batch, max_word_length]</span></span><br><span class="line">        batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        first_letter = word.new_zeros(batch, <span class="number">1</span>)</span><br><span class="line">        x = torch.cat((first_letter, word[:, <span class="number">0</span>:-<span class="number">1</span>]), <span class="number">1</span>)</span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=word.device)</span><br><span class="line">        emb = self.drop(self.encoder(x))</span><br><span class="line">        output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        y = self.decoder(output.reshape(batch * Tx, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y.reshape(batch, Tx, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>初始化时，我们用<code>nn.Embedding</code>表示单词的向量。词嵌入（Embedding）是《深度学习专项-RNN》第二门课的内容，我会在下一篇笔记里介绍。这里我们把<code>nn.Embedding</code>看成一种代替one-hot编码的更高级的向量就行。这些向量和线性层参数<code>W</code>一样，是可以被梯度下降优化的。这样，不仅是RNN可以优化，每一个单词的表示方法也可以被优化。</p>
<p>注意，使用<code>nn.Embedding</code>后，输入的张量不再是one-hot编码，而是数字标签。代码中的其他地方也要跟着修改。</p>
<p><code>nn.GRU</code>可以创建GRU。其第一个参数是输入的维度，第二个参数是隐变量<code>a</code>的维度，第三个参数是层数，这里我们只构建1层RNN，<code>batch_first</code>表示输入张量的格式是<code>[batch, Tx, embedding_length]</code>还是<code>[Tx,  batch, embedding_length]</code>。</p>
<p>貌似RNN中常用的正则化是靠dropout实现的。我们要提前准备好dropout层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_units=<span class="number">64</span>, embeding_dim=<span class="number">64</span>, dropout_rate=<span class="number">0.2</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.drop = nn.Dropout(dropout_rate)</span><br><span class="line">    self.encoder = nn.Embedding(EMBEDDING_LENGTH, embeding_dim)</span><br><span class="line">    self.rnn = nn.GRU(embeding_dim, hidden_units, <span class="number">1</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">    self.decoder = torch.nn.Linear(hidden_units, EMBEDDING_LENGTH)</span><br><span class="line">    self.hidden_units = hidden_units</span><br><span class="line"></span><br><span class="line">    self.init_weights()</span><br></pre></td></tr></table></figure>
<p>准备好了计算层后，在forward里只要依次调用它们就行了。其底层原理和我们之前手写的是一样的。其中，<code>self.rnn(emb, hidden)</code>这个调用完成了循环遍历的计算。</p>
<p>由于输入格式改了，令第一轮输入为空字符的操作也更繁琐了一点。我们要先定义一个空字符张量，再把它和输入的第一至倒数第二个元素拼接起来，作为网络的真正输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, word: torch.Tensor</span>):</span></span><br><span class="line">    <span class="comment"># word shape: [batch, max_word_length]</span></span><br><span class="line">    batch, Tx = word.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    first_letter = word.new_zeros(batch, <span class="number">1</span>)</span><br><span class="line">    x = torch.cat((first_letter, word[:, <span class="number">0</span>:-<span class="number">1</span>]), <span class="number">1</span>)</span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=word.device)</span><br><span class="line">    emb = self.drop(self.encoder(x))</span><br><span class="line">    output, hidden = self.rnn(emb, hidden)</span><br><span class="line">    y = self.decoder(output.reshape(batch * Tx, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y.reshape(batch, Tx, -<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>PyTorch里的RNN用起来非常灵活。我们不仅能够给它一个序列，一次输出序列的所有结果，还可以只输入一个元素，得到一轮的结果。在采样单词时，我们不得不每次输入一个元素。有关采样的逻辑如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_word</span>(<span class="params">self, device=<span class="string">&#x27;cuda:0&#x27;</span></span>):</span></span><br><span class="line">    batch = <span class="number">1</span></span><br><span class="line">    output = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, batch, self.hidden_units, device=device)</span><br><span class="line">    x = torch.zeros(batch, <span class="number">1</span>, device=device, dtype=torch.long)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        emb = self.drop(self.encoder(x))</span><br><span class="line">        rnn_output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        hat_y = self.decoder(rnn_output)</span><br><span class="line">        hat_y = F.softmax(hat_y, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        np_prob = hat_y[<span class="number">0</span>, <span class="number">0</span>].detach().cpu().numpy()</span><br><span class="line">        letter = np.random.choice(LETTER_LIST, p=np_prob)</span><br><span class="line">        output += letter</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> letter == <span class="string">&#x27; &#x27;</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        x = torch.zeros(batch, <span class="number">1</span>, device=device, dtype=torch.long)</span><br><span class="line">        x[<span class="number">0</span>] = LETTER_MAP[letter]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>以上就是PyTorch高级RNN组件的使用方法。在使用PyTorch的RNN时，主要的改变就是输入从one-hot向量变成了标签，数据预处理会更加方便一些。另外，PyTorch的RNN会自动完成循环，可以给它输入任意长度的序列。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我展示了一个字母级语言模型项目。这个项目涉及到的编程知识有：</p>
<ul>
<li>one-hot编码的处理</li>
<li>RNN的底层实现</li>
<li>如何用RNN对语言模型任务建模</li>
<li>如何用RNN求出语言模型的概率</li>
<li>如何对语言模型采样</li>
<li>PyTorch的RNN组件</li>
</ul>
<p>这篇文章只展示了部分关键代码。想阅读整个项目完整的代码，可以访问该项目的<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicRNN">GitHub链接</a>。</p>
<p>如果大家正在学深度学习，强烈建议大家从头写一遍这个项目。编写代码能够学到很多细节，加深对RNN的理解。</p>
<p>在编写这个项目时，我总结了项目中几个比较有挑战性的部分。大家阅读代码或自己动手时可以格外注意这些部分。第一个比较难的部分是和batch有关的计算。RNN本身必须得顺序处理序列，效率较低，同时处理一个batch的数据是一个很重要的加速手段。我们的代码都得尽量符合向量化编程要求，一次处理一个batch。</p>
<p>另外，相比一般的数据，序列数据多了一个时间维度（或者说序列维度），在向量化计算中考虑这个维度是很耗费脑力的。我们可以在代码中加入对中间变量形状的注释。在使用PyTorch或者其他框架时，要注意是batch维度在前面，还是时间维度在前面。注意初始化RNN的<code>batch_first</code>这个参数。还有，一个张量到底是one-hot编码，还是embedding，还是标签序列，这个也要想清楚来。</p>
<blockquote>
<p>PyTorch里的<code>CrossEntropyLoss</code>自带了softmax操作，千万不能和softmax混用！我之前写了这个bug，调了很久才调出来，真是气死人了。</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/tags/%E7%BC%96%E7%A8%8B/" rel="tag"># 编程</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/08/15/20220813-SRGAN/" rel="prev" title="图像超分经典网络 SRGAN 解析 ~ 如何把 GAN 运用在其他视觉任务上">
      <i class="fa fa-chevron-left"></i> 图像超分经典网络 SRGAN 解析 ~ 如何把 GAN 运用在其他视觉任务上
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/09/21/DLS-note-14/" rel="next" title="吴恩达《深度学习专项》笔记（十四）：循环神经网络基础">
      吴恩达《深度学习专项》笔记（十四）：循环神经网络基础 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A5%E8%AF%86%E8%83%8C%E6%99%AF"><span class="nav-number">1.</span> <span class="nav-text">知识背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN"><span class="nav-number">1.1.</span> <span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.</span> <span class="nav-text">语言模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">RNN 语言模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%AE%B2%E8%A7%A3"><span class="nav-number">2.</span> <span class="nav-text">代码讲解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%8E%B7%E5%8F%96"><span class="nav-number">2.1.</span> <span class="nav-text">数据集获取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%BB%E5%8F%96"><span class="nav-number">2.2.</span> <span class="nav-text">数据集读取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-number">2.3.</span> <span class="nav-text">模型定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">2.4.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95"><span class="nav-number">2.5.</span> <span class="nav-text">测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E6%A0%B7%E5%8D%95%E8%AF%8D"><span class="nav-number">2.6.</span> <span class="nav-text">采样单词</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PyTorch%E9%87%8C%E7%9A%84RNN%E5%87%BD%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">PyTorch里的RNN函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">101</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">51</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
