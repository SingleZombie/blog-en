<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/en/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/en/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/en/images/logo.svg" color="#222">

<link rel="stylesheet" href="/en/css/main.css">


<link rel="stylesheet" href="/en/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/en/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="我之前的一篇文章介绍了如何给PyTorch添加CPU上的简单的加法算子。在这篇文章里，我将继续展示一个更具体的PyTorch自定义算子示例——自己动手复现二维卷积算子。这个示例是基于PyTorch Extension的，在迁移项目时，不需要自己生成动态库，只需要用setup.py重新编译一遍即可。我会同时介绍CPU版和CUDA版的实现。 许多前沿的神经网络都会对卷积进行一些修改。比如大名鼎鼎的可变">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch 自定义算子：复现CPU和CUDA版的二维卷积">
<meta property="og:url" content="https://zhouyifan.net/en/2022/08/09/20220712-custom-op-2/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="我之前的一篇文章介绍了如何给PyTorch添加CPU上的简单的加法算子。在这篇文章里，我将继续展示一个更具体的PyTorch自定义算子示例——自己动手复现二维卷积算子。这个示例是基于PyTorch Extension的，在迁移项目时，不需要自己生成动态库，只需要用setup.py重新编译一遍即可。我会同时介绍CPU版和CUDA版的实现。 许多前沿的神经网络都会对卷积进行一些修改。比如大名鼎鼎的可变">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-08-09T06:20:00.000Z">
<meta property="article:modified_time" content="2022-08-28T10:52:09.390Z">
<meta property="article:author" content="Zhou Yifan">
<meta property="article:tag" content="编程">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="C++">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/en/2022/08/09/20220712-custom-op-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>PyTorch 自定义算子：复现CPU和CUDA版的二维卷积 | 周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/en/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/en/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/en/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/en/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/en/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/en/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net" rel="section"><i class="fa fa-language fa-fw"></i>简体中文</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2022/08/09/20220712-custom-op-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch 自定义算子：复现CPU和CUDA版的二维卷积
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:20:00" itemprop="dateCreated datePublished" datetime="2022-08-09T14:20:00+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>我之前的<a href="https://zhouyifan.net/2022/03/18/20220315-custom-op/">一篇文章</a>介绍了如何给PyTorch添加CPU上的简单的加法算子。在这篇文章里，我将继续展示一个更具体的PyTorch自定义算子示例——自己动手复现二维卷积算子。这个示例是基于PyTorch Extension的，在迁移项目时，不需要自己生成动态库，只需要用<code>setup.py</code>重新编译一遍即可。我会同时介绍CPU版和CUDA版的实现。</p>
<p>许多前沿的神经网络都会对卷积进行一些修改。比如大名鼎鼎的可变形卷积(deformable convolution)。相信看完这篇文章后，大家能看懂PyTorch卷积的实现代码，并大概了解如何修改卷积的实现细节，并把新写好的卷积运用到自己的PyTorch项目中。</p>
<h1 id="PyTorch-Extension-实现二维卷积"><a href="#PyTorch-Extension-实现二维卷积" class="headerlink" title="PyTorch Extension 实现二维卷积"></a>PyTorch Extension 实现二维卷积</h1><h2 id="搭建项目"><a href="#搭建项目" class="headerlink" title="搭建项目"></a>搭建项目</h2><p>在开始写代码前，要准备一个崭新的目录，在这个文件夹里搭建项目。</p>
<p>在根目录下，先创建一个<code>setup.py</code>，之后要填写这份安装文件。</p>
<p>之后，创建一个文件夹，其名字是项目名。在这个文件夹里合适的地方新建一个子文件夹，专门用来放和算子相关的文件。我的项目名叫做<code>panoflow</code>，算子相关文件放在了<code>panoflow/core/op</code>子文件夹下。</p>
<p>接下来，和算子实现相关的文件都应该放在算子文件夹里。使用和测试算子的文件可以放在项目文件夹的其他地方。</p>
<p>由于在实现中我借用了MMCV的代码，还要提前准备好一些头文件。首先新建一个文件<code>pytorch_cpp_helper.hpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> PYTORCH_CPP_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTORCH_CPP_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> at;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.device().is_cuda(), #x <span class="meta-string">&quot; must be a CUDA tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CPU(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(!x.device().is_cuda(), #x <span class="meta-string">&quot; must be a CPU tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CONTIGUOUS(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.is_contiguous(), #x <span class="meta-string">&quot; must be contiguous&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CUDA(x);            \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CPU_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CPU(x);            \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// PYTORCH_CPP_HELPER</span></span></span><br></pre></td></tr></table></figure>
<p>再创建一个文件<code>pytorch_cuda_helper.hpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> PYTORCH_CUDA_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTORCH_CUDA_HELPER</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/ATen.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/cuda/CUDAContext.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;c10/cuda/CUDAGuard.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/cuda/CUDAApplyUtils.cuh&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;THC/THCAtomics.cuh&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;common_cuda_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> at::Half;</span><br><span class="line"><span class="keyword">using</span> at::Tensor;</span><br><span class="line"><span class="keyword">using</span> phalf = at::Half;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __PHALF(x) (x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// PYTORCH_CUDA_HELPER</span></span></span><br></pre></td></tr></table></figure>
<p>还有一个<code>common_cuda_helper.hpp</code>：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> COMMON_CUDA_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> COMMON_CUDA_HELPER</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_1D_KERNEL_LOOP(i, n)                              \</span></span><br><span class="line"><span class="meta">  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n); \</span></span><br><span class="line"><span class="meta">       i += blockDim.x * gridDim.x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_2D_KERNEL_LOOP(i, n, j, m)                             \</span></span><br><span class="line"><span class="meta">  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n);   \</span></span><br><span class="line"><span class="meta">       i += blockDim.x * gridDim.x)                                 \</span></span><br><span class="line"><span class="meta">    for (size_t j = blockIdx.y * blockDim.y + threadIdx.y; j &lt; (m); \</span></span><br><span class="line"><span class="meta">         j += blockDim.y * gridDim.y)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_2D_KERNEL_BLOCK_LOOP(i, n, j, m)          \</span></span><br><span class="line"><span class="meta">  for (size_t i = blockIdx.x; i &lt; (n); i += gridDim.x) \</span></span><br><span class="line"><span class="meta">    for (size_t j = blockIdx.y; j &lt; (m); j += gridDim.y)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THREADS_PER_BLOCK 512</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">GET_BLOCKS</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> num_threads = THREADS_PER_BLOCK)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> optimal_block_num = (N + num_threads - <span class="number">1</span>) / num_threads;</span><br><span class="line">  <span class="keyword">int</span> max_block_num = <span class="number">4096</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">min</span>(optimal_block_num, max_block_num);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ T <span class="title">bilinear_interpolate</span><span class="params">(<span class="keyword">const</span> T* input, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="keyword">const</span> <span class="keyword">int</span> width, T y, T x,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="keyword">const</span> <span class="keyword">int</span> index <span class="comment">/* index for debug only*/</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// deal with cases that inverse elements are out of feature map boundary</span></span><br><span class="line">  <span class="keyword">if</span> (y &lt; <span class="number">-1.0</span> || y &gt; height || x &lt; <span class="number">-1.0</span> || x &gt; width) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y &lt;= <span class="number">0</span>) y = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (x &lt;= <span class="number">0</span>) x = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> y_low = (<span class="keyword">int</span>)y;</span><br><span class="line">  <span class="keyword">int</span> x_low = (<span class="keyword">int</span>)x;</span><br><span class="line">  <span class="keyword">int</span> y_high;</span><br><span class="line">  <span class="keyword">int</span> x_high;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y_low &gt;= height - <span class="number">1</span>) &#123;</span><br><span class="line">    y_high = y_low = height - <span class="number">1</span>;</span><br><span class="line">    y = (T)y_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    y_high = y_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (x_low &gt;= width - <span class="number">1</span>) &#123;</span><br><span class="line">    x_high = x_low = width - <span class="number">1</span>;</span><br><span class="line">    x = (T)x_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    x_high = x_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T ly = y - y_low;</span><br><span class="line">  T lx = x - x_low;</span><br><span class="line">  T hy = <span class="number">1.</span> - ly, hx = <span class="number">1.</span> - lx;</span><br><span class="line">  <span class="comment">// do bilinear interpolation</span></span><br><span class="line">  T v1 = input[y_low * width + x_low];</span><br><span class="line">  T v2 = input[y_low * width + x_high];</span><br><span class="line">  T v3 = input[y_high * width + x_low];</span><br><span class="line">  T v4 = input[y_high * width + x_high];</span><br><span class="line">  T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;</span><br><span class="line"></span><br><span class="line">  T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">bilinear_interpolate_gradient</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> height, <span class="keyword">const</span> <span class="keyword">int</span> width, T y, T x, T&amp; w1, T&amp; w2, T&amp; w3, T&amp; w4,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span>&amp; x_low, <span class="keyword">int</span>&amp; x_high, <span class="keyword">int</span>&amp; y_low, <span class="keyword">int</span>&amp; y_high,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> index <span class="comment">/* index for debug only*/</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// deal with cases that inverse elements are out of feature map boundary</span></span><br><span class="line">  <span class="keyword">if</span> (y &lt; <span class="number">-1.0</span> || y &gt; height || x &lt; <span class="number">-1.0</span> || x &gt; width) &#123;</span><br><span class="line">    <span class="comment">// empty</span></span><br><span class="line">    w1 = w2 = w3 = w4 = <span class="number">0.</span>;</span><br><span class="line">    x_low = x_high = y_low = y_high = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y &lt;= <span class="number">0</span>) y = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (x &lt;= <span class="number">0</span>) x = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  y_low = (<span class="keyword">int</span>)y;</span><br><span class="line">  x_low = (<span class="keyword">int</span>)x;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y_low &gt;= height - <span class="number">1</span>) &#123;</span><br><span class="line">    y_high = y_low = height - <span class="number">1</span>;</span><br><span class="line">    y = (T)y_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    y_high = y_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (x_low &gt;= width - <span class="number">1</span>) &#123;</span><br><span class="line">    x_high = x_low = width - <span class="number">1</span>;</span><br><span class="line">    x = (T)x_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    x_high = x_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T ly = y - y_low;</span><br><span class="line">  T lx = x - x_low;</span><br><span class="line">  T hy = <span class="number">1.</span> - ly, hx = <span class="number">1.</span> - lx;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// reference in forward</span></span><br><span class="line">  <span class="comment">// T v1 = input[y_low * width + x_low];</span></span><br><span class="line">  <span class="comment">// T v2 = input[y_low * width + x_high];</span></span><br><span class="line">  <span class="comment">// T v3 = input[y_high * width + x_low];</span></span><br><span class="line">  <span class="comment">// T v4 = input[y_high * width + x_high];</span></span><br><span class="line">  <span class="comment">// T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);</span></span><br><span class="line"></span><br><span class="line">  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// COMMON_CUDA_HELPER</span></span></span><br></pre></td></tr></table></figure></p>
<p>这些文件添加了CPU和CUDA实现时需要的头文件和定义，后面的C++源码会用到它们。</p>
<h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="C-实现"><a href="#C-实现" class="headerlink" title="C++实现"></a>C++实现</h3><p>在用C++实现一个算子时，我们要编写一个形如这样的文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">my_add</span><span class="params">(torch::Tensor t1, torch::Tensor t2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">return</span> t1 + t2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">	m.<span class="built_in">def</span>(<span class="string">&quot;my_add&quot;</span>, my_add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个C++文件主要包含两部分内容：算子的实现函数和C++接口绑定。在实现卷积时，也是要实现这两部分内容。</p>
<p>在修改一个现有的算子时，最好的方法不是从头写一个，而是去开源库里找一份实现，并在这个基础上进行修改。</p>
<blockquote>
<p>我在MMCV的仓库里找到了<a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops">可变形卷积的实现</a>，并把它拆解回了普通的卷积。我参考了这篇教程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464492627">手把手教你如何高效地在 MMCV 中贡献算子</a>。另外，这份笔记还参考了<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension">PyTorch官方Extension教程</a>。</p>
</blockquote>
<p>找到了卷积的实现后，在算子文件夹下新建一个cpp源文件。比如我的文件路径就是<code>panoflow/core/op/my_conv.cpp</code>。这样一个普通卷积的实现如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;pytorch_cpp_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_shape_check</span><span class="params">(at::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">                         at::Tensor weight, <span class="keyword">int</span> kH,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> kW, <span class="keyword">int</span> dH, <span class="keyword">int</span> dW, <span class="keyword">int</span> padH, <span class="keyword">int</span> padW,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> dilationH, <span class="keyword">int</span> dilationW, <span class="keyword">int</span> group)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">        weight.<span class="built_in">ndimension</span>() == <span class="number">4</span>,</span><br><span class="line">        <span class="string">&quot;4D weight tensor (nOutputPlane,nInputPlane,kH,kW) expected, but got: %s&quot;</span>,</span><br><span class="line">        weight.<span class="built_in">ndimension</span>());</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(weight.<span class="built_in">is_contiguous</span>(), <span class="string">&quot;weight tensor has to be contiguous&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(kW &gt; <span class="number">0</span> &amp;&amp; kH &gt; <span class="number">0</span>,</span><br><span class="line">                <span class="string">&quot;kernel size should be greater than zero, but got kH: %d kW: %d&quot;</span>,</span><br><span class="line">                kH, kW);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>((weight.<span class="built_in">size</span>(<span class="number">2</span>) == kH &amp;&amp; weight.<span class="built_in">size</span>(<span class="number">3</span>) == kW),</span><br><span class="line">                <span class="string">&quot;kernel size should be consistent with weight, &quot;</span>,</span><br><span class="line">                <span class="string">&quot;but got kH: %d kW: %d weight.size(2): %d, weight.size(3): %d&quot;</span>,</span><br><span class="line">                kH, kW, weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(dW &gt; <span class="number">0</span> &amp;&amp; dH &gt; <span class="number">0</span>,</span><br><span class="line">                <span class="string">&quot;stride should be greater than zero, but got dH: %d dW: %d&quot;</span>, dH,</span><br><span class="line">                dW);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">        dilationW &gt; <span class="number">0</span> &amp;&amp; dilationH &gt; <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;dilation should be greater than 0, but got dilationH: %d dilationW: %d&quot;</span>,</span><br><span class="line">        dilationH, dilationW);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> ndim = input.<span class="built_in">ndimension</span>();</span><br><span class="line">    <span class="keyword">int</span> dimf = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> dimh = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> dimw = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ndim == <span class="number">4</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        dimf++;</span><br><span class="line">        dimh++;</span><br><span class="line">        dimw++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(ndim == <span class="number">3</span> || ndim == <span class="number">4</span>,</span><br><span class="line">                <span class="string">&quot;3D or 4D input tensor expected but got: %s&quot;</span>, ndim);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> nInputPlane = weight.<span class="built_in">size</span>(<span class="number">1</span>) * group;</span><br><span class="line">    <span class="keyword">long</span> inputHeight = input.<span class="built_in">size</span>(dimh);</span><br><span class="line">    <span class="keyword">long</span> inputWidth = input.<span class="built_in">size</span>(dimw);</span><br><span class="line">    <span class="keyword">long</span> nOutputPlane = weight.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> outputHeight =</span><br><span class="line">        (inputHeight + <span class="number">2</span> * padH - (dilationH * (kH - <span class="number">1</span>) + <span class="number">1</span>)) / dH + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> outputWidth =</span><br><span class="line">        (inputWidth + <span class="number">2</span> * padW - (dilationW * (kW - <span class="number">1</span>) + <span class="number">1</span>)) / dW + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (outputWidth &lt; <span class="number">1</span> || outputHeight &lt; <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">AT_ERROR</span>(</span><br><span class="line">            <span class="string">&quot;Given input size: (%ld x %ld x %ld). &quot;</span></span><br><span class="line">            <span class="string">&quot;Calculated output size: (%ld x %ld x %ld). Output size is too small&quot;</span>,</span><br><span class="line">            nInputPlane, inputHeight, inputWidth, nOutputPlane, outputHeight,</span><br><span class="line">            outputWidth);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(input.<span class="built_in">size</span>(<span class="number">1</span>) == nInputPlane,</span><br><span class="line">                <span class="string">&quot;invalid number of input planes, expected: %d, but got: %d&quot;</span>,</span><br><span class="line">                nInputPlane, input.<span class="built_in">size</span>(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>((inputHeight &gt;= kH &amp;&amp; inputWidth &gt;= kW),</span><br><span class="line">                <span class="string">&quot;input image is smaller than kernel&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_forward</span><span class="params">(Tensor input, Tensor weight, Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Tensor output, Tensor columns, <span class="keyword">int</span> kW,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> kH, <span class="keyword">int</span> dW, <span class="keyword">int</span> dH, <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> dilationW, <span class="keyword">int</span> dilationH, <span class="keyword">int</span> group,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> im2col_step)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> isCuda = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (input.<span class="built_in">device</span>().<span class="built_in">is_cuda</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(input);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(weight);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(bias);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(output);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(columns);</span><br><span class="line">        isCuda = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(input);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(weight);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(bias);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(output);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(columns);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">my_conv_shape_check</span>(input, weight, kH, kW, dH, dW, padH,</span><br><span class="line">                        padW, dilationH, dilationW, group);</span><br><span class="line">    <span class="function">at::DeviceGuard <span class="title">guard</span><span class="params">(input.device())</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> batch = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (input.<span class="built_in">ndimension</span>() == <span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Force batch</span></span><br><span class="line">        batch = <span class="number">0</span>;</span><br><span class="line">        input.<span class="built_in">unsqueeze_</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> batchSize = input.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> nInputPlane = input.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">long</span> inputHeight = input.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">long</span> inputWidth = input.<span class="built_in">size</span>(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> nOutputPlane = weight.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> outputWidth =</span><br><span class="line">        (inputWidth + <span class="number">2</span> * padW - (dilationW * (kW - <span class="number">1</span>) + <span class="number">1</span>)) / dW + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> outputHeight =</span><br><span class="line">        (inputHeight + <span class="number">2</span> * padH - (dilationH * (kH - <span class="number">1</span>) + <span class="number">1</span>)) / dH + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    output = output.<span class="built_in">view</span>(&#123;batchSize / im2col_step, im2col_step, nOutputPlane,</span><br><span class="line">                          outputHeight, outputWidth&#125;);</span><br><span class="line">    columns = at::<span class="built_in">zeros</span>(</span><br><span class="line">        &#123;nInputPlane * kW * kH, im2col_step * outputHeight * outputWidth&#125;,</span><br><span class="line">        input.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    input = input.<span class="built_in">view</span>(&#123;batchSize / im2col_step, im2col_step, nInputPlane,</span><br><span class="line">                        inputHeight, inputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    Tensor output_buffer = at::<span class="built_in">zeros</span>(&#123;batchSize / im2col_step, nOutputPlane,</span><br><span class="line">                                      im2col_step * outputHeight, outputWidth&#125;,</span><br><span class="line">                                     output.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(</span><br><span class="line">        &#123;output_buffer.<span class="built_in">size</span>(<span class="number">0</span>), group, output_buffer.<span class="built_in">size</span>(<span class="number">1</span>) / group,</span><br><span class="line">         output_buffer.<span class="built_in">size</span>(<span class="number">2</span>), output_buffer.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize / im2col_step; elt++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (isCuda)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">my_conv_im2col_cuda</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                            inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                            dilationW, im2col_step, columns);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">my_conv_im2col_cpu</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                            inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                            dilationW, im2col_step, columns);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        columns = columns.<span class="built_in">view</span>(&#123;group, columns.<span class="built_in">size</span>(<span class="number">0</span>) / group, columns.<span class="built_in">size</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">        weight = weight.<span class="built_in">view</span>(&#123;group, weight.<span class="built_in">size</span>(<span class="number">0</span>) / group, weight.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">                              weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group; g++)</span><br><span class="line">        &#123;</span><br><span class="line">            output_buffer[elt][g] = output_buffer[elt][g]</span><br><span class="line">                                        .<span class="built_in">flatten</span>(<span class="number">1</span>)</span><br><span class="line">                                        .<span class="built_in">addmm_</span>(weight[g].<span class="built_in">flatten</span>(<span class="number">1</span>), columns[g])</span><br><span class="line">                                        .<span class="built_in">view_as</span>(output_buffer[elt][g]);</span><br><span class="line">        &#125;</span><br><span class="line">        columns =</span><br><span class="line">            columns.<span class="built_in">view</span>(&#123;columns.<span class="built_in">size</span>(<span class="number">0</span>) * columns.<span class="built_in">size</span>(<span class="number">1</span>), columns.<span class="built_in">size</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">        weight = weight.<span class="built_in">view</span>(&#123;weight.<span class="built_in">size</span>(<span class="number">0</span>) * weight.<span class="built_in">size</span>(<span class="number">1</span>), weight.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">                              weight.<span class="built_in">size</span>(<span class="number">3</span>), weight.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(</span><br><span class="line">        &#123;output_buffer.<span class="built_in">size</span>(<span class="number">0</span>), output_buffer.<span class="built_in">size</span>(<span class="number">1</span>) * output_buffer.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">         output_buffer.<span class="built_in">size</span>(<span class="number">3</span>), output_buffer.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(&#123;batchSize / im2col_step, nOutputPlane,</span><br><span class="line">                                        im2col_step, outputHeight, outputWidth&#125;);</span><br><span class="line">    output_buffer.<span class="built_in">transpose_</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    output.<span class="built_in">copy_</span>(output_buffer);</span><br><span class="line">    output = output.<span class="built_in">view</span>(&#123;batchSize, nOutputPlane, outputHeight, outputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    bias = bias.<span class="built_in">view</span>(&#123;<span class="number">1</span>, bias.<span class="built_in">size</span>(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>&#125;);</span><br><span class="line">    output.<span class="built_in">add_</span>(bias);</span><br><span class="line"></span><br><span class="line">    input = input.<span class="built_in">view</span>(&#123;batchSize, nInputPlane, inputHeight, inputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (batch == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        output = output.<span class="built_in">view</span>(&#123;nOutputPlane, outputHeight, outputWidth&#125;);</span><br><span class="line">        input = input.<span class="built_in">view</span>(&#123;nInputPlane, inputHeight, inputWidth&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; index++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_cpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">      m.<span class="built_in">def</span>(<span class="string">&quot;my_conv_forward&quot;</span>, my_conv_forward, <span class="string">&quot;my_conv_forward&quot;</span>,</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;input&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;weight&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;bias&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;output&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;columns&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;kW&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;kH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dW&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;padW&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;padH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dilationW&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dilationH&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;group&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;im2col_step&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这份实现非常长，我挑一些重点的内容讲解。</p>
<p>从最下面的<code>PYBIND11_MODULE(my_ops, m)</code>看起。这里的<code>my_ops</code>是生成的库名，可以随便取名。待会要import这个库名。代码块里<code>m.def</code>用于定义C++函数的Python接口。<code>&quot;my_conv_forward&quot;</code>是Python调用时的函数名称，<code>my_conv_forward</code>是被Python代码调用的这份代码里的C++函数名称。也就是说，这份卷积实现的入口函数就是<code>my_conv_forward</code>。我们从这个函数看起。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_forward</span><span class="params">(Tensor input, Tensor weight, Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Tensor output, Tensor columns, <span class="keyword">int</span> kW,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> kH, <span class="keyword">int</span> dW, <span class="keyword">int</span> dH, <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> dilationW, <span class="keyword">int</span> dilationH, <span class="keyword">int</span> group,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> im2col_step)</span></span></span><br></pre></td></tr></table></figure>
<p><code>my_conv_forward</code>就是卷积的主函数。它的参数除了PyTorch的<code>Conv2d</code>传入的参数外，还多了两个参数<code>output, columus</code>。这两个张量是保存中间结果的，在PyTorch侧是看不到的。<code>output</code>用于保存卷积输出，<code>columns</code>用于保存卷积时的列矩阵。底层实现卷积时，会先把图像转换成一个用列表示的矩阵，再把卷积操作当成一个矩阵乘法来完成。其中，第一步操作叫做”im2col”。对此原理不熟的话可以参考这篇文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63974249。">https://zhuanlan.zhihu.com/p/63974249。</a></p>
<p><code>my_conv_forward</code>函数的大部分内容都是在做类型检查和张量形状转换。在修改卷积实现时，这些东西都可以不用改。整个卷积操作的核心都在这一部分：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize / im2col_step; elt++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (isCuda)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">my_conv_im2col_cuda</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                        inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                        dilationW, im2col_step, columns);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">my_conv_im2col_cpu</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                        inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                        dilationW, im2col_step, columns);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    columns = columns.<span class="built_in">view</span>(&#123;group, columns.<span class="built_in">size</span>(<span class="number">0</span>) / group, columns.<span class="built_in">size</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">    weight = weight.<span class="built_in">view</span>(&#123;group, weight.<span class="built_in">size</span>(<span class="number">0</span>) / group, weight.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">                          weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group; g++)</span><br><span class="line">    &#123;</span><br><span class="line">        output_buffer[elt][g] = output_buffer[elt][g]</span><br><span class="line">                                    .<span class="built_in">flatten</span>(<span class="number">1</span>)</span><br><span class="line">                                    .<span class="built_in">addmm_</span>(weight[g].<span class="built_in">flatten</span>(<span class="number">1</span>), columns[g])</span><br><span class="line">                                    .<span class="built_in">view_as</span>(output_buffer[elt][g]);</span><br><span class="line">    &#125;</span><br><span class="line">    columns =</span><br><span class="line">        columns.<span class="built_in">view</span>(&#123;columns.<span class="built_in">size</span>(<span class="number">0</span>) * columns.<span class="built_in">size</span>(<span class="number">1</span>), columns.<span class="built_in">size</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">    weight = weight.<span class="built_in">view</span>(&#123;weight.<span class="built_in">size</span>(<span class="number">0</span>) * weight.<span class="built_in">size</span>(<span class="number">1</span>), weight.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">                          weight.<span class="built_in">size</span>(<span class="number">3</span>), weight.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码先做了<code>im2col</code>操作，再做了矩阵乘法。其实，包括可变形卷积在内，各种稀奇古怪的卷积操作通过靠修改<code>im2col</code>来完成的。CPU和CUDA版卷积的主要区别，也体现在<code>im2col</code>中（后面的矩阵乘法在CPU和CUDA上都能用）。</p>
<p>由于是讲CPU实现，这里的CUDA实现我暂时放了一个空函数。<code>my_conv_im2col_cpu</code>的内容如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_cpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数其实只是处理了一下输入，真正的实现在<code>my_conv_im2col_cpu_kernel</code>里。<code>AT_DISPATCH_FLOATING_TYPES_AND_HALF</code>可以让实现兼容半精度和普通float，所以实现<code>my_conv_im2col_cpu_kernel</code>得写成一个模板函数。</p>
<p><code>my_conv_im2col_cpu_kernel</code>的实现如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; index++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>它的作用就是把图像里的数据搬到做卷积运算的<code>column</code>里。循环遍历每一次卷积的每一个位置，把待运算的量填入<code>column</code>。卷积里的所有参数(pad, stride, …)都是在这段函数里生效的。想实现可变形卷积等改进，也要修改这个函数。</p>
<h3 id="Python封装"><a href="#Python封装" class="headerlink" title="Python封装"></a>Python封装</h3><p>实现好了后，如果编译完了的话，刚刚的卷积接口可以通过以下方式在Python里调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> my_ops</span><br><span class="line">my_ops.my_conv_forward(...)</span><br></pre></td></tr></table></figure>
<p>这里的<code>my_ops</code>这个名称必须和开始<code>PYBIND11_MODULE(my_ops, m)</code>里面那个库名称对应。</p>
<p>基于这个接口，可以仿照PyTorch中<code>Conv2d</code>的接口，编写一个和<code>Conv2d</code>等价的<code>torch.nn.Module</code>出来。我的这个Python文件的路径是<code>panoflow/core/op/my_conv.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.utils <span class="keyword">import</span> _pair</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> my_ops</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyConvF</span>(<span class="params">Function</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="built_in">input</span>: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                weight,</span></span></span><br><span class="line"><span class="params"><span class="function">                bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                groups=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                im2col_step=<span class="number">32</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">input</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">input</span>.dim() != <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&#x27;Expected 4D tensor as input, got <span class="subst">&#123;<span class="built_in">input</span>.dim()&#125;</span>D tensor \</span></span><br><span class="line"><span class="string">                  instead.&#x27;</span>)</span><br><span class="line">        ctx.stride = _pair(stride)</span><br><span class="line">        ctx.padding = _pair(padding)</span><br><span class="line">        ctx.dilation = _pair(dilation)</span><br><span class="line">        ctx.groups = groups</span><br><span class="line">        ctx.im2col_step = im2col_step</span><br><span class="line"></span><br><span class="line">        weight = weight.type_as(<span class="built_in">input</span>)</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>, weight)</span><br><span class="line"></span><br><span class="line">        output = <span class="built_in">input</span>.new_empty(MyConvF._output_size(ctx, <span class="built_in">input</span>, weight))</span><br><span class="line"></span><br><span class="line">        ctx.bufs_ = [<span class="built_in">input</span>.new_empty(<span class="number">0</span>), <span class="built_in">input</span>.new_empty(<span class="number">0</span>)]  <span class="comment"># columns, ones</span></span><br><span class="line"></span><br><span class="line">        cur_im2col_step = <span class="built_in">min</span>(ctx.im2col_step, <span class="built_in">input</span>.size(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">assert</span> (<span class="built_in">input</span>.size(<span class="number">0</span>) % cur_im2col_step</span><br><span class="line">                ) == <span class="number">0</span>, <span class="string">&#x27;batch size must be divisible by im2col_step&#x27;</span></span><br><span class="line"></span><br><span class="line">        my_ops.my_conv_forward(</span><br><span class="line">            <span class="built_in">input</span>,</span><br><span class="line">            weight,</span><br><span class="line">            bias,</span><br><span class="line">            output,</span><br><span class="line">            ctx.bufs_[<span class="number">0</span>],</span><br><span class="line">            kW=weight.size(<span class="number">3</span>),</span><br><span class="line">            kH=weight.size(<span class="number">2</span>),</span><br><span class="line">            dW=ctx.stride[<span class="number">1</span>],</span><br><span class="line">            dH=ctx.stride[<span class="number">0</span>],</span><br><span class="line">            padW=ctx.padding[<span class="number">1</span>],</span><br><span class="line">            padH=ctx.padding[<span class="number">0</span>],</span><br><span class="line">            dilationW=ctx.dilation[<span class="number">1</span>],</span><br><span class="line">            dilationH=ctx.dilation[<span class="number">0</span>],</span><br><span class="line">            group=ctx.groups,</span><br><span class="line">            im2col_step=cur_im2col_step)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_output_size</span>(<span class="params">ctx, <span class="built_in">input</span>, weight</span>):</span></span><br><span class="line">        channels = weight.size(<span class="number">0</span>)</span><br><span class="line">        output_size = (<span class="built_in">input</span>.size(<span class="number">0</span>), channels)</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">input</span>.dim() - <span class="number">2</span>):</span><br><span class="line">            in_size = <span class="built_in">input</span>.size(d + <span class="number">2</span>)</span><br><span class="line">            pad = ctx.padding[d]</span><br><span class="line">            kernel = ctx.dilation[d] * (weight.size(d + <span class="number">2</span>) - <span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">            stride_ = ctx.stride[d]</span><br><span class="line">            output_size += ((in_size + (<span class="number">2</span> * pad) - kernel) // stride_ + <span class="number">1</span>, )</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">all</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> s: s &gt; <span class="number">0</span>, output_size)):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&#x27;convolution input is too small (output would be &#x27;</span> +</span><br><span class="line">                <span class="string">&#x27;x&#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, output_size)) + <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> output_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_conv = MyConvF.apply</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyConv2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 in_channels: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 out_channels: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 kernel_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 bias: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        kernel_size_ = _pair(kernel_size)</span><br><span class="line">        stride_ = _pair(stride)</span><br><span class="line">        padding_ = _pair(padding)</span><br><span class="line">        dilation_ = _pair(dilation)</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.kernel_size = kernel_size_</span><br><span class="line">        self.stride = stride_</span><br><span class="line">        self.padding = padding_</span><br><span class="line">        self.dilation = dilation_</span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.weight = Parameter(</span><br><span class="line">            torch.Tensor(out_channels, in_channels // groups, *kernel_size_))</span><br><span class="line">        self.bias = Parameter(torch.Tensor(out_channels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Useless attributes</span></span><br><span class="line">        self.transposed = <span class="literal">None</span></span><br><span class="line">        self.output_padding = <span class="literal">None</span></span><br><span class="line">        self.padding_mode = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">return</span> my_conv(<span class="built_in">input</span>, self.weight, self.bias, self.stride,</span><br><span class="line">                       self.padding, self.dilation, self.groups)</span><br></pre></td></tr></table></figure>
<p>以后，用自己的卷积<code>MyConv2d</code>就和用普通的<code>Conv2d</code>一样了。</p>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>打开外面的<code>setup.py</code>，填写以下内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">src_root = <span class="string">&#x27;panoflow/core/op&#x27;</span></span><br><span class="line">cpp_src = [<span class="string">&#x27;my_conv.cpp&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    include_dirs = [<span class="string">&#x27;panoflow/core/op&#x27;</span>]</span><br><span class="line">    cpp_path = [os.path.join(src_root, src) <span class="keyword">for</span> src <span class="keyword">in</span> cpp_src]</span><br><span class="line"></span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;panoflow&#x27;</span>,</span><br><span class="line">        ext_modules=[</span><br><span class="line">            cpp_extension.CppExtension(</span><br><span class="line">                <span class="string">&#x27;my_ops&#x27;</span>, cpp_path, include_dirs=include_dirs)</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>其中的路径要根据自己的实际情况修改。</p>
<p>和编译相关的内容都写在<code>cpp_extension.CppExtension</code>里。其中，源文件要写在第二个参数里，头文件目录要写在<code>include_dirs</code>。由于我的源文件放在<code>panoflow/core/op</code>里，我写了个源文件名数组<code>cpp_src</code>，在传参前把路径组合了一下。由于<code>include_dirs</code>和源文件在同一个目录下，我也填的是<code>panoflow/core/op</code>。</p>
<p>写完了<code>setup.py</code>后，运行<code>python setup.py develop</code>，就能一键编译和安装。如果运行后没有报编译错误，就可以把实现的卷积用起来了。</p>
<h3 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h3><p>用单元测试可以快速地验证卷积是否实现成功。我写了一个简单的单元测试文件，在任意一个文件夹下创建该文件即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> panoflow.core.op.my_conv <span class="keyword">import</span> MyConv2d</span><br><span class="line"></span><br><span class="line">inc = <span class="number">3</span></span><br><span class="line">outc = <span class="number">4</span></span><br><span class="line">img_shaspe = (<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># device_name = &#x27;cuda:0&#x27;</span></span><br><span class="line">device_name = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">open_bias = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one</span>():</span></span><br><span class="line">    ts = torch.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_two</span>():</span></span><br><span class="line">    ts = torch.rand([<span class="number">1</span>, inc, *img_shaspe]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test_one()</span><br><span class="line">    test_two()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中，<code>panoflow.core.op.my_conv</code>是我刚刚放<code>MyConv2d</code>的Python模块。</p>
<p>直接运行这个Python文件，如果没有任何输出（报错信息），就说明卷积实现成功了。</p>
<h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h2><h3 id="C-实现-1"><a href="#C-实现-1" class="headerlink" title="C++实现"></a>C++实现</h3><p>在刚刚的实现中，有一个<code>my_conv_im2col_cuda</code>的实现是空着的。在CUDA版本中，我们要实现这个函数。不过，这个函数要放在一个用<code>nvcc</code>编译的<code>.cu</code>文件里。<strong>注意！注意！注意！</strong> 因此，<code>my_conv.cpp</code>里那个空的<code>my_conv_im2col_cuda</code>实现应该全部删掉。</p>
<p>新建一个文件<code>my_conv_cuda.cu</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Modify from https://github.com/open-mmlab/mmcv/blob/my_conv/mmcv/ops/csrc/common/cuda/deform_conv_cuda_kernel.cuh</span></span><br><span class="line"><span class="comment">// Copyright (c) OpenMMLab. All rights reserved.</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/types.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;pytorch_cuda_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">my_conv_im2col_gpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CUDA_1D_KERNEL_LOOP</span>(index, n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;my_conv_im2col_gpu&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_gpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;&lt;&lt;&lt;<span class="built_in">GET_BLOCKS</span>(num_kernels),</span><br><span class="line">                                                THREADS_PER_BLOCK, <span class="number">0</span>,</span><br><span class="line">                                                at::cuda::<span class="built_in">getCurrentCUDAStream</span>()&gt;&gt;&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_CUDA_CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和CPU版的类似，<code>my_conv_im2col_cuda</code>也是预处理了输入，并调用核函数<code>my_conv_im2col_gpu_kernel</code>来实现<code>im2col</code>。</p>
<p>CUDA实现和CPU几乎一样，唯一的区别就是for循环变成了<code>CUDA_1D_KERNEL_LOOP(index, n)</code>。这个宏是头文件里帮我们定义的，它简化了CUDA的一维循环。</p>
<h3 id="编译-1"><a href="#编译-1" class="headerlink" title="编译"></a>编译</h3><p>修改<code>setup.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">src_root = <span class="string">&#x27;panoflow/core/op&#x27;</span></span><br><span class="line">cpp_src = [<span class="string">&#x27;my_conv.cpp&#x27;</span>, <span class="string">&#x27;my_conv_cuda.cu&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    include_dirs = [<span class="string">&#x27;panoflow/core/op&#x27;</span>]</span><br><span class="line">    cpp_path = [os.path.join(src_root, src) <span class="keyword">for</span> src <span class="keyword">in</span> cpp_src]</span><br><span class="line"></span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;panoflow&#x27;</span>,</span><br><span class="line">        ext_modules=[</span><br><span class="line">            cpp_extension.CUDAExtension(</span><br><span class="line">                <span class="string">&#x27;my_ops&#x27;</span>, cpp_path, include_dirs=include_dirs)</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>首先，要把源文件加入<code>cpp_src</code>里。之后，把<code>CppExtension</code>改成<code>CUDAExtension</code>。这样，就能编译新写的CUDA文件了。</p>
<p>写完了之后，再次<code>python setup.py develop</code>编译即可。</p>
<blockquote>
<p>编译小技巧：不拿IDE直接写C++和CUDA源代码是很容易出错误的。但如果你想只用<code>setup.py</code>来验证代码的正确性，可以<code>python setup.py develop &gt; tmp.txt</code>把编译输出重定向到一个文件里来查看。由于编译时的信息过多，在命令行里很难从一堆编译warning里找到最重要的error。</p>
</blockquote>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>由于Python部分在之前都已经写好了，可以直接用刚刚的单元测试文件测试了。只要把刚刚那份文件的<code>device_name</code>改成<code>cuda:0</code>即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> panoflow.core.op.my_conv <span class="keyword">import</span> MyConv2d</span><br><span class="line"></span><br><span class="line">inc = <span class="number">3</span></span><br><span class="line">outc = <span class="number">4</span></span><br><span class="line">img_shaspe = (<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">device_name = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line"><span class="comment"># device_name = &#x27;cpu&#x27;</span></span><br><span class="line">open_bias = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one</span>():</span></span><br><span class="line">    ts = torch.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_two</span>():</span></span><br><span class="line">    ts = torch.rand([<span class="number">1</span>, inc, *img_shaspe]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test_one()</span><br><span class="line">    test_two()</span><br></pre></td></tr></table></figure>
<p>同样，没报错就说明写对了。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/en/tags/%E7%BC%96%E7%A8%8B/" rel="tag"># 编程</a>
              <a href="/en/tags/PyTorch/" rel="tag"># PyTorch</a>
              <a href="/en/tags/Python/" rel="tag"># Python</a>
              <a href="/en/tags/C/" rel="tag"># C++</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/en/2022/08/09/20220807-ResNet/" rel="prev" title="ResNet 论文概览与精读">
      <i class="fa fa-chevron-left"></i> ResNet 论文概览与精读
    </a></div>
      <div class="post-nav-item">
    <a href="/en/2022/08/15/20220813-SRGAN/" rel="next" title="图像超分经典网络 SRGAN 解析 ~ 如何把 GAN 运用在其他视觉任务上">
      图像超分经典网络 SRGAN 解析 ~ 如何把 GAN 运用在其他视觉任务上 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch-Extension-%E5%AE%9E%E7%8E%B0%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF"><span class="nav-number">1.</span> <span class="nav-text">PyTorch Extension 实现二维卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="nav-number">1.1.</span> <span class="nav-text">搭建项目</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CPU"><span class="nav-number">1.2.</span> <span class="nav-text">CPU</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C-%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.2.1.</span> <span class="nav-text">C++实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Python%E5%B0%81%E8%A3%85"><span class="nav-number">1.2.2.</span> <span class="nav-text">Python封装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E8%AF%91"><span class="nav-number">1.2.3.</span> <span class="nav-text">编译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95"><span class="nav-number">1.2.4.</span> <span class="nav-text">单元测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA"><span class="nav-number">1.3.</span> <span class="nav-text">CUDA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C-%E5%AE%9E%E7%8E%B0-1"><span class="nav-number">1.3.1.</span> <span class="nav-text">C++实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E8%AF%91-1"><span class="nav-number">1.3.2.</span> <span class="nav-text">编译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95"><span class="nav-number">1.3.3.</span> <span class="nav-text">测试</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/en/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/en/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/en/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/en/lib/anime.min.js"></script>
  <script src="/en/lib/velocity/velocity.min.js"></script>
  <script src="/en/lib/velocity/velocity.ui.min.js"></script>

<script src="/en/js/utils.js"></script>

<script src="/en/js/motion.js"></script>


<script src="/en/js/schemes/muse.js"></script>


<script src="/en/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
