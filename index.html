<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:type" content="website">
<meta property="og:title" content="周弈帆的博客">
<meta property="og:url" content="https://zhouyifan.net/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhou Yifan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/07/14/20240703-SD3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/07/14/20240703-SD3/" class="post-title-link" itemprop="url">Stable Diffusion 3 论文及源码概览</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-14 10:26:06" itemprop="dateCreated datePublished" datetime="2024-07-14T10:26:06+08:00">2024-07-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>近期，最受开源社区欢迎的文生图模型 Stable Diffusion 的最新版本 Stable Diffusion 3 开放了源码和模型参数。开发者宣称，Stable Diffusion 3 使用了全新的模型结构和文本编码方法，能够生成更符合文本描述且高质量的图片。得知 Stable Diffusion 3 开源后，社区用户们纷纷上手测试，在网上分享了许多测试结果。而在本文中，我将面向之前已经熟悉 Stable Diffusion 的科研人员，快速讲解 Stable Diffusion 3 论文的主要内容及其在 Diffusers 中的源码。对于 Stable Diffusion 3 中的一些新技术，我并不会介绍其细节，而是会讲清其设计动机并指明进一步学习的参考文献。</p>
<h2 id="内容索引"><a href="#内容索引" class="headerlink" title="内容索引"></a>内容索引</h2><p>本文会从多个角度简单介绍 SD3，具体要介绍的方面如下所示。读者可以根据自己的需求，跳转到感兴趣的部分阅读。</p>
<h3 id="流匹配原理简介"><a href="#流匹配原理简介" class="headerlink" title="流匹配原理简介"></a>流匹配原理简介</h3><p>流匹配是一种定义图像生成目标的方法，它可以兼容当前扩散模型的训练目标。流匹配中一个有代表性的工作是整流 (rectified flow)，它也正是 SD3 用到的训练目标。我们会在本文中通过简单的可视化示例学习流匹配的思想。</p>
<h3 id="SD3-中的-DiT"><a href="#SD3-中的-DiT" class="headerlink" title="SD3 中的 DiT"></a>SD3 中的 DiT</h3><p>我们会从一个简单的类 ViT 架构开始，学习 SD3 中的去噪网络 DiT 模型是怎么一步一步搭起来的。读者不需要提前学过 DiT，只需要了解 Transformer 的结构，并大概知道视觉任务里的 Transformer 会做哪些通用的修改（如图块化），即可学懂 SD3 里的 DiT。</p>
<h3 id="SD3-模型与训练策略改进细节"><a href="#SD3-模型与训练策略改进细节" class="headerlink" title="SD3 模型与训练策略改进细节"></a>SD3 模型与训练策略改进细节</h3><p>除了将去噪网络从 U-Net 改成 DiT 外，SD3 还在模型结构与训练策略上做了很多小改进：</p>
<ul>
<li>改变训练时噪声采样方法</li>
<li>将一维位置编码改成二维位置编码</li>
<li>提升 VAE 隐空间通道数</li>
<li>对注意力 QK 做归一化以确保高分辨率下训练稳定</li>
</ul>
<p>本文会简单介绍这些改进。</p>
<h3 id="大型消融实验"><a href="#大型消融实验" class="headerlink" title="大型消融实验"></a>大型消融实验</h3><p>对于想训练大型文生图模型的开发者，SD3 论文提供了许多极有价值的大型消融实验结果。本文会简单分析论文中的两项实验结果：各训练目标在文生图任务中的表现、SD3 的参数扩增实验结果。</p>
<h3 id="SD3-Diffusers-源码解读"><a href="#SD3-Diffusers-源码解读" class="headerlink" title="SD3 Diffusers 源码解读"></a>SD3 Diffusers 源码解读</h3><p>本文会介绍如何配置 Diffusers 环境以用代码运行 SD3，并简单介绍相比于 SD，SD3 的采样代码和模型代码有哪些变动。</p>
<h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h3 id="核心贡献"><a href="#核心贡献" class="headerlink" title="核心贡献"></a>核心贡献</h3><p>介绍 Stable Diffusion 3 (SD3) 的文章标题为 <em>Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</em>。与其说它是一篇技术报告，更不如说它是一篇论文，因为它确实是按照撰写学术论文的一般思路，将正文的叙述重点放到了方法的核心创新点上，而没有过多叙述工程细节。正如其标题所示，这篇文章的内容很简明，就是用<strong>整流 (rectified flow)</strong> 生成模型、<strong>Transformer</strong> 神经网络做了模型<strong>参数扩增</strong>实验，以实现高质量文生图大模型。</p>
<p>由于这是一篇实验主导而非思考主导的文章，论文的开头没有太多有价值的内容。从我们读者学习论文的角度，文章的核心贡献如下：</p>
<p>从方法设计上：</p>
<ul>
<li>首次在大型文生图模型上使用了整流模型。</li>
<li>用一种新颖的 Diffusion Transformer (DiT) 神经网络来更好地融合文本信息。</li>
<li>使用了各种小设计来提升模型的能力。如使用二维位置编码来实现任意分辨率的图像生成。</li>
</ul>
<p>从实验上：</p>
<ul>
<li>开展了一场大规模、系统性的实验，以验证哪种扩散模型/整流模型的学习目标最优。</li>
<li>开展了扩增模型参数的实验 (scaling study)，以证明提升参数量能提升模型的效果。</li>
</ul>
<h3 id="整流模型简介"><a href="#整流模型简介" class="headerlink" title="整流模型简介"></a>整流模型简介</h3><p>由于 SD3 最后用了整流模型来建模图像生成，所以文章是从一种称为流匹配 (Flow Matching) 的角度而非更常见的扩散模型的角度来介绍各种训练目标。鉴于 SD3 并没有对其他论文中提出的整流模型做太多更改，我们在阅读本文时可以主要关注整流的想法及其与扩散模型的关系，后续再从其他论文中学习整流的具体原理。在此，我们来大致认识一下流匹配与整流的想法。</p>
<p>所谓图像生成，其实就是让神经网络模型学习一个图像数据集所表示的分布，之后从分布里随机采样。比如我们想让模型生成人脸图像，就是要让模型学习一个人脸图像集的分布。为了直观理解，我们可以用二维点来表示一张图像的数据。比如在下图中我们希望学习红点表示的分布，即我们希望随机生成点，生成的点都落在红点处，而不是落在灰点处。</p>
<p><img src="/2024/07/14/20240703-SD3/1.jpg" alt></p>
<p>我们很难表示出一个适合采样的复杂分布。因此，我们会把学习一个分布的问题转换成学习一个简单好采样的分布到复杂分布的映射。一般这个简单分布都是标准正态分布。如下图所示，我们可以用简单的算法采样在原点附近的来自标准正态分布的蓝点，我们要想办法得到蓝点到红点的映射方法。</p>
<p><img src="/2024/07/14/20240703-SD3/2.jpg" alt></p>
<p>学习这种映射依然是很困难的。而近年来包括扩散模型在内的几类生成模型用一种巧妙的方法来学习这种映射：从纯噪声（标准正态分布里的数据）到真实数据的映射很难表示，但从真实数据到纯噪声的逆映射很容易表示。所以，我们先人工定义从图像数据集到噪声的变换路线（红线），再让模型学习逆路线（蓝线）。让噪声数据沿着逆路线走，就实现了图像生成。</p>
<p><img src="/2024/07/14/20240703-SD3/3.jpg" alt></p>
<p>我们又可以用一种巧妙的方法间接学习图像生成路线。知道了预定义的数据到噪声的路线后，我们其实就知道了数据在路线上每一位置的速度（红箭头）。那么，我们可以以每一位置的反向速度（蓝箭头）为真值，学习噪声到真实数据的速度场。这样的学习目标被称为流匹配。</p>
<p><img src="/2024/07/14/20240703-SD3/4.jpg" alt></p>
<p>对于不同的扩散模型及流匹配模型，其本质区别在于图像到噪声的路线的定义方式。在扩散模型中，图像到噪声的路线是由一个复杂的公式表示的。而整流模型将图像到噪声的路线定义为了直线。比如根据论文的介绍，整流中 $t$ 时刻数据 $z_t$ 由真实图像 $x_0$ 变换成纯噪声 $\epsilon$ 的位置为:</p>
<script type="math/tex; mode=display">
z_t = (1 - t) x_0 + t \epsilon</script><p>而较先进的扩散模型 EDM 提出的路线公式为（$b_t$ 是一个形式较为复杂的变量）：</p>
<script type="math/tex; mode=display">
z_t = x_0 + b_t \epsilon</script><p>由于整流最后学习出来的生成路线近乎是直线，这种模型在设计上就支持少步数生成。</p>
<blockquote>
<p>虽然整流模型是这样宣传的，但实际上 SD3 还是默认用了 28 步来生成图像。单看这篇文章，原整流论文里的很多设计并没有用上。对整流感兴趣的话，可以去阅读原论文 <em>Flow straight and fast: Learning to generate and transfer data with rectified flow</em></p>
<p>流匹配模型和扩散模型的另一个区别是，流匹配模型天然支持 image2image 任务。从纯噪声中生成图像只是流匹配模型的一个特例。</p>
</blockquote>
<h3 id="非均匀训练噪声采样"><a href="#非均匀训练噪声采样" class="headerlink" title="非均匀训练噪声采样"></a>非均匀训练噪声采样</h3><p>在学习这样一种生成模型时，会先随机采样一个时刻 $t \in [0, 1]$，根据公式获取此时刻对应位置在生成路线上的速度，再让神经网络学习这个速度。直观上看，刚开始和快到终点的路线很好学，而路线的中间处比较难学。因此，在采样时刻 $t$ 时，SD3 使用了一种非均匀采样分布。</p>
<p>如下图所示，SD3 主要考虑了两种公式: mode（左）和 logit-norm （右）。二者的共同点是中间多，两边少。mode 相比 logit-norm，在开始和结束时概率不会过分接近 0。</p>
<p><img src="/2024/07/14/20240703-SD3/5.jpg" alt></p>
<h3 id="网络整体架构"><a href="#网络整体架构" class="headerlink" title="网络整体架构"></a>网络整体架构</h3><p>以上内容都是和训练相关的理论基础，下面我们来看多数用户更加熟悉的文生图架构。</p>
<p>从整体架构上来看，和之前的 SD 一样，SD3 主要基于隐扩散模型（latent diffusion model, LDM）。这套方法是一个两阶段的生成方法：先用一个 LDM 生成隐空间低分辨率的图像，再用一个自编码器把图像解码回真实图像。</p>
<p>扩散模型 LDM 会使用一个神经网络模型来对噪声图像去噪。为了实现文生图，该去噪网络会以输入文本为额外约束。相比之前多数扩散模型，SD3 的主要改进是把去噪模型的结构从 U-Net 变为了 DiT。</p>
<blockquote>
<p>DiT 的论文为 <em>Scalable Diffusion Models with Transformers</em>。如果只是对 DiT 的结构感兴趣的话，可以去直接通过读 SD3 的源码来学习。读 DiT 论文时只需要着重学习 AdaLayerNormZero 模块。</p>
</blockquote>
<h3 id="提升自编码器通道数"><a href="#提升自编码器通道数" class="headerlink" title="提升自编码器通道数"></a>提升自编码器通道数</h3><p>在当时设计整套自编码器 + LDM 的生成架构时，SD 的开发者并没有仔细改进自编码器，用了一个能把图像下采样 8 倍，通道数变为 4 的隐空间图像。比如输入 $512 \times 512 \times 3$ 的图像会被自编码器编码成 $64 \times 64 \times 4$。而近期有些工作发现，这个自编码器不够好，提升隐空间的通道数能够提升自编码器的重建效果。因此，SD3 把隐空间图像的通道数从 $4$ 改为了 $16$。</p>
<h3 id="多模态-DiT-MM-DiT"><a href="#多模态-DiT-MM-DiT" class="headerlink" title="多模态 DiT (MM-DiT)"></a>多模态 DiT (MM-DiT)</h3><p>SD3 的去噪模型是一个 Diffusion Transformer (DiT)。如果去噪模型只有带噪图像这一种输入的话，DiT 则会是一个结构非常简单的模型，和标准 ViT 一样：图像过图块化层 (Patching) 并与位置编码相加，得到序列化的数据。这些数据会像标准 Transformer 一样，经过若干个子模块，再过反图块层得到模型输出。DiT 的每个子模块 DiT-Block 和标准 Transformer 块一样，由 LayerNorm, Self-Attention, 一对一线性层 (Pointwise Feedforward, FF) 等模块构成。</p>
<blockquote>
<p>图块化层会把 $2\times2$ 个像素打包成图块，反图块化层则会把图块还原回像素。</p>
</blockquote>
<p><img src="/2024/07/14/20240703-SD3/6.jpg" alt></p>
<p>然而，扩散模型中的去噪网络一定得支持带约束生成。这是因为扩散模型约束于去噪时刻 $t$。此外，作为文生图模型，SD3 还得支持文本约束。DiT 及本文的 MM-DiT 把模型设计的重点都放在了处理额外约束上。</p>
<p>我们先看一下模块是怎么处理较简单的时刻约束的。此处，如下图所示，SD3 的模块保留了 DiT 的设计，用自适应 LayerNorm (Adaptive LayerNorm, AdaLN) 来引入额外约束。具体来说，过了 LayerNorm 后，数据的均值、方差会根据时刻约束做调整。另外，过完 Attention 层或 FF 层后，数据也会乘上一个和约束相关的系数。</p>
<p><img src="/2024/07/14/20240703-SD3/7.jpg" alt></p>
<p>我们再来看文本约束的处理。文本约束以两种方式输入进模型：与时刻编码拼接、在注意力层中融合。具体数据关联细节可参见下图。如图所示，为了提高 SD3 的文本理解能力，描述文本 (“Caption”) 经由三种编码器编码，得到两组数据。一组较短的数据会经由 MLP 与文本编码加到一起；另一组数据会经过线性层，输入进 Transformer 的主模块中。</p>
<blockquote>
<p>将约束编码与时刻编码相加是一种很常见的做法。此前 U-Net 去噪网络中处理简单约束（如 ImageNet 类型约束）就是用这种方法。</p>
</blockquote>
<p><img src="/2024/07/14/20240703-SD3/8.jpg" alt></p>
<p>SD3 的 DiT 的子模块结构图如下所示。我们可以分几部分来看它。先看时刻编码 $y$ 的那些分支。和标准 DiT 子模块一样，$y$ 通过修改 LayerNorm 后数据的均值、方差及部分层后的数据大小来实现约束。再看输入的图像编码 $x$ 和文本编码 $c$。二者以相同的方式做了 DiT 里的 LayerNorm, FF 等操作。不过，相比此前多数基于 DiT 的模型，此模块用了一种特殊的融合注意力层。具体来说，在过注意力层之前，$x$ 和 $c$ 对应的 $Q, K, V$ 会分别拼接到一起，而不是像之前的模型一样，$Q$ 来自图像，$K, V$ 来自文本。过完注意力层，输出的数据会再次拆开，回到原本的独立分支里。由于 Transformer 同时处理了文本、图像的多模态信息，所以作者将模型取名为 MM-DiT (Multimodal DiT)。</p>
<p><img src="/2024/07/14/20240703-SD3/9.jpg" alt></p>
<blockquote>
<p>论文里讲:「这个结构可以等价于两个模态各有一个 Transformer，但是在注意力操作时做了拼接，使得两种表示既可以在独自的空间里工作也可以考虑到另一个表示。」然而，我不太喜欢这种尝试去凭空解读神经网络中间表示的表述。仅从数据来源来看，过了一个注意力层后，图像信息和文本信息就混在了一起。你很难说，也很难测量，之后的 $x$ 主要是图像信息，$c$ 主要是文本信息。只能说 $x, c$ 都蕴含了多模态的信息。之前 SD U-Net 里的 $x, c$ 可以认为是分别包含了图像信息和文本信息，因为之前的 $x$ 保留了二维图像结构，而 $c$ 仅由文本信息决定。</p>
</blockquote>
<h3 id="比例可变的位置编码"><a href="#比例可变的位置编码" class="headerlink" title="比例可变的位置编码"></a>比例可变的位置编码</h3><p>此前多数方法在使用类 ViT 架构时，都会把图像的图块从左上到右下编号，把二维图块拆成一维序列，再用这种一维位置编码来对待图块。</p>
<p><img src="/2024/07/14/20240703-SD3/10.jpg" alt></p>
<p>这样做有一个很大的坏处：生成的图像的分辨率是无法修改的。比如对于上图，假如采样时输入大小不是 $4 \times 4$，而是 $4 \times 5$，那么 $0$ 号图块的下面就是 $5$ 而不是 $4$ 了，模型训练时学习到的图块之间的位置关系全部乱套。</p>
<p>解决此问题的方法很简单，只需要将一维的编码改为二维编码。这样 Transformer 就不会搞混二维图块间的关系了。</p>
<p><img src="/2024/07/14/20240703-SD3/11.jpg" alt></p>
<p>SD3 的 MM-DiT 一开始是在 $256^2$ 固定分辨率上训练的。之后在高分辨率图像上训练时，开发者用了一些巧妙的位置编码设置技巧，让不同比例的高分辨率图像也能共享之前学到的这套位置编码。详细公式请参见原论文。</p>
<h3 id="训练数据预处理"><a href="#训练数据预处理" class="headerlink" title="训练数据预处理"></a>训练数据预处理</h3><p>看完了模块设计，我们再来看一下 SD3 在训练中的一些额外设计。在大规模训练前，开发者用三个方式过滤了数据：</p>
<ol>
<li>用了一个 NSFW 过滤器过滤图片，似乎主要是为了过滤色情内容。</li>
<li>用美学打分器过滤了美学分数太低的图片。</li>
<li>移除了看上去语义差不多的图片。</li>
</ol>
<p>虽然开发者们自信满满地向大家介绍了这些数据过滤技术，但根据社区用户们的反馈，可能正是因为色情过滤器过分严格，导致 SD3 经常会生成奇怪的人体。</p>
<p>由于在训练 LDM 时，自编码器和文本编码器是不变的，因此可以提前处理好所有训练数据的图像编码和文本编码。当然，这是一项非常基础的工程技巧，不应该写在正文里的。</p>
<h3 id="用-QK-归一化提升训练稳定度"><a href="#用-QK-归一化提升训练稳定度" class="headerlink" title="用 QK 归一化提升训练稳定度"></a>用 QK 归一化提升训练稳定度</h3><p>按照之前高分辨率文生图模型的训练方法，SD3 会先在 $256^2$ 的图片上训练，再在高分辨率图片上微调。然而，开发者发现，开始微调后，混合精度训练常常会训崩。根据之前工作的经验，这是由于注意力输入的熵会不受控制地增长。解决方法也很简单，只要在做注意力计算之前对 Q, K 做一次归一化就行，具体做计算的位置可以参考上文模块图中的 “RMSNorm”。不过，开发者也承认，这个技巧并不是一个长久之策，得具体问题具体分析。看来这种 DiT 模型在大规模训练时还是会碰到许多训练不稳定的问题，且这些问题没有一个通用解。 </p>
<h3 id="哪种扩散模型训练目标最适合文生图任务？"><a href="#哪种扩散模型训练目标最适合文生图任务？" class="headerlink" title="哪种扩散模型训练目标最适合文生图任务？"></a>哪种扩散模型训练目标最适合文生图任务？</h3><p>最后我们来看论文的实验结果部分。首先，为了寻找最好的扩散模型/流匹配模型，开发者开展了一场声势浩大的实验。实验涉及 61 种训练公式，其中的可变项有：</p>
<ul>
<li>对于普通扩散模型，考虑 $\epsilon$- 或 $\mathbf{v}$-prediction，考虑线性或 cosine 噪声调度。</li>
<li>对于整流，考虑不同的噪声调度。</li>
<li>对于 EDM，考虑不同的噪声调度，且尽可能与整流的调度机制相近以保证可比较。</li>
</ul>
<p>在训练时，除了训练目标公式可变外，优化算法、模型架构、数据集、采样器都不可变。所有模型在 ImageNet 和 CC12M 数据集上训练，在 COCO-2014 验证集上评估 FID 和 CLIP Score。根据评估结果，可以选出每个模型的最优停止训练的步数。基于每种目标下的最优模型，开发者对模型进行最后的排名。由于在最终评估时，仍有采样步数、是否使用 EMA 模型等可变采样配置，开发者在所有 24 种采样配置下评估了所有模型，并用一种算法来综合所有采样配置的结果，得到一个所有模型的最终排名。最终的排名结果如下面的表 1 所示。训练集上的一些指标如表 2 所示。</p>
<p><img src="/2024/07/14/20240703-SD3/12.jpg" alt></p>
<p>根据实验结果，我们可以得到一些直观的结论：整流领先于扩散模型。惊人的是，较新推出的 EDM 竟然没有战胜早期的 LDM (“eps/linear”)。</p>
<p>当然，我个人认为，应该谨慎看待这份实验结果。一般来说，大家做图像生成会用一个统一的指标，比如 ImageNet 上的 FID。这篇论文相当于是新提出了一种昂贵的评价方法。这种评价方法是否合理，是否能得到公认还犹未可知。另外，想说明一个生成模型的拟合能力不错，用 ImageNet 上的 FID 指标就足够有说服力了，大家不会对一个简单的生成模型有太多要求。然而，对于大型文生图模型，大家更关心的是模型的生成效果，而 FID 和 CLIP Score 并不能直接反映文生图模型的质量。因此，光凭这份实验结果，我们并不能说整流一定比之前的扩散模型要好。</p>
<p>会关注这份实验结果的应该都是公司里的文生图开发者。我建议体量小的公司直接参考这份实验结果，无脑使用整流来代替之前的训练目标。而如果有能力做同等级的实验的话，则不应该错过改良后的扩散模型，如最新的 EDM2，说不定以后还会有更好的文生图训练目标。</p>
<h3 id="参数扩增实验结果"><a href="#参数扩增实验结果" class="headerlink" title="参数扩增实验结果"></a>参数扩增实验结果</h3><p>现在多数生成模型都会做参数扩增实验，即验证模型表现随参数量增长而增长，确保模型在资源足够的情况下可以被训练成「大模型」。SD3 也做了类似的实验。开发者用参数 $d$ 来控制 MM-DiT 的大小，Transformer 块的个数为 $d$，且所有特征的通道数与 $d$ 成正比。开发者在 $256^2$ 的数据上训练了所有模型 500k 步，每 50k 步在 CoCo 数据集上统计验证误差。最终所有评估指标如下图所示。可以说，所有指标都表明，模型的表现的确随参数量增长而增长。更多结果请参见论文。</p>
<p><img src="/2024/07/14/20240703-SD3/13.jpg" alt></p>
<h2 id="Diffusers-源码阅读"><a href="#Diffusers-源码阅读" class="headerlink" title="Diffusers 源码阅读"></a>Diffusers 源码阅读</h2><h3 id="测试脚本"><a href="#测试脚本" class="headerlink" title="测试脚本"></a>测试脚本</h3><p>我们来阅读一下 SD3 在最流行的扩散模型框架 Diffusers 中的源码。在读源码前，我们先来跑通官方的示例脚本。</p>
<p>由于使用协议的限制，SD3 的环境搭起来稍微有点麻烦。首先，我们要确保 Diffuers 和 Transformers 都用的是最新版本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade diffusers transformers</span><br></pre></td></tr></table></figure>
<p>之后，我们要注册 HuggingFace 账号，再在 SD3 的模型网站 <code>https://huggingface.co/stabilityai/stable-diffusion-3-medium</code> 里确认同意某些使用协议。之后，我们要设置 Access Token。具体操作如下所示，先点右上角的 “settings”，再点左边的 “Access Tokens”，创建一个新 token。将这个 token 复制保存在本地后，点击 token 右上角选项里的 “Edit Permission”，在权限里开启 “… public gated repos …”。</p>
<p><img src="/2024/07/14/20240703-SD3/14.jpg" alt></p>
<p>最后，我们用命令行登录 HuggingFace 并使用 SD3。先用下面的命令安装 HuggingFace 命令行版。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U &quot;huggingface_hub[cli]&quot;</span><br></pre></td></tr></table></figure>
<p>再输入 <code>huggingface-cli login</code>，命令行会提示输入 token 信息。把刚刚保存好的 token 粘贴进去，即可完成登录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br><span class="line"></span><br><span class="line">Enter your token (input will not be visible): 在这里粘贴 token</span><br></pre></td></tr></table></figure>
<p>做完准备后，我们就可以执行下面的测试脚本了。注意，该脚本会自动下载模型，我们需要保证当前环境能够访问 HuggingFace。执行完毕后，生成的 $1024 \times 1024$ 大小的图片会保存在 <code>tmp.png</code> 里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableDiffusion3Pipeline</span><br><span class="line"></span><br><span class="line">pipe = StableDiffusion3Pipeline.from_pretrained(</span><br><span class="line">    <span class="string">&quot;stabilityai/stable-diffusion-3-medium-diffusers&quot;</span>, torch_dtype=torch.float16)</span><br><span class="line">pipe = pipe.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line">image = pipe(</span><br><span class="line">    <span class="string">&quot;A cat holding a sign that says hello world&quot;</span>,</span><br><span class="line">    negative_prompt=<span class="string">&quot;&quot;</span>,</span><br><span class="line">    num_inference_steps=<span class="number">28</span>,</span><br><span class="line">    guidance_scale=<span class="number">7.0</span>,</span><br><span class="line">).images[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">image.save(<span class="string">&#x27;tmp.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>我得到的图片如下所示。看起来 SD3 理解文本的能力还是挺强的。</p>
<p><img src="/2024/07/14/20240703-SD3/15.jpg" alt></p>
<h3 id="模型组件"><a href="#模型组件" class="headerlink" title="模型组件"></a>模型组件</h3><p>接下来我们来快速浏览一下 SD3 流水线 <code>StableDiffusion3Pipeline</code> 的源码。在 IDE 里使用源码跳转功能可以在 <code>diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py</code> 里找到该类的源码。</p>
<p>通过流水线的 <code>__init__</code> 方法，我们能知道 SD3 的所有组件。组件包括自编码器 <code>vae</code>, MM-DiT <code>Transformer</code>, 流匹配噪声调度器 <code>scheduler</code>，以及三个文本编码器。每个编码器由一个 tokenizer 和一个 text encoder 组成.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    transformer: SD3Transformer2DModel,</span></span></span><br><span class="line"><span class="params"><span class="function">    scheduler: FlowMatchEulerDiscreteScheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">    vae: AutoencoderKL,</span></span></span><br><span class="line"><span class="params"><span class="function">    text_encoder: CLIPTextModelWithProjection,</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">    text_encoder_2: CLIPTextModelWithProjection,</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer_2: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">    text_encoder_3: T5EncoderModel,</span></span></span><br><span class="line"><span class="params"><span class="function">    tokenizer_3: T5TokenizerFast,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br></pre></td></tr></table></figure>
<p><code>vae</code> 的用法和之前 SD 的一模一样，编码时用 <code>vae.encode</code> 并乘 <code>vae.config.scaling_factor</code>，解码时除以 <code>vae.config.scaling_factor</code> 并用 <code>vae.decode</code>。</p>
<p>文本编码器的用法可以参见 <code>encode_prompt</code> 方法。文本会分别过各个编码器的 tokenizer 和 text encoder，得到三种文本编码，并按照论文中的描述拼接成两种约束信息。这部分代码十分繁杂，多数代码都是在处理数据形状，没有太多有价值的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_prompt</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        prompt,</span></span></span><br><span class="line"><span class="params"><span class="function">        prompt_2,</span></span></span><br><span class="line"><span class="params"><span class="function">        prompt_3,</span></span></span><br><span class="line"><span class="params"><span class="function">        device,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_images_per_prompt,</span></span></span><br><span class="line"><span class="params"><span class="function">        do_classifier_free_guidance,</span></span></span><br><span class="line"><span class="params"><span class="function">        negative_prompt,</span></span></span><br><span class="line"><span class="params"><span class="function">        negative_prompt_2,</span></span></span><br><span class="line"><span class="params"><span class="function">        negative_prompt_3,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> prompt_embeds, negative_prompt_embeds,</span><br><span class="line">     pooled_prompt_embeds, negative_pooled_prompt_embeds</span><br></pre></td></tr></table></figure>
<h3 id="采样流水线"><a href="#采样流水线" class="headerlink" title="采样流水线"></a>采样流水线</h3><p>我们再来通过阅读流水线的 <code>__call__</code> 方法了解 SD3 采样的过程。由于 SD3 并没有修改 LDM 的这套生成框架，其采样流水线和 SD 几乎完全一致。SD3 和 SD 的 <code>__call__</code> 方法的主要区别是，生成文本编码时会生成两种编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(</span><br><span class="line">    prompt_embeds,</span><br><span class="line">    negative_prompt_embeds,</span><br><span class="line">    pooled_prompt_embeds,</span><br><span class="line">    negative_pooled_prompt_embeds,</span><br><span class="line">) = self.encode_prompt(...)</span><br></pre></td></tr></table></figure>
<p>在调用去噪网络时，那个较小的文本编码 <code>pooled_prompt_embeds</code> 会作为一个额外参数输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">noise_pred = self.transformer(</span><br><span class="line">    hidden_states=latent_model_input,</span><br><span class="line">    timestep=timestep,</span><br><span class="line">    encoder_hidden_states=prompt_embeds,</span><br><span class="line">    pooled_projections=pooled_prompt_embeds,</span><br><span class="line">    joint_attention_kwargs=self.joint_attention_kwargs,</span><br><span class="line">    return_dict=<span class="literal">False</span>,</span><br><span class="line">)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="MM-DiT-去噪模型"><a href="#MM-DiT-去噪模型" class="headerlink" title="MM-DiT 去噪模型"></a>MM-DiT 去噪模型</h3><p>相比之下，SD3 的去噪网络 MM-DiT 的改动较大。我们来看一下对应的 <code>SD3Transformer2DModel</code> 类，它位于文件 <code>diffusers\models\transformers\transformer_sd3.py</code>。</p>
<p>类的构造函数里有几个值得关注的模块：二维位置编码类 <code>PatchEmbed</code>、组合时刻编码和文本编码模块 <code>CombinedTimestepTextProjEmbeddings</code>、主模块类 <code>JointTransformerBlock</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    self.pos_embed = PatchEmbed(...)</span><br><span class="line">    self.time_text_embed = CombinedTimestepTextProjEmbeddings(...)</span><br><span class="line">    ...</span><br><span class="line">    self.transformer_blocks = nn.ModuleList(</span><br><span class="line">          [</span><br><span class="line">              JointTransformerBlock(..)</span><br><span class="line">              <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.num_layers)</span><br><span class="line">          ]</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>
<p>类的前向传播函数 <code>forward</code> 里都是比较常规的操作。数据会依次经过前处理、若干个 Transformer 块、后处理。所有实现细节都封装在各个模块类里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">...</span>):</span></span><br><span class="line">    hidden_states = self.pos_embed(hidden_states)</span><br><span class="line">    temb = self.time_text_embed(timestep, pooled_projections)</span><br><span class="line">    encoder_hidden_states = self.context_embedder(encoder_hidden_states)</span><br><span class="line">    <span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.transformer_blocks):</span><br><span class="line">       encoder_hidden_states, hidden_states = block(...)</span><br><span class="line">    </span><br><span class="line">    encoder_hidden_states, hidden_states = block(</span><br><span class="line">    hidden_states=hidden_states, encoder_hidden_states=encoder_hidden_states, temb=temb</span><br><span class="line">)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>接下来我们来看这几个较为重要的子模块。<code>PatchEmbed</code> 类的实现写在 <code>diffusers/models/embeddings.py</code> 里。这个类的实现写得非常清晰。<code>PatchEmbed</code> 类本身用于维护位置编码宽高、特征长度这些信息，计算位置编码的关键代码在 <code>get_2d_sincos_pos_embed</code> 中。<code>get_2d_sincos_pos_embed</code> 会生成 <code>(0, 0), (1, 0), ...</code> 这样的二维坐标网格，再调用 <code>get_2d_sincos_pos_embed_from_grid</code> 生成二维位置编码。<code>get_2d_sincos_pos_embed_from_grid</code> 会调用两次一维位置编码函数 <code>get_1d_sincos_pos_embed_from_grid</code>，也就是 Transformer 里那种标准位置编码生成函数，来分别生成两个方向的编码，最后拼接成二维位置编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchEmbed</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, latent</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        pos_embed = get_2d_sincos_pos_embed(...)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_2d_sincos_pos_embed</span>(<span class="params">...</span>):</span></span><br><span class="line">    grid_h = np.arange(...)</span><br><span class="line">    grid_w = np.arange(...)</span><br><span class="line">    grid = np.meshgrid(grid_w, grid_h)</span><br><span class="line">    ...</span><br><span class="line">    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_2d_sincos_pos_embed_from_grid</span>(<span class="params">...</span>):</span></span><br><span class="line">    <span class="comment"># use half of dimensions to encode grid_h</span></span><br><span class="line">    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // <span class="number">2</span>, grid[<span class="number">0</span>])  <span class="comment"># (H*W, D/2)</span></span><br><span class="line">    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // <span class="number">2</span>, grid[<span class="number">1</span>])  <span class="comment"># (H*W, D/2)</span></span><br><span class="line"></span><br><span class="line">    emb = np.concatenate([emb_h, emb_w], axis=<span class="number">1</span>)  <span class="comment"># (H*W, D)</span></span><br><span class="line">    <span class="keyword">return</span> emb</span><br></pre></td></tr></table></figure>
<p>组合时刻编码和文本编码模块 <code>CombinedTimestepTextProjEmbeddings</code> 的代码非常短。它实际上就是用通常的 <code>Timesteps</code> 类获取时刻编码，用一个 <code>text_embedder</code> 模块再次处理文本编码，最后把两个编码加起来。<br><code>text_embedder</code> 是一个线性层、激活函数、线性层构成的简单模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CombinedTimestepTextProjEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim, pooled_projection_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.time_proj = Timesteps(num_channels=<span class="number">256</span>, flip_sin_to_cos=<span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">        self.timestep_embedder = TimestepEmbedding(in_channels=<span class="number">256</span>, time_embed_dim=embedding_dim)</span><br><span class="line">        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn=<span class="string">&quot;silu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, timestep, pooled_projection</span>):</span></span><br><span class="line">        timesteps_proj = self.time_proj(timestep)</span><br><span class="line">        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=pooled_projection.dtype))  <span class="comment"># (N, D)</span></span><br><span class="line"></span><br><span class="line">        pooled_projections = self.text_embedder(pooled_projection)</span><br><span class="line"></span><br><span class="line">        conditioning = timesteps_emb + pooled_projections</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> conditioning</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PixArtAlphaTextProjection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, caption</span>):</span></span><br><span class="line">        hidden_states = self.linear_1(caption)</span><br><span class="line">        hidden_states = self.act_1(hidden_states)</span><br><span class="line">        hidden_states = self.linear_2(hidden_states)</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>MM-DiT 的主要模块 <code>JointTransformerBlock</code> 在 <code>diffusers/models/attention.py</code> 文件里。这个类的代码写得比较乱。它主要负责处理 LayerNorm 及数据的尺度变换操作，具体的注意力计算由注意力处理器 <code>JointAttnProcessor2_0</code> 负责。两处 LayerNorm 的实现方式竟然是不一样的。</p>
<p><img src="/2024/07/14/20240703-SD3/9.jpg" alt></p>
<p>我们先简单看一下构造函数里初始化了哪些模块。代码中，<code>norm1, ff, norm2</code> 等模块都是普通 Transformer 块中的模块。而加了 <code>_context</code> 的模块则表示处理文本分支 $c$ 的模块，如 <code>norm1_context, ff_context</code>。<code>context_pre_only</code> 表示做完了注意力计算后，还要不要给文本分支加上 LayerNorm 和 FeedForward。如前文所述，具体的注意力计算由 <code>JointAttnProcessor2_0</code> 负责。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointTransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_attention_heads, attention_head_dim, context_pre_only=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.context_pre_only = context_pre_only</span><br><span class="line">        context_norm_type = <span class="string">&quot;ada_norm_continous&quot;</span> <span class="keyword">if</span> context_pre_only <span class="keyword">else</span> <span class="string">&quot;ada_norm_zero&quot;</span></span><br><span class="line"></span><br><span class="line">        self.norm1 = AdaLayerNormZero(dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> context_norm_type == <span class="string">&quot;ada_norm_continous&quot;</span>:</span><br><span class="line">            self.norm1_context = AdaLayerNormContinuous(</span><br><span class="line">                dim, dim, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>, bias=<span class="literal">True</span>, norm_type=<span class="string">&quot;layer_norm&quot;</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">elif</span> context_norm_type == <span class="string">&quot;ada_norm_zero&quot;</span>:</span><br><span class="line">            self.norm1_context = AdaLayerNormZero(dim)</span><br><span class="line">        </span><br><span class="line">        processor = JointAttnProcessor2_0()</span><br><span class="line">        self.attn = Attention(</span><br><span class="line">            query_dim=dim,</span><br><span class="line">            cross_attention_dim=<span class="literal">None</span>,</span><br><span class="line">            added_kv_proj_dim=dim,</span><br><span class="line">            dim_head=attention_head_dim,</span><br><span class="line">            heads=num_attention_heads,</span><br><span class="line">            out_dim=dim,</span><br><span class="line">            context_pre_only=context_pre_only,</span><br><span class="line">            bias=<span class="literal">True</span>,</span><br><span class="line">            processor=processor,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.norm2 = nn.LayerNorm(dim, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.ff = FeedForward(dim=dim, dim_out=dim, activation_fn=<span class="string">&quot;gelu-approximate&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> context_pre_only:</span><br><span class="line">            self.norm2_context = nn.LayerNorm(dim, elementwise_affine=<span class="literal">False</span>, eps=<span class="number">1e-6</span>)</span><br><span class="line">            self.ff_context = FeedForward(dim=dim, dim_out=dim, activation_fn=<span class="string">&quot;gelu-approximate&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.norm2_context = <span class="literal">None</span></span><br><span class="line">            self.ff_context = <span class="literal">None</span>     </span><br></pre></td></tr></table></figure>
<p>我们再来看 <code>forward</code> 方法。在前向传播时，图像分支和文本分支会分别过 <code>norm1</code>，再一起过注意力操作，再分别过 <code>norm2</code> 和 <code>ff</code>。大概的代码如下所示，我把较复杂的 context 分支的代码略过了。</p>
<p>这份代码写得很不漂亮，按理说模块里两个 LayerNorm + 尺度变换 (即 Adaptive LayerNorm) 的操作是一样的，应该用同样的代码来处理。但是这个模块里 <code>norm1</code> 是 <code>AdaLayerNormZero</code> 类，<code>norm2</code> 是 <code>LayerNorm</code> 类。<code>norm1</code> 会自动做完 AdaLayerNorm 的运算，并把相关变量返回。而在 <code>norm2</code> 处，代码会先执行普通的 LayerNorm，再根据之前的变量手动调整数据的尺度。我们心里知道这份代码是在实现论文里那张结构图就好，没必要去仔细阅读。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self, hidden_states: torch.FloatTensor, encoder_hidden_states: torch.FloatTensor, temb: torch.FloatTensor</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(hidden_states, emb=temb)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.context_pre_only:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Attention.</span></span><br><span class="line">    attn_output, context_attn_output = self.attn(</span><br><span class="line">        hidden_states=norm_hidden_states, encoder_hidden_states=norm_encoder_hidden_states</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Process attention outputs for the `hidden_states`.</span></span><br><span class="line">    attn_output = gate_msa.unsqueeze(<span class="number">1</span>) * attn_output</span><br><span class="line">    hidden_states = hidden_states + attn_output</span><br><span class="line"></span><br><span class="line">    norm_hidden_states = self.norm2(hidden_states)</span><br><span class="line">    norm_hidden_states = norm_hidden_states * (<span class="number">1</span> + scale_mlp[:, <span class="literal">None</span>]) + shift_mlp[:, <span class="literal">None</span>]</span><br><span class="line">    ff_output = self.ff(norm_hidden_states)</span><br><span class="line">    ff_output = gate_mlp.unsqueeze(<span class="number">1</span>) * ff_output</span><br><span class="line"></span><br><span class="line">    hidden_states = hidden_states + ff_output</span><br><span class="line">    <span class="keyword">if</span> self.context_pre_only:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> encoder_hidden_states, hidden_states</span><br></pre></td></tr></table></figure>
<p>融合注意力的实现方法很简单。和普通的注意力计算相比，这种注意力就是把另一条数据分支 <code>encoder_hidden_states</code> 也做了 QKV 的线性变换，并在做注意力运算前与原来的 QKV 拼接起来。做完注意力运算后，两个数据又会拆分回去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointAttnProcessor2_0</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Attention processor used typically in processing the SD3-like self-attention projections.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        attn: Attention,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: torch.FloatTensor = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        *args,</span></span></span><br><span class="line"><span class="params"><span class="function">        **kwargs,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; torch.FloatTensor:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># `sample` projections.</span></span><br><span class="line">        query = attn.to_q(hidden_states)</span><br><span class="line">        key = attn.to_k(hidden_states)</span><br><span class="line">        value = attn.to_v(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># `context` projections.</span></span><br><span class="line">        encoder_hidden_states_query_proj = attn.add_q_proj(encoder_hidden_states)</span><br><span class="line">        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)</span><br><span class="line">        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention</span></span><br><span class="line">        query = torch.cat([query, encoder_hidden_states_query_proj], dim=<span class="number">1</span>)</span><br><span class="line">        key = torch.cat([key, encoder_hidden_states_key_proj], dim=<span class="number">1</span>)</span><br><span class="line">        value = torch.cat([value, encoder_hidden_states_value_proj], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the attention outputs.</span></span><br><span class="line">        hidden_states, encoder_hidden_states = (</span><br><span class="line">            hidden_states[:, : residual.shape[<span class="number">1</span>]],</span><br><span class="line">            hidden_states[:, residual.shape[<span class="number">1</span>] :],</span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们学习了 SD3 论文及源码中的主要内容。相比于 SD，SD3 做了两项较大的改进：用整流代替原来的 DDPM 中的训练目标；将去噪模型从 U-Net 变成了能更好地处理多模态信息的 MM-DiT。SD3 还在模型结构和训练目标上做了许多小改进，如调整训练噪声采样分布、使用二维位置编码。SD3 论文展示了多项大型消融实验的结果，证明当前的 SD3 是以最优配置训练得到的。SD3 可以在 Diffusers 中使用。当然，由于 SD3 的使用协议较为严格，我们需要做一些配置，才能在代码中使用 SD3。SD3 的采样流水线基本没变，原来 SD 的多数编辑方法能够无缝迁移过来。而 SD3 的去噪模型变动较大，和 U-Net 相关的编辑方法则无法直接用过来。在学习源码时，主要值得学习的是新 MM-DiT 模型中每个 Transformer 层的实现细节。</p>
<p>尽管 SD3 并没有提出新的流匹配方法，但其实验结果表明流匹配模型可能更适合文生图任务。作为研究者，受此启发，我们或许需要关注一下整流等流匹配模型，知道它们的思想，分析它们与原扩散模型训练目标的异同，以拓宽自己的视野。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/06/24/20240622-CVPR2024/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/06/24/20240622-CVPR2024/" class="post-title-link" itemprop="url">顽抗生活中的厄运</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-24 19:18:57" itemprop="dateCreated datePublished" datetime="2024-06-24T19:18:57+08:00">2024-06-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E8%AE%B0%E5%8F%99%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">记叙文</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今年第一次参加 CVPR 的前后发生了很多糟心事。我被气得受不了了。</p>
<hr>
<p>美国签证不太好申，我周围所有因参加学术会议而申请签证的人都被审查了一个月。我决定参会与申请签证的时间较晚，过签的时间非常仅限。在会议开始前的倒数第二周，我收到了签证通过的通知，并得知我需要在一周后，也就是会议开始前的最后几个工作日领取带了美国签证的护照。能够恰好拿到签证，算得上是运气不错。可下一周发生的种种事情，却谈不上了幸运了。</p>
<p>会议开始前一周的周三早上，我赶早骑行前往学校参加会议前的最后一次周会。骑至半途，忽然天降暴雨。在新加坡，这种现象我已经见怪不怪了。我先穿上日常携带的雨衣，再将书包背到雨衣外，以免背上出太多汗。我又用手机向导师报告我可能会推迟参会，随后将手机和浸满了雨水的眼镜装进口袋。</p>
<p>这场雨就像老天给我开的一个玩笑。我一到目的地，雨就小了下来。上楼，掀开笔记本电脑，发现电脑被穿透了书包的一点雨水浸湿，开不了机。伸手一摸口袋，眼镜竟然也不见了。</p>
<p>这种雨中骑车的事我已经做过好多回了，但我从来没碰到过这么多意外。我有多次电脑进水的经历，可这是我第一次知道穿透书包的那一点水也能弄坏电脑。另外，本来我这条裤子的口袋是不会掉东西的，可这次恰好我要用手机发消息，而眼镜和手机放在了一起，顺势掉了出来。</p>
<p>无论多么令人懊恼，生活都得继续。我用上了我所有的规划能力来处理问题。我先和导师商议将周会延期。随后，我请同学陪我一起找眼镜。还好，眼镜在楼下就找到了，但是被我自己或者其他人踩了一脚，一边的镜框断了。我艰难地用半副眼镜处理完一天的杂事，于傍晚赶往市中心将电脑送修。晚上回家，我用强力胶将断裂的镜框部件粘在一起，眼镜勉强可以用了。第二天周四，我领完美国签证，同时得知电脑严重进水，一天修不好。我回家立刻拿出老电脑并配好工作环境。周五，我准备好一切出国开会所需资料，总算安顿好了一切。</p>
<p>周六，出发前一天，我去换美元。付款时要输入储蓄卡密码，我多次尝试也没能把密码输对，卡被冻结。这时我才想起来为什么我会不记得卡的密码：这张卡的密码是系统随机设置的，平时用卡根本不需要密码，所以我既忘了密码，也忘了密码不是我自己设的。这下去美国用不了这张卡了。还好，我还有国内的信用卡，完成了在新加坡后续的付款。</p>
<p>启程的飞机将于周日早上九点起飞。我起了个大早，提前前往机场。兴冲冲地首次坐上长途国际航班后，我立刻收到了飞机因故障无法起飞的通知，灰溜溜地下了飞机。航空公司给我安排的新航班是 36 小时之后，周一晚上九点。为表赔偿，航空公司给我们预订了新加坡五星级酒店的住宿，并附带了早中晚餐各 20 新币的代金券。走进五星级酒店最普通的房间，我原以为能享受一番，却又被正餐 30 新币起步（还不含税）的菜单气晕了过去。上午我在机场吃过了，中午我就随便点了个 20 新币的汤应付了过去。晚上我兼顾价格与饱腹度，点了一个 30 多新币的披萨。晚餐送至房间，我将代金券和 20 新币交给服务员后，服务员将代金券收走，现金还回，说道：“可以拿明天的早餐钱来抵这顿晚饭。”唉，我又亏了将近 10 新币，早知道点一个更好的披萨了。</p>
<p>周一，白天没有安排，我决定去解决新加坡储蓄卡的事。手机应用有挂失换卡的功能，却楞是没有申请解冻的功能。我只好顶着烈日，跨越条条街道，又穿梭于人山人海的购物中心，找到银行的支行。这个支行只有通过视频交流的虚拟柜台。为了确认身份，柜员问了我三个有关账户的验证问题。有一个问题的华文名词我听不懂，没答上来。因此，卡解冻失败，柜员建议我前往线下柜台。我真是纳闷，我有银行账户的一切访问权限，有身份证、护照，为什么要用这种方法来验证我的身份。但我今天时间多，不跟你们计较了。我又匆忙坐上满载的地铁，赶往下一处有线下柜台的支行。地图上说银行 4 点关门，我是三点半到的，可银行已经关门了。我突然感到一股违和感：今天是周一，为什么地铁那么多人？为什么购物中心那么多人？为什么银行提早下班？我一查，果然如我所料，原来今天是新加坡法定假日。今天，到美国开会的同学在当地开开心心地旅游，留在本地的人则能享受假期。就我一个人，飞机没坐上，假没休息到，事情没办成，从一对对情侣旁边擦身而过，在我本不该在的繁华市区里奔波。没办法，休息一下，晚上早点去坐飞机吧。</p>
<p>晚上，刚进机场，我的镜框因没有粘牢又断开了。还好我早有准备，拿出透明胶勉强固定了镜框。</p>
<p>过安检，我被选为 SSSS 级“幸运”用户，被安检警察拎到队伍前面，进行细致的搜身检查。也好，我也不会带什么危险物品，顺便插了个队。</p>
<blockquote>
<p>SSSS 是美国TSA （美国运输安全管理局）随机选取需要接受二次安检的乘客。安检人员看到SSSS的登机牌后会对乘客格外注意，在经过普通的X光扫描检查以后，行李会被拆开进行人工检查，人工彻底搜身，电子产品会接受爆炸物探测。</p>
</blockquote>
<p>上了飞机，我总算坐上了有着小屏幕的美联航高级国际航班。航班要航行 15 个小时，我计划在这段时间里写一篇文章。一看时间还早，我就开了两把小游戏。玩到一半，电脑啪地屏幕一黑，没电了。飞机上的插座怎么充不进电啊？过了好一会儿，机长发来广播：“很抱歉，有乘客反映插座失灵，我们将重启设备，尽快修好。（翻译自英文）”我满怀期待地等着电脑的开机，这一等就等到了飞机降落。临走前，贴心的乘务长向大家诚恳地致歉：“今天有一些旅客一直没有接上插座，我们真是深表歉意呢~（翻译自英文）”</p>
<p>第一趟飞机在旧金山降落，我还需要乘坐中转西雅图的飞机。由于下一站是国内航班，我需要在这里完成入境。入境的队伍很长，一眼望去，四五十分钟都不见得能排完队。帮我安排航班的工作人员想必之前是一位极限运动的教练，给我安排了一小时后结束登机、一个半小时后起飞的第二趟航班，让我在这里锻炼极限的时间管理。排队五十分钟后，我极速应付完安检人员的问题，总算顺利入境。眼下飞机只有十分钟就要停止登机了，在这种极限条件下，我的身体潜能与语言表达潜能被猛然激活。我背着沉甸甸的书包——我唯一的行李，仔细地看遍每一个路标，在机场里沿着最短路线狂奔，流畅地办理登机、安检。我这一个多小时一直没来得及上厕所。跑至空荡荡的登机口前，我遗憾地望了一眼对面的厕所后，急忙向柜台后的空乘人员出示机票。“啊！你很幸运啊！快点上飞机。”在我的极限运营下，我成功在起飞前登机，就是肚子下面不太舒服。到了原定的起飞时间，我正双目紧闭静待起飞，忽然听见了机长包含温情的广播：“我们得知有部分旅客还在转机，飞机将在约十五分钟后起飞。”不早说啊！我这么赶是为了什么啊！快把厕所门打开！</p>
<p>飞机起飞，我如愿以偿地解了个手，满足地回座位睡了一觉。可能是剧烈运动导致新陈代谢加快，醒来后，我又是一阵内急。可又好巧不巧，飞机刚开始降落，我还要等半个多小时才能下飞机上厕所。在这段时间里，我仿佛领悟了长生不老之道，一分钟可以当十分钟来过。我思考了我为什么这么难受，为什么上这趟飞机，为什么会存在这个世界上。恍惚的精神慢慢回归，我规划出了飞机降落后最快的下机方式。总算到站了，舱门一打开，我就背起书包，右手紧扣腹部，面露难色，名正言顺地在下机的队伍里插队。一路小跑到厕所前，这下没人看，不用演戏了。可我的手却怎么都不肯从肚子上松开——哦，原来我真的憋得肚子不舒服了。好在厕所只有一步之遥了。我特意找了个高级包厢，坐下来不紧不慢地上起了厕所。这是我第一次憋得这么难受，也是第一次觉得上厕所是人生中最幸福的事。</p>
<p>当地时间凌晨两点，我总算来到了西雅图机场出口。没有飞机要赶，没有厕所要上，充足休息了近 20 个小时的我，即将迎来光明的前程。我没有办理美国的流量，因为我知道在需要用网络的地方都有免费无线网络。按照网上的攻略，我前往停车场的某处，准备用 Uber 打车。一到停车场，凌冽的寒气扑面而来。这冷空调开得有点大啊。不对！这是室外的正常气温。我来美国之前是看了天气预报的，每天的温度都是 10 到 20 多度。我过了太久的夏天，对 10 度的天气没有概念，也没有料到我大晚上还会待在室外。但没事啊，打上车就好了。我在机场就配置好了 Uber，只差呼叫司机这一步了。欸，怎么付款前还要验证手机号？这里信号很差，接收国内手机的短信要三分钟左右。当然，这个延迟是我在多次发送短信后才意识到的。由于我多次输入了上一次短信的验证码，Uber 最后忍无可忍，把我的账号禁用了。寒风中，我身着短袖短裤，在空荡的候车处死盯没有回应的手机，打着牙颤，默默无语。冷静了一会儿，我想我有信用卡有钱，为什么非得用软件打车不可。于是，我走到了普通打车处，稍等片刻，上车，最终总算抵达了酒店。</p>
<p>第一次出国开会，第一次前往美国旅行。对于多数人来说快乐的事情，到我这为什么就成了渡劫呢？这些事或许算不上什么人生大事，但攒在一起总是会令人烦恼。诚然，有些事是我考虑不周，但大多数事是我完全无法控制的。不过这些都没关系。在麻将等概率游戏中搏杀已久的我知道，过去是无法改变的，结果通常是无法控制的，我们能做的只有在此刻寻找问题的最优解。哪怕是碰到了这么多事，我还是拿出了我玩解谜游戏的所有实力，尽力去处理好每一件事。<del>如果还是对生活的不幸感到愤愤不平，不如带着幸灾乐祸的心理想一想我这次事件里的美联航。他们的航班延误，影响了那么多乘客，最后给每个人都赔了一晚的五星级酒店，还不是把问题解决了。</del></p>
<hr>
<p>以上是我头脑冷静时写的东西。参会过程省略以示愤怒。</p>
<hr>
<p>本来我准备了很多要写的东西，回程的时候又被气到了，累死了，不写了。这“一天”里，我提前3小时到机场，坐2小时飞机，等5小时飞机，又坐15小时飞机。累计睡眠5~6乘1个小时。到了新加坡我还想省钱，没打车，在机场走了半小时，又坐了1小时地铁。下地铁后走回家，莫名其妙绕了路，在太阳底下穿长裤走了1个多小时。好不容易回到家，洗完澡想刷个牙，牙膏还没了。真作假时假亦真，没睡醒时醒亦困。看上去我这几天睡了很多次觉，但我根本没有睡醒。算上在美国的时间，我已经颠沛流离了好几天，各种硬抗时差，已经不知道睡够六小时是怎样的感觉了。别人比我早一两天来，晚一两天走，玩得开开心心。我除了在会场当3天“好学生”外，剩下近一周时间全在参与人生的极限挑战。怪不得学术大佬都不愿意出来开会，从明年开始我也是学术大佬了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/06/05/20240407-SVD-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/06/05/20240407-SVD-1/" class="post-title-link" itemprop="url">Stable Video Diffusion 源码解读 (Diffusers 版)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-05 15:52:35" itemprop="dateCreated datePublished" datetime="2024-06-05T15:52:35+08:00">2024-06-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在上篇文章中，我们浏览了 Stable Video Diffusion (SVD) 的论文，并特别学习了没有在论文中提及的模型结构、噪声调度器这两个模块。在这篇文章中，让我们来看看 SVD 在 Diffusers 中的源码实现。我们会先学习 SVD 的模型结构，再学习 SVD 的采样流水线。在本文的多数章节中，我都会将 SVD 的结构与 Stable Diffusion (SD) 的做对比，帮助之前熟悉 SD 的读者快速理解 SVD 的性质。强烈建议读者在阅读本文前先熟悉 SD 及其在 Diffusers 中的实现。</p>
<p><a href="https://zhouyifan.net/2024/01/23/20230713-SD3/">Stable Diffusion Diffusers 实现源码解读</a></p>
<h2 id="简单采样实验"><a href="#简单采样实验" class="headerlink" title="简单采样实验"></a>简单采样实验</h2><p>目前开源的 SVD 仅有图生视频模型，即给定视频首帧，模型生成视频的后续内容。在首次开源时，SVD 有 1.0 和 1.0-xt 两个版本。二者模型结构配置相同，主要区别在于训练数据上。SVD 1.0 主要用于生成 14 帧 576x1024 的视频，而 1.0-xt 版本由 1.0 模型微调而来，主要用于生成 25 帧 576x1024 的视频。后来，开发团队又开源了 SVD 1.1-xt，该模型在固定帧率的视频数据上微调，输出视频更加连贯。为了做实验方便，在这篇文章中，我们将使用最基础的 SVD 1.0 模型。</p>
<p>参考 Diffusers 官方文档: <a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/main/en/using-diffusers/svd">https://huggingface.co/docs/diffusers/main/en/using-diffusers/svd</a> ，我们来创建一个关于 SVD 的 “Hello World” 项目。如果你的电脑可以访问 HuggingFace 原站的话，直接运行下面的脚本就行了；如果不能访问原网站，可以尝试取消代码里的那行注释，访问 HuggingFace 镜像站；如果还是不行，则需要手动下载 “stabilityai/stable-video-diffusion-img2vid” 仓库，并将仓库路径改成本地下载的仓库路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os.environ[&#x27;HF_ENDPOINT&#x27;] = &#x27;https://hf-mirror.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableVideoDiffusionPipeline</span><br><span class="line"><span class="keyword">from</span> diffusers.utils <span class="keyword">import</span> load_image, export_to_video</span><br><span class="line"></span><br><span class="line">pipe = StableVideoDiffusionPipeline.from_pretrained(</span><br><span class="line">    <span class="string">&quot;stabilityai/stable-video-diffusion-img2vid&quot;</span>, torch_dtype=torch.float16, variant=<span class="string">&quot;fp16&quot;</span></span><br><span class="line">)</span><br><span class="line">pipe.enable_model_cpu_offload()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the conditioning image</span></span><br><span class="line">image = load_image(<span class="string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png&quot;</span>)</span><br><span class="line">image = image.resize((<span class="number">1024</span>, <span class="number">576</span>))</span><br><span class="line"></span><br><span class="line">generator = torch.manual_seed(<span class="number">42</span>)</span><br><span class="line">frames = pipe(image, decode_chunk_size=<span class="number">8</span>, generator=generator).frames[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">export_to_video(frames, <span class="string">&quot;generated.mp4&quot;</span>, fps=<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<p>成功运行后，我们能得到这样的一个火箭升空视频。它的第一帧会和我们的输入图片一模一样。</p>
<p><img src="/2024/06/05/20240407-SVD-1/rocket.gif" alt></p>
<h2 id="SVD-概览"><a href="#SVD-概览" class="headerlink" title="SVD 概览"></a>SVD 概览</h2><p>由于 SVD 并没有在论文里对其图生视频模型做详细的介绍，我们没有官方资料可以参考，只能靠阅读源码来了解 SVD 的实现细节。为了让大家在读代码时不会晕头转向，我会在读代码前简单概述一下 SVD 的模型结构和采样方法。</p>
<p>SVD 和 SD 一样，是一个隐扩散模型（Latent Diffusion Model, LDM）。图像（视频帧）的生成由两个阶段组成：先由扩散模型生成压缩图像，再由 VAE 解码成真实图像。</p>
<p>扩散模型在生成图像时，会用一个去噪 U-Net $\epsilon_\theta$ 反复对纯噪声图像 $z_T$ 去噪，直至得到一幅有意义的图片 $z$。为了让模型输出我们想要的图像，我们会用一些额外的信息来约束模型，或者说将约束信息也输入进 U-Net。对于文生图 SD 来说，额外约束是文本。对于图生视频 SVD 来说，额外约束是图像。LDM 提出了两种输入约束信息的方式：与输入噪声图像拼接、作为交叉注意力模块的 K, V。SD 仅使用了交叉注意力的方式，而 SVD 同时使用了两种方式。</p>
<p><img src="/2024/06/05/20240407-SVD-1/0-1.jpg" alt></p>
<p>上面这两种添加约束信息的方法适用于信息量比较大的约束。实际上，还有一种更简单的输入实数约束信息的方法。除了噪声输入外，去噪模型还必须输入当前的去噪时刻 $t$。自最早的 DDPM 以来，时刻 $t$ 都是先被转换成位置编码，再输入进 U-Net 的所有残差块中。仿照这种输入机制，如果有其他的约束信息和 $t$ 一样可以用一个实数表示，则不必像前面那样将这种约束信息与输入拼接或输入交叉注意力层，只需要把约束也转换成位置编码，再与 $t$ 的编码加在一起。</p>
<p>SVD 给模型还添加了三种额外约束：噪声增强程度、帧率、运动程度。这三种约束都是用和时刻编码相加的形式实现的。</p>
<p><img src="/2024/06/05/20240407-SVD-1/0-2.jpg" alt></p>
<p>即使现在不完全理解这三种额外约束的意义也不要紧。稍后我们会在学习 U-Net 结构时看到这种额外约束是怎么添加进 U-Net 的，在学习采样流水线时了解这三种约束的意义。</p>
<p>总结一下，除了添加了少数模块外，SVD 和 SD 的整体架构一样，都是以去噪 U-Net 为核心的 LDM。除了原本扩散模型要求的噪声、去噪时刻这两种输入外，SVD 还加入了 4 种约束信息：约束图像（视频首帧）、噪声增强程度、帧率、运动程度。约束图像是最主要的约束信息，它会与噪声输入拼接，且输入进 U-Net 的交叉注意力层中。后三种额外约束会以和处理去噪时刻类似的方式输入进 U-Net 中。</p>
<h2 id="去噪模型结构"><a href="#去噪模型结构" class="headerlink" title="去噪模型结构"></a>去噪模型结构</h2><p>接下来，我们来学习 SVD 的去噪模型的结构。在 Diffusers 中，一个扩散模型的参数、配置全部放在一个模型文件夹里，该文件夹的各个子文件夹存储了模型的各个模块，如自编码器、去噪模型、调度器等。我们可以在 <code>https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/tree/main</code> 找到 SVD 的模型文件夹，或者访问我们本地下载好的模型文件夹。</p>
<p>SVD 的去噪 U-Net 放在模型文件夹的 <code>unet</code> 子文件夹里。通过阅读子文件夹里的 <code>config.json</code>，我们就能知道模型类的名字是什么，并知道初始化模型的参数有哪些。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;UNetSpatioTemporalConditionModel&quot;</span>,</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;down_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;DownBlockSpatioTemporal&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;up_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;UpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlockSpatioTemporal&quot;</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>通过在本地 Diffusers 库文件夹里搜索类名 <code>UNetSpatioTemporalConditionModel</code>，或者利用 IDE 的 Python 智能提示功能，在前文的示例脚本里跳转到 <code>StableVideoDiffusionPipeline</code> 所在文件，再跳转到 <code>UNetSpatioTemporalConditionModel</code> 所在文件，我们就能知道 SVD 的去噪 U-Net 类定义在 <code>diffusers/models/unet_spatio_temporal_condition.py</code> 里。我们可以对照位于 <code>diffusers/models/unet_2d_condition.py</code> 的 SD 的 2D U-Net 类 <code>UNet2DConditionModel</code> 来看一下 SVD 的 U-Net 有何不同。</p>
<p>先来看 <code>__init__</code> 构造函数。SVD U-Net 几乎就是一个写死了许多参数的特化版 2D U-Net，其构造函数也基本上是 SD 2D U-Net 的构造函数的子集。比如 2D U-Net 允许用 <code>act_fn</code> 来指定模型的激活函数，默认为 <code>&quot;silu&quot;</code>，而 SVD U-Net 直接把所有模块的激活函数写死成 <code>&quot;silu&quot;</code>。经过简化后，SVD U-Net 的构造函数可读性高了很多。我们从参数开始读起，逐一了解构造函数每一个参数的意义：</p>
<ul>
<li><code>sample_size=None</code>：隐空间图片边长。供其他代码调用，与 U-Net 无关。</li>
<li><code>in_channels=8</code>：输入通道数。</li>
<li><code>out_channels=4</code>: 输出通道数。</li>
<li><code>down_block_types</code>：每一大层下采样模块的类名。</li>
<li><code>up_block_types</code>：每一大层上采样模块的类名。</li>
<li><code>block_out_channels = (320, 640, 1280, 1280)</code>：每一大层的通道数。</li>
<li><code>addition_time_embed_dim=256</code>: 每个额外约束的通道数。</li>
<li><code>projection_class_embeddings_input_dim=768</code>: 所有额外约束的通道数。</li>
<li><code>layers_per_block=2</code>: 每一大层有几个结构相同的模块。</li>
<li><code>cross_attention_dim=1024</code>: 交叉注意力层的通道数。</li>
<li><code>transformer_layers_per_block=1</code>: 每一大层的每一个模块里有几个 Transformer 层。</li>
<li><code>num_attention_heads=(5, 10, 10, 20)</code>: 各大层多头注意力层的头数。</li>
<li><code>num_frames=25</code>: 训练时的帧数。供其他代码调用，与 U-Net 无关。</li>
</ul>
<p>SVD U-Net 的参数基本和 SD 的一致，不同之处有：1）稍后我们会在采样流水线里看到，SVD 把图像约束拼接到了噪声图像上，所以整个噪声输入的通道数是原来的两倍，从 4 变为 8；2）多了一个给采样代码用的 <code>num_frames</code> 参数，它其实没有被 U-Net 用到。</p>
<p>我们再来大致过一下构造函数的实现细节。SVD U-Net 的整体结构和 2D U-Net 的几乎一致。数据先经过下采样模块，再经过中间模块，最后过上采样模块。下采样模块和上采样模块之间有短路连接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, down_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(down_block_types):</span><br><span class="line">    ...</span><br><span class="line">    down_block = get_down_block(...)</span><br><span class="line">    self.down_blocks.append(down_block)</span><br><span class="line"></span><br><span class="line">self.mid_block = UNetMidBlockSpatioTemporal(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, up_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(up_block_types):</span><br><span class="line">    ...</span><br><span class="line">    up_block = get_up_block(...)</span><br><span class="line">    self.up_blocks.append(up_block)</span><br><span class="line"></span><br><span class="line">self.conv_norm_out = nn.GroupNorm(...)</span><br><span class="line">self.conv_act = nn.SiLU()</span><br><span class="line">self.conv_out = nn.Conv2d(...)</span><br></pre></td></tr></table></figure>
<p>扩散模型还需要处理去噪时刻约束 $t$。U-Net 会先用正弦编码（Transformer 里的位置编码）<code>time_proj</code> 来将时刻转为向量，再用一系列线性层 <code>time_embedding</code> 预处理这个编码。该编码后续会输入进 U-Net 主体的每一个模块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.time_proj = Timesteps(block_out_channels[<span class="number">0</span>], <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">timestep_input_dim = block_out_channels[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim)</span><br></pre></td></tr></table></figure>
<p>除了多数扩散模型都有的 U-Net 模块外，SVD 还加入了额外约束模块。如前文所述，对于能用一个实数表示的约束，可以使用和处理时刻类似的方式，先让其过位置编码层，再过线性层，最后把得到的输出编码和时刻编码加起来。所以，和这种额外约束相关的模块在代码里叫做 <code>add_time</code>。在 2D U-Net 里，额外约束是可选的。SD 没有用到额外约束。而 SVD 把额外约束设为了必选模块。稍后我们会在采样流水线里看到，SVD 将视频的帧率、运动程度、噪声增强强度作为了生成时的额外约束。这些约束都是用这种与时刻编码相加的形式实现的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.add_time_proj = Timesteps(addition_time_embed_dim, <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">self.add_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)</span><br></pre></td></tr></table></figure></p>
<p>构造函数的代码就看完了。在构造函数中，我们认识了 SVD U-Net 的各个模块，但对其工作原理或许还存在着些许疑惑。我们来模型的前向传播函数 <code>forward</code> 里看一下各个模块是怎么处理输入的。</p>
<p>看代码前，我们先回顾一下概念，整理一下 U-Net 的数据处理流程。下面是我之前给 SD U-Net 画的示意图。该图对 SVD 同样适用。和 SD 相比，SVD 的输入 <code>x</code> 不仅包括噪声图像（准确说是多个表示视频帧的图像），还包括作为约束的首帧图像； <code>c</code> 换成了首帧图像的 CLIP 编码；<code>t</code> 不仅包括时刻，还包括一些额外约束。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-1.jpg" alt></p>
<p>和上图所示的一样，SVD U-Net 的 <code>forward</code> 方法的输入包含图像 <code>sample</code>，时刻 <code>timestep</code>，交叉注意力层约束（图像编码） <code>encoder_hidden_states</code> , 额外约束 <code>added_time_ids</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    sample: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    timestep: <span class="type">Union</span>[torch.Tensor, <span class="built_in">float</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    encoder_hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    added_time_ids: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    return_dict: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<p>方法首先会处理去噪时刻和额外参数，我们来看一下这两个输入是怎么拼到一起的。</p>
<p>做完一系列和形状相关的处理后，输入时刻 <code>timestep</code> 变成了 <code>timesteps</code>。随后，该变量会先过正弦编码（位置编码）层 <code>time_proj</code>，再过一些线性层 <code>time_embedding</code>，得到最后输入 U-Net 主体的时刻嵌入 <code>emb</code>。这两个模块的命名非常容易混淆，千万别弄反了。类似地，额外约束也是先过正弦编码层 <code>add_time_proj</code>，再过一些线性层 <code>add_embedding</code>，最后其输出 <code>aug_emb</code> 会加到 <code>emb</code> 上。当然，为了确保结果可以相加，<code>time_embedding</code> 和 <code>add_time_proj</code> 的输出通道数是相同的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># preprocessing</span></span><br><span class="line"><span class="comment"># timesteps = timestep</span></span><br><span class="line"></span><br><span class="line">t_emb = self.time_proj(timesteps)</span><br><span class="line">t_emb = t_emb.to(dtype=sample.dtype)</span><br><span class="line"></span><br><span class="line">emb = self.time_embedding(t_emb)</span><br><span class="line"></span><br><span class="line">time_embeds = self.add_time_proj(added_time_ids.flatten())</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">time_embeds = time_embeds.to(emb.dtype)</span><br><span class="line">aug_emb = self.add_embedding(time_embeds)</span><br><span class="line">emb = emb + aug_emb</span><br></pre></td></tr></table></figure>
<p>这里有关额外约束的处理写得很差，逻辑也很难读懂。在构造函数里，额外约束的正弦编码层 <code>add_time_proj</code> 的输出通道数 <code>addition_time_embed_dim</code> 是 256, 线性模块 <code>add_embedding</code> 的输入通道数 <code>projection_class_embeddings_input_dim</code> 是 768。两个通道数不一样的模块是怎么接起来的？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">  ...</span></span></span><br><span class="line"><span class="params"><span class="function">  addition_time_embed_dim: <span class="built_in">int</span> = <span class="number">256</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">  projection_class_embeddings_input_dim: <span class="built_in">int</span> = <span class="number">768</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">  ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">self</span>.<span class="title">add_time_proj</span> = <span class="title">Timesteps</span>(<span class="params">addition_time_embed_dim, <span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span></span>)</span></span><br><span class="line"><span class="function"><span class="title">self</span>.<span class="title">add_embedding</span> = <span class="title">TimestepEmbedding</span>(<span class="params">projection_class_embeddings_input_dim, time_embed_dim</span>)</span></span><br></pre></td></tr></table></figure>
<p>原来，在下面这份模块前向传播代码中，<code>added_time_ids</code> 的形状是 <code>[batch_size, 3]</code>。其中的 <code>3</code> 表示有三个额外约束。做了 <code>flatten()</code> 再过 <code>add_time_proj</code> 后，可以得到形状为 <code>[3 * batch_size, 256]</code> 的正弦编码 <code>time_embeds</code>。之所以三个约束可以用同一个模块来处理，是因为正弦编码没有学习参数，对所有输入都会产生同样的输出。得到 <code>time_embeds</code> 后，再根据从输入噪声图像里得到的 <code>batch_size</code>，用 <code>reshape</code> 把 <code>time_embeds</code> 的形状变成 <code>[batch_size, 768]</code>。这样，<code>time_embeds</code> 就可以输入进 <code>add_embedding</code> 里了。 <code>add_embedding</code> 是有可学习参数的，三个约束必须分别处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">time_embeds = self.add_time_proj(added_time_ids.flatten())</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">time_embeds = time_embeds.to(emb.dtype)</span><br><span class="line">aug_emb = self.add_embedding(time_embeds)</span><br></pre></td></tr></table></figure>
<p>这些代码不应该这样写的。当前的写法不仅可读性差，还不利于维护。比较好的写法是在构造函数里把输入参数从<code>projection_class_embeddings_input_dim</code> 改为 <code>num_add_time</code>，表示额外约束的数量。之后，把 <code>add_embedding</code> 的输入通道数改成 <code>num_add_time * addition_time_embed_dim</code>。这样，使用者不必手动设置合理的 <code>add_embedding</code> 的输入通道数（比如保证 768 必须是 256 的 3 倍），只设置有几个额外约束就行了。这样改了之后，为了提升可读性，还可以像下面那样把 <code>reshape</code> 里的那个 <code>-1</code> 写清楚来。Diffusers 采用这种比较混乱的写法，估计是因为这段代码是从 2D U-Net 里摘抄出来的。而原 2D U-Net 需要兼容更复杂的情况，所以 <code>add_time_proj</code> 和 <code>add_embedding</code> 的通道数需要分别指定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">time_embeds = time_embeds.reshape((batch_size, -<span class="number">1</span>))</span><br><span class="line">-&gt;</span><br><span class="line">time_embeds = time_embeds.reshape((batch_size, self.num_add_time * self.addition_time_embed_dim))</span><br></pre></td></tr></table></figure>
<p>预处理完时刻和额外约束后，方法还会修改所有输入的形状，使得它们第一维的长度都是 <code>batch_size</code> 乘视频帧数。正如我们在上一篇文章中学到的，为了兼容图像模型里的模块，我们要先把视频长度那一维和 batch 那一维合并，等到了和时序相关的模块再对视频长度那一维单独处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Flatten the batch and frames dimensions</span></span><br><span class="line"><span class="comment"># sample: [batch, frames, channels, height, width] -&gt; [batch * frames, channels, height, width]</span></span><br><span class="line">sample = sample.flatten(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># Repeat the embeddings num_video_frames times</span></span><br><span class="line"><span class="comment"># emb: [batch, channels] -&gt; [batch * frames, channels]</span></span><br><span class="line">emb = emb.repeat_interleave(num_frames, dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># encoder_hidden_states: [batch, 1, channels] -&gt; [batch * frames, 1, channels]</span></span><br><span class="line">encoder_hidden_states = encoder_hidden_states.repeat_interleave(num_frames, dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>后面的代码就和 2D U-Net 的几乎一样了。数据依次经过下采样块、中间块、上采样块。下采样块的中间结果还会保存在栈 <code>down_block_res_samples</code> 里，作为上采样模块的额外输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sample = self.conv_in(sample)</span><br><span class="line"></span><br><span class="line">image_only_indicator = torch.zeros(batch_size, num_frames, dtype=sample.dtype, device=sample.device)</span><br><span class="line"></span><br><span class="line">down_block_res_samples = (sample,)</span><br><span class="line"><span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">    sample, res_samples = downsample_block(...)</span><br><span class="line">    down_block_res_samples += res_samples</span><br><span class="line"></span><br><span class="line">sample = self.mid_block(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">    res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">    down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">    sample = upsample_block(...)</span><br><span class="line"></span><br><span class="line">sample = self.conv_norm_out(sample)</span><br><span class="line">sample = self.conv_act(sample)</span><br><span class="line">sample = self.conv_out(sample)</span><br></pre></td></tr></table></figure>
<p>光看 U-Net 类，我们还看不出 SVD 的 3D U-Net 和 2D U-Net 的区别。接下来，我们来看一看 U-Net 中某一个具体的模块是怎么实现的。由于 U-Net 下采样块、中间块、上采样块的结构是类似的，我们只挑某一大层的下采样模块类 <code>CrossAttnDownBlockSpatioTemporal</code> 来学习。</p>
<p>在 <code>CrossAttnDownBlockSpatioTemporal</code> 类中，我们可以看到 SVD U-Net 的每一个子模块都可以拆成残差卷积块和 Transformer 块。数据在经过子模块时，会先过残差块，再过 Transformer 块。我们来继续深究时序残差块类 <code>SpatioTemporalResBlock</code> 和时序 Transformer 块 <code>TransformerSpatioTemporalModel</code> 的实现细节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __init__</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">    in_channels = in_channels <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> out_channels</span><br><span class="line">    resnets.append(</span><br><span class="line">        SpatioTemporalResBlock(...)</span><br><span class="line">    )</span><br><span class="line">    attentions.append(</span><br><span class="line">        TransformerSpatioTemporalModel(...)</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># forward</span></span><br><span class="line">blocks = <span class="built_in">list</span>(<span class="built_in">zip</span>(self.resnets, self.attentions))</span><br><span class="line"><span class="keyword">for</span> resnet, attn <span class="keyword">in</span> blocks:</span><br><span class="line">    hidden_states = resnet(hidden_states, ...)</span><br><span class="line">    hidden_states = attn(hidden_states, ...)</span><br></pre></td></tr></table></figure>
<p>在开始看代码之前，我们再回顾一下论文里有关 3D U-Net 块的介绍。SVD 的 U-Net 是从 Video LDM 的 U-Net 改过来的。下面的模块结构图源自 Video LDM 论文，我将其改成了能描述 SVD U-Net 块的图。图中红框里的模块表示在原 SD 2D U-Net 块的基础上新加入的模块。可以看出，SVD 实际上就是在原来的 2D 残差块后面加了一个 3D 卷积层，原空间注意力块后面加了一个时序注意力层。旧模块输出和新模块输出之间用一个比例 $\alpha$ 来线性混合。中间数据形状变换的细节我们已经在上篇文章里学过了，这篇文章里我们主要关心这些模块在代码里大概是怎么定义的。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-2.jpg" alt></p>
<p>3D 残差块类 <code>SpatioTemporalResBlock</code> 在 <code>diffusers/models/resnet.py</code> 文件中。它有三个子模块，分别对应上文示意图中的 2D 残差块、时序残差块（3D 卷积）、混合模块。在运算时，旧模块的输出会缓存到<br> <code>hidden_states_mix</code> 中，新模块的输出为 <code>hidden_states</code>，二者最终会送入混合模块 <code>time_mixer</code> 做一个线性混合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpatioTemporalResBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.spatial_res_block = ResnetBlock2D(...)</span><br><span class="line">        self.temporal_res_block = TemporalResnetBlock(...)</span><br><span class="line">        self.time_mixer = AlphaBlender(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        hidden_states = self.spatial_res_block(hidden_states, temb)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        hidden_states_mix = hidden_states</span><br><span class="line">        </span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        hidden_states = self.temporal_res_block(hidden_states, temb)</span><br><span class="line">        hidden_states = self.time_mixer(</span><br><span class="line">            x_spatial=hidden_states_mix,</span><br><span class="line">            x_temporal=hidden_states,</span><br><span class="line">        )</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p><code>ResnetBlock2D</code> 是 SD 2D U-Net 的残差模块，我们在这篇文章里就不去学习它了。 时序残差块 <code>TemporalResnetBlock</code> 和 2D 残差块的结构几乎完全一致，唯一的区别在于 2D 卷积被换成了 3D 卷积。从代码中我们可以知道，这个模块是一个标准的残差块，数据会依次过两个卷积层，并在最后输出前与输入相加。扩散模型中的时刻约束 <code>temb</code> 会在数据过完第一个卷积层后，加到数据上。值得注意的是，虽然类里面的卷积层名字叫 3D 卷积，但实际上它的卷积核形状为 <code>(3, 1, 1)</code>，这说明这个卷积层实际上只是一个时序维度上窗口大小为 3 的 1D 卷积层。</p>
<p><img src="/2024/06/05/20240407-SVD-1/1-3.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemporalResnetBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        kernel_size = (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        padding = [k // <span class="number">2</span> <span class="keyword">for</span> k <span class="keyword">in</span> kernel_size]</span><br><span class="line"></span><br><span class="line">        self.norm1 = torch.nn.GroupNorm(...)</span><br><span class="line">        self.conv1 = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> temb_channels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.time_emb_proj = nn.Linear(temb_channels, out_channels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.time_emb_proj = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.norm2 = torch.nn.GroupNorm(...)</span><br><span class="line"></span><br><span class="line">        self.dropout = torch.nn.Dropout(<span class="number">0.0</span>)</span><br><span class="line">        self.conv2 = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">        self.nonlinearity = get_activation(<span class="string">&quot;silu&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.use_in_shortcut = self.in_channels != out_channels</span><br><span class="line"></span><br><span class="line">        self.conv_shortcut = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.use_in_shortcut:</span><br><span class="line">            self.conv_shortcut = nn.Conv3d(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_tensor, temb</span>):</span></span><br><span class="line">        hidden_states = input_tensor</span><br><span class="line"></span><br><span class="line">        hidden_states = self.norm1(hidden_states)</span><br><span class="line">        hidden_states = self.nonlinearity(hidden_states)</span><br><span class="line">        hidden_states = self.conv1(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.time_emb_proj <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            temb = self.nonlinearity(temb)</span><br><span class="line">            temb = self.time_emb_proj(temb)[:, :, :, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            temb = temb.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">            hidden_states = hidden_states + temb</span><br><span class="line"></span><br><span class="line">        hidden_states = self.norm2(hidden_states)</span><br><span class="line">        hidden_states = self.nonlinearity(hidden_states)</span><br><span class="line">        hidden_states = self.dropout(hidden_states)</span><br><span class="line">        hidden_states = self.conv2(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.conv_shortcut <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_tensor = self.conv_shortcut(input_tensor)</span><br><span class="line"></span><br><span class="line">        output_tensor = input_tensor + hidden_states</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output_tensor</span><br></pre></td></tr></table></figure>
<p>混合模块 <code>AlphaBlender</code> 其实就只是定义了一个可学习的混合比例 <code>mix_factor</code>，之后用这个比例来混合空间层输出和时序层输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlphaBlender</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        alpha: <span class="built_in">float</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.register_parameter(<span class="string">&quot;mix_factor&quot;</span>, torch.nn.Parameter(torch.Tensor([alpha])))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        x_spatial,</span></span></span><br><span class="line"><span class="params"><span class="function">        x_temporal,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; torch.Tensor:</span></span><br><span class="line">        <span class="comment"># Get mix_factor</span></span><br><span class="line">        alpha = self.get_alpha(...)</span><br><span class="line">        alpha = alpha.to(x_spatial.dtype)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.switch_spatial_to_temporal_mix:</span><br><span class="line">            alpha = <span class="number">1.0</span> - alpha</span><br><span class="line"></span><br><span class="line">        x = alpha * x_spatial + (<span class="number">1.0</span> - alpha) * x_temporal</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>看完了3D 残差块 <code>SpatioTemporalResBlock</code> 的内容，我们接着来看 3D 注意力块 <code>TransformerSpatioTemporalModel</code> 的内容。<code>TransformerSpatioTemporalModel</code> 也主要由 2D Transformer 块 <code>BasicTransformerBlock</code>、时序 Transformer 块 <code>TemporalBasicTransformerBlock</code> 、混合模块组成 <code>AlphaBlender</code>。它们的连接方式和上面的残差块类似。时序 Transformer 块和普通 2D Transformer 块一样，都是有自注意力、交叉注意力、全连接层的标准 Transformer 模块，它们的区别只在于时序 Transformer 块对输入做形状变换的方式不同，会让数据在时序维度上做信息交互。这里我们就不去进一步深究它们的实现细节了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerSpatioTemporalModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_attention_heads: <span class="built_in">int</span> = <span class="number">16</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_head_dim: <span class="built_in">int</span> = <span class="number">88</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        in_channels: <span class="built_in">int</span> = <span class="number">320</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        out_channels: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_layers: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        cross_attention_dim: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                BasicTransformerBlock(...)</span><br><span class="line">                <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.temporal_transformer_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                TemporalBasicTransformerBlock(...)</span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.time_pos_embed = TimestepEmbedding(in_channels, time_embed_dim, out_dim=in_channels)</span><br><span class="line">        self.time_proj = Timesteps(in_channels, <span class="literal">True</span>, <span class="number">0</span>)</span><br><span class="line">        self.time_mixer = AlphaBlender(alpha=<span class="number">0.5</span>, ...)</span><br></pre></td></tr></table></figure>
<p>这个时序 Transformer 模块类有一个地方值得注意。我们知道，Transformer 模型本身是不知道输入数据的顺序的。无论是注意力层还是全连接层，它们都与顺序无关。为了让模型知道数据的先后顺序，比如在 NLP 里我们希望模型知道一句话里每个单词的前后顺序，我们会给输入数据加上位置编码。而有些时候我们觉得模型不用知道数据的先后顺序。比如在 SD 的 2D 图像 Transformer 块里，我们把每个像素当成一个 token，每个像素在 Transformer 块的运算方式是相同的，与其所在位置无关。而在处理视频时序的 Transformer 块中，知道视频每一帧的先后顺序看起来还是很重要的。所以，和 SD 的 2D Transformer 块不同，SVD 的时序 Transformer 块根据视频的帧号设置了位置编码，用和 NLP 里处理文本类似的方式处理视频。SVD 的时序 Transformer 类在构造函数里定义了生成位置编码的模块 <code>TimestepEmbedding</code>, <code>Timesteps</code>。在前向传播时，<code>forward</code> 方法会用 <code>torch.arange(num_frames)</code> 根据总帧数生成帧号列表，并经过两个模块得到最终的位置编码嵌入 <code>emb</code>。嵌入 <code>emb</code> 会在数据过时序 Transformer 块前与输入 <code>hidden_states_mix</code> 相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerSpatioTemporalModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.time_pos_embed = TimestepEmbedding(in_channels, time_embed_dim, out_dim=in_channels)</span><br><span class="line">        self.time_proj = Timesteps(in_channels, <span class="literal">True</span>, <span class="number">0</span>)</span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        num_frames_emb = torch.arange(num_frames, device=hidden_states.device)</span><br><span class="line">        num_frames_emb = num_frames_emb.repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        num_frames_emb = num_frames_emb.reshape(-<span class="number">1</span>)</span><br><span class="line">        t_emb = self.time_proj(num_frames_emb)</span><br><span class="line">        emb = self.time_pos_embed(t_emb)</span><br><span class="line">        emb = emb[:, <span class="literal">None</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> block, temporal_block <span class="keyword">in</span> <span class="built_in">zip</span>(self.transformer_blocks, self.temporal_transformer_blocks):</span><br><span class="line">            hidden_states = block(</span><br><span class="line">                ...</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            hidden_states_mix = hidden_states</span><br><span class="line">            hidden_states_mix = hidden_states_mix + emb</span><br><span class="line"></span><br><span class="line">            hidden_states_mix = temporal_block(...)</span><br><span class="line">            hidden_states = self.time_mixer(...)</span><br><span class="line"></span><br><span class="line">        hidden_states = self.proj_out(hidden_states)</span><br><span class="line">        hidden_states = hidden_states.reshape(batch_frames, height, width, inner_dim).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).contiguous()</span><br><span class="line"></span><br><span class="line">        output = hidden_states + residual</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>到这里，我们就读完了 SVD U-Net 的主要代码。相比 SD U-Net，SVD U-Net 主要做了以下修改：</p>
<ul>
<li>由于输入多了一张约束图像，输入通道数变为原来的两倍。</li>
<li>多加了三个和视频相关的额外约束。它们是通过和扩散模型的时刻嵌入相加输入进模型的。它们的命名通常与 <code>add_time</code> 相关。</li>
<li>仿照 Video LDM 的结构设计，SVD 也在 2D 残差块后面加入了由 3D 卷积组成的时序残差块，在空间 Transformer 块后面加入了对时序维度做注意力的时序 Transformer 块。新旧模块的输出会以一个可学习的比例线性混合。</li>
</ul>
<h2 id="VAE-结构"><a href="#VAE-结构" class="headerlink" title="VAE 结构"></a>VAE 结构</h2><p>SVD 不仅微调了 SD 的 U-Net，还微调了 VAE 的解码器，让输出视频在时序上更加连贯。由于更新 VAE 和更新 U-Net 的方法几乎一致，我们就来快速看一下 SVD 的时序 VAE 的结构，而跳过每个模块的更新细节。</p>
<p>通过阅读 VAE 的配置文件，我们可以知道时序 VAE 的类名为  <code>AutoencoderKLTemporalDecoder</code>，它位于文件 <code>diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py</code> 中。从它的构造函数里我们可以知道，时序 VAE 的编码器类是 <code>Encoder</code>，和 SD 的一样，只是解码器类从 <code>Decoder</code> 变成了 <code>TemporalDecoder</code>。我们来看一下这个新解码器类的代码做了哪些改动。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AutoencoderKLTemporalDecoder</span>(<span class="params">ModelMixin, ConfigMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        in_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        out_channels: <span class="built_in">int</span> = <span class="number">3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.encoder = Encoder(...)</span><br><span class="line"></span><br><span class="line">        self.decoder = TemporalDecoder(...)</span><br><span class="line"></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在 SD 中，VAE 和 U-Net 的组成模块是几乎一致的，二者的结构主要有三个区别：1）由于 VAE 的解码器和编码器是独立的，它们之间没有残差连接。而 U-Net 是一个整体，它的编码器（下采样块）和解码器（上采样块）之间有残差连接，以减少数据在下采样中的信息损失; 2）由于 VAE 中图像的尺寸较大，仅在 VAE 最深层图像尺寸为 <code>64x64</code> 时才有自注意力层。具体来说，这个自注意力层加到了 VAE 解码器的一开头，代码中相关模块称为 <code>mid_block</code>；3）VAE 仅有空间自注意力，而 SD U-Net 用了完整的 Transformer 块（包含自注意力层、交叉注意力层、全连接层）。由于 SD VAE 和 U-Net 结构上的相似性，SVD 的开发者直接把对 U-Net 的更新也搬到了 VAE 上来。</p>
<p>SVD VAE 解码器仅做了两项更新：1）将所有模块里的 2D 残差块都被换成了我们在上文中见过的 3D 残差块；2）在最终输出前加了一个 3D 卷积（时序维度上的 1D 卷积）。VAE 的自注意力层的结构并没有更新。更新 2D 残差块的方法和 U-Net 的是一致的。比如在新的上采样块类 <code>UpBlockTemporalDecoder</code> 中，我们就可以看到之前在新 U-Net 里看过的 3D 残差块类 <code>SpatioTemporalResBlock</code> 的身影。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> ..unet_3d_blocks <span class="keyword">import</span> MidBlockTemporalDecoder, UpBlockTemporalDecoder</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpBlockTemporalDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            ...</span><br><span class="line">            resnets.append(SpatioTemporalResBlock(...))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemporalDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers_per_block = layers_per_block</span><br><span class="line"></span><br><span class="line">        self.conv_in = nn.Conv2d(...)</span><br><span class="line">        self.mid_block = MidBlockTemporalDecoder(...)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(block_out_channels)):</span><br><span class="line">            ...</span><br><span class="line">            up_block = UpBlockTemporalDecoder(...)</span><br><span class="line">            self.up_blocks.append(up_block)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        conv_out_kernel_size = (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.time_conv_out = torch.nn.Conv3d(...)</span><br></pre></td></tr></table></figure>
<h2 id="采样流水线"><a href="#采样流水线" class="headerlink" title="采样流水线"></a>采样流水线</h2><p>看完了 U-Net 和 VAE 的代码后，我们来看整套 SVD 的采样代码。和其他方法一样，在 Diffusers 中，一套采样方法会用一个流水线类 (<code>xxxPipeline</code>)来表示。SVD 对应的流水线类叫做 <code>StableVideoDiffusionPipeline</code>。我们可以利用 IDE 的代码跳转功能，在本文开头的示例采样脚本中跳转至 <code>StableVideoDiffusionPipeline</code> 所在源文件 <code>diffusers/pipelines/stable_video_diffusion/pipeline_stable_video_diffusion.py</code>。</p>
<p>如示例脚本所示，使用流水线类时，可以将类实例 <code>pipe</code> 当成一个函数来用。这种用法实际上会调用实例的 <code>__call__</code> 方法。所以，在阅读流水线类的代码时，我们可以先忽略其他部分，直接看 <code>__call__</code> 方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipe = StableVideoDiffusionPipeline.from_pretrained(...)</span><br><span class="line">frames = pipe(image, decode_chunk_size=<span class="number">8</span>, generator=generator).frames[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><code>__call__</code> 的参数定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    image: <span class="type">Union</span>[PIL.Image.Image, <span class="type">List</span>[PIL.Image.Image], torch.FloatTensor],</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="built_in">int</span> = <span class="number">576</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="built_in">int</span> = <span class="number">1024</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_frames: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">25</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    min_guidance_scale: <span class="built_in">float</span> = <span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_guidance_scale: <span class="built_in">float</span> = <span class="number">3.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    fps: <span class="built_in">int</span> = <span class="number">7</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    motion_bucket_id: <span class="built_in">int</span> = <span class="number">127</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    noise_aug_strength: <span class="built_in">float</span> = <span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    decode_chunk_size: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_videos_per_prompt: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    generator: <span class="type">Optional</span>[<span class="type">Union</span>[torch.Generator, <span class="type">List</span>[torch.Generator]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    latents: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    output_type: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="string">&quot;pil&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    callback_on_step_end: <span class="type">Optional</span>[<span class="type">Callable</span>[[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="type">Dict</span>], <span class="literal">None</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    callback_on_step_end_tensor_inputs: <span class="type">List</span>[<span class="built_in">str</span>] = [<span class="string">&quot;latents&quot;</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    return_dict: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br></pre></td></tr></table></figure>
<p><code>__call__</code> 的参数就是我们在使用 SVD 采样时能修改的参数，我们需要把其中的主要参数弄懂。各参数的解释如下：</p>
<ul>
<li><code>image</code>：SVD 会根据哪张图片生成视频。</li>
<li><code>height, width</code>: 生成视频的尺寸。如果输入图片与这个尺寸对不上，会将输入图片的尺寸调整为该尺寸。</li>
<li><code>num_frames</code>: 生成视频的帧数。SVD 1.0 版默认 14 帧，1.0-xt 版默认 25 帧。</li>
<li><code>min_guidance_scale</code>, <code>max_guidance_scale</code>: 使用 Classifiser-free Guidance (CFG) 的强度范围。SVD 用了一种特殊的设置 CFG 强度的机制，稍后我们会在采样代码里见到。</li>
<li><code>fps</code>：输出视频期望的帧率。SVD 的额外约束。实际上这个帧率肯定是不准的，只不过提高这个值可以让视频更平滑。</li>
<li><code>motion_bucket_id</code>: SVD 的额外约束。官方没有解释该值的原理，只说明了提高该值能让输出视频的运动更多。</li>
<li><code>noise_aug_strength</code>: 对输入图片添加的噪声强度。值越低输出视频越像原图。</li>
<li><code>decode_chunk_size</code>: 一次放几张图片进时序 VAE 做解码，用于在内存占用和效果之间取得一个平衡。按理说一次处理所有图片得到的视频连续性最好，但那样也会消耗过多的内存。</li>
<li><code>num_videos_per_prompt</code>: 对于每张输入图片 （prompt），输出几段视频。</li>
<li><code>generator</code>: PyTorch 的随机数生成器。如果想要手动控制生成中的随机种子，就手动设置这个变量。</li>
<li><code>latents</code>: 强制指定的扩散模型的初始高斯噪声。</li>
<li><code>output_type</code>: 输出图片格式，是 NumPy、PIL，还是 PyTorch。</li>
<li><code>callback_on_step_end</code>，<code>callback_on_step_end_tensor_inputs</code> 用于在不修改原流水线代码的情况下向采样过程中添加额外的处理逻辑。学习代码的时候可以忽略。</li>
<li><code>return_dict</code>: 流水线是返回一个词典，还是像普通 Python 函数一样返回用元组表示的多个返回值。</li>
</ul>
<p>大致搞清楚了输入参数的意义后，我们来看流水线的执行代码。一开始的代码都是在预处理输入，可以直接跳过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0. Default height and width to unet</span></span><br><span class="line">height = height <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">width = width <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line"></span><br><span class="line">num_frames = num_frames <span class="keyword">if</span> num_frames <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> self.unet.config.num_frames</span><br><span class="line">decode_chunk_size = decode_chunk_size <span class="keyword">if</span> decode_chunk_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> num_frames</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Check inputs. Raise error if not correct</span></span><br><span class="line">self.check_inputs(image, height, width)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Define call parameters</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(image, PIL.Image.Image):</span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line"><span class="keyword">elif</span> <span class="built_in">isinstance</span>(image, <span class="built_in">list</span>):</span><br><span class="line">    batch_size = <span class="built_in">len</span>(image)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    batch_size = image.shape[<span class="number">0</span>]</span><br><span class="line">device = self._execution_device</span><br><span class="line">self._guidance_scale = max_guidance_scale</span><br></pre></td></tr></table></figure>
<p>之后，代码开始预处理交叉注意力层的约束信息。在 SD 里，约束信息是文本，所以这一步会用 CLIP 文本编码器得到约束文本的嵌入。而 SVD 是一个图生视频模型，所以这一步会用 CLIP 图像编码器得到约束图像的嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Encode input image</span></span><br><span class="line">image_embeddings = self._encode_image(image, device, num_videos_per_prompt, self.do_classifier_free_guidance)</span><br></pre></td></tr></table></figure>
<p>代码还把额外约束帧率 <code>fps</code> 减了个一，因为训练的时候模型实际上输入的额外约束是 <code>fps - 1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fps = fps - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>接着，代码开始处理与噪声拼接的约束图像。回顾一下，SVD 的约束图像以两种形式输入进模型：一种是过 CLIP 图像编码器，以交叉注意力 K，V 的形式输入，其预处理如上部分的代码所示；另一种形式是与原去噪 U-Net 的噪声输入拼接，其预处理如当前这部分代码所示。</p>
<p>在预处理要拼接的图像时，代码会先调用预处理器 <code>image_processor.preprocess</code>，把其他格式的图像转成 PyTorch 的 <code>Tensor</code> 类型。之后，代码会随机生成一点高斯噪声，并把噪声根据噪声增强强度 <code>noise_aug_strength</code> 加到这张约束图像上。这种做法来自于之前有约束图像的扩散模型 <em>Cascaded diffusion models</em>。<code>noise_aug_strength</code> 稍后会作为额外约束输入进 U-Net 里，与去噪时刻的编码相加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image = self.image_processor.preprocess(image, height=height, width=width).to(device)</span><br><span class="line">noise = randn_tensor(image.shape, generator=generator, device=device, dtype=image.dtype)</span><br><span class="line">image = image + noise_aug_strength * noise</span><br></pre></td></tr></table></figure>
<p>加了这个噪声后，图像会过 VAE 的编码器，得到 <code>image_latents</code>。<code>image_latents</code> 会通过 <code>repeat</code> 操作复制成多份，并于稍后拼接到每一帧带噪图像上。注意，一般图像在过 VAE 的编码器后，要乘一个系数 <code>vae.config.scaling_factor</code>; 在过 VAE 的解码器前，要除以这个系数。然而，只有在这个地方，<code>image_latents</code> 没有乘系数。我个人觉得这是开发者的一个失误。当然，做不做这个操作对于模型来说区别不大，因为模型能很快学会这种系数上的差异。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Encode input image using VAE</span></span><br><span class="line">image_latents = self._encode_vae_image(</span><br><span class="line">    image,</span><br><span class="line">    device=device,</span><br><span class="line">    num_videos_per_prompt=num_videos_per_prompt,</span><br><span class="line">    do_classifier_free_guidance=self.do_classifier_free_guidance,</span><br><span class="line">)</span><br><span class="line">image_latents = image_latents.to(image_embeddings.dtype)</span><br><span class="line">image_latents = image_latents.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, num_frames, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>下一步，代码会把三个额外约束拼接在一起，得到 <code>added_time_ids</code>。它会接入到 U-Net 中，与时刻编码加到一起。在训练时，帧率 <code>fps</code> 和 运动程度 <code>motion_bucket_id</code> 完全来自于数据集标注，而 <code>noise_aug_strength</code> 是可以随机设置的。在采样时，这三个参数都可以手动设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5. Get Added Time IDs</span></span><br><span class="line">added_time_ids = self._get_add_time_ids(</span><br><span class="line">    fps,</span><br><span class="line">    motion_bucket_id,</span><br><span class="line">    noise_aug_strength,</span><br><span class="line">    image_embeddings.dtype,</span><br><span class="line">    batch_size,</span><br><span class="line">    num_videos_per_prompt,</span><br><span class="line">    self.do_classifier_free_guidance,</span><br><span class="line">)</span><br><span class="line">added_time_ids = added_time_ids.to(device)</span><br></pre></td></tr></table></figure>
<p>再下一步，代码会将采样的总步数 <code>num_inference_steps</code> 告知采样调度器 <code>scheduler</code>。这一步是 Diffusers API 的要求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 6. Prepare timesteps</span></span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, <span class="literal">None</span>, sigmas)</span><br></pre></td></tr></table></figure>
<p>然后，代码会随机生成初始高斯噪声。不同的随机噪声即对应不同的输出视频。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 7. Prepare latent variables</span></span><br><span class="line">num_channels_latents = self.unet.config.in_channels</span><br><span class="line">latents = self.prepare_latents(</span><br><span class="line">    batch_size * num_videos_per_prompt,</span><br><span class="line">    num_frames,</span><br><span class="line">    num_channels_latents,</span><br><span class="line">    height,</span><br><span class="line">    width,</span><br><span class="line">    image_embeddings.dtype,</span><br><span class="line">    device,</span><br><span class="line">    generator,</span><br><span class="line">    latents,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>开始采样前，SVD 对约束图像的强度做了一种很特殊的设定。在看代码之前，我们先回顾一下约束强度的意义。现在的扩散模型普遍使用了 CFG (Classifier-free Guidance) 技术，它允许我们在采样时灵活地调整模型和约束信息的相符程度。这个强度默认取 1.0。我们可以通过增大强度来提升模型的生成效果，比如在 SD 中，这个强度一般取 7.5，这代表模型会更加贴近输入文本。</p>
<p>而 SVD 中，约束信息为图像。开发者对视频的不同帧采用了不同的约束强度：首帧为 <code>min_guidance_scale</code>, 末帧为 <code>max_guidance_scale</code>。强度从首帧到末帧线性增加。默认情况下，约束强度的范围是 [1, 3]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 8. Prepare guidance scale</span></span><br><span class="line">guidance_scale = torch.linspace(min_guidance_scale, max_guidance_scale, num_frames).unsqueeze(<span class="number">0</span>)</span><br><span class="line">guidance_scale = guidance_scale.to(device, latents.dtype)</span><br><span class="line">guidance_scale = guidance_scale.repeat(batch_size * num_videos_per_prompt, <span class="number">1</span>)</span><br><span class="line">guidance_scale = _append_dims(guidance_scale, latents.ndim)</span><br><span class="line"></span><br><span class="line">self._guidance_scale = guidance_scale</span><br></pre></td></tr></table></figure>
<p>最后，就来到了扩散模型的去噪循环了。根据之前采样调度器返回的采样时刻列表 <code>timesteps</code>，代码从中取出去噪时刻，对纯噪声输入迭代去噪。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_warmup_steps = <span class="built_in">len</span>(timesteps) - num_inference_steps * self.scheduler.order</span><br><span class="line">self._num_timesteps = <span class="built_in">len</span>(timesteps)</span><br><span class="line"><span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">    <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br></pre></td></tr></table></figure>
<p>去噪迭代的一开始，代码会根据是否要执行 CFG 来决定是否要把输入额外复制一份。这是因为做 CFG 时，我们需要把同一个输入过两次去噪模型，一次带约束，一次不带约束。为了简化这个流程，我们可以直接把输入复制一遍，这样只要过一次去噪模型就能得到两个输出了。下一行的 <code>scale_model_input</code> 是 Diffusers 的 API 要求，可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br></pre></td></tr></table></figure>
<p>接着，加了噪声、过了 VAE 解码器、没有乘系数的约束图像 <code>image_latents</code> 会与普通的噪声拼接到一起，作为模型的直接输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Concatenate image_latents over channels dimension</span></span><br><span class="line">latent_model_input = torch.cat([latent_model_input, image_latents], dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>准备好了所有输入后，代码调用 U-Net 对输入噪声图像去噪。输入包括直接输入 <code>latent_model_input</code>，去噪时刻 <code>t</code>，约束图像的 CLIP 嵌入 <code>image_embeddings</code>，三个额外约束的拼接 <code>added_time_ids</code>。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict the noise residual</span></span><br><span class="line">noise_pred = self.unet(</span><br><span class="line">    latent_model_input,</span><br><span class="line">    t,</span><br><span class="line">    encoder_hidden_states=image_embeddings,</span><br><span class="line">    added_time_ids=added_time_ids,</span><br><span class="line">    return_dict=<span class="literal">False</span>,</span><br><span class="line">)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>去噪结束后，代码根据公式做 CFG。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># perform guidance</span></span><br><span class="line"><span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_cond - noise_pred_uncond)</span><br></pre></td></tr></table></figure>
<p>有了去噪的输出 <code>noise_pred</code> 还不够，我们还需要用一些比较复杂的公式计算才能得到下一时刻的噪声图像。这一切都被 Diffusers 封装进调度器里了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">latents = self.scheduler.step(noise_pred, t, latents).prev_sample</span><br></pre></td></tr></table></figure>
<p>以上就是一步去噪迭代的主要内容。代码会反复执行去噪迭代。这后面除了下面这行会调用 VAE 解码器将隐空间的视频解码回真实视频外，没有其他重要代码了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frames = self.decode_latents(latents, num_frames, decode_chunk_size)</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们学习了图生视频模型 SVD 的模型结构和采样代码。整体上看，SVD 相较 SD 在模型上的修改不多，只是在原来的 2D 模块后面加了一些在时序维度上交互信息的卷积块和 Transformer 块。在学习时，我们应该着重关注 SVD 的采样流水线。SVD 使用拼接和交叉注意力两种方式添加了图像约束，并以与时刻编码相加的方式额外输入了三种约束信息。由于视频不同帧对于首帧的依赖情况不同，SVD 还使用了一种随帧号线性增长的 CFG 强度设置方式。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/06/05/20240405-SVD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/06/05/20240405-SVD/" class="post-title-link" itemprop="url">Stable Video Diffusion 结构浅析与论文速览</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-05 15:52:11" itemprop="dateCreated datePublished" datetime="2024-06-05T15:52:11+08:00">2024-06-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">扩散模型</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>近期，各个科技公司纷纷展示了自己在视频生成模型上的最新成果。虽然不少模型的演示效果都非常惊艳，但其中可供学术界研究的开源模型却少之又少。Stable Video Diffusion (SVD) 算得上是目前开源视频生成模型中的佼佼者，有认真一学的价值。在这篇文章中，我将面向之前已经熟悉 Stable Diffusion (SD) 的读者，简要解读 SVD 的论文。由于 SVD 的部分结构复用了之前的工作，并没有在论文正文中做详细介绍，所以我还会补充介绍一下 SVD 的模型结构、调度器。后续我还会在其他文章中详细介绍 SVD 的代码实现及使用方法。</p>
<p><img src="/2024/06/05/20240405-SVD/1.jpg" alt></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Stable Video Diffusion 是 Stability 公司于 2023 年 11 月 21 日公布并开源的一套用扩散模型实现的视频生成模型。由于该模型是从 Stability 公司此前发布的著名文生图模型 Stable Diffusion 2.1 微调而成的，因而得名 Stable Video Diffusion。SVD 的技术报告论文与模型同日发布，它对 SVD 的训练过程做了一个详细的分享。由于该论文过分偏向实践，这里我们仅对它的开头及中间模型设计的几处关键部分做解读。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近，有许多视频生成模型都是在图像生成模型 SD 的基础上，添加和视频时序相关的模块，并在小规模高质量视频数据集上微调新模型。而 SVD 作者认为，该领域在训练方法及精制数据集的策略上并未达成统一。这篇文章的主要贡献，也正是提出了一套训练方法与精制数据集的方法。具体而言，SVD 的训练由三个阶段组成：文生图预训练、视频预训练、高质量视频微调。同时，SVD 提出了一种系统性的数据精制流程，包含数据的标注与过滤这两部分的策略。论文会分享诸多的实验成果，包括验证精心构建的数据集对生成高质量视频的必要性、探究视频预训练与微调这两步的重要性、展示基础模型如何为图生视频等下游任务提供强大的运动表示、演示模型如何提供多视角三维先验并可以作为微调多视角扩散模型的基础模型在一轮神经网络推理中同时生成多视角的图片。</p>
<blockquote>
<p>「构建」一个数据集在论文中通常用动词 curate 及名词 curation 指代。curate 原指展出画作时，选择、组织和呈现艺术品的过程。而现代将这个词用在数据集上时，则转变为表示精心选择、组织和管理数据的过程。中文中并没有完全对应的翻译，我暂时将这个词翻译为「精制」，以区别于随便收集一些数据来构成一个数据集。</p>
</blockquote>
<p>总结一下，SVD 并没有强调在模型设计或者采样算法上的创新，而主要宣传了该工作在数据集精制及训练策略上的创新。对于大部分普通研究人员来说，由于没有训练大视频模型的需求，该文章的很多内容都价值不大。我们就只是来大致过一遍这篇文章的主要内容。</p>
<h2 id="SVD-模型架构回顾"><a href="#SVD-模型架构回顾" class="headerlink" title="SVD 模型架构回顾"></a>SVD 模型架构回顾</h2><h3 id="Video-LDM-与-SVD"><a href="#Video-LDM-与-SVD" class="headerlink" title="Video-LDM 与 SVD"></a>Video-LDM 与 SVD</h3><p>在阅读正文之前，我们先来回顾一下此前视频生成模型的开发历程，并重点探究 SVD 的模型架构——Video LDM 的具体组成。绝大多数工作在训练一个基于扩散模型的视频生成模型时，都是在预训练的 SD 上加入时序模块，如 3D 卷积，并通过微调把一个图像生成模型转换成视频生成模型。由于 SD 是一种 LDM (Latent Diffusion Model)，所以这些视频模型都可以归类为 Video-LDM。所谓 LDM，就是一种先生成压缩图像，再用解码模型把压缩图像还原成真实图像的模型。而对于视频，Video-LDM 则会先生成边长压缩过的视频，再把压缩视频还原。</p>
<p><img src="/2024/06/05/20240405-SVD/2.jpg" alt></p>
<p>虽然 Video-LDM 严格上来说是一个视频扩散模型的种类，但大家一般会用Video LDM （没有横杠） 来指代 <em>Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</em> 这篇工作。这篇论文已在 CVPR 2023 上发布，两个主要作者正是前一年在 CVPR 上发表 SD 论文的主要作者，也是现在这篇 SVD 论文的主要作者。从署名上来看，似乎两个作者在毕业后就加入了 Stability 公司，并将 Video LDM 拓展成了 SVD。论文中也讲到，SVD 完全复用了 Video LDM 的结构。为了了解 SVD 的模型结构，我们再来回顾一下 Video LDM 的结构。</p>
<p>在 SD 的基础上，Video LDM 做对模型结构了两项改动：在扩散模型的去噪模型 U-Net 中加入时序层、在对图像压缩和解压的 VAE 的解码器中加入时序层。</p>
<h3 id="添加时序层"><a href="#添加时序层" class="headerlink" title="添加时序层"></a>添加时序层</h3><p>Video LDM 在 U-Net 中加入时序层的方法与多数同期方法相同，是在每个原来处理图像的空间层后面加上处理视频的时序层。Video LDM 加入的时序层包括 3D 卷积层与时序注意力层。这些新模块本身不难理解，但我们需要着重关注这些新模块是怎么与原模型兼容的。</p>
<p>要兼容各个模块，其实就是要兼容数据的形状。本来，图像生成模型的 U-Net 的输入形状为 <code>B C H W</code>，分别表示图像数、通道数、高、宽。而视频数据的形状是 <code>B T C H W</code>，即视频数、视频长度、通道数、高、宽。要让视频数据复用之前的图像模型的结构，只要把数据前两维合并，变成 <code>(B T) C H W</code> 即可。这种做法就是把 B 组长度为 T 的视频看成了 $B \cdot T$ 张图片。</p>
<p>对于之前已有的空间层，只要把数据形状变成 <code>(B T) C H W</code> 就没问题了。而 SVD 又新加入了两种时序层：3D 卷积和时序注意力。我们来看一下数据是怎么经过这些新的时序层的。</p>
<p>2D 卷积会对 <code>B C H W</code> 的数据的后两个高、宽维度做卷积。类似地，3D 卷积会对数据最后三个时间、高、宽维度做卷积。所以，过 3D 卷积前，要把形状从 <code>(B T) C H W</code> 变成 <code>B C T H W</code>，做完卷积再还原。</p>
<p>接下来我们来看新的时序注意力。这个地方稍微有点难理解，我们从最简单的注意力开始一点一点学习。最早的 NLP 中的注意力层的输入形状为 <code>B L C</code>，表示数据数、token 长度、token 通道数。<code>L</code> 这一维最为重要，它表示了 <code>L</code>个 token 之间互相交换信息。如果把其拓展成图像空间注意力，则 token 表示图像的每一个像素。在这种注意力层中，<code>L</code> 是 <code>(H W)</code>，<code>B C H W</code> 的数据会被转换成 <code>B (H W) C</code> 输入进注意力层。这表示同一组图像中，每个像素两两之间交换信息。而让视频数据过空间注意力层时，只需要把 <code>B</code> 换成 <code>(B T)</code> 即可，即把数据形状从 <code>(B T) C H W</code> 变为 <code>(B T) (H W) C</code>。这表示同一组、同一帧的图像的每个像素之间，两两交换信息。</p>
<p>在 SVD 新加入的时序注意力层中，token 依旧指代是某一组、某一帧上的一个像素。然而，这次我们不是让同一张图像的像素互相交换信息，而是让不同时刻的像素互相交换信息。因此，这次 token 长度 <code>L</code> 是 <code>T</code>，它表示要像素在时间维度上交换信息。这样，在视频数据过时序层里的自注意力层时，要把数据形状从 <code>(B T) C H W</code> 变成 <code>(B H W) T C</code>。这表示每一组、图像每一处的像素独立处理，它们仅与同一位置不同时间的像素进行信息交换。</p>
<blockquote>
<p>此处如果你没有理解注意力层的形状变换也不要紧，它只是一个实现细节，不影响后面的阅读。如果感兴趣的话，可以回顾一下 Transformer 论文的代码，看一下注意力运算为什么是对 <code>B L C</code> 的数据做操作的。</p>
</blockquote>
<p><img src="/2024/06/05/20240405-SVD/3.jpg" alt></p>
<h3 id="微调-VAE-解码器"><a href="#微调-VAE-解码器" class="headerlink" title="微调 VAE 解码器"></a>微调 VAE 解码器</h3><p>Video LDM 的另一项改动是修改了图像压缩模型 VAE 的解码器。具体来说，方法先在 VAE 的解码器中加入类似的时序层，并在 VAE 配套的 GAN 的判别器里也加入了时序层，随后开始微调。在微调时，编码器不变，仅训练解码器和判别器。</p>
<blockquote>
<p>如果你没听过这套 VAE + GAN 的架构的话，请回顾 Stable Diffusion 论文及与其紧密相关的 VQGAN 论文。</p>
</blockquote>
<p><img src="/2024/06/05/20240405-SVD/4.jpg" alt></p>
<p>以上就是 Video LDM 的模型结构。SVD 对其没有做任何更改，所以也没有在论文里对模型结构做详细介绍。稍有不同的是，Video LDM 仅微调了新加入的模块，而 SVD 在加入新模块后对模型的所有参数都进行了重新训练。</p>
<h2 id="SVD-训练细节"><a href="#SVD-训练细节" class="headerlink" title="SVD 训练细节"></a>SVD 训练细节</h2><p>SVD 分四节介绍了模型训练过程。第一节介绍了数据精制的过程，后三节分别介绍了训练的三个阶段：文生图预训练、视频预训练、高质量视频微调。</p>
<p>获取了一个大规模视频数据集后，SVD 的数据精制主要由预处理和标注这两步组成。由于视频生成模型主要关注生成同一个场景的视频，而不考虑转场的问题，每段训练视频也应该尽量只包含一个场景。为此，预处理主要是在用一些自动化视频剪切工具把收集到的视频进一步切成连续的片段。经切片后，视频片段数变为原来的4倍。标注主要是给视频加上文字描述，以训练一个文生视频的模型。SVD 在添加文字描述时用到了多个标注模型，并使用大语言模型来润色描述。经预处理和标注后，得到的数据集被称作 LVD (Large Video Dataset)。</p>
<p>SVD 数据精制的细节中，比较值得注意的是有关视频帧数的处理。由于开发团队发现视频数据的播放速度快慢不一，于是他们使用光流预测模型来大致估计每段视频的播放速度（以帧率 FPS 表示），并将视频的帧率也作为标注。这样，在训练时，视频的帧率也可以作为一种约束信息。这样的好处是，在我们在生成视频时，可以用该约束来指定视频的播放速度。</p>
<p>之后我们来看 SVD 模型训练的三个阶段。对于第一个文生图预训练阶段，论文没有对模型结构做过多修改，因为他们在这一步使用了之前训练好的 SD 2.1。不过，SVD 在这一步做了一个非常重要的改进：SVD 的噪声调度器从原版的 DDPM 改成了 EDM，采样方法也改成了 EDM 的。</p>
<p>EDM 的论文全称为 <em>Elucidating the Design Space of Diffusion-Based Generative Models</em> 。这篇论文用一种概括性较强的数学模型统一表示了此前各种各样的扩散模型结构，并提出了改进版模型的训练及采样策略。简单来说，EDM 把扩散模型不同时刻的噪声强度表示成 $\sigma_t$，它表示在 $t$ 时刻时，对来自数据集的图像加了标准差为 $\sigma_t$ 的高斯噪声 $\mathcal{N}(\mathbf{0}, \sigma_t^2\mathbf{I})$。一开始，对于没加噪声的图像，$\sigma_0=0$。对于最后一个时刻 $T$ 的图像，$\sigma_T$ 要足够大，使得原图像的内容被完全破坏。</p>
<blockquote>
<p>这里时刻 $0$ 与时刻 $T$ 的定义与 DDPM 论文相同，与 EDM 论文相反。</p>
</blockquote>
<p>有了这样一种统一的表示后，EDM 对扩散模型的训练和采样都做了不少改进。这里我们仅关注其中最重要的一条改进：将离散噪声改进成连续噪声。原来 DDPM 的去噪模型会输入时刻 $t$ 这个参数。EDM 论文指出，$t$ 实际上表示了噪声强度 $\sigma_t$，应该把 $\sigma_t$ 输入进模型。与其用离散的 $t$ 训练一个只认识离散噪声强度的去噪模型，不如训练一个认识连续噪声强度 $\sigma$ 的模型。这样，在采样 $n$ 步时，我们不再是选择离散去噪时刻<code>[timestep[n], timestep[n - 1], ..., 0]</code>，而是可以选择连续噪声强度<code>[sigma[n], sigma[n - 1], ..., 0]</code> 。这样采样更灵活，效果也更好。在第一个训练阶段中，SVD 照搬了 EDM 的这种训练方法，改进了原来的 DDPM。SVD 的默认采样策略也使用了 EDM 的 。我们会在之后的代码实践文章中详细学习这种新采样方法。</p>
<p>对于第二个视频预训练阶段，或许是因为视频模型和图像模型的训练过程毫无区别，论文的介绍重点依然放在了这一阶段的数据处理上，而没有强调训练方法上的创新。简单来看，这一阶段的目标是得到一个过滤后的高质量数据集 LVD-F。为了找到这样一种合适的过滤方案，开发团队先用排列组合生成了大量的过滤方案：对每类指标（文本视频匹配度、美学分数、帧率等）都设置 12.5%, 25% 或 50% 的过滤条件，然后不同指标的条件之间排列组合。之后，开发团队抽取原数据集的一个子集 LVD-10M，用各个方案得到过滤后的视频子集 LVD-10-F。最后，用这样得到的子数据集分别训练模型，比较模型输出的好坏，以决定在完整数据集上使用的最优过滤方案。</p>
<p>在第三个阶段，参考以往多阶段训练图像模型的经验，SVD 也在另一个小而精的视频数据集上进行微调。此数据集的获取方法并没有在论文中给出，大概率是人工手动收集并标注。</p>
<h2 id="SVD-应用"><a href="#SVD-应用" class="headerlink" title="SVD 应用"></a>SVD 应用</h2><p>经上述训练后，开发团队得到了一个低分辨率的基础文生视频模型。在实验部分，SVD 论文除了给出视频生成模型在各大公开数据集上的指标外，还分享了几个基于基础模型的应用。</p>
<h3 id="高分辨率文生视频"><a href="#高分辨率文生视频" class="headerlink" title="高分辨率文生视频"></a>高分辨率文生视频</h3><p>基础文生视频最直接的应用就是高分辨率文生视频。实现的方法很简单，只要准备一个高分辨率的视频数据集，在此数据集上微调原基础模型即可。SVD 高分辨率文生视频模型能生成 576 x 1024 的视频。</p>
<p><img src="/2024/06/05/20240405-SVD/5.jpg" alt></p>
<h3 id="高分辨率图生视频"><a href="#高分辨率图生视频" class="headerlink" title="高分辨率图生视频"></a>高分辨率图生视频</h3><p>除了文生视频外，也可以用基础模型来微调出一个图生视频模型。为了把约束从文本换成图像，开发团队将 U-Net 交叉注意力层的约束从文本嵌入变成了约束图像的图像嵌入，并将约束图像与原 U-Net 的噪声输入在通道维度上拼接在一起。特别地，参考以往 <em>Cascaded diffusion models</em> 论文的经验，约束图像在与噪声输入拼接前，会加上一些噪声。除此之外，由于约束机制的变动，像文生图模型一样将约束强度（CFG scale）设成 7.5 会让 SVD 图生视频模型产生瑕疵。因此，SVD 图生视频模型每一帧的约束强度不同，从第一帧到最后一帧以 1.0 到 3.0 线性增长。</p>
<p><img src="/2024/06/05/20240405-SVD/6.jpg" alt></p>
<p>参考之前 <em>AnimateDiff</em> 工作，SVD 也成功训练了相机运动 LoRA，使得图生视频模型只会生成平移、缩放等某一种特定相机运动的视频。</p>
<p><img src="/2024/06/05/20240405-SVD/7.jpg" alt></p>
<h3 id="视频插帧"><a href="#视频插帧" class="headerlink" title="视频插帧"></a>视频插帧</h3><p>Video LDM 曾提出了一种把基础视频模型变成视频插帧模型方法。该方法以视频片段的首末帧为额外约束，在此新约束下把视频生成模型微调成了预测中间帧的视频预测模型。SVD 以同样方式实现了这一应用。</p>
<h3 id="多视角生成"><a href="#多视角生成" class="headerlink" title="多视角生成"></a>多视角生成</h3><p>多视角生成是计算机视觉中另一类重要的任务：给定 3D 物体某个视角的图片，需要算法生成物体另外视角的图片，从而还原 3D 物体的原貌。而视频生成模型从数据中学到了物体的平滑变换规律，恰好能帮助到多视角生成任务。SVD 论文用不少篇幅介绍了如何在 3D 数据集上生成视频并微调基础模型，从而得到一个能生成环绕物体旋转的视频的模型。</p>
<p><img src="/2024/06/05/20240405-SVD/8.jpg" alt></p>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>Stable Video Diffusion 是在文生图模型 Stable Diffusion 2.1 的基础上添加了和 Video LDM 相同的视频模块微调而成的一套视频生成模型。SVD 的论文主要介绍了其精制数据集的细节，并展示了几个微调基础模型能实现的应用。通过微调基础低分辨率文生视频模型，SVD 可以用于高分辨率文生视频、高分辨率图生视频、视频插帧、多视角生成。</p>
<p>对于没有资源与需求训练大视频模型的多数科研人员而言，没有深究这篇文章细节的必要。并且，由于 SVD <strong>只开源了图生视频模型</strong> （3D模型后来是在 SV3D 论文中正式公布的），这篇文章比较有用的只有和图生视频相关的部分。为了彻底搞懂 SVD 的原理，读这篇论文是不够的，我们还需要通过回顾 Video LDM 论文来了解模型结构，学习 EDM 论文来了解训练及采样机制。</p>
<p>这篇文章主要是面向熟悉 Stable Diffusion 的读者的。如果你缺少某些背景知识，欢迎读我之前介绍 Stable Diffusion 的文章。我没有在本文过多介绍 SVD 的实现细节，欢迎阅读我之后发表的 SVD 代码实践文章。</p>
<p>Stable Diffusion 解读（一）：回顾早期工作</p>
<p>Stable Diffusion 解读（二）：论文精读</p>
<p>Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/04/04/20240221-TorchEval-FID/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/04/04/20240221-TorchEval-FID/" class="post-title-link" itemprop="url">FID 指标简介与修正 TorchEval FID 计算接口经历分享</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-04-04 14:08:44" itemprop="dateCreated datePublished" datetime="2024-04-04T14:08:44+08:00">2024-04-04</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">记录</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%B0%E5%BD%95/%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>FID 是一种衡量图像生成模型质量的指标。对于这种常见的指标，一般都能找到好用的 PyTorch 计算接口。然而，当我用 PyTorch 的官方库 TorchEval 来算 FID 指标时，却发现它的结果和多数非官方库无法对齐。我花了不少时间，总算把 TorchEval 的 FID 计算接口修好了。在这篇文章中，我将分享有关 FID 计算的知识以及我调试 TorchEval 的经历，并总结用 pytorch-fid, torch-fidelity, TorchEval 算 FID 的方法。文章最后，我还会分享一个偶然发现的用于反映模型训练时的当前 FID 的方法。</p>
<h2 id="FID-指标简介"><a href="#FID-指标简介" class="headerlink" title="FID 指标简介"></a>FID 指标简介</h2><p>FID 的全称是 Fréchet Inception Distance，它用于衡量两个图像分布之间的差距。如果令一个图像分布是训练集，再用生成模型输出的图像构成另一个分布，那么 FID 指标就表示了生成出来的图片和训练集整体上的相似度，也就间接反映了模型对训练集的拟合程度。FID 名字中的 Fréchet Distance 是一种描述两个样本分布的距离的指标，其定位和 KL 散度一样，但某些情况下会比 KL 散度更加合适。FID 用来算 Fréchet Distance 的样本来自预训练 InceptionV3 模型，它名称中的 Inception 由此而来。</p>
<p>计算 FID 的过程如下：</p>
<ol>
<li>准备两个图片文件夹。一般一个是训练集，另一个存储了生成模型随机生成的图片。 </li>
<li>用预训练的 InceptionV3 模型把每个输入图片转换成一个 2048 维的向量。</li>
<li>计算训练集、生成集上输出向量的均值、协方差。</li>
<li>把均值、协方差代入进下面这个算 Fréchet Distance 的公式，就得到了 FID。</li>
</ol>
<script type="math/tex; mode=display">
FID = ||\mu - \mu_w||^2 + tr(\Sigma + \Sigma_w - 2(\Sigma\Sigma_w)^{\frac{1}{2}})</script><p>实际上，在用 FID 的时候我们完全不用管它的原理，只要知道它的值越小就越好，并且会调用相关接口即可。需注意的是，由于 FID 是一种和集合相关的指标，算 FID 时一定要给足图片。在构建自己模型的输出集合时，至少得有 10000 张图片，推荐生成 50000 张。否则 FID 的结果会不准确。</p>
<h2 id="用-PyTorch-计算-FID-的第三方库"><a href="#用-PyTorch-计算-FID-的第三方库" class="headerlink" title="用 PyTorch 计算 FID 的第三方库"></a>用 PyTorch 计算 FID 的第三方库</h2><p>由于 FID 的计算需要用到一个预训练的 InceptionV3 模型，只有在模型实现完全一致的情况下，FID 的输出结果才是可比的。因此，所有论文汇报的 FID 都基于提出 FID 的作者的官方实现。这份官方实现是用 TensorFlow 写的，后来也有完全等价的 PyTorch 实现。在这一节里，我们就来学习如何用这些基于 PyTorch 的库算 FID。</p>
<p>GitHub 上点赞最多的 PyTorch FID 库是 <code>pytorch-fid</code>。这个库被 FID 官方仓库推荐，且 Stable Diffusion 论文也用了这个库，结果绝对可靠。使用该库的方法很简单，只需要先安装它。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pytorch-fid</span><br></pre></td></tr></table></figure>
<p>再准备好两个用于计算 FID 的文件夹，将文件夹路径传给脚本即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pytorch_fid path/to/dataset1 path/to/dataset2</span><br></pre></td></tr></table></figure>
<p>另一个较为常见的用 PyTorch 算指标的库叫做 <code>torch-fidelity</code>。它用起来和 <code>pytorch-fid</code> 一样简单。一开始，需要用 pip 安装它。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch-fidelity</span><br></pre></td></tr></table></figure>
<p>之后，同样是准备好两个图片文件夹，将文件夹路径传给脚本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fidelity --gpu <span class="number">0</span> --fid --input1 path/to/dataset1 --input2 path/to/dataset2</span><br></pre></td></tr></table></figure>
<p>除了命令行脚本外，<code>torch-fidelity</code> 还提供了 Python API。我们可以在 Python 脚本里加入算 FID 的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch_fidelity</span><br><span class="line"></span><br><span class="line">metrics_dict = torch_fidelity.calculate_metrics(</span><br><span class="line">    input1=<span class="string">&#x27;path1&#x27;</span>,</span><br><span class="line">    input2=<span class="string">&#x27;path2&#x27;</span>,</span><br><span class="line">    fid=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(metrics_dict)</span><br></pre></td></tr></table></figure>
<p><code>torch-fidelity</code> 还提供了其他便捷的功能。比如直接以某个生成模型为 API 的输入 <code>input1</code>，而不是先把图像生成到一个文件夹里，再把文件夹路径传给 <code>input1</code>。同时，<code>torch-fidelity</code> 还支持计算其他指标，我们只需要在命令行脚本或者 API 里多加几个参数就行了。</p>
<h2 id="修正-TorchEval-里的-FID-计算接口"><a href="#修正-TorchEval-里的-FID-计算接口" class="headerlink" title="修正 TorchEval 里的 FID 计算接口"></a>修正 TorchEval 里的 FID 计算接口</h2><p>尽管这些第三方库已经足够好用了，我还是想用 PyTorch 官方近年来推出的指标计算库 TorchEval 来算 FID 指标。原因有两点：</p>
<ol>
<li>我的项目其他地方都是用 PyTorch 官方库实现的 （<code>torch</code> 以及 <code>torchvision</code>），算指标也用官方库会让整体代码风格更加统一。我已经用 TorchEval 算了 PSNR、SSIM，使用体验还可以。</li>
<li>目前，似乎只有 TorchEval 支持在线更新指标的值。也就是说，我可以先生成一部分图片，储存算 FID 需要的中间结果；再生成一部分图片，最终计算此前所有图片与训练集的 FID。这种计算方法的好处我会在文章后面介绍。</li>
</ol>
<p>以前我都是用 pytorch-fid 来算 FID。而当我换成用 TorchEval 后，却发现结果对不齐。于是，漫长的调试之路开始了。</p>
<p>当你有两块时间不一样的手表时，应该怎样确认时间呢？答案是，再找到第三块表。如果三块表中能有两块表时间一样，那么它们的时间就是正确的。一开始，我并不能确定是哪个库写错了，所以我又测试了 torch-fidelity 的结果。实验发现，torch-fidelity 和 pytorch-fid 的结果是一致的。并且我去确认了 Stable Diffusion 的论文，其中用来计算 FID 的库也是 pytorch-fid。看来，是 TorchEval 结果不对。</p>
<p>像 FID 这么常见的指标，大家的中间计算过程肯定都没错，就是一些细微的预处理不太一样。抱着这样的想法，我随意地比对了一下二者的代码，很快就发现 TorchEval 把输入尺寸调成 <code>[299, 299]</code> 了，而 pytorch-fid 没做。可删掉这段代码，程序直接报错了。我深入阅读了 pytorch-fid 的代码，发现它的写法和 TorchEval 不一样，把调整尺寸为 <code>[299, 299]</code> 写到了另一个地方。且通过调查发现，InceptionV3 网络的输入尺寸必须是 <code>[299, 299]</code> 的，是我孤陋寡闻了。唉，看来这次的调试不能太随意啊。</p>
<p>我准备拿出我的真实实力来调 bug。我认真整理了一下算 FID 的步骤，将其主要过程总结为以下几步：</p>
<ol>
<li>用预训练权重初始化 InceptionV3</li>
<li>用 InceptionV3 算两个数据集输出的均值、协方差</li>
<li>根据均值、协方差算距离</li>
</ol>
<p>最后那个算距离的过程不涉及任何神经网络，输出该是什么就是什么。这一块是最不容易出错，且最容易调试的。于是，我决定先排除第三步是否对齐。我把 TorchEval 得到的均值、协方差存下来，用 pytorch-fid 算距离。发现结果和原 TorchEval 的输出差不多。看来算距离这一步没有问题。</p>
<p>接下来，我很自然地想到是不是均值和协方差算错了。我存下了两个库得到的均值、协方差，算了两个库输出之间的误差。结果发现，均值的误差在 0.09 左右，协方差的误差在 0.0002 左右。图像的数据范围在 0~1 之间，0.09 算是一个很大的误差了。可见，第一步和第二步一定存在着无法对齐的部分。</p>
<p>模型输出不同，最容易想到的是模型权重不同。于是，我尝试交换使用二者的模型权重，再比较输出的 FID。两个库的模型定义不太一样，不能直接换模型文件名。我用强大的代码魔改实力强行让新权重分别都跑起来了。结果非常神奇，算上之前的两个 FID，我一共得到了 4 个不一样的 FID 结果。也就是说，A 库 A 模型、B 库 B 模型、A 库 B 模型，B 库 A 模型，结果均不一样。</p>
<p>我被这两个库气得不行，决定认真研究对比二者的模型定义。眼尖的我发现初始化 pytorch-fid 的 InceptionV3 时有一个参数叫 <code>use_fid_inception</code>。作者对此的注释写道：「如果设置为 true，则用 TensorFlow 版 FID 实现；否则，用 torchvision 版 Inception 模型。TensorFlow 的 FID Inception 模型和 torchvision 的在权重和结构上有细微的差别。如果你要计算 FID，强烈推荐将此值设置为 true，以得到和其他论文可比的结果。」总结来说，TorchEval 用的是 torchvision 里的标准 PyTorch 版 InceptionV3，而 pytorch-fid 在标准 PyTorch 版 InceptionV3 外又封装了一层，改了一些模块的定义。为什么要改这些东西呢？这是因为原来的 FID Inception 模型是在 TensorFlow 里实现的，需要改一些结构来将 PyTorch 模型对齐过去。除了模型结构外，二者的权重也有一定差别。大家都是用 TensorFlow 版模型算 FID，一切都应该以 pytorch-fid 的为准。这个 TorchEval 太离谱了，我也懒得认真修改了，直接注释掉 TorchEval 里原 <code>FIDInceptionV3</code> 的定义，然后大笔一挥：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_fid.inception <span class="keyword">import</span> \</span><br><span class="line">    InceptionV3 <span class="keyword">as</span> FIDInceptionV3</span><br></pre></td></tr></table></figure>
<p>按理说，这下权重和模型结构都对齐了。FID 计算的第一、第二步绝对不会有错。而开始的结果表明，FID 计算的第三步也没有错。那么，两个库就应该对齐了。我激动地又测了 TorchEval 的结果，发现结果还是无法对齐！</p>
<p>这不应该啊？难道哪步测错了？人生就是在不断自我怀疑中度过的。而怀疑自我，首先会怀疑最久远的自我。所以，我感觉是最早测第三步的时候有问题。之前我是把 TorchEval 的均值、协方差放到 pytorch-fid 里，结果与 TorchEval 自己的输出一致。这次我反过来，把 pytorch-fid 的均值、协方差放到 TorchEval 的算距离函数里算。这次，我第一次见到 TorchEval 输出了正确的 FID。由此可见，第三步没错。难道是均值和协方差又没对齐了？</p>
<p>自我怀疑开始进一步推进，我开始怀疑第二步输出的均值、协方差还是没有对齐。我再次计算了 pytorch-fid 和 TorchEval 的输出之间的误差，发现误差这次仅有 1e-16，可以认为没有区别。我花了很多时间复习协方差的计算，想找出 TorchEval 里的 bug。可是越学习，越觉得 TorchEval 写得很对。这一回，我找不到错误了。</p>
<p>调试代码，不怕到处有错，而怕「没错却有错」。「没错」，指的是每一步中间步骤都找不到错误；「有错」，指的是最终结果还是错了。没有错误，就得创造错误。我开启了随机乱调模式，希望能触发一个错误。回忆一下，算 FID 要用到两个数据集，一般一个是训练集，一个是模型输出的集合。在 TorchEval 最后一步算距离时，我乱改代码，让一个集合的均值、协方差不变，即来自原 TorchEval 的 Inception 模型的输出；而让另一个的集合的均值、协方差来自 pytorch-fid。理论上说，如果两个库的均值、协方差是对齐的，那么这次输出的 FID 也应该是正确的。欸，这回代码报错了，运行不了。报错说数据精度不统一。原来，TorchEval 的输出精度是 float32，而 pytorch-fid 的输出精度是 float64。之前测试距离计算函数时，数据要么全来自 TorchEval，要么全来自 pytorch-fid，所以没报过这个错。可是这个错只是一个运行上的错误，稍微改改就好了。</p>
<p>我把 pytorch-fid 相关数据的精度统一成了 float32。这下代码跑起来了，可 FID 不对了。调试过程中，如果上一次成功，而这一次失败，则应该想办法把代码退回上一次的，再次测试。因此，我又修改了最后用 TorchEval 计算距离的数据来源，让所有数据都来自 pytorch-fid。可是，修改后，FID 输出没变，还是错的。</p>
<p>为什么两轮测试之前，我全用 pytorch-fid 的输出、TorchEval 的距离计算函数没有错，这次却错了？到底是哪里不同？当测试两份差不多的代码后，一份对了，一份错了，那么错误就可以定位到两份代码的差异处。仔细回顾一下我的调试经历，相信你可以推理出 bug 出自哪了。</p>
<p>没错！我仔细比对了当前代码和我记忆中两轮测试前的代码，仅发现了一处不同——我把 pytorch-fid 的输出数据的精度改成了 float32。把精度改回 float64 就对了。同样，如果把 TorchEval 的输出数据的精度改成 float64，再扔进 TorchEval 的距离计算函数里算，结果也是对的。问题出在 TorchEval 的距离计算函数的数据精度上。</p>
<p>定位到了 bug 的位置，再找出 bug 的原因就很简单了。对比 pytorch-fid 的距离计算函数和 TorchEval 的，可以发现二者描述的计算公式完全相同。然而，pytorch-fid 是用 NumPy 算的，而 TorchEval 是用 PyTorch 算的。算 FID 的距离时，会涉及矩阵特征值等较为复杂的运算，它们对数据精度要求较高。像 NumPy 这种久经考验的库应该会自动把数据变成高精度再计算，而 PyTorch 就没做这么多细腻的处理了。</p>
<p>汇总一下我调试的结论。TorchEval 在权重初始化、模型计算、距离计算这三步中均有错误。前两步没有让 InceptionV3 模型和普遍使用的 TensorFlow 版对齐，最后一步没有考虑输入精度，用了不够稳定的 PyTorch API 来做复杂矩阵运算。要用 TorchEval 算出正确的 FID，需要做以下修改：</p>
<ul>
<li>安装 pytorch-fid 和 TorchEval</li>
<li>打开 torcheval/metrics/image/fid.py</li>
<li>注释掉 <code>FIDInceptionV3</code> 类，在文件开头加上 <code>from pytorch_fid.inception import InceptionV3 as FIDInceptionV3</code></li>
<li>在 <code>FrechetInceptionDistance</code> 类的构造函数中，在定义所有浮点数据时加上 <code>dtype=torch.float64</code></li>
</ul>
<p>这里点名批评 TorchEval。开源的时候吹得天花乱坠，结果根本没人用，这么简单的有关 FID 的 bug 也发现不了。我发了一个修正此 bug 的相关 issue <code>https://github.com/pytorch/torcheval/issues/192</code>，截至目前还是没有官方人员回复。这个库的开发水平实在太逆天了，希望他们能尽快维护好。</p>
<h2 id="在线计算-FID"><a href="#在线计算-FID" class="headerlink" title="在线计算 FID"></a>在线计算 FID</h2><p>前文提到，我用 TorchEval 的原因是它支持在线计算 FID。具体来说，可以建立一个 FID 管理类，之后用 <code>update</code> 方法来不断往某个集合加入新图片，并随时使用 <code>compute</code> 方法算出当前所有图片的 FID。我之前写代码忘了清空旧图片的中间结果时发现了一个相关应用。经我使用下来，这种应用非常有用，我们可以用它高效估计训练时的当前 FID。</p>
<p>回顾一下，要得到准确的 FID 值，一般需要 50000 张图片。而训练图像生成模型时，如果每次验证都要生成这么多图片，则大部分时间都会消耗在验证上了。为了加快 FID 的验证，我发现可以用一种 「全局 FID」来近似表示当前的模型拟合情况。具体来说，我先用训练集的所有图片初始化 FID 的集合 1 的中间结果，再在模型训练中每次验证时随机生成 500 张图片，将其中间结果加到 FID 的集合 2 中，并输出一次当前 FID。这样，随着训练不断推进，算 FID 的图片的数量会逐渐满足 50000 张的要求，但是这些图片并不是来自同一个模型，而是来自不同训练程度的模型。这样得到的 FID 仅能大致反映当前的真实 FID 值，有时偏高、有时偏低。但经我测试发现，这种全局 FID 的相对关系很能反映最终的真实 FID 的相对关系。训练两个不同超参的模型时，如果一个全局 FID 较大，那它最终的 FID 一般也会较大。同时，如果训练一切正常，则全局 FID 会随验证轮数单调递减（因为图片数量变多，且拟合情况不会变差）。如果某一次验证时全局 FID 增加了，则模型也一定在这段时间里变差了。通过这种验证方式，我们能够大致评估模型在训练中的拟合情况。这应该是一种很容易想到的工程技巧，但由于分享自己训练生成模型的经验帖较少，且重要性不足以写进论文，我没有在任何地方看到有人介绍这种技巧。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>FID 是评估图像生成模型的重要指标。通过 pytorch-fid 等库，我们能轻松地用 PyTorch 计算两个图像分布间的 FID。而通过计算输出分布和训练分布之间的 FID，我们就能评估当前模型的拟合情况。</p>
<p>FID 的计算本身是很简单的。所以在介绍 FID 的计算方法之外，我分享了我调试 TorchEval 的漫长过程。这段经历很有意思，我学到了不少调 bug 的新知识。此前我从来没想到过数据精度竟然会大幅影响某个值的结果。这段经历启示我们，做一些复杂运算时，不要用 PyTorch 算，最好拿 NumPy 等更稳定的库来计算。如果你调 bug 的经验不足，这段经历也能给你许多参考。</p>
<p>文章最后我分享了一种算全局 FID 的方法。它可以高效反映生成模型在训练时的拟合情况。该功能很容易实现，感兴趣的话可以自己尝试一下。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/03/09/20240228-DiffMorpher/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/03/09/20240228-DiffMorpher/" class="post-title-link" itemprop="url">CVPR 2024 | DiffMorpher：实现两张图像间的平滑变形</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-03-09 22:58:43" itemprop="dateCreated datePublished" datetime="2024-03-09T22:58:43+08:00">2024-03-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>相信大家都在网上看过这种「笑容逐渐消失」的表情包：一张图片经过经过平滑的变形，逐渐变成另一张截然不同的图片。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/1.png" alt></p>
<p>对此，计算机科学中有一种专门描述此应用的任务——图像变形（image morphing）。给定两张图像，图像变形算法会输出一系列合理的插值图像。当按顺序显示这些插值图像时，它们应该能构成一个描述两张输入图像平滑变换的视频。</p>
<p>图像变形可以广泛运用于创意制作中。比如在做 PPT 时，我们可以在翻页处用图像变形做出炫酷的过渡效果。当然，图像变形也可以用在更严谨的场合。比如在制作游戏中的 2D 人物动画时，可以让画师只画好一系列关键帧，再用图像变形来补足中间帧。可是，这种任务对于中间插值图像的质量有着很高的要求。而传统的基于优化的图像变形算法只能对两张输入图像的像素进行一定程度的变形与混合，难以生成高质量的中间帧。有没有一种更好的图像变形算法呢？</p>
<p>针对这一需求，我们提出了 DiffMorpher —— 一种基于预训练扩散模型 （Stable Diiffusion）的图像变形算法。该研究由 DragGAN 作者潘新钢教授指导，经清华大学、上海人工智能实验室、南洋理工大学 S-Lab 合作完成。目前，该工作已经被 CVPR 2024 接收。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/2.png" alt></p>
<p>我们可以借助 DiffMorpher 实现许多应用。最简单的玩法是输入两张人脸，生成人脸的渐变图。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/yifei.gif" alt></p>
<p>如果输入一系列图片，我们还能制作更长更丰富的渐变图。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/human.gif" alt></p>
<p>而当输入的两张图片很相似时，我们可以用该工具制作出质量尚可的补间动画。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/anime.gif" alt></p>
<p>在这篇文章中，让我们来浏览一下 DiffMorpher 的工作原理，并学习如何使用这一工具。学习 DiffMorpher 的一些技术也能为我们开发其他基于扩散模型的编辑工具提供启发。</p>
<p>项目官网：<a target="_blank" rel="noopener" href="https://kevin-thu.github.io/DiffMorpher_page/">https://kevin-thu.github.io/DiffMorpher_page/</a></p>
<p>代码仓库：<a target="_blank" rel="noopener" href="https://github.com/Kevin-thu/DiffMorpher">https://github.com/Kevin-thu/DiffMorpher</a></p>
<h2 id="隐变量插值"><a href="#隐变量插值" class="headerlink" title="隐变量插值"></a>隐变量插值</h2><p>如前所述，图像变形任务在生成插值图像时不仅需要混合输入图像的内容，还需要补充生成一些内容。用预训练的图像生成模型来完成图像变形是再自然不过的想法了。前几年，已经有一些工作探究了如何用 GAN 来完成图像变形。使用 GAN 做图像变形的方法非常直接：在 GAN 中，每张被生成的图片都由一个高维隐变量决定。可以说，隐变量蕴含了生成一张图像所需的所有信息。那么，只要先使用 GAN 反演（inversion）把输入图片变成隐变量，再对隐变量做插值，就能用其生成两张输入图像的一系列中间过渡图像了。</p>
<blockquote>
<p>对于隐变量，我们一般使用球面插值（slerp）而不是线性插值。</p>
</blockquote>
<p><img src="/2024/03/09/20240228-DiffMorpher/3.png" alt></p>
<p>然而，GAN 生成的图像往往局限于某一类别，泛用性差。因此，用 GAN 做图像变形时，往往得不到高质量的图像插值结果。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/gan.gif" alt></p>
<p>而以 Stable Diffusion（SD）为代表的图像生成扩散模型以能生成各式各样的图像而著称。我们可以在 SD 上也用类似的过程来实现图像插值。具体来说，我们需要对 DDIM 反演得到的纯噪声图像（隐变量）进行插值，并对输入文本的嵌入进行插值，最后根据插值结果生成图像。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/4.png" alt></p>
<p>可是，扩散模型也存在缺陷：扩散模型的隐变量没有 GAN 的那么适合编辑。如下面的动图所示，如果仅使用简单的隐变量插值，会存在着两个问题：1）早期和晚期的中间帧和输入图像非常相近，而中期的中间帧又变化过快，图像的过渡非常突兀；2）中间帧的图像质量较低。这样的结果无法满足实际应用的要求。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/thu_nolora.gif" alt></p>
<h2 id="LoRA-插值"><a href="#LoRA-插值" class="headerlink" title="LoRA 插值"></a>LoRA 插值</h2><p>扩散模型的隐变量不适合编辑，准确来说是其所在隐空间的性质导致的。模型只能处理隐空间中部分区域的隐变量。如果对隐变量稍加修改，让隐变量「跑」到了一个模型处理不了的区域，那模型就生成不了高质量的结果。而在对两个输入隐变量做插值时，插值的隐变量很可能就位于一个模型处理不了的区域。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/6.png" alt></p>
<p>想解决此问题，我们需要参考一些其他的工作。为了提升扩散模型的编辑单张图像的能力，一些往期工作会在单张图片上微调预训练扩散模型（即训练集只由同一张图片构成，让模型在单张图片上过拟合）。这样，无论是调整初始的隐变量还是文本输入，模型总是能够生成一些和该图片很相近的图片。比如在 Imagic 工作中，为了编辑输入图片，算法会先在输入图片上微调扩散模型，再用新的文本描述重新生成一次图片。这样，最终生成的图片既和原图很接近（鸟的外观差不多），又符合新的文本描述（鸟张开了翅膀）。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/5.png" alt></p>
<p>后来，许多工作用 LoRA 代替了全参数微调。LoRA 是一种高效的模型微调技术。在训练 LoRA 时，原来的模型权重不用修改，只需要训练额外引入的少量参数即可。假设原模型的参数为$W$，则 LoRA 参数可以表示为 $\Delta W$，新模型可以表示为$W + \Delta W$，其中 $\Delta W$ 里的参数比 $W$ 要少得多。训练 LoRA 的目的和全参数微调是一样的，只不过 LoRA 相对而言更加高效。</p>
<p>对单张图片训练 LoRA，其实就是在把整个隐空间都变成一个能生成高质量图像的空间。但付出的代价是，模型只能生成和该图片差不多的图片。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/7.png" alt></p>
<p>我们能不能把 LoRA 的这种性质放在隐变量插值上呢？我们可以认为，LoRA 的参数 $\Delta W$ 存储了新的隐空间的一些信息。如果我们不仅对两个输入图片的隐变量做插值，还对分别对两个输入图片训练一个 LoRA，得到$\Delta W_1, \Delta W_2$，再对两个 LoRA 的参数进行插值，得到$\Delta W = \alpha \Delta W_1 + (1-\alpha) \Delta W_2$，就能让中间的插值隐变量也能生成有意义的图片，且该图片会保留两个输入图片的性质。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/8.png" alt></p>
<p>相关实验结果能支撑我们的假设。下图展示了不同 LoRA 配置下，对隐变量做插值得到的结果。第一行和第二行表示分别仅使用左图或右图的 LoRA，第三行表示对 LoRA 也进行插值。可以看出，使用 LoRA 后，所有图片的质量都还不错。固定 LoRA，对隐变量做插值时，图像的风格会随隐变量变化而变化，而图像的语义内容会与训练该 LoRA 的图像相同。而对 LoRA 也进行插值的话，图像的风格、语义都会平滑过渡。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/9.png" alt></p>
<p>下图是前文那个例子的某些中间帧不使用 LoRA 插值和使用 LoRA 插值的结果。可以看出，使用了 LoRA 后，图像质量提升了很多。而通过对 LoRA 的插值，输出图像也会保留两个输入图像的特征。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/10.png" alt></p>
<h2 id="自注意力输入的插值与替换"><a href="#自注意力输入的插值与替换" class="headerlink" title="自注意力输入的插值与替换"></a>自注意力输入的插值与替换</h2><p>使用 LoRA 插值后，中间帧的图像质量得到了大幅提升，可是图像变形不连贯的问题还是没有得到解决。要提升图像变换的连贯性，还需要使用到一项和自注意力相关的技术。</p>
<p>深度学习中常见的注意力运算都可以表示成交叉注意力 $CrossAttn(\mathbf{x}, \mathbf{y})$，它表示数据 $\mathbf{x}$ 从数据 $\mathbf{y}$ 中获取了一次信息。交叉注意力的特例是自注意力 $SelfAttn(\mathbf{x}) = CrossAttn(\mathbf{x}, \mathbf{x})$，它表示数据 $\mathbf{x}$ 自己内部做了一次信息聚合。多数扩散模型的 U-Net 都使用了自注意力层。</p>
<p>由于自注意力本质上是一种交叉注意力，我们可以把另一个图像的自注意力输入替换某图像的自注意力输入。具体来说，我们可以先生成一张参考图像，将自注意力输入$\mathbf{x’}$缓存下来。再开始生成当前图片，对于原来的自注意力计算 $CrossAttn(\mathbf{x}, \mathbf{x})$，我们把第二个 $\mathbf{x}$ 换成 $\mathbf{x’}$，让计算变成 $CrossAttn(\mathbf{x}, \mathbf{x’})$。这样，在生成当前图片时，当前图片会和参考图片更加相似一些。</p>
<p>在扩散模型生成图像时，每个去噪时刻的每个自注意力模块的输入都有自己的意义。在替换输入时，我们必须用参考图像当前时刻当前模块的输入来做替换。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/11.png" alt></p>
<p>这种注意力替换技巧通常用在基于图像扩散模型的视频编辑任务里。一般我们会以输出视频的第一帧为参考图像，让生成后续帧的自注意力模块参考第一帧的信息。这样视频每一帧的风格都会更加一致。</p>
<p>我们可以把视频编辑任务的这种技巧挪用到图像变形任务里。在图像变形中，每一个中间帧要以一定的混合比例参考两个输入图像。那么，我们也可以先分别生成两个输入图像，缓存它们的自注意力输入$\mathbf{x}_0, \mathbf{x}_1$。在生成混合比例为 $\alpha$ 的中间帧时，我们先混合自注意力输入$\mathbf{x’} = \alpha \mathbf{x}_0 + (1-\alpha) \mathbf{x}_1$，再以 $\mathbf{x’}$ 为自注意力的第二个参数，计算 $CrossAttn(\mathbf{x_{\alpha}}, \mathbf{x’})$。</p>
<p>下面是不使用/使用自注意力替换的结果。可以看出，不使用注意力替换时，视频中间某帧会出现突变。而使用了注意力替换后，视频平滑了很多。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/cmp_attn.gif" alt></p>
<p>在实验中我们也发现，直接用 $\mathbf{x’}$ 来替换注意力输入会降低中间帧的质量。为了权衡质量与过渡性，我们会让替换的注意力输入在原输入 $\mathbf{x_{\alpha}}$ 和 $\mathbf{x’}$ 之间做一个混合，即令插入的注意力输入为 $\mathbf{x’} \gets \lambda \mathbf{x’} + (1-\lambda) \mathbf{x_{\alpha}}$。最终实验中我们令 $\lambda=0.6$。</p>
<h2 id="重调度采样"><a href="#重调度采样" class="headerlink" title="重调度采样"></a>重调度采样</h2><p>通过注意力插值，我们解决了中间帧跳变的问题。然而，视频的变换速度还是不够平均。在开始和结束时，视频的变化速度较慢；而在中间时刻，视频的变化又过快。</p>
<p>我们使用了一种重新选择混合比例 $\alpha$ 的重调度策略来解决这一问题。之前，我们在选择混合比例时，是均匀地在 0~1 之间采样。比如要生成 10 段过渡，9个中间帧，我们就可以令混合比例为 $[0, 0.1, 0.2, …, 0.9, 1]$。但是，由于不同比例处插值图像的变化率不同，这样选取混合比例会导致每两帧之间变化量不均匀。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/12.png" alt></p>
<p>上图是一个可能的变化率分布图。图的横坐标是插值的混合比例，或者说视频渐变的时刻，图的纵坐标是图像内容随时间的变化率。每个矩形的面积表示相邻两帧之间的 LPIPS 感知误差。如果等间距采样混合比例的话，由于每个时刻的变化率不同，矩形的面积也不同，图像的变化会时快时慢。</p>
<p>我们希望重新选择一些采样的横坐标，使得相邻帧构成的矩形的面积尽可能一致。通过使用类似于平均颜色分布的直方图均衡化（histogram equalization）算法，我们可以得到重采样的混合比例 $[0, \alpha_1, \alpha_2, …, \alpha_{n-1}, 1]$，达到下面这种相邻帧变化量几乎相同的结果。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/13.png" alt></p>
<p>下面是不使用/使用重采样的结果。可以看出，二者生成的中间图像几乎是一致的，但左边的视频在开头和结尾会停顿一会儿，而右边的视频的内容一直都在均匀地变化。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/res_cmp.gif" alt></p>
<h2 id="在线示例与代码"><a href="#在线示例与代码" class="headerlink" title="在线示例与代码"></a>在线示例与代码</h2><p>看完了该工作的原理，我们来动手使用一下 DiffMorpher。我们先来运行一下在线示例。在线示例可以在 OpenXLab (<a target="_blank" rel="noopener" href="https://openxlab.org.cn/apps/detail/KaiwenZhang/DiffMorpher">https://openxlab.org.cn/apps/detail/KaiwenZhang/DiffMorpher</a> ) 或者 HuggingFace（<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kevin-thu/DiffMorpher">https://huggingface.co/spaces/Kevin-thu/DiffMorpher</a> ）上访问。</p>
<p>使用 WebUI 时，可以直接点击 Run 直接运行示例，或者手动上传两张图片并给定 prompt 再运行。</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/14.png" alt></p>
<p>如果你对一些细节感兴趣，也可以手动 clone GitHub 仓库。配置环境的过程也很简单，只需要准备一个有 PyTorch 的运行环境，再安装相关 Pip 包即可。注意，该项目用的 Diffusers 版本较旧，最新的 Diffusers 可能无法成功运行，建议直接照着 <code>requirements.txt</code> 里的版本来。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/Kevin-thu/DiffMorpher.git</span><br><span class="line">cd DiffMorpher</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>配置好了环境后，可以直接尝试仓库里自带的示例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python main.py \</span><br><span class="line">  --image_path_0 ./assets/Trump.jpg --image_path_1 ./assets/Biden.jpg \ </span><br><span class="line">  --prompt_0 &quot;A photo of an American man&quot; --prompt_1 &quot;A photo of an American man&quot; \</span><br><span class="line">  --output_path &quot;./results/Trump_Biden&quot; \</span><br><span class="line">  --use_adain --use_reschedule --save_inter</span><br></pre></td></tr></table></figure>
<p>运行后，就能得到下面的结果：</p>
<p><img src="/2024/03/09/20240228-DiffMorpher/biden.gif" alt></p>
<h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>图像变形任务的目标是在两个有对应关系的图像之间产生一系列合理的过渡帧。传统基于像素变形与混合的方法无法在中间帧里生成新内容。我们希望用包含丰富图像信息的预训练扩散模型来完成图像变形任务。然而，直接对扩散模型的隐变量插值，会出现中间帧质量低、结果不连贯这两个问题。为了解决这两个问题，我们对扩散模型生成两个输入图像时的诸多属性进行了插值，包括 LoRA 插值、自注意力插值，分别解决了中间帧质量与结果连贯性的问题。另外，加入了重调度采样后，输出视频的连贯性得到了进一步的提升。</p>
<p>受限于图像变形这一任务本身的上限，DiffMorpher 在实际应用中的质量难以比拟专门面向某一任务的方法（比如只做拖拽式编辑，或者只做视频插帧）。这篇工作在科研上的贡献会远大于其在应用上的贡献。方法中一些较为新颖的插值手段或许会帮助到未来的图像编辑工作。</p>
<p>尽管 DiffMorpher 已经算是一个不错的图像变形工具了，该方法并没有从本质上提升扩散模型的可编辑性。相比 GAN 而言，逐渐对扩散模型的隐变量修改难以产生平滑的输出结果。比如在拖拽式编辑任务中，DragGAN 只需要优化 GAN 的隐变量就能产生合理的编辑效果，而扩散模型中的类似工具（如 DragDiffusion, DragonDiffusion）需要更多设计才能达到同样的结果。从本质上提升扩散模型的可编辑性依然是一个值得研究的问题。</p>
<p>出于可读性的考虑，本文没有过多探讨技术细节。如果你对相关技术感兴趣，欢迎阅读我之前的文章：</p>
<p>LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例</p>
<p>Stable Diffusion 中的自注意力替换技术与 Diffusers 实现</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/02/21/20240216-Sora-Comment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/02/21/20240216-Sora-Comment/" class="post-title-link" itemprop="url">OpenAI 视频模型 Sora 科研贡献速览</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-02-21 16:46:58" itemprop="dateCreated datePublished" datetime="2024-02-21T16:46:58+08:00">2024-02-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天，一则重磅消息席卷了 AI 圈：OpenAI 发布了视频模型 Sora，能根据文本生成长达一分钟的高质量 1920x1080 视频，生成能力远超此前只能生成 25 帧 576x1024 图像的顶尖视频生成模型 Stable Video Diffusion。</p>
<p>同时，OpenAI 也公布了一篇非常简短的技术报告。报告仅大致介绍了 Sora 的架构及应用场景，并未对模型的原理详加介绍。让我们来快速浏览一下这份报告，看看科研人员从这份报告中能学到什么。</p>
<p>官网链接：<a target="_blank" rel="noopener" href="https://openai.com/sora">https://openai.com/sora</a></p>
<p>技术报告链接：<a target="_blank" rel="noopener" href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a></p>
<p>这篇文章没怎么贴视频，感兴趣的话可以对照着原报告中的视频阅读。</p>
<h2 id="LDM-与-DiT-的结合"><a href="#LDM-与-DiT-的结合" class="headerlink" title="LDM 与 DiT 的结合"></a>LDM 与 DiT 的结合</h2><p>简单来说，Sora 就是 Latent Diffusion Model (LDM) [1] 加上 Diffusion Transformer (DiT) [2]。我们先简要回顾一下这两种模型架构。</p>
<p>LDM 就是 Stable Diffusion 使用的模型架构。扩散模型的一大问题是计算需求大，难以拟合高分辨率图像。为了解决这一问题，实现 LDM时，会先训练一个几乎能无损压缩图像的自编码器，能把 512x512 的真实图像压缩成 64x64 的压缩图像并还原。接着，再训练一个扩散模型去拟合分辨率更低的压缩图像。这样，仅需少量计算资源就能训练出高分辨率的图像生成模型。</p>
<p>LDM 的扩散模型使用的模型是 U-Net。而根据其他深度学习任务中的经验，相比 U-Net，Transformer 架构的参数可拓展性强，即随着参数量的增加，Transformer 架构的性能提升会更加明显。这也是为什么大模型普遍都采用了 Transformer 架构。从这一动机出发，DiT 应运而生。DiT 在 LDM 的基础上，把 U-Net 换成了 Transformer。</p>
<p>顺带一提，Transformer 本来是用于文本任务的，它只能处理一维的序列数据。为了让 Transformer 处理二维图像，通常会把输入图像先切成边长为 $p$ 的图块，再把每个图块处理成一项数据。也就是说，原来边长为 $I$ 的正方形图片，经图块化后，变成了长度为 $(I/p)^2$ 的一维序列数据。</p>
<p><img src="/2024/02/21/20240216-Sora-Comment/1.png" alt></p>
<p>Transformer 是一种和顺序无关的计算。比如对于输入”abc”和”bca”，Transformer 会输出一模一样的值。为了描述数据的先后顺序，使用 Transformer 时，一般会给数据加一个位置编码。</p>
<p>Sora 是一个视频版的 DiT 模型。让我们看一下 Sora 在 DiT 上做了哪些改进。</p>
<h2 id="时空自编码器"><a href="#时空自编码器" class="headerlink" title="时空自编码器"></a>时空自编码器</h2><p>在此之前，许多工作都尝试把预训练 Stable Diffusion 拓展成视频生成模型。在拓展时，视频的每一帧都会单独输入进 Stable Diffusion 的自编码器，再重新构成一个压缩过的图像序列。而 VideoLDM[3] 工作发现，直接对视频使用之前的图像自编码器，会令输出视频出现闪烁的现象。为此，该工作对自编码器的解码器进行了微调，加入了一些能够处理时间维度的模块，使之能一次性处理整段压缩视频，并输出连贯的真实视频。</p>
<p>Sora 则是从头训练了一套能直接压缩视频的自编码器。相比之前的工作，Sora 的自编码器不仅能在空间上压缩图像，还能在时间上压缩视频长度。这估计是为什么 Sora 能生成长达一分钟的视频。</p>
<blockquote>
<p>报告中提到，Sora 也能处理图像，即长度为1的视频。那么，自编码器怎么在时间上压缩长度为1的视频呢？报告中并没有给出细节。我猜测该自编码器在时间维度做了填充（比如时间被压缩成原来的 1/2，那么就对输入视频填充空数据直至视频长度为偶数），也可能是输入了视频长度这一额外约束信息。</p>
</blockquote>
<h2 id="时空压缩图块"><a href="#时空压缩图块" class="headerlink" title="时空压缩图块"></a>时空压缩图块</h2><p>输入视频经过自编码器后，会被转换成一段空间和时间维度上都变小的压缩视频。这段压缩视频就是 Sora 的 DiT 的拟合对象。在处理视频数据时，DiT 较 U-Net 又有一些优势。</p>
<p>之前基于 U-Net 的去噪模型在处理视频数据时（如 [3])，都需要额外加入一些和时间维度有关的操作，比如时间维度上的卷积、自注意力。而 Sora 的 DiT 是一种完全基于图块的 Transformer 架构。要用 DiT 处理视频数据，不需要这种设计，只要把视频看成一个 3D 物体，再把 3D 物体分割成「图块」，并重组成一维数据输入进 DiT 即可。和原本图像 DiT 一样，假设视频边长为 $I$，时长也为 $I$，要切成边长为 $p$ 的图块，最后会得到 $(I/p)^3$ 个数据。</p>
<blockquote>
<p>报告没有给出视频图块化的细节。</p>
</blockquote>
<h2 id="处理任意分辨率、时长的视频"><a href="#处理任意分辨率、时长的视频" class="headerlink" title="处理任意分辨率、时长的视频"></a>处理任意分辨率、时长的视频</h2><p>报告中反复提及，Sora 在训练和生成时使用的视频可以是任何分辨率（在 1920x1080 以内）、任何长宽比、任何时长的。这意味着视频训练数据不需要做缩放、裁剪等预处理。这些特性是绝大多数其他视频生成模型做不到的，让我们来着重分析一下这一特性的原理。</p>
<p>Sora 的这种性质还是得益于 Transformer 架构。前文提到，Transformer 的计算与输入顺序无关，必须用位置编码来指明每个数据的位置。尽管报告没有提及，我觉得 Sora 的 DiT 使用了类似于 $(x, y, t)$ 的位置编码来表示一个图块的时空位置。这样，不管输入的视频的大小如何，长度如何，只要给每个图块都分配一个位置编码，DiT 就能分清图块间的相对关系了。</p>
<p>相比以前的工作，Sora 的这种设计是十分新颖的。之前基于 U-Net 的 Stable Diffusion 为了保证所有训练数据可以统一被处理，输入图像都会被缩放与裁剪至同一大小。由于训练数据中有被裁剪的图像，模型偶尔也会生成被裁剪的图像。生成训练分辨率以外的图像时，模型的表现有时也会不太好。SDXL [4] 的解决方式是把裁剪的长宽做为额外信息输入进 U-Net。为了生成没有裁剪的图像，只要令输入的裁剪长宽为 0 即可。类似地，SDXL 也把图像分辨率做为额外输入，使得 U-Net 学习不同分辨率、长宽比的图像。相比 SDXL，Sora 的做法就简洁多了。</p>
<p>之前基于 DiT 的模型 （比如华为的 PixArt [5]）似乎都没有利用到 Transformer 可以随意设置位置编码这一性质。DiT 在处理输入图块时，会先把图块变形成一维数据，再从左到右编号，即从从左到右，从上到下地给二维图块组编号。这种位置编码并没有保留图像的二维空间信息，因此，在这种编码下，模型的输入分辨率必须固定。比如对于下面这个$4\times4$的图块组，如果是从左到右、从上到下编码，模型等于是强行学习到了「1号在0号右边、4号在0号下面」这样的位置信息。如果输入的图块形状为 $4 \times 5$，那么图块间的相对关系就完全对不上了。而如果像 Sora 这样以视频图块的 $(x, y, t)$ 来生成位置编码的话，就没有这种问题了，输入视频可以是任何分辨率、任何长度。</p>
<p><img src="/2024/02/21/20240216-Sora-Comment/2.png" alt></p>
<h2 id="Transformer-在视频生成的可拓展性"><a href="#Transformer-在视频生成的可拓展性" class="headerlink" title="Transformer 在视频生成的可拓展性"></a>Transformer 在视频生成的可拓展性</h2><p>前文提过，Transformer 的特点就是可拓展性强，即模型越大，训练越久，效果越好。报告中展示了1倍、4倍、16倍某单位训练时间下的生成结果，可以看出模型确实一直有进步。</p>
<p><img src="/2024/02/21/20240216-Sora-Comment/3.png" alt></p>
<h2 id="语言理解能力"><a href="#语言理解能力" class="headerlink" title="语言理解能力"></a>语言理解能力</h2><p>之前大部分文生图扩散模型都是在人工标注的图片-文字数据集上训练的。后来大家发现，人工标注的图片描述质量较低，纷纷提出了各种提升标注质量的方法。Sora 复用了自家 DALL·E 3 的重标注技术，用一个训练的能生成详细描述的标注器来重新为训练视频生成标注。这种做法不仅解决了视频缺乏标注的问题，且相比人工标注质量更高。Sora 的部分结果展示了其强大了抽象理解能力（如理解人和猫之间的交互），这多半是因为视频标注模型足够强大，视频生成模型学到了视频标注模型的知识。但同样，视频标注模型的相关细节完全没有公开。</p>
<h2 id="其他生成功能"><a href="#其他生成功能" class="headerlink" title="其他生成功能"></a>其他生成功能</h2><ul>
<li>基于已有图像和视频进行生成：除了约束文本外，Sora 还支持在一个视频前后补充内容（如果是在一张图片后面补充内容，就是图生视频）。报告没有给出实现细节，我猜测是直接做了反演（inversion）再把反演得到的隐变量替换到随机初始隐变量中。</li>
<li>视频编辑：报告明确写出，只用简单的 SDEdit （即目前 Stable Diffusion 中的图生图）即可实现视频编辑。</li>
<li>视频内容融合：可能是对两个视频的初始隐变量做了插值。</li>
<li>图像生成：当然，Sora 也可以生成图像。报告表明，Sora 可以生成最大 2048x2048 的图像。</li>
</ul>
<h2 id="涌现出的能力"><a href="#涌现出的能力" class="headerlink" title="涌现出的能力"></a>涌现出的能力</h2><p>通过学习大量数据，Sora 还涌现出一些意想不到的能力。</p>
<ul>
<li>3D 一致性：视频中包含自然的相机视角变换。之前的 Stable Video Diffusion 也有类似发现。</li>
<li>长距离连贯性：AI 生成出来的视频往往有物体在中途突然消失的情况。而 Sora 有时候能克服这一问题。</li>
<li>与世界的交互：比如在描述画画的视频中，画纸上的内容随画笔生成。</li>
<li>模拟数字世界：报告展示了在输入文本有”Minecraft”时，模型能生成非常真实的 Minecraft 游戏视频。这大概只能说明模型的拟合能力太强了，以至于学会了生成 Minecraft 这一种特定风格的视频。 </li>
</ul>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><p>报告结尾还是给出了一些失败的生成示例，比如玻璃杯在桌子上没有摔碎。这表明模型还不能完全学会某些物理性质。然而，我觉得现阶段 Sora 已经展示了足够强大的学习能力。想模拟现有视频中已经包含的物理现象，只需要增加数据就行了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Sora 是一个惊艳的视频生成模型，它以卓越的生成能力（高分辨率、长时间）与生成质量令一众同期的视频生成模型黯然失色。Sora 的技术报告非常简短，不过我们从中还是可以学到一些东西。从技术贡献上来看，Sora 的创新主要有两点：</p>
<ol>
<li>让 LDM 的自编码器也在视频时间维度上压缩。</li>
<li>使用了一种不限制输入形状的 DiT</li>
</ol>
<p>其中，第二点贡献是非常有启发性的。DiT 能支持不同形状的输入，大概率是因为它以视频的3D位置生成位置编码，打破了一维编码的分辨率限制。后续大家或许会逐渐从 U-Net 转向 DiT 来建模扩散模型的去噪模型。</p>
<p>我认为 Sora 的成功有三个原因。前两个原因对应两项创新。第一，由于在时间维度上也进行了压缩，Sora 最终能生成长达一分钟的视频；第二，使用 DiT 不仅去除了视频空间、时间长度上的限制，还充分利用了 Transformer 本身的可拓展性，使训练一个视频生成大模型变得可能。第三个原因来自于视频标注模型。之前 Stable Diffusion 能够成功，很大程度上是因为有一个能够关联图像与文本的 CLIP 模型，且有足够多的带标注图片。相比图像，视频训练本来就少，带标注的视频就更难获得了。一个能够理解视频内容，生成详细视频标注的标注器，一定是让视频生成模型理解复杂文本描述的关键。除了这几点原因外，剩下的就是砸钱、扩大模型、加数据了。</p>
<p>Sora 显然会对 AIGC 社区产生一定影响。对于 AIGC 爱好者而言，他们或许会多了一些生成创意视频的方法，比如给部分帧让 Sora 来根据文本补全剩余帧。当然，目前 Sora 依然不能取代视频创作者，长视频的质量依然有待观察。对于正在开发相似应用的公司，我觉得他们应该要连夜撤销之前的方案，转换为这套没有分辨率限制的 DiT 的方案。他们的压力应该会很大。对于相关科研人员而言，除了学习这种较为新颖的 DiT 用法外，也没有太多收获了。这份技术报告透露出一股「我绝对不会开源」的意思。没有开源模型，普通的研究者也就什么都做不了。新技术的诞生绝对不可能靠一家公司，一个模型就搞定。像之前的 Stable Diffusion，也是先开源了一个基础模型，科研者和爱好者再补充了各种丰富的应用。我呼吁各大公司尽快训练并开源一个这种不限分辨率的 DiT，这样科研界或许会抛开 U-Net，基于 DiT 开发出新的扩散模型应用。</p>
<h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><ol>
<li>Latent Diffusion Model, Stable Difusion: High-Resolution Image Synthesis with Latent Diffusion Models</li>
<li>DiT: Scalable Diffusion Models with Transformers</li>
<li>VideoLDM: Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</li>
<li>SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</li>
<li>PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/27/20240123-SD-Attn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/27/20240123-SD-Attn/" class="post-title-link" itemprop="url">Stable Diffusion 中的自注意力替换技术与 Diffusers 实现</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-27 16:48:55" itemprop="dateCreated datePublished" datetime="2024-01-27T16:48:55+08:00">2024-01-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在使用预训练 Stable Diffusion (SD) 生成图像时，如果将其 U-Net 的自注意力层在某去噪时刻的输入 K, V 替换成另一幅参考图像的，则输出图像会和参考图像更加相似。许多无需训练的 SD 编辑科研工作都运用了此性质。尤其对于是对于视频编辑任务，如果在生成某一帧时将注意力输入替换成之前帧的，则输出视频会更加连贯。在这篇文章中，我们将快速学习 SD 自注意力替换技术的原理，并在 Diffusers 里实现一个基于此技术的视频编辑流水线。</p>
<h2 id="注意力计算"><a href="#注意力计算" class="headerlink" title="注意力计算"></a>注意力计算</h2><p>我们先来回顾一下 Transformer 论文中提出的注意力机制。所有注意力机制都基于一种叫做放缩点乘注意力（Scaled Dot-Product Attention）的运算：</p>
<script type="math/tex; mode=display">
Attention(Q, K, V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中，$Q \in \mathbb{R}^{a \times d_k}, K \in \mathbb{R}^{b \times d_k}, V \in \mathbb{R}^{b \times d_v}$。注意力计算可以理解成先算 $a$ 个长度为 $d_k$ 的向量对 $b$ 个长度为 $d_k$ 的向量的相似度，再以此相似度为权重算 $a$ 个向量对 $b$ 个长度为 $d_v$ 的向量的加权和。</p>
<p>注意力计算是没有可学习参数的。为了加入参数，Transformer 设计了如下所示的注意力层，其中 $W^Q, W^K, W^V, W^O$ 都是参数。</p>
<script type="math/tex; mode=display">
AttnLayer(Q, K, V) = Attention(QW^Q, KW^K, VW^V)W^O</script><p>一般在使用注意力层时，会让$K=V$。这种注意力叫做交叉注意力。交叉注意力可以理解成数据 $A$ 想从数据 $B$ 里提取信息，提取的根据是 $A$ 里每个向量和 $B$ 里每个向量的相似度。 </p>
<script type="math/tex; mode=display">
CrossAttnLayer(A, B) = Attention(AW^Q, BW^K, BW^V)W^O</script><p>交叉注意力的特例是自注意力，此时 $Q=K=V$ 。这表示数据里的向量两两之间交换了一次信息。</p>
<script type="math/tex; mode=display">
SelfAttnLayer(A) = Attention(AW^Q, AW^K, AW^V)W^O</script><h2 id="SD-中的自注意力替换"><a href="#SD-中的自注意力替换" class="headerlink" title="SD 中的自注意力替换"></a>SD 中的自注意力替换</h2><p>SD 的 U-Net 既用到了自注意力，也用到了交叉注意力。自注意力用于图像特征自己内部信息聚合。交叉注意力用于让生成图像对齐文本，其 Q 来自图像特征，K, V 来自文本编码。</p>
<p><img src="/2024/01/27/20240123-SD-Attn/1.png" alt></p>
<p>由于自注意力其实可以看成一种特殊的交叉注意力，我们可以把自注意力的 K, V 替换成来自另一幅参考图像的特征。这样，扩散模型的生成图片会既和原本要生成的图像相似，又和参考图像相似。当然，用来替换的特征必须和原来的特征「格式一致」，不然就生成不了有意义的结果了。</p>
<p><img src="/2024/01/27/20240123-SD-Attn/2.png" alt></p>
<p>什么叫「格式一致」呢？我们知道，扩散模型在采样时有很多步，U-Net 中又有许多自注意力层。每一步时的每一个自注意力层的输入都有自己的「格式」。也就是说，如果你要把某时刻某自注意力层的 K, V 替换，就得先生成参考图像，用生成参考图像过程中此时刻此自注意力层的输入替换，而不能用其他时刻或者其他自注意力层的。</p>
<p><img src="/2024/01/27/20240123-SD-Attn/3.png" alt></p>
<p><img src="/2024/01/27/20240123-SD-Attn/4.png" alt></p>
<p>一般这种编辑技术只会用在自注意力层而不是交叉注意力层上，这是因为 SD 中的交叉注意力是用来关联图像与文字的，另一幅图像的信息无法输入。当然，除了 SD，只要是用到了自注意力模块的扩散模型，都能用此方法编辑，只不过大部分工作都是基于 SD 开发的。</p>
<h2 id="自注意力替换的应用"><a href="#自注意力替换的应用" class="headerlink" title="自注意力替换的应用"></a>自注意力替换的应用</h2><p>自注意力替换最常见的应用是提升 SD 视频编辑的连续性。在此任务中，一般会先正常编辑第一帧，再将后续帧的自注意力的 K, V 替换成第一帧的。这种技术在文献中一般被称为帧间注意力（cross-frame attention）。较早提出此论文的工作是 Text2Video-Zero。</p>
<p>自注意力替换也可以用于提升单幅图像编辑的保真度。一个例子是拖拽单幅图像的 DragonDiffusion。此应用可以拓展到图像插值上，比如 DiffMorpher 在图像插值时对两幅参考图像的自注意力输入等比例插值，再替换掉对应插值图像的自注意力的 K, V。</p>
<h2 id="在-Diffusers-里实现自注意力替换"><a href="#在-Diffusers-里实现自注意力替换" class="headerlink" title="在 Diffusers 里实现自注意力替换"></a>在 Diffusers 里实现自注意力替换</h2><p>Diffusers 的 U-Net 专门提供了用于修改注意力计算的 <code>AttentionProcessor</code> 类。借助相关接口，我们可以方便地修改注意力的计算方法。在这个示例项目中，我们来用 Diffusers 实现一个参考第一帧和上一帧的注意力输入的 SD 视频编辑流水线。相比逐帧生成编辑图片，该流水线的结果会更加平滑一点。项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/ReplaceAttn">https://github.com/SingleZombie/DiffusersExample/tree/main/ReplaceAttn</a> 。</p>
<h3 id="AttentionProcessor"><a href="#AttentionProcessor" class="headerlink" title="AttentionProcessor"></a><code>AttentionProcessor</code></h3><p>在 Diffusers 中，U-Net 的每一个注意力模块都有一个 <code>AttentionProcessor</code> 类的实例。<code>AttentionProcessor</code> 类的 <code>__call__</code> 方法描述了注意力计算的过程。如果我们想修改某些注意力模块的计算，就需要自己定义一个注意力处理类，其 <code>__call__</code> 方法的参数需与 <code>AttentionProcessor</code> 的兼容。之后，我们再调用相关接口把原来的处理类换成我们自己写的处理类。下面我们将先看一下 <code>AttentionProcessor</code> 类的实现细节，再实现我们自己的<br>注意力处理类。</p>
<p><code>AttentionProcessor</code> 类在 <code>diffusers/models/attention_processor.py</code> 文件里。它只有一个 <code>__call__</code> 方法，其主要内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnProcessor</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        attn: Attention,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        temb: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        scale: <span class="built_in">float</span> = <span class="number">1.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>) -&gt; torch.Tensor:</span></span><br><span class="line">        residual = hidden_states</span><br><span class="line">        query = attn.to_q(hidden_states, *args)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_hidden_states = hidden_states</span><br><span class="line"></span><br><span class="line">        key = attn.to_k(encoder_hidden_states, *args)</span><br><span class="line">        value = attn.to_v(encoder_hidden_states, *args)</span><br><span class="line"></span><br><span class="line">        query = attn.head_to_batch_dim(query)</span><br><span class="line">        key = attn.head_to_batch_dim(key)</span><br><span class="line">        value = attn.head_to_batch_dim(value)</span><br><span class="line"></span><br><span class="line">        attention_probs = attn.get_attention_scores(query, key, attention_mask)</span><br><span class="line">        hidden_states = torch.bmm(attention_probs, value)</span><br><span class="line">        hidden_states = attn.batch_to_head_dim(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear proj</span></span><br><span class="line">        hidden_states = attn.to_out[<span class="number">0</span>](hidden_states, *args)</span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        hidden_states = attn.to_out[<span class="number">1</span>](hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn.residual_connection:</span><br><span class="line">            hidden_states = hidden_states + residual</span><br><span class="line"></span><br><span class="line">        hidden_states = hidden_states / attn.rescale_output_factor</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>方法参数中，<code>hidden_states</code> 是 Q， <code>encoder_hidden_states</code> 是 K, V。如果 K, V 没有传入（为 <code>None</code>），则 K, V 会被赋值成 Q。该方法的实现细节和 Tranformer 中的注意力层完全一样，此处就不多加解释了。一般替换注意力的输入时，我们不用改这个方法的实现，只会在需要的时候调用这个方法。</p>
<script type="math/tex; mode=display">
AttnLayer(Q, K, V) = Attention(QW^Q, KW^K, VW^V)W^O</script><p><code>attention_processor.py</code> 文件中还有一个功能类似的类 <code>AttnProcessor2_0</code>，它和 <code>AttentionProcessor</code> 的区别在于它调用了 PyTorch 2.0 起启用的算子 <code>F.scaled_dot_product_attention</code> 代替手动实现的注意力计算。这个算子更加高效，如果你确定 PyTorch 版本至少为 2.0，就可以用 <code>AttnProcessor2_0</code> 代替 <code>AttentionProcessor</code>。</p>
<p>看完了 <code>AttentionProcessor</code> 类后，我们来看该怎么在 U-Net 里将原注意力处理类替换成我们自己写的。U-Net 类的 <code>attn_processors</code> 属性会返回一个词典，它的 key 是每个处理类所在位置，比如 <code>down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor</code>，它的 value 是每个处理类的实例。为了替换处理类，我们需要构建一个格式一样的词典<code>attn_processor_dict</code>，再调用 <code>unet.set_attn_processor(attn_processor_dict)</code> ，取代原来的 <code>attn_processors</code>。假如我们自己实现了处理类 <code>MyAttnProcessor</code>，我们可以编写下面的代码来实现替换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">attn_processor_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">    <span class="keyword">if</span> we_want_to_modify(k):</span><br><span class="line">        attn_processor_dict[k] = MyAttnProcessor()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">unet.set_attn_processor(attn_processor_dict)</span><br></pre></td></tr></table></figure>
<h3 id="实现帧间注意力处理类"><a href="#实现帧间注意力处理类" class="headerlink" title="实现帧间注意力处理类"></a>实现帧间注意力处理类</h3><p>熟悉了 <code>AttentionProcessor</code> 类的相关内容，我们来编写自己的帧间注意力处理类。在处理第一帧时，该类的行为不变。对于之后的每一帧，该类的 K, V 输入会被替换成视频第一帧和上一帧的输入在序列长度维度上的拼接结果，即：</p>
<script type="math/tex; mode=display">
CrossFrameAttn(A, A_1, A_{prev}) = CrossAttnLayer(A, [A_1, A_{prev}])</script><blockquote>
<p>你是否会感到疑惑：为什么 K, V 的序列长度可以修改？别忘了，在注意力计算中，Q, K, V 的形状分别是：$Q \in \mathbb{R}^{a \times d_k}, K \in \mathbb{R}^{b \times d_k}, V \in \mathbb{R}^{b \times d_v}$。注意力计算只要求 K，V 的序列长度 $b$ 相同，并没有要求 Q, K 的序列长度相同。</p>
</blockquote>
<p>现在，注意力计算不再是一个没有状态的计算，它的运算结果取决于第一帧和上一帧的输入。因此，我们在注意力处理类中需要额外维护这两个变量。我们可以按照如下代码编写类的构造函数。除了处理继承外，我们还需要创建两个数据词典来存储不同时间戳下第一帧和上一帧的注意力输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossFrameAttnProcessor</span>(<span class="params">AttnProcessor</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.first_maps = &#123;&#125;</span><br><span class="line">        self.prev_maps = &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>在运行方法中，我们根据 <code>encoder_hidden_states</code> 是否为空来判断该注意力是自注意力还是交叉注意力。我们仅修改自注意力。当该注意力为自注意力时，假设我们知道了当前时刻 <code>t</code>，我们就可以根据 <code>t</code> 获取当前时刻第一帧和前一帧的输入，并将它们拼接起来得到 <code>cross_map</code>。以此 <code>cross_map</code> 为当前注意力的 K, V，我们就实现了帧间注意力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, attn: Attention, hidden_states, encoder_hidden_states=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Is self attention</span></span><br><span class="line">        cross_map = torch.cat(</span><br><span class="line">            (self.first_maps[t], self.prev_maps[t]), dim=<span class="number">1</span>)</span><br><span class="line">        res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Is cross attention</span></span><br><span class="line">        res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<blockquote>
<p>由于 Diffusers 经常修改函数接口，在调用普通的注意力计算接口时，最好原封不动地按照 <code>super().__call__(..., **kwargs)</code> 写，不然这份代码就不能兼容后续版本的 Diffusers。</p>
</blockquote>
<p>上述代码只描述了后续帧的行为。如前所述，我们的注意力计算有两种行为：对于第一帧，我们不修改注意力的计算过程，只缓存其输入；对于之后每一帧，我们替换注意力的输入，同时维护当前「上一帧」的输入。既然注意力在不同情况下有不同行为，我们就应该用一个变量来记录当前状态，让 <code>__call__</code> 能根据此变量决定当前的行为。相关的伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, attn: Attention, hidden_states, encoder_hidden_states=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># Is self attention</span></span><br><span class="line">        <span class="keyword">if</span> self.state == FIRST_FRAME:</span><br><span class="line">            res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line">            <span class="comment"># update maps</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cross_map = torch.cat(</span><br><span class="line">                (self.first_maps[t], self.prev_maps[t]), dim=<span class="number">1</span>)</span><br><span class="line">            res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line">            <span class="comment"># update maps</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Is cross attention</span></span><br><span class="line">        res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>在伪代码中，<code>self.state</code> 表示当前注意力的状态，它的值表明注意力计算是在处理第一帧还是后续帧。在视频编辑流水线中，我们应按照下面的伪代码，先编辑第一帧，再修改注意力状态后编辑后续帧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">edit(frames[<span class="number">0</span>])</span><br><span class="line">set_attn_state(SUBSEQUENT_FRAMES)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(frames)):</span><br><span class="line">    edit(frames[i])</span><br></pre></td></tr></table></figure>
<p>现在，有一个问题：我们该怎么修改怎么每一个注意力模块的处理器的状态呢？显然，最直接的方式是想办法访问每一个注意力模块的处理器，再直接修改对象的属性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">modules = unet.get_attn_moduels</span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> modules:</span><br><span class="line">    <span class="keyword">if</span> we_want_to_modify(module):</span><br><span class="line">        module.processor.state = ...</span><br></pre></td></tr></table></figure>
<p>但是，每次都去遍历所有模块会让代码更加凌乱。同时，这样写也会带来代码维护上的问题：我们每次遍历注意力模块时，都可能要判断该注意力模块是否应该修改。而在用前面讲过的处理类替换方法 <code>unet.set_attn_processor</code> 时，我们也得判断一遍。同一段逻辑重复写在两个地方，非常不利于代码更新。</p>
<p>一种更优雅的实现方式是：我们定义一个状态管理类，所有注意力处理器都从同一个全局状态管理类对象里获取当前的状态信息。想修改每一个处理器的状态，不需要遍历所有对象，只需要改一次全局状态管理类对象就行了。</p>
<p>按照这种实现方式，我们先编写一个状态类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnState</span>:</span></span><br><span class="line">    STORE = <span class="number">0</span></span><br><span class="line">    LOAD = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.reset()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__state = AttnState.STORE</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_load</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__state = AttnState.LOAD</span><br></pre></td></tr></table></figure>
<p>在注意力处理类中，我们在初始化时保存状态类对象的引用，在运行时根据状态类对象获取当前状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossFrameAttnProcessor</span>(<span class="params">AttnProcessor</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, attn_state: AttnState</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn_state = attn_state</span><br><span class="line">        self.first_maps = &#123;&#125;</span><br><span class="line">        self.prev_maps = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, attn: Attention, hidden_states, encoder_hidden_states=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Is self attention</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.attn_state.state == AttnState.STORE:</span><br><span class="line">                res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cross_map = torch.cat(</span><br><span class="line">                    (self.first_maps[t], self.prev_maps[t]), dim=<span class="number">1</span>)</span><br><span class="line">                res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Is cross attention</span></span><br><span class="line">            res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>到目前为止，假设已经维护好了之前的输入，我们的注意力处理类能执行两种不同的行为了。现在，我们来实现之前输入的维护。使用之前的注意力输入时，我们其实需要知道当前的时刻 <code>t</code>。当前的时刻也算是另一个状态，最好是也在状态管理类里维护。但为了简化我们的代码，我们可以偷懒让每个处理类自己维护当前时刻。具体做法是：如果知道了去噪迭代的总时刻数，我们就可以令当前时刻从0开始不断自增，直到最大时刻时，再重置为0。加入了时刻处理及之前输入维护的完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttnState</span>:</span></span><br><span class="line">    STORE = <span class="number">0</span></span><br><span class="line">    LOAD = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.reset()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">timestep</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__timestep</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_timestep</span>(<span class="params">self, t</span>):</span></span><br><span class="line">        self.__timestep = t</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__state = AttnState.STORE</span><br><span class="line">        self.__timestep = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">to_load</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.__state = AttnState.LOAD</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossFrameAttnProcessor</span>(<span class="params">AttnProcessor</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, attn_state: AttnState</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn_state = attn_state</span><br><span class="line">        self.cur_timestep = <span class="number">0</span></span><br><span class="line">        self.first_maps = &#123;&#125;</span><br><span class="line">        self.prev_maps = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, attn: Attention, hidden_states, encoder_hidden_states=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Is self attention</span></span><br><span class="line"></span><br><span class="line">            tot_timestep = self.attn_state.timestep</span><br><span class="line">            <span class="keyword">if</span> self.attn_state.state == AttnState.STORE:</span><br><span class="line">                self.first_maps[self.cur_timestep] = hidden_states.detach()</span><br><span class="line">                self.prev_maps[self.cur_timestep] = hidden_states.detach()</span><br><span class="line">                res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tmp = hidden_states.detach()</span><br><span class="line">                cross_map = torch.cat(</span><br><span class="line">                    (self.first_maps[self.cur_timestep], self.prev_maps[self.cur_timestep]), dim=<span class="number">1</span>)</span><br><span class="line">                res = <span class="built_in">super</span>().__call__(attn, hidden_states, cross_map, **kwargs)</span><br><span class="line">                self.prev_maps[self.cur_timestep] = tmp</span><br><span class="line"></span><br><span class="line">            self.cur_timestep += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> self.cur_timestep == tot_timestep:</span><br><span class="line">                self.cur_timestep = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Is cross attention</span></span><br><span class="line">            res = <span class="built_in">super</span>().__call__(attn, hidden_states, encoder_hidden_states, **kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>代码中，<code>tot_timestep</code> 为总时刻数，<code>cur_timestep</code> 为当前时刻。每运算一次，<code>cur_timestep</code> 加一，直至总时刻时再归零。在处理第一帧时，我们把当前时刻的输入同时存入第一帧缓存 <code>first_maps</code> 和上一帧缓存 <code>prev_maps</code> 中。对于后续帧，我们先做替换过输入的注意力计算，再更新上一帧缓存 <code>prev_maps</code>。</p>
<h3 id="视频编辑流水线"><a href="#视频编辑流水线" class="headerlink" title="视频编辑流水线"></a>视频编辑流水线</h3><p>准备好了我们自己写的帧间注意力处理类后，我们来编写一个简单的 Diffusers 视频处理流水线。该流水线基于 ControlNet 与图生图流水线，其主要代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VideoEditingPipeline</span>(<span class="params">StableDiffusionControlNetImg2ImgPipeline</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(...)</span><br><span class="line">        self.attn_state = AttnState()</span><br><span class="line">        attn_processor_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">            <span class="keyword">if</span> k.startswith(<span class="string">&quot;up&quot;</span>):</span><br><span class="line">                attn_processor_dict[k] = CrossFrameAttnProcessor(</span><br><span class="line">                    self.attn_state)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">        self.unet.set_attn_processor(attn_processor_dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, *args, images=<span class="literal">None</span>, control_images=<span class="literal">None</span>,  **kwargs</span>):</span></span><br><span class="line">        self.attn_state.reset()</span><br><span class="line">        self.attn_state.set_timestep(</span><br><span class="line">            <span class="built_in">int</span>(kwargs[<span class="string">&#x27;num_inference_steps&#x27;</span>] * kwargs[<span class="string">&#x27;strength&#x27;</span>]))</span><br><span class="line">        outputs = [<span class="built_in">super</span>().__call__(</span><br><span class="line">            *args, **kwargs, image=images[<span class="number">0</span>], control_image=control_images[<span class="number">0</span>]).images[<span class="number">0</span>]]</span><br><span class="line">        self.attn_state.to_load()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(images)):</span><br><span class="line">            image = images[i]</span><br><span class="line">            control_image = control_images[i]</span><br><span class="line">            outputs.append(<span class="built_in">super</span>().__call__(</span><br><span class="line">                *args, **kwargs, image=image, control_image=control_image).images[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>在构造函数中，我们创建了一个全局注意力状态对象 <code>attn_state</code>。它的引用会传给每一个帧间注意力处理对象。一般修改自注意力模块时，只会修改 U-Net 上采样部分的，而不会动下采样部分和中间部分的。因此，在过滤注意力模块时，我们的判断条件是 <code>k.startswith(&quot;up&quot;)</code>。把新的注意力处理器词典填完后，用 <code>unet.set_attn_processor</code> 更新所有的处理类对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">self.attn_state = AttnState()</span><br><span class="line">attn_processor_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">    <span class="keyword">if</span> k.startswith(<span class="string">&quot;up&quot;</span>):</span><br><span class="line">        attn_processor_dict[k] = CrossFrameAttnProcessor(</span><br><span class="line">            self.attn_state)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">self.unet.set_attn_processor(attn_processor_dict)</span><br></pre></td></tr></table></figure>
<p>在 <code>__call__</code> 方法中，我们要基于原图像编辑流水线 <code>super().__call__()</code>，实现我们的视频编辑流水线。在这个过程中，我们的主要任务是维护好注意力管理对象中的状态。一开始，我们要把管理类重置，根据参数设置最大去噪时刻数。经重置后，注意力处理器的状态默认为 <code>STORE</code>，即会保存第一帧的输入。处理完第一帧后，我们运行 <code>attn_state.to_load()</code> 改变注意力处理器的状态，让它们每次做注意力运算时先读第一帧和上一帧的输入，再维护上一帧输入的缓存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, *args, images=<span class="literal">None</span>, control_images=<span class="literal">None</span>,  **kwargs</span>):</span></span><br><span class="line">    self.attn_state.reset()</span><br><span class="line">    self.attn_state.set_timestep(</span><br><span class="line">        <span class="built_in">int</span>(kwargs[<span class="string">&#x27;num_inference_steps&#x27;</span>] * kwargs[<span class="string">&#x27;strength&#x27;</span>]))</span><br><span class="line">    outputs = [<span class="built_in">super</span>().__call__(</span><br><span class="line">        *args, **kwargs, image=images[<span class="number">0</span>], control_image=control_images[<span class="number">0</span>]).images[<span class="number">0</span>]]</span><br><span class="line">    self.attn_state.to_load()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(images)):</span><br><span class="line">        image = images[i]</span><br><span class="line">        control_image = control_images[i]</span><br><span class="line">        outputs.append(<span class="built_in">super</span>().__call__(</span><br><span class="line">            *args, **kwargs, image=image, control_image=control_image).images[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>运行该流水线的示例脚本在项目根目录下的 <code>replace_attn.py</code> 文件中。示例中使用的视频可以在 <code>https://github.com/williamyang1991/Rerender_A_Video/blob/main/videos/pexels-koolshooters-7322716.mp4</code> 下载，下载后应重命名为 <code>woman.mp4</code>。不使用和使用新注意力处理器的输出结果如下：</p>
<p><img src="/2024/01/27/20240123-SD-Attn/output.gif" alt></p>
<p>可以看出，虽然注意力替换不能解决生成视频的闪烁问题，但帧间的一致性提升了不少。将注意力替换技术和其他技术结合起来的话，我们就能得到一个不错的 SD 视频生成工具。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>扩散模型中的自注意力替换是一种常见的提升图片一致性的技术。该技术的实现方法是将扩散模型 U-Net 中自注意力的 K, V 输入替换成另一幅图片的。在这篇文章中，我们学习了一个较为复杂的基于 Diffusers 开发的自注意力替换示例项目，用于提升 SD 视频生成的一致性。在这个过程中，我们学习了和 <code>AttentionProcessor</code> 相关接口函数的使用，并了解了如何基于全局管理类实现一个代码可维护性强的多行为注意力处理类。如果你能看懂这篇文章的示例，那你在开发 Diffusers 的注意力处理类时基本上不会碰到任何难题。</p>
<p>项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/ReplaceAttn">https://github.com/SingleZombie/DiffusersExample/tree/main/ReplaceAttn</a></p>
<p>如果你想进一步学习 Diffusers 中视频编辑流水线的开发，可以参考我给 Diffusers 写的流水线：<a target="_blank" rel="noopener" href="https://github.com/huggingface/diffusers/tree/main/examples/community#Rerender_A_Video">https://github.com/huggingface/diffusers/tree/main/examples/community#Rerender_A_Video</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/23/20240114-SD-LoRA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/23/20240114-SD-LoRA/" class="post-title-link" itemprop="url">LoRA 在 Stable Diffusion 中的三种应用：原理讲解与代码示例</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-23 19:42:10" itemprop="dateCreated datePublished" datetime="2024-01-23T19:42:10+08:00">2024-01-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>如果你一直关注 Stable Diffusion (SD) 社区，那你一定不会对 “LoRA” 这个名词感到陌生。社区用户分享的 SD LoRA 模型能够修改 SD 的画风，使之画出动漫、水墨或像素等风格的图片。但实际上，LoRA 不仅仅能改变 SD 的画风，还有其他的妙用。在这篇文章中，我们会先简单学习 LoRA 的原理，再认识科研中 LoRA 的三种常见应用：1） 还原单幅图像；2）风格调整；3）训练目标调整，最后阅读两个基于 Diffusers 的 SD LoRA 代码实现示例。</p>
<h2 id="LoRA-的原理"><a href="#LoRA-的原理" class="headerlink" title="LoRA 的原理"></a>LoRA 的原理</h2><p>在认识 LoRA 之前，我们先来回顾一下迁移学习的有关概念。迁移学习指在一次新的训练中，复用之前已经训练过的模型的知识。如果你自己动手训练过深度学习模型，那你应该不经意间地使用到了迁移学习：比如你一个模型训练了 500 步，测试后发现效果不太理想，于是重新读取该模型的参数，又继续训练了 100 步。之前那个被训练过的模型叫做预训练模型（pre-trained model），继续训练预训练模型的过程叫做微调（fine-tune）。</p>
<p>知道了微调的概念，我们就能来认识 LoRA 了。LoRA 的全称是 Low-Rank Adaptation (低秩适配)，它是一种 Parameter-Efficient Fine-Tuning (参数高效微调，PEFT) 方法，即在微调时只训练原模型中的部分参数，以加速微调的过程。相比其他的 PEFT 方法，LoRA 之所以能脱颖而出，是因为它有几个明显的优点：</p>
<ul>
<li>从性能上来看，使用 LoRA 时，只需要存储少量被微调过的参数，而不需要把整个新模型都保存下来。同时，LoRA 的新参数可以和原模型的参数合并到一起，不会增加模型的运算时间。</li>
<li>从功能上来看，LoRA 维护了模型在微调中的「变化量」。通过用一个介于 0~1 之间的混合比例乘变化量，我们可以控制模型的修改程度。此外，基于同一个原模型独立训练的多个 LoRA 可以同时使用。</li>
</ul>
<p>这些优点在 SD LoRA 中的体现为：</p>
<ul>
<li>SD LoRA 模型一般都很小，一般只有几十 MB。</li>
<li>SD LoRA 模型的参数可以合并到 SD 基础模型里，得到一个新的 SD 模型。</li>
<li>可以用一个 0~1 之间的比例来控制 SD LoRA 新画风的程度。</li>
<li>可以把不同画风的 SD LoRA 模型以不同比例混合。</li>
</ul>
<p>为什么 LoRA 能有这些优点呢？LoRA 名字中的 「低秩」又是什么意思呢？让我们从 LoRA 的优点入手，逐步揭示它原理。</p>
<p>上文提到过，LoRA 之所以那么灵活，是因为它维护了模型在微调过程中的变化量。那么，假设我们正在修改模型中的一个参数 $W \in \mathbb{R}^{d \times d}$，我们就应该维护它的变化量 $\Delta W \in \mathbb{R}^{d \times d}$，训练时的参数用 $W + \Delta W$ 表示。这样，想要在推理时控制模型的修改程度，只要添加一个 $\alpha \in [0, 1]$，令使用的参数为 $W + \alpha \Delta W$即可。</p>
<p>可是，这样做我们还是要记录一个和原参数矩阵一样大的参数矩阵 $\Delta W$，这就算不上是参数<strong>高效</strong>微调了。为此，LoRA 的作者提出假设：模型参数在微调时的变化量中蕴含的信息没有那么多。为了用更少的信息来表示参数的变化量$\Delta W$，我们可以把$\Delta W$拆解成两个低秩矩阵的乘积：</p>
<script type="math/tex; mode=display">
\Delta W = BA</script><p>其中，$A \in \mathbb{R}^{r \times d}$, $B \in \mathbb{R}^{d \times r}$，$d$ 是一个比 $r$ 小得多的数。这样，通过用两个参数量少得多的矩阵 $A, B$ 来维护变化量，我们不仅提高了微调的效率，还保持了使用变化量来描述微调过程的灵活性。这就是 LoRA 的全部原理，它十分简单，用 $\Delta W = BA$ 这一行公式足以表示。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/20240114-SD-LORA/1.jpg" alt></p>
<p>了解了 LoRA 的原理，我们再回头看前文提及的 LoRA 的四项优点。LoRA 模型由许多参数量较少的矩阵 $A, B$ 来表示，它可以被单独存储，且占用空间不大。由于 $\Delta W = BA$ 维护的其实是参数的变化量，我们既可以把它与预训练模型的参数加起来得到一个新模型以提高推理速度，也可以在线地用一个混合比例来灵活地组合新旧模型。LoRA 的最后一个优点是各个基于同一个原模型独立训练出来的 LoRA 模型可以混合使用。LoRA 甚至可以作用于被其他方式修改过的原模型，比如 SD LoRA 支持带 ControlNet 的 SD。这一点其实来自于社区用户的实践。一个可能的解释是，LoRA 用低秩矩阵来表示变化量，这种低秩的变化量恰好与其他方法的变化量「错开」，使得 LoRA 能向着一个不干扰其他方法的方向修改模型。</p>
<p>我们最后来学习一下 LoRA 的实现细节。LoRA 有两个超参数，除了上文中提到的$r$，还有一个叫$\alpha$的参数。LoRA 的作者在实现 LoRA 模块时，给修改量乘了一个 $\frac{\alpha}{r}$ 的系数，即对于输入$x$，带了 LoRA 模块后的输出为 $Wx + \frac{\alpha}{r}BAx$。作者解释说，调这个参数几乎等于调学习率，一开始令$\alpha=r$即可。在我们要反复调超参数$r$时，只要保持$\alpha$不变，就不用改其他超参数了（因为不加$\alpha$的话，改了$r$后，学习率等参数也得做相应调整以维持同样的训练条件）。当然，实际运用中，LoRA 的超参数很好调。一般令$r=4, 8, 16$即可。由于我们不怎么会变$r$，总是令$\alpha=r$就够了。</p>
<p>为了使用 LoRA，除了确定超参数外，我们还得指定需要被微调的参数矩阵。在 SD 中使用 LoRA 时，大家一般会对 SD 的 U-Net 的所有多头注意力模块的所有参数矩阵做微调。即对于多头注意力模块的四个矩阵 $W_Q, W_K, W_V, W_{out}$ 进行微调。</p>
<h2 id="LoRA-在-SD-中的三种运用"><a href="#LoRA-在-SD-中的三种运用" class="headerlink" title="LoRA 在 SD 中的三种运用"></a>LoRA 在 SD 中的三种运用</h2><p>LoRA 在 SD 的科研中有着广泛的应用。按照使用 LoRA 的动机，我们可以把 LoRA 的应用分成：1） 还原单幅图像；2）风格调整；3）训练目标调整。通过学习这些应用，我们能更好地理解 LoRA 的本质。</p>
<h3 id="还原单幅图像"><a href="#还原单幅图像" class="headerlink" title="还原单幅图像"></a>还原单幅图像</h3><p>SD 只是一个生成任意图片的模型。为了用 SD 来编辑一张给定的图片，我们一般要让 SD 先学会生成一张一模一样的图片，再在此基础上做修改。可是，由于训练集和输入图片的差异，SD 或许不能生成完全一样的图片。解决这个问题的思路很简单粗暴：我们只用这一张图片来微调 SD，让 SD 在这张图片上过拟合。这样，SD 的输出就会和这张图片非常相似了。</p>
<p>较早介绍这种提高输入图片保真度方法的工作是 Imagic，只不过它采取的是完全微调策略。后续的 DragDiffusion 也用了相同的方法，并使用 LoRA 来代替完全微调。近期的 DiffMorpher 为了实现两幅图像间的插值，不仅对两幅图像单独训练了 LoRA，还通过两个 LoRA 间的插值来平滑图像插值的过程。</p>
<h3 id="风格调整"><a href="#风格调整" class="headerlink" title="风格调整"></a>风格调整</h3><p>LoRA 在 SD 社区中最受欢迎的应用就是风格调整了。我们希望 SD 只生成某一画风，或者某一人物的图片。为此，我们只需要在一个符合我们要求的训练集上直接训练 SD LoRA 即可。</p>
<p>由于这种调整 SD 风格的方法非常直接，没有特别介绍这种方法的论文。稍微值得一提的是基于 SD 的视频模型 AnimateDiff，它用 LoRA 来控制输出视频的视角变换，而不是控制画风。</p>
<p>由于 SD 风格化 LoRA 已经被广泛使用，能否兼容 SD 风格化 LoRA 决定了一个工作是否易于在社区中传播。</p>
<h3 id="训练目标调整"><a href="#训练目标调整" class="headerlink" title="训练目标调整"></a>训练目标调整</h3><p>最后一个应用就有一点返璞归真了。LoRA 最初的应用就是把一个预训练模型适配到另一任务上。比如 GPT 一开始在大量语料中训练，随后在问答任务上微调。对于 SD 来说，我们也可以修改 U-Net 的训练目标，以提升 SD 的能力。</p>
<p>有不少相关工作用 LoRA 来改进 SD。比如 Smooth Diffusion 通过在训练目标中添加一个约束项并进行 LoRA 微调来使得 SD 的隐空间更加平滑。近期比较火的高速图像生成方法 LCM-LoRA 也是把原本作用于 SD 全参数上的一个模型蒸馏过程用 LoRA 来实现。</p>
<h3 id="SD-LoRA-应用总结"><a href="#SD-LoRA-应用总结" class="headerlink" title="SD LoRA 应用总结"></a>SD LoRA 应用总结</h3><p>尽管上述三种 SD LoRA 应用的设计出发点不同，它们本质上还是在利用微调这一迁移学习技术来调整模型的数据分布或者训练目标。LoRA 只是众多高效微调方法中的一种，只要是微调能实现的功能，LoRA 基本都能实现，只不过 LoRA 更轻便而已。如果你想微调 SD 又担心计算资源不够，那么用 LoRA 准没错。反过来说，你想用 LoRA 在 SD 上设计出一个新应用，就要去思考微调 SD 能够做到哪些事。</p>
<h2 id="Diffusers-SD-LoRA-代码实战"><a href="#Diffusers-SD-LoRA-代码实战" class="headerlink" title="Diffusers SD LoRA 代码实战"></a>Diffusers SD LoRA 代码实战</h2><p>看完了原理，我们来尝试用 Diffusers 自己训一训 LoRA。我们会先学习 Diffusers 训练 LoRA 的脚本，再学习两个简单的 LoRA 示例： SD 图像插值与 SD 图像风格迁移。</p>
<p>项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA">https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA</a></p>
<h3 id="Diffusers-脚本"><a href="#Diffusers-脚本" class="headerlink" title="Diffusers 脚本"></a>Diffusers 脚本</h3><p>我们将参考 Diffusers 中的 SD LoRA 文档 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/training/lora">https://huggingface.co/docs/diffusers/training/lora</a> ，使用官方脚本 <code>examples/text_to_image/train_text_to_image_lora.py</code> 训练 LoRA。为了使用这个脚本，建议直接克隆官方仓库，并安装根目录和 <code>text_to_image</code> 目录下的依赖文件。本文使用的 Diffusers 版本是 0.26.0，过旧的 Diffusers 的代码可能和本文展示的有所出入。目前，官方文档也描述的是旧版的代码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/huggingface/diffusers</span><br><span class="line">cd diffusers</span><br><span class="line">pip install .</span><br><span class="line">cd examples/text_to_image</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<p>这份代码使用 accelerate 库管理 PyTorch 的训练。对同一份代码，只需要修改 accelerate 的配置，就能实现单卡训练或者多卡训练。默认情况下，用 <code>accelerate launch</code> 命令运行 Python 脚本会使用所有显卡。如果你需要修改训练配置，请参考相关文档使用 <code>accelerate config</code> 命令配置环境。</p>
<p>做好准备后，我们来开始阅读 <code>examples/text_to_image/train_text_to_image_lora.py</code> 的代码。这份代码写得十分易懂，复杂的地方都有注释。我们跳过命令行参数部分，直接从 <code>main</code> 函数开始读。</p>
<p>一开始，函数会配置 accelerate 库及日志记录器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">args = parse_args()</span><br><span class="line">logging_dir = Path(args.output_dir, args.logging_dir)</span><br><span class="line"></span><br><span class="line">accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator(</span><br><span class="line">    gradient_accumulation_steps=args.gradient_accumulation_steps,</span><br><span class="line">    mixed_precision=args.mixed_precision,</span><br><span class="line">    log_with=args.report_to,</span><br><span class="line">    project_config=accelerator_project_config,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> args.report_to == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_wandb_available():</span><br><span class="line">        <span class="keyword">raise</span> ImportError(<span class="string">&quot;Make sure to install wandb if you want to use it for logging during training.&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> wandb</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make one log on every process with the configuration for debugging.</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">)</span><br><span class="line">logger.info(accelerator.state, main_process_only=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">if</span> accelerator.is_local_main_process:</span><br><span class="line">    datasets.utils.logging.set_verbosity_warning()</span><br><span class="line">    transformers.utils.logging.set_verbosity_warning()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_info()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    datasets.utils.logging.set_verbosity_error()</span><br><span class="line">    transformers.utils.logging.set_verbosity_error()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_error()</span><br></pre></td></tr></table></figure>
<p>随后的代码决定是否手动设置随机种子。保持默认即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If passed along, set the training seed now.</span></span><br><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    set_seed(args.seed)</span><br></pre></td></tr></table></figure>
<p>接着，函数会创建输出文件夹。如果我们想把模型推送到在线仓库上，函数还会创建一个仓库。我们的项目不必上传，忽略所有 <code>args.push_to_hub</code> 即可。另外，<code>if accelerator.is_main_process:</code> 表示多卡训练时只有主进程会执行这段代码块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Handle the repository creation</span></span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        os.makedirs(args.output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        repo_id = create_repo(</span><br><span class="line">            repo_id=args.hub_model_id <span class="keyword">or</span> Path(args.output_dir).name, exist_ok=<span class="literal">True</span>, token=args.hub_token</span><br><span class="line">        ).repo_id</span><br></pre></td></tr></table></figure>
<p>准备完辅助工具后，函数正式开始着手训练。训练前，函数会先实例化好一切处理类，包括用于维护扩散模型中间变量的 <code>DDPMScheduler</code>，负责编码输入文本的 <code>CLIPTokenizer, CLIPTextModel</code>，压缩图像的VAE <code>AutoencoderKL</code>，预测噪声的 U-Net <code>UNet2DConditionModel</code>。参数 <code>args.pretrained_model_name_or_path</code> 是 Diffusers 在线仓库的地址（如<code>runwayml/stable-diffusion-v1-5</code>），或者本地的 Diffusers 模型文件夹。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load scheduler, tokenizer and models.</span></span><br><span class="line">noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;scheduler&quot;</span>)</span><br><span class="line">tokenizer = CLIPTokenizer.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;tokenizer&quot;</span>, revision=args.revision</span><br><span class="line">)</span><br><span class="line">text_encoder = CLIPTextModel.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;text_encoder&quot;</span>, revision=args.revision</span><br><span class="line">)</span><br><span class="line">vae = AutoencoderKL.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;vae&quot;</span>, revision=args.revision, variant=args.variant</span><br><span class="line">)</span><br><span class="line">unet = UNet2DConditionModel.from_pretrained(</span><br><span class="line">    args.pretrained_model_name_or_path, subfolder=<span class="string">&quot;unet&quot;</span>, revision=args.revision, variant=args.variant</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>函数还会设置各个带参数模型是否需要计算梯度。由于我们待会要优化的是新加入的 LoRA 模型，所有预训练模型都不需要计算梯度。另外，函数还会根据 accelerate 配置自动设置这些模型的精度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># freeze parameters of models to save more memory</span></span><br><span class="line">unet.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">vae.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">text_encoder.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze the unet parameters before adding adapters</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> unet.parameters():</span><br><span class="line">    param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to half-precision</span></span><br><span class="line"><span class="comment"># as these weights are only used for inference, keeping weights in full precision is not required.</span></span><br><span class="line">weight_dtype = torch.float32</span><br><span class="line"><span class="keyword">if</span> accelerator.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.float16</span><br><span class="line"><span class="keyword">elif</span> accelerator.mixed_precision == <span class="string">&quot;bf16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.bfloat16</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move unet, vae and text_encoder to device and cast to weight_dtype</span></span><br><span class="line">unet.to(accelerator.device, dtype=weight_dtype)</span><br><span class="line">vae.to(accelerator.device, dtype=weight_dtype)</span><br><span class="line">text_encoder.to(accelerator.device, dtype=weight_dtype)</span><br></pre></td></tr></table></figure></p>
<p>把预训练模型都调好了后，函数会配置 LoRA 模块并将其加入 U-Net 模型中。最近，Diffusers 更新了添加 LoRA 的方式。Diffusers 用 Attention 处理器来描述 Attention 的计算。为了把 LoRA 加入到 Attention 模块中，早期的 Diffusers 直接在 Attention 处理器里加入可训练参数。现在，为了和其他 Hugging Face 库统一，Diffusers 使用 PEFT 库来管理 LoRA。我们不需要关注 LoRA 的实现细节，只需要写一个 <code>LoraConfig</code> 就行了。</p>
<blockquote>
<p>PEFT 中的 LoRA 文档参见 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/peft/conceptual_guides/lora">https://huggingface.co/docs/peft/conceptual_guides/lora</a></p>
</blockquote>
<p><code>LoraConfig</code> 中有四个主要参数: <code>r, lora_alpha, init_lora_weights, target_modules</code>。 <code>r, lora_alpha</code> 的意义我们已经在前文中见过了，前者决定了 LoRA 矩阵的大小，后者决定了训练速度。默认配置下，它们都等于同一个值 <code>args.rank</code>。<code>init_lora_weights</code> 表示如何初始化训练参数，<code>gaussian</code>是论文中使用的方法。<code>target_modules</code> 表示 Attention 模块的哪些层需要添加 LoRA。按照通常的做法，会给所有层，即三个输入变换矩阵 <code>to_k, to_q, to_v</code> 和一个输出变换矩阵 <code>to_out.0</code> 加 LoRA。</p>
<p>创建了配置后，用 <code>unet.add_adapter(unet_lora_config)</code> 就可以创建 LoRA 模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">unet_lora_config = LoraConfig(</span><br><span class="line">    r=args.rank,</span><br><span class="line">    lora_alpha=args.rank,</span><br><span class="line">    init_lora_weights=<span class="string">&quot;gaussian&quot;</span>,</span><br><span class="line">    target_modules=[<span class="string">&quot;to_k&quot;</span>, <span class="string">&quot;to_q&quot;</span>, <span class="string">&quot;to_v&quot;</span>, <span class="string">&quot;to_out.0&quot;</span>],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">unet.add_adapter(unet_lora_config)</span><br><span class="line"><span class="keyword">if</span> args.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> unet.parameters():</span><br><span class="line">        <span class="comment"># only upcast trainable parameters (LoRA) into fp32</span></span><br><span class="line">        <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">            param.data = param.to(torch.float32)</span><br></pre></td></tr></table></figure>
<p>更新完了 U-Net 的结构，函数会尝试启用 <code>xformers</code> 来提升 Attention 的效率。PyTorch 在 2.0 版本也加入了类似的 Attention 优化技术。如果你的显卡性能有限，且 PyTorch 版本小于 2.0，可以考虑使用 <code>xformers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.enable_xformers_memory_efficient_attention:</span><br><span class="line">  <span class="keyword">if</span> is_xformers_available():</span><br><span class="line">      <span class="keyword">import</span> xformers</span><br><span class="line"></span><br><span class="line">      xformers_version = version.parse(xformers.__version__)</span><br><span class="line">      <span class="keyword">if</span> xformers_version == version.parse(<span class="string">&quot;0.0.16&quot;</span>):</span><br><span class="line">          logger.warn(</span><br><span class="line">              ...</span><br><span class="line">          )</span><br><span class="line">      unet.enable_xformers_memory_efficient_attention()</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&quot;xformers is not available. Make sure it is installed correctly&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>做完了 U-Net 的处理后，函数会过滤出要优化的模型参数，这些参数稍后会传递给优化器。过滤的原则很简单，如果参数要求梯度，就是待优化参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lora_layers = <span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, unet.parameters())</span><br></pre></td></tr></table></figure>
<p>之后是优化器的配置。函数先是配置了一些细枝末节的训练选项，一般可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.gradient_checkpointing:</span><br><span class="line">    unet.enable_gradient_checkpointing()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable TF32 for faster training on Ampere GPUs,</span></span><br><span class="line"><span class="comment"># cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices</span></span><br><span class="line"><span class="keyword">if</span> args.allow_tf32:</span><br><span class="line">    torch.backends.cuda.matmul.allow_tf32 = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>然后是优化器的选择。我们可以忽略其他逻辑，直接用 <code>AdamW</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the optimizer</span></span><br><span class="line"><span class="keyword">if</span> args.use_8bit_adam:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">import</span> bitsandbytes <span class="keyword">as</span> bnb</span><br><span class="line">    <span class="keyword">except</span> ImportError:</span><br><span class="line">        <span class="keyword">raise</span> ImportError(</span><br><span class="line">            <span class="string">&quot;...&quot;</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    optimizer_cls = bnb.optim.AdamW8bit</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    optimizer_cls = torch.optim.AdamW</span><br></pre></td></tr></table></figure>
<p>选择了优化器类，就可以实例化优化器了。优化器的第一个参数是之前准备好的待优化 LoRA 参数，其他参数是 Adam 优化器本身的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optimizer_cls(</span><br><span class="line">    lora_layers,</span><br><span class="line">    lr=args.learning_rate,</span><br><span class="line">    betas=(args.adam_beta1, args.adam_beta2),</span><br><span class="line">    weight_decay=args.adam_weight_decay,</span><br><span class="line">    eps=args.adam_epsilon,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>准备了优化器，之后需要准备训练集。这个脚本用 Hugging Face 的 datasets 库来管理数据集。我们既可以读取在线数据集，也可以读取本地的图片文件夹数据集。在本文的示例项目中，我们将使用图片文件夹数据集。稍后我们再详细学习这样的数据集文件夹该怎么构建。相关的文档可以参考 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder">https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</a> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.dataset_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># Downloading and loading a dataset from the hub.</span></span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        args.dataset_name,</span><br><span class="line">        args.dataset_config_name,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">        data_dir=args.train_data_dir,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_files = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> args.train_data_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        data_files[<span class="string">&quot;train&quot;</span>] = os.path.join(args.train_data_dir, <span class="string">&quot;**&quot;</span>)</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        <span class="string">&quot;imagefolder&quot;</span>,</span><br><span class="line">        data_files=data_files,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># See more about loading custom images at</span></span><br><span class="line">    <span class="comment"># https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</span></span><br></pre></td></tr></table></figure>
<p>训练 SD 时，每一个数据样本需要包含两项信息：图像数据与对应的文本描述。在数据集 <code>dataset</code> 中，每个数据样本包含了多项属性。下面的代码用于从这些属性中取出图像与文本描述。默认情况下，第一个属性会被当做图像数据，第二个属性会被当做文本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets.</span></span><br><span class="line"><span class="comment"># We need to tokenize inputs and targets.</span></span><br><span class="line">column_names = dataset[<span class="string">&quot;train&quot;</span>].column_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. Get the column names for input/target.</span></span><br><span class="line">dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, <span class="literal">None</span>)</span><br><span class="line"><span class="keyword">if</span> args.image_column <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    image_column = dataset_columns[<span class="number">0</span>] <span class="keyword">if</span> dataset_columns <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> column_names[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    image_column = args.image_column</span><br><span class="line">    <span class="keyword">if</span> image_column <span class="keyword">not</span> <span class="keyword">in</span> column_names:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;--image_column&#x27; value &#x27;<span class="subst">&#123;args.image_column&#125;</span>&#x27; needs to be one of: <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(column_names)&#125;</span>&quot;</span></span><br><span class="line">        )</span><br><span class="line"><span class="keyword">if</span> args.caption_column <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    caption_column = dataset_columns[<span class="number">1</span>] <span class="keyword">if</span> dataset_columns <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> column_names[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    caption_column = args.caption_column</span><br><span class="line">    <span class="keyword">if</span> caption_column <span class="keyword">not</span> <span class="keyword">in</span> column_names:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">f&quot;--caption_column&#x27; value &#x27;<span class="subst">&#123;args.caption_column&#125;</span>&#x27; needs to be one of: <span class="subst">&#123;<span class="string">&#x27;, &#x27;</span>.join(column_names)&#125;</span>&quot;</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>准备好了数据集，接下来要定义数据预处理流程以创建 <code>DataLoader</code>。函数先定义了一个把文本标签预处理成 token ID 的 token 化函数。我们不需要修改它。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_captions</span>(<span class="params">examples, is_train=<span class="literal">True</span></span>):</span></span><br><span class="line">    captions = []</span><br><span class="line">    <span class="keyword">for</span> caption <span class="keyword">in</span> examples[caption_column]:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(caption, <span class="built_in">str</span>):</span><br><span class="line">            captions.append(caption)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(caption, (<span class="built_in">list</span>, np.ndarray)):</span><br><span class="line">            <span class="comment"># take a random caption if there are multiple</span></span><br><span class="line">            captions.append(random.choice(caption) <span class="keyword">if</span> is_train <span class="keyword">else</span> caption[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&quot;Caption column `<span class="subst">&#123;caption_column&#125;</span>` should contain either strings or lists of strings.&quot;</span></span><br><span class="line">            )</span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        captions, max_length=tokenizer.model_max_length, padding=<span class="string">&quot;max_length&quot;</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> inputs.input_ids</span><br></pre></td></tr></table></figure>
<p>接着，函数定义了图像数据的预处理流程。该流程是用 <code>torchvision</code> 中的 <code>transforms</code> 实现的。如代码所示，处理流程中包括了 resize 至指定分辨率 <code>args.resolution</code>、将图像长宽均裁剪至指定分辨率、随机翻转、转换至 tensor 和归一化。</p>
<p>经过这一套预处理后，所有图像的长宽都会被设置为 <code>args.resolution</code> 。统一图像的尺寸，主要的目的是对齐数据，以使多个数据样本能拼接成一个 batch。注意，数据预处理流程中包括了随机裁剪。如果数据集里的多数图片都长宽不一致，模型会倾向于生成被裁剪过的图片。为了解决这一问题，要么自己手动预处理图片，使训练图片都是分辨率至少为 <code>args.resolution</code> 的正方形图片，要么令 batch size 为 1 并取消掉随机裁剪。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets.</span></span><br><span class="line">train_transforms = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize(</span><br><span class="line">            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),</span><br><span class="line">        transforms.CenterCrop(</span><br><span class="line">            args.resolution) <span class="keyword">if</span> args.center_crop <span class="keyword">else</span> transforms.RandomCrop(args.resolution),</span><br><span class="line">        transforms.RandomHorizontalFlip() <span class="keyword">if</span> args.random_flip <span class="keyword">else</span> transforms.Lambda(<span class="keyword">lambda</span> x: x),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>定义了预处理流程后，函数对所有数据进行预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_train</span>(<span class="params">examples</span>):</span></span><br><span class="line">    images = [image.convert(<span class="string">&quot;RGB&quot;</span>) <span class="keyword">for</span> image <span class="keyword">in</span> examples[image_column]]</span><br><span class="line">    examples[<span class="string">&quot;pixel_values&quot;</span>] = [</span><br><span class="line">        train_transforms(image) <span class="keyword">for</span> image <span class="keyword">in</span> images]</span><br><span class="line">    examples[<span class="string">&quot;input_ids&quot;</span>] = tokenize_captions(examples)</span><br><span class="line">    <span class="keyword">return</span> examples</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> accelerator.main_process_first():</span><br><span class="line">    <span class="keyword">if</span> args.max_train_samples <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        dataset[<span class="string">&quot;train&quot;</span>] = dataset[<span class="string">&quot;train&quot;</span>].shuffle(</span><br><span class="line">            seed=args.seed).select(<span class="built_in">range</span>(args.max_train_samples))</span><br><span class="line">    <span class="comment"># Set the training transforms</span></span><br><span class="line">    train_dataset = dataset[<span class="string">&quot;train&quot;</span>].with_transform(preprocess_train)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>之后函数用预处理过的数据集创建 <code>DataLoader</code>。这里要注意的参数是 batch size <code>args.train_batch_size</code> 和读取数据的进程数 <code>args.dataloader_num_workers</code> 。这两个参数的用法和一般的 PyTorch 项目一样。<code>args.train_batch_size</code> 决定了训练速度，一般设置到不爆显存的最大值。如果要读取的数据过多，导致数据读取成为了模型训练的速度瓶颈，则应该提高 <code>args.dataloader_num_workers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">examples</span>):</span></span><br><span class="line">    pixel_values = torch.stack([example[<span class="string">&quot;pixel_values&quot;</span>]</span><br><span class="line">                                <span class="keyword">for</span> example <span class="keyword">in</span> examples])</span><br><span class="line">    pixel_values = pixel_values.to(</span><br><span class="line">        memory_format=torch.contiguous_format).<span class="built_in">float</span>()</span><br><span class="line">    input_ids = torch.stack([example[<span class="string">&quot;input_ids&quot;</span>] <span class="keyword">for</span> example <span class="keyword">in</span> examples])</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;pixel_values&quot;</span>: pixel_values, <span class="string">&quot;input_ids&quot;</span>: input_ids&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataLoaders creation:</span></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(</span><br><span class="line">    train_dataset,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=collate_fn,</span><br><span class="line">    batch_size=args.train_batch_size,</span><br><span class="line">    num_workers=args.dataloader_num_workers,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>如果想用更大的 batch size，显存又不够，则可以使用梯度累计技术。使用这项技术时，训练梯度不会每步优化，而是累计了若干步后再优化。<code>args.gradient_accumulation_steps</code> 表示要累计几步再优化模型。实际的 batch size 等于输入 batch size 乘 GPU 数乘梯度累计步数。下面的代码维护了训练步数有关的信息，并创建了学习率调度器。我们按照默认设置使用一个常量学习率即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scheduler and math around the number of training steps.</span></span><br><span class="line">overrode_max_train_steps = <span class="literal">False</span></span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line"><span class="keyword">if</span> args.max_train_steps <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch</span><br><span class="line">    overrode_max_train_steps = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    args.lr_scheduler,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,</span><br><span class="line">    num_training_steps=args.max_train_steps * accelerator.num_processes,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare everything with our `accelerator`.</span></span><br><span class="line">unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(</span><br><span class="line">    unet, optimizer, train_dataloader, lr_scheduler</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We need to recalculate our total training steps as the size of the training dataloader may have changed.</span></span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line"><span class="keyword">if</span> overrode_max_train_steps:</span><br><span class="line">    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch</span><br><span class="line"><span class="comment"># Afterwards we recalculate our number of training epochs</span></span><br><span class="line">args.num_train_epochs = math.ceil(</span><br><span class="line">    args.max_train_steps / num_update_steps_per_epoch)</span><br></pre></td></tr></table></figure>
<p>在准备工作的最后，函数会用 accelerate 库记录配置信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    accelerator.init_trackers(<span class="string">&quot;text2image-fine-tune&quot;</span>, config=<span class="built_in">vars</span>(args))</span><br></pre></td></tr></table></figure>
<p>终于，要开始训练了。训练开始前，函数会准备全局变量并记录日志。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train!</span></span><br><span class="line">total_batch_size = args.train_batch_size * \</span><br><span class="line">    accelerator.num_processes * args.gradient_accumulation_steps</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;***** Running training *****&quot;</span>)</span><br><span class="line">...</span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line">first_epoch = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>此时，如果设置了 <code>args.resume_from_checkpoint</code>，则函数会读取之前训练过的权重。一般继续训练时可以把该参数设为 <code>latest</code>，程序会自动找最新的权重。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Potentially load in the weights and states from a previous save</span></span><br><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint:</span><br><span class="line">    <span class="keyword">if</span> args.resume_from_checkpoint != <span class="string">&quot;latest&quot;</span>:</span><br><span class="line">        path = ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the most recent checkpoint</span></span><br><span class="line">        path = ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        args.resume_from_checkpoint = <span class="literal">None</span></span><br><span class="line">        initial_global_step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        accelerator.load_state(os.path.join(args.output_dir, path))</span><br><span class="line">        global_step = <span class="built_in">int</span>(path.split(<span class="string">&quot;-&quot;</span>)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        initial_global_step = global_step</span><br><span class="line">        first_epoch = global_step // num_update_steps_per_epoch</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    initial_global_step = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>随后，函数根据总步数和已经训练过的步数设置迭代器，正式进入训练循环。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">progress_bar = tqdm(</span><br><span class="line">    <span class="built_in">range</span>(<span class="number">0</span>, args.max_train_steps),</span><br><span class="line">    initial=initial_global_step,</span><br><span class="line">    desc=<span class="string">&quot;Steps&quot;</span>,</span><br><span class="line">    <span class="comment"># Only show the progress bar once on each machine.</span></span><br><span class="line">    disable=<span class="keyword">not</span> accelerator.is_local_main_process,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(first_epoch, args.num_train_epochs):</span><br><span class="line">    unet.train()</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        <span class="keyword">with</span> accelerator.accumulate(unet):</span><br></pre></td></tr></table></figure>
<p>训练的过程基本和 LDM 论文中展示的一致。一开始，要取出图像<code>batch[&quot;pixel_values&quot;]</code> 并用 VAE 把它压缩进隐空间。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert images to latent space</span></span><br><span class="line">latents = vae.encode(batch[<span class="string">&quot;pixel_values&quot;</span>].to(</span><br><span class="line">    dtype=weight_dtype)).latent_dist.sample()</span><br><span class="line">latents = latents * vae.config.scaling_factor</span><br></pre></td></tr></table></figure><br>再随机生成一个噪声。该噪声会套入扩散模型前向过程的公式，和输入图像一起得到 <code>t</code> 时刻的带噪图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sample noise that we&#x27;ll add to the latents</span></span><br><span class="line">noise = torch.randn_like(latents)</span><br></pre></td></tr></table></figure>
<p>下一步，这里插入了一个提升扩散模型训练质量的小技巧，用上它后输出图像的颜色分布会更合理。原理见注释中的链接。<code>args.noise_offset</code> 默认为 0。如果要启用这个特性，一般令 <code>args.noise_offset = 0.1</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.noise_offset:</span><br><span class="line">    <span class="comment"># https://www.crosslabs.org//blog/diffusion-with-offset-noise</span></span><br><span class="line">    noise += args.noise_offset * torch.randn(</span><br><span class="line">        (latents.shape[<span class="number">0</span>], latents.shape[<span class="number">1</span>], <span class="number">1</span>, <span class="number">1</span>), device=latents.device</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>然后是时间戳的随机生成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bsz = latents.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># Sample a random timestep for each image</span></span><br><span class="line">timesteps = torch.randint(</span><br><span class="line">    <span class="number">0</span>, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)</span><br><span class="line">timesteps = timesteps.long()</span><br></pre></td></tr></table></figure></p>
<p>时间戳和前面随机生成的噪声一起经 DDPM 的前向过程得到带噪图片 <code>noisy_latents</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Add noise to the latents according to the noise magnitude at each timestep</span></span><br><span class="line"><span class="comment"># (this is the forward diffusion process)</span></span><br><span class="line">noisy_latents = noise_scheduler.add_noise(</span><br><span class="line">    latents, noise, timesteps)</span><br></pre></td></tr></table></figure>
<p>再把文本 <code>batch[&quot;input_ids&quot;]</code> 编码，为之后的 U-Net 前向传播做准备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the text embedding for conditioning</span></span><br><span class="line">encoder_hidden_states = text_encoder(batch[<span class="string">&quot;input_ids&quot;</span>])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>在 U-Net 推理开始前，函数这里做了一个关于 U-Net 输出类型的判断。一般 U-Net 都是输出预测的噪声 <code>epsilon</code>，可以忽略这段代码。当 U-Net 是想预测噪声时，要拟合的目标是之前随机生成的噪声 <code>noise</code> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the target for loss depending on the prediction type</span></span><br><span class="line"><span class="keyword">if</span> args.prediction_type <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># set prediction_type of scheduler if defined</span></span><br><span class="line">    noise_scheduler.register_to_config(</span><br><span class="line">        prediction_type=args.prediction_type)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> noise_scheduler.config.prediction_type == <span class="string">&quot;epsilon&quot;</span>:</span><br><span class="line">    target = noise</span><br><span class="line"><span class="keyword">elif</span> noise_scheduler.config.prediction_type == <span class="string">&quot;v_prediction&quot;</span>:</span><br><span class="line">    target = noise_scheduler.get_velocity(</span><br><span class="line">        latents, noise, timesteps)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">f&quot;Unknown prediction type <span class="subst">&#123;noise_scheduler.config.prediction_type&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>之后把带噪图像、时间戳、文本编码输入进 U-Net，U-Net 输出预测的噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict the noise residual and compute loss</span></span><br><span class="line">model_pred = unet(noisy_latents, timesteps,</span><br><span class="line">                  encoder_hidden_states).sample</span><br></pre></td></tr></table></figure>
<p>有了预测值，下一步是算 loss。这里又可以选择是否使用一种加速训练的技术。如果使用，则 <code>args.snr_gamma</code> 推荐设置为 5.0。原 DDPM 的做法是直接算预测噪声和真实噪声的均方误差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.snr_gamma <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    loss = F.mse_loss(model_pred.<span class="built_in">float</span>(),</span><br><span class="line">                      target.<span class="built_in">float</span>(), reduction=<span class="string">&quot;mean&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>训练迭代的最后，要用 accelerate 库来完成梯度计算和反向传播。在更新梯度前，可以通过设置 <code>args.max_grad_norm</code> 来裁剪梯度，以防梯度过大。<code>args.max_grad_norm</code> 默认为 1.0。代码中的 <code>if accelerator.sync_gradients:</code> 可以保证所有 GPU 都同步了梯度再执行后续代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Backpropagate</span></span><br><span class="line">accelerator.backward(loss)</span><br><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    params_to_clip = lora_layers</span><br><span class="line">    accelerator.clip_grad_norm_(</span><br><span class="line">        params_to_clip, args.max_grad_norm)</span><br><span class="line">optimizer.step()</span><br><span class="line">lr_scheduler.step()</span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>一步训练结束后，更新和步数相关的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    global_step += <span class="number">1</span></span><br><span class="line">    accelerator.log(&#123;<span class="string">&quot;train_loss&quot;</span>: train_loss&#125;, step=global_step)</span><br><span class="line">    train_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>脚本默认每 <code>args.checkpointing_steps</code> 步保存一次中间结果。当需要保存时，函数会清理多余的 checkpoint，再把模型状态和 LoRA 模型分别保存下来。<code>accelerator.save_state(save_path)</code> 负责把模型及优化器等训练用到的所有状态存下来，后面的 <code>StableDiffusionPipeline.save_lora_weights</code> 负责存储 LoRA 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> global_step % args.checkpointing_steps == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">        <span class="comment"># _before_ saving state, check if this save would set us over the `checkpoints_total_limit`</span></span><br><span class="line">        <span class="keyword">if</span> args.checkpoints_total_limit <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            checkpoints = ...</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(checkpoints) &gt;= args.checkpoints_total_limit:</span><br><span class="line">                <span class="comment"># remove ckpt</span></span><br><span class="line">                ...</span><br><span class="line"></span><br><span class="line">        save_path = os.path.join(</span><br><span class="line">            args.output_dir, <span class="string">f&quot;checkpoint-<span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">        accelerator.save_state(save_path)</span><br><span class="line"></span><br><span class="line">        unwrapped_unet = accelerator.unwrap_model(unet)</span><br><span class="line">        unet_lora_state_dict = convert_state_dict_to_diffusers(</span><br><span class="line">            get_peft_model_state_dict(unwrapped_unet)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        StableDiffusionPipeline.save_lora_weights(</span><br><span class="line">            save_directory=save_path,</span><br><span class="line">            unet_lora_layers=unet_lora_state_dict,</span><br><span class="line">            safe_serialization=<span class="literal">True</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        logger.info(<span class="string">f&quot;Saved state to <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>训练循环的最后，函数会更新进度条上的信息，并根据当前的训练步数决定是否停止训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logs = &#123;<span class="string">&quot;step_loss&quot;</span>: loss.detach().item(</span><br><span class="line">), <span class="string">&quot;lr&quot;</span>: lr_scheduler.get_last_lr()[<span class="number">0</span>]&#125;</span><br><span class="line">progress_bar.set_postfix(**logs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> global_step &gt;= args.max_train_steps:</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>训完每一个 epoch 后，函数会进行验证。默认的验证方法是新建一个图像生成 pipeline，生成一些图片并保存。如果有其他验证方法，如计算某一指标，可以自行编写这部分的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">   <span class="keyword">if</span> args.validation_prompt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> epoch % args.validation_epochs == <span class="number">0</span>:</span><br><span class="line">       logger.info(</span><br><span class="line">           <span class="string">f&quot;Running validation... \n Generating <span class="subst">&#123;args.num_validation_images&#125;</span> images with prompt:&quot;</span></span><br><span class="line">           <span class="string">f&quot; <span class="subst">&#123;args.validation_prompt&#125;</span>.&quot;</span></span><br><span class="line">       )</span><br><span class="line">       pipeline = DiffusionPipeline.from_pretrained(...)</span><br><span class="line">       ...</span><br></pre></td></tr></table></figure>
<p>所有训练结束后，函数会再存一次最终的 LoRA 模型权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save the lora layers</span></span><br><span class="line">accelerator.wait_for_everyone()</span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    unet = unet.to(torch.float32)</span><br><span class="line"></span><br><span class="line">    unwrapped_unet = accelerator.unwrap_model(unet)</span><br><span class="line">    unet_lora_state_dict = convert_state_dict_to_diffusers(</span><br><span class="line">        get_peft_model_state_dict(unwrapped_unet))</span><br><span class="line">    StableDiffusionPipeline.save_lora_weights(</span><br><span class="line">        save_directory=args.output_dir,</span><br><span class="line">        unet_lora_layers=unet_lora_state_dict,</span><br><span class="line">        safe_serialization=<span class="literal">True</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>函数还会再测试一次模型。具体方法和之前的验证是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Final inference</span></span><br><span class="line"><span class="comment"># Load previous pipeline</span></span><br><span class="line"><span class="keyword">if</span> args.validation_prompt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>运行完了这里，函数也就结束了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerator.end_training()</span><br></pre></td></tr></table></figure>
<p>为了方便使用，我把这个脚本改写了一下：删除了部分不常用的功能，并且配置参数能通过配置文件而不是命令行参数传入。新的脚本为项目根目录下的 <code>train_lora.py</code>，示例配置文件在 <code>cfg</code> 目录下。</p>
<p>以 <code>cfg</code> 中的某个配置文件为例，我们来回顾一下训练脚本主要用到的参数：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;log_dir&quot;</span>: <span class="string">&quot;log&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;ckpt&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;data_dir&quot;</span>: <span class="string">&quot;dataset/mountain&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;ckpt_name&quot;</span>: <span class="string">&quot;mountain&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;gradient_accumulation_steps&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;pretrained_model_name_or_path&quot;</span>: <span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;rank&quot;</span>: <span class="number">8</span>,</span><br><span class="line">    <span class="attr">&quot;enable_xformers_memory_efficient_attention&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span>,</span><br><span class="line">    <span class="attr">&quot;adam_beta1&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">    <span class="attr">&quot;adam_beta2&quot;</span>: <span class="number">0.999</span>,</span><br><span class="line">    <span class="attr">&quot;adam_weight_decay&quot;</span>: <span class="number">1e-2</span>,</span><br><span class="line">    <span class="attr">&quot;adam_epsilon&quot;</span>: <span class="number">1e-08</span>,</span><br><span class="line">    <span class="attr">&quot;resolution&quot;</span>: <span class="number">512</span>,</span><br><span class="line">    <span class="attr">&quot;n_epochs&quot;</span>: <span class="number">200</span>,</span><br><span class="line">    <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">500</span>,</span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;dataloader_num_workers&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;lr_scheduler_name&quot;</span>: <span class="string">&quot;constant&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;resume_from_checkpoint&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">&quot;noise_offset&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="attr">&quot;max_grad_norm&quot;</span>: <span class="number">1.0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要关注的参数：<code>output_dir</code> 为输出 checkpoint 的文件夹，<code>ckpt_name</code> 为输出 checkpoint 的文件名。<code>data_dir</code> 是训练数据集所在文件夹。<code>pretrained_model_name_or_path</code> 为 SD 模型文件夹。<code>rank</code> 是决定 LoRA 大小的参数。<code>learning_rate</code> 是学习率。<code>adam</code> 打头的是 AdamW 优化器的参数。<code>resolution</code> 是训练图片的统一分辨率。<code>n_epochs</code> 是训练的轮数。<code>checkpointing_steps</code> 指每过多久存一次 checkpoint。<code>train_batch_size</code> 是 batch size。<code>gradient_accumulation_steps</code> 是梯度累计步数。</p>
<p>要修改这个配置文件，要先把文件夹的路径改对，填上训练时的分辨率，再通过 <code>gradient_accumulation_steps</code> 和 <code>train_batch_size</code> 决定 batch size，接着填 <code>n_epochs</code> (一般训 10~20 轮就会过拟合)。最后就可以一边改 LoRA 的主要超参数 <code>rank</code> 一边反复训练了。</p>
<h3 id="SD-图像插值"><a href="#SD-图像插值" class="headerlink" title="SD 图像插值"></a>SD 图像插值</h3><p>在这个示例中，我们来实现 DiffMorpher 工作的一小部分，完成一个简单的图像插值工具。在此过程中，我们将学会怎么在单张图片上训练 SD LoRA，以验证我们的训练环境。</p>
<p>这个工具的原理很简单：我们对两张图片分别训练一个 LoRA。之后，为了获取两张图片的插值，我们可以对两张图片 DDIM Inversion 的初始隐变量及两个 LoRA 分别插值，用插值过的隐变量在插值过的 SD LoRA 上生成图片就能得到插值图片。</p>
<p>该示例的所有数据和代码都已经在项目文件夹中给出。首先，我们看一下该怎么在单张图片上训 LoRA。训练之前，我们要准备一个数据集文件夹。数据集文件夹及包含所有图片及一个描述文件 <code>metadata.jsonl</code>。比如单图片的数据集文件夹的结构应如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">├── mountain</span><br><span class="line">│       ├── metadata.jsonl</span><br><span class="line">│       └── mountain.jpg</span><br></pre></td></tr></table></figure>
<p><code>metadata.jsonl</code> 元数据文件的每一行都是一个 json 结构，包含该图片的路径及文本描述。单图片的元数据文件如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>如果是多图片，就应该是：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;mountain_up.jpg&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;mountain&quot;</span>&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>我们可以运行项目目录下的数据集测试文件 <code>test_dataset.py</code> 来看看 datasets 库的数据集对象包含哪些信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;imagefolder&quot;</span>, data_dir=<span class="string">&quot;dataset/mountain&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(dataset)</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>].column_names)</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>][<span class="string">&#x27;image&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(dataset[<span class="string">&quot;train&quot;</span>][<span class="string">&#x27;text&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>其输出大致为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Generating train split: 1 examples [00:00, 66.12 examples/s]</span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [&#x27;image&#x27;, &#x27;text&#x27;],</span><br><span class="line">        num_rows: 1</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br><span class="line">[&#x27;image&#x27;, &#x27;text&#x27;]</span><br><span class="line">[&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F0400246670&gt;]</span><br><span class="line">[&#x27;mountain&#x27;]</span><br></pre></td></tr></table></figure></p>
<p>这说明数据集对象实际上是一个词典。默认情况下，数据集放在词典的 <code>train</code> 键下。数据集的 <code>column_names</code> 属性可以返回每项数据有哪些属性。在我们的数据集里，数据的 <code>image</code> 是图像数据，<code>text</code> 是文本标签。训练脚本默认情况下会把每项数据的第一项属性作为图像，第二项属性作为文本标签。我们的这个数据集定义与训练脚本相符。</p>
<p>认识了数据集，我们可以来训练模型了。用下面的两行命令就可以分别在两张图片上训练 LoRA。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python train_lora.py cfg/mountain.json</span><br><span class="line">python train_lora.py cfg/mountain_up.json</span><br></pre></td></tr></table></figure>
<p>如果要用所有显卡训练，则应该用 accelerate。当然，对于这个简单的单图片训练，不需要用那么多显卡。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accelerate launch train_lora.py cfg/mountain.json</span><br><span class="line">accelerate launch train_lora.py cfg/mountain_up.json</span><br></pre></td></tr></table></figure>
<p>这两个 LoRA 模型的配置文件我们已经在前文见过了。相比普通的风格化 LoRA，这两个 LoRA 的训练轮数非常多，有 200 轮。设置较大的训练轮数能保证模型在单张图片上过拟合。</p>
<p>训练结束后，项目的 <code>ckpt</code> 文件夹下会多出两个 LoRA 权重文件: <code>mountain.safetensor</code>, <code>mountain_up.safetensor</code>。我们可以用它们来做图像插值了。</p>
<p>图像插值的脚本为 <code>morph.py</code>，它的主要内容为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> inversion_pipeline <span class="keyword">import</span> InversionPipeline</span><br><span class="line"></span><br><span class="line">lora_path = <span class="string">&#x27;ckpt/mountain.safetensor&#x27;</span></span><br><span class="line">lora_path2 = <span class="string">&#x27;ckpt/mountain_up.safetensor&#x27;</span></span><br><span class="line">sd_path = <span class="string">&#x27;runwayml/stable-diffusion-v1-5&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pipeline: InversionPipeline = InversionPipeline.from_pretrained(</span><br><span class="line">    sd_path).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path, adapter_name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path2, adapter_name=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img1_path = <span class="string">&#x27;dataset/mountain/mountain.jpg&#x27;</span></span><br><span class="line">img2_path = <span class="string">&#x27;dataset/mountain_up/mountain_up.jpg&#x27;</span></span><br><span class="line">prompt = <span class="string">&#x27;mountain&#x27;</span></span><br><span class="line">latent1 = pipeline.inverse(img1_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">latent2 = pipeline.inverse(img2_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">n_frames = <span class="number">10</span></span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_frames + <span class="number">1</span>):</span><br><span class="line">    alpha = i / n_frames</span><br><span class="line">    pipeline.set_adapters([<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>], adapter_weights=[<span class="number">1</span> - alpha, alpha])</span><br><span class="line">    latent = slerp(latent1, latent2, alpha)</span><br><span class="line">    output = pipeline(prompt=prompt, latents=latent,</span><br><span class="line">                      guidance_scale=<span class="number">1.0</span>).images[<span class="number">0</span>]</span><br><span class="line">    images.append(output)</span><br></pre></td></tr></table></figure>
<p>对于每一个 Diffusers 的 Pipeline 类实例，都可以用 <code>pipeline.load_lora_weights</code> 来读取 LoRA 权重。如果我们在同一个模型上使用了多个 LoRA，为了区分它们，我们要加上 <code>adapter_name</code> 参数为每个 LoRA 命名。稍后我们会用到这些名称。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipeline.load_lora_weights(lora_path, adapter_name=<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">pipeline.load_lora_weights(lora_path2, adapter_name=<span class="string">&#x27;b&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>读好了文件，使用已经写好的 DDIM Inversion 方法来得到两张图片的初始隐变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img1_path = <span class="string">&#x27;dataset/mountain/mountain.jpg&#x27;</span></span><br><span class="line">img2_path = <span class="string">&#x27;dataset/mountain_up/mountain_up.jpg&#x27;</span></span><br><span class="line">prompt = <span class="string">&#x27;mountain&#x27;</span></span><br><span class="line">latent1 = pipeline.inverse(img1_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br><span class="line">latent2 = pipeline.inverse(img2_path, prompt, <span class="number">50</span>, guidance_scale=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>最后开始生成不同插值比例的图片。根据混合比例 <code>alpha</code>，我们可以用 <code>pipeline.set_adapters([&quot;a&quot;, &quot;b&quot;], adapter_weights=[1 - alpha, alpha])</code> 来融合 LoRA 模型的比例。随后，我们再根据 <code>alpha</code> 对隐变量插值。用插值隐变量在插值 SD LoRA 上生成图片即可得到最终的插值图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_frames = <span class="number">10</span></span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_frames + <span class="number">1</span>):</span><br><span class="line">    alpha = i / n_frames</span><br><span class="line">    pipeline.set_adapters([<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>], adapter_weights=[<span class="number">1</span> - alpha, alpha])</span><br><span class="line">    latent = slerp(latent1, latent2, alpha)</span><br><span class="line">    output = pipeline(prompt=prompt, latents=latent,</span><br><span class="line">                      guidance_scale=<span class="number">1.0</span>).images[<span class="number">0</span>]</span><br><span class="line">    images.append(output)</span><br></pre></td></tr></table></figure>
<p>下面两段动图中，左图和右图分别是无 LoRA 和有 LoRA 的插值结果。可见，通过 LoRA 权重上的插值，图像插值的过度会更加自然。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/output.gif" alt></p>
<h3 id="图片风格迁移"><a href="#图片风格迁移" class="headerlink" title="图片风格迁移"></a>图片风格迁移</h3><p>接下来，我们来实现最流行的 LoRA 应用——风格化 LoRA。当然，训练一个每张随机输出图片都质量很高的模型是很困难的。我们退而求其次，来实现一个能对输入图片做风格迁移的 LoRA 模型。</p>
<p>训练风格化 LoRA 对技术要求不高，其主要难点其实是在数据收集上。大家可以根据自己的需求，准备自己的数据集。我在本文中会分享我的实验结果。我希望把《弹丸论破》的画风——一种颜色渐变较多的动漫画风——应用到一张普通动漫画风的图片上。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/2.png" alt></p>
<p>由于我的目标是拟合画风而不是某一种特定的物体，我直接选取了 50 张左右的游戏 CG 构成训练数据集，且没有对图片做任何处理。训风格化 LoRA 时，文本标签几乎没用，我把所有数据的文本都设置成了游戏名 <code>danganronpa</code>。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;1.png&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;danganronpa&quot;</span>&#125;</span><br><span class="line">...</span><br><span class="line">&#123;<span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;59.png&quot;</span>, <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;danganronpa&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>我的配置文件依然和前文的相同，LoRA rank 设置为 8。我一共训了 100 轮，但发现训练后期模型的过拟合很严重，其实令 <code>n_epochs</code> 为 10 到 20 就能有不错的结果。50 张图片训 10 轮最多几十分钟就训完。</p>
<p>由于训练图片的内容不够多样，且图片预处理时加入了随机裁剪，我的 LoRA 模型随机生成的图片质量较低。于是我决定在图像风格迁移任务上测试该模型。具体来说，我使用了 ControlNet Canny 加上图生图 （SDEdit）技术。相关的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> StableDiffusionControlNetImg2ImgPipeline, ControlNetModel</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">lora_path = <span class="string">&#x27;...&#x27;</span></span><br><span class="line">sd_path = <span class="string">&#x27;runwayml/stable-diffusion-v1-5&#x27;</span></span><br><span class="line">controlnet_canny_path = <span class="string">&#x27;lllyasviel/sd-controlnet-canny&#x27;</span></span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&#x27;1 man, look at right, side face, Ace Attorney, Phoenix Wright, best quality, danganronpa&#x27;</span></span><br><span class="line">neg_prompt = <span class="string">&#x27;longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, &#123;multiple people&#125;&#x27;</span></span><br><span class="line">img_path = <span class="string">&#x27;...&#x27;</span></span><br><span class="line">init_image = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&quot;RGB&quot;</span>)</span><br><span class="line">init_image = init_image.resize((<span class="number">768</span>, <span class="number">512</span>))</span><br><span class="line">np_image = np.array(init_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get canny image</span></span><br><span class="line">np_image = cv2.Canny(np_image, <span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line">np_image = np_image[:, :, <span class="literal">None</span>]</span><br><span class="line">np_image = np.concatenate([np_image, np_image, np_image], axis=<span class="number">2</span>)</span><br><span class="line">canny_image = Image.fromarray(np_image)</span><br><span class="line">canny_image.save(<span class="string">&#x27;tmp_edge.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line">controlnet = ControlNetModel.from_pretrained(controlnet_canny_path)</span><br><span class="line">pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(</span><br><span class="line">    sd_path, controlnet=controlnet</span><br><span class="line">)</span><br><span class="line">pipe.load_lora_weights(lora_path)</span><br><span class="line"></span><br><span class="line">output = pipe(</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    negative_prompt=neg_prompt,</span><br><span class="line">    strength=<span class="number">0.5</span>,</span><br><span class="line">    guidance_scale=<span class="number">7.5</span>,</span><br><span class="line">    controlnet_conditioning_scale=<span class="number">0.5</span>,</span><br><span class="line">    num_inference_steps=<span class="number">50</span>,</span><br><span class="line">    image=init_image,</span><br><span class="line">    cross_attention_kwargs=&#123;<span class="string">&quot;scale&quot;</span>: <span class="number">1.0</span>&#125;,</span><br><span class="line">    control_image=canny_image,</span><br><span class="line">).images[<span class="number">0</span>]</span><br><span class="line">output.save(<span class="string">&quot;tmp.png&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><code>StableDiffusionControlNetImg2ImgPipeline</code> 是 Diffusers 中 ControlNet 加图生图的 Pipeline。使用它生成图片的重要参数有：</p>
<ul>
<li><code>strength</code>：0~1 之间重绘比例。越低越接近输入图片。</li>
<li><code>controlnet_conditioning_scale</code>： 0~1 之间的 ControlNet 约束比例。越高越贴近约束。</li>
<li><code>cross_attention_kwargs=&#123;&quot;scale&quot;: scale&#125;</code>：此处的 <code>scale</code> 是 0~1 之间的 LoRA 混合比例。越高越贴近 LoRA 模型的输出。</li>
</ul>
<p>这里贴一下输入图片和两张编辑后的图片。</p>
<p><img src="/2024/01/23/20240114-SD-LoRA/3.png" alt><br><img src="/2024/01/23/20240114-SD-LoRA/4.png" alt><br><img src="/2024/01/23/20240114-SD-LoRA/5.png" alt></p>
<p>可以看出，输出图片中人物的画风确实得到了修改，颜色渐变更加丰富。我在几乎没有调试 LoRA 参数的情况下得到了这样的结果，可见虽然训练一个高质量的随机生成新画风的 LoRA 难度较高，但只是做风格迁移还是比较容易的。</p>
<p>尽管实验的经历不多，我还是基本上了解了 SD LoRA 风格化的能力边界。LoRA 风格化的本质还是修改输出图片的分布，数据集的质量基本上决定了生成的质量，其他参数的影响不会很大（包括训练图片的文本标签）。数据集最好手动裁剪至 512x512。如果想要生成丰富的风格化内容而不是只生成人物，就要丰富训练数据，减少人物数据的占比。训练时，最容易碰到的机器学习上的问题是过拟合问题。解决此问题的最简单的方式是早停，即不用最终的训练结果而用中间某一步的结果。如果你想实现改变输出数据分布以外的功能，比如精确生成某类物体、向模型中加入一些改变画风的关键词，那你应该使用更加先进的技术，而不仅仅是用最基本的 LoRA 微调。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LoRA 是当今深度学习领域中常见的技术。对于 SD，LoRA 则是能够编辑单幅图片、调整整体画风，或者是通过修改训练目标来实现更强大的功能。LoRA 的原理非常简单，它其实就是用两个参数量较少的矩阵来描述一个大参数矩阵在微调中的变化量。Diffusers 库提供了非常便利的 SD LoRA 训练脚本。相信读完了本文后，我们能知道如何用 Diffusers 训练 LoRA，修改训练中的主要参数，并在简单的单图片 LoRA 编辑任务上验证训练的正确性。利用这些知识，我们也能把 LoRA 拓展到风格化生成及其他应用上。</p>
<p>本文的项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA">https://github.com/SingleZombie/DiffusersExample/tree/main/LoRA</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2024/01/23/20230713-SD3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/23/20230713-SD3/" class="post-title-link" itemprop="url">Stable Diffusion 解读（三）：原版实现及Diffusers实现源码解读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-23 19:42:00" itemprop="dateCreated datePublished" datetime="2024-01-23T19:42:00+08:00">2024-01-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>看完了Stable Diffusion的论文，在最后这篇文章里，我们来学习Stable Diffusion的代码实现。具体来说，我们会学习Stable Diffusion官方仓库及Diffusers开源库中有关采样算法和U-Net的代码，而不会学习有关训练、VAE、text encoder (CLIP) 的代码。如今大多数工作都只会用到预训练的Stable Diffusion，只学采样算法和U-Net代码就能理解大多数工作了。</p>
<blockquote>
<p>建议读者在阅读本文之前了解DDPM、ResNet、U-Net、Transformer。</p>
<p>本文用到的Stable Diffusion版本是v1.5。Diffusers版本是0.25.0。为了提升可读性，本文对源代码做了一定的精简，部分不会运行到的分支会被略过。</p>
</blockquote>
<h2 id="算法梳理"><a href="#算法梳理" class="headerlink" title="算法梳理"></a>算法梳理</h2><p>在正式读代码之前，我们先用伪代码梳理一下Stable Diffusion的采样过程，并回顾一下U-Net架构的组成。实现Stable Diffusion的代码库有很多，各个库之间的API差异很大。但是，它们实际上都是在描述同一个算法，同一个模型。如果我们理解了算法和模型本身，就可以在学习时主动去找一个算法对应哪一段代码，而不是被动地去理解每一行代码在干什么。</p>
<h3 id="LDM-采样算法"><a href="#LDM-采样算法" class="headerlink" title="LDM 采样算法"></a>LDM 采样算法</h3><p>让我们从最早的DDPM开始，一步一步还原Latent Diffusion Model (LDM)的采样算法。DDPM的采样算法如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddpm_sample</span>(<span class="params">image_shape</span>):</span></span><br><span class="line">  ddpm_scheduler = DDPMScheduler()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  xt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> T ... <span class="number">1</span>:</span><br><span class="line">    eps = unet(xt, t)</span><br><span class="line">    std = ddpm_scheduler.get_std(t)</span><br><span class="line">    xt = ddpm_scheduler.get_xt_prev(xt, t, eps, std)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>在DDPM的实现中，一般会有一个类专门维护扩散模型的$\alpha, \beta$等变量。我们这里把这个类称为<code>DDPMScheduler</code>。此外，DDPM会用到一个U-Net神经网络<code>unet</code>，用于计算去噪过程中图像应该去除的噪声<code>eps</code>。准备好这两个变量后，就可以用<code>randn()</code>从标准正态分布中采样一个纯噪声图像<code>xt</code>。它会被逐渐去噪，最终变成一幅图片。去噪过程中，时刻<code>t</code>会从总时刻<code>T</code>遍历至<code>1</code>(总时刻<code>T</code>一般取<code>1000</code>)。在每一轮去噪步骤中，U-Net会根据这一时刻的图像<code>xt</code>和当前时间戳<code>t</code>估计出此刻应去除的噪声<code>eps</code>，根据<code>xt</code>和<code>eps</code>就能知道下一步图像的均值。除了均值，我们还要获取下一步图像的方差，这一般可以从DDPM调度类中直接获取。有了下一步图像的均值和方差，我们根据DDPM的公式，就能采样出下一步的图像。反复执行去噪循环，<code>xt</code>会从纯噪声图像变成一幅有意义的图像。</p>
<p>DDIM对DDPM的采样过程做了两点改进：1) 去噪的有效步数可以少于<code>T</code>步，由另一个变量<code>ddim_steps</code>决定；2) 采样的方差大小可以由<code>eta</code>决定。因此，改进后的DDIM算法可以写成这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddim_sample</span>(<span class="params">image_shape, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  xt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> timesteps:</span><br><span class="line">    eps = unet(xt, t)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    xt = ddim_scheduler.get_xt_prev(xt, t, eps, std)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>其中，<code>ddim_steps</code>是去噪循环的执行次数。根据<code>ddim_steps</code>，DDIM调度器可以生成所有被使用到的<code>t</code>。比如对于<code>T=1000, ddim_steps=20</code>，被使用到的就只有<code>[1000, 950, 900, ..., 50]</code>这20个时间戳，其他时间戳就可以跳过不算了。<code>eta</code>会被用来计算方差，一般这个值都会设成<code>0</code>。</p>
<blockquote>
<p>DDIM是早期的加速扩散模型采样的算法。如今有许多比DDIM更好的采样方法，但它们多数都保留了<code>steps</code>和<code>eta</code>这两个参数。因此，在使用所有采样方法时，我们可以不用关心实现细节，只关注多出来的这两个参数。</p>
</blockquote>
<p>在DDIM的基础上，LDM从生成像素空间上的图像变为生成隐空间上的图像。隐空间图像需要再做一次解码才能变回真实图像。从代码上来看，使用LDM后，只需要多准备一个VAE，并对最后的隐空间图像<code>zt</code>解码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_ddim_sample</span>(<span class="params">image_shape, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  vae = VAE()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  zt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> timesteps:</span><br><span class="line">    eps = unet(zt, t)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure></p>
<p>而想用LDM实现文生图，则需要给一个额外的文本输入<code>text</code>。文本编码器会把文本编码成张量<code>c</code>，输入进<code>unet</code>。其他地方的实现都和之前的LDM一样。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>):</span></span><br><span class="line">  ddim_scheduler = DDIMScheduler()</span><br><span class="line">  vae = VAE()</span><br><span class="line">  unet = UNet()</span><br><span class="line">  zt = randn(image_shape)</span><br><span class="line">  T = <span class="number">1000</span></span><br><span class="line">  timesteps = ddim_scheduler.get_timesteps(T, ddim_steps) <span class="comment"># [1000, 950, 900, ...]</span></span><br><span class="line"></span><br><span class="line">  text_encoder = CLIP()</span><br><span class="line">  c = text_encoder.encode(text)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> t = timesteps:</span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure><br>最后这个能实现文生图的LDM就是我们熟悉的Stable Diffusion。Stable Diffusion的采样算法看上去比较复杂，但如果能够从DDPM开始把各个功能都拆开来看，理解起来就不是那么困难了。</p>
<h3 id="U-Net-结构组成"><a href="#U-Net-结构组成" class="headerlink" title="U-Net 结构组成"></a>U-Net 结构组成</h3><p>Stable Diffusion代码实现中的另一个重点是去噪网络U-Net的实现。仿照上一节的学习方法，我们来逐步学习Stable Diffusion中的U-Net是怎么从最经典的纯卷积U-Net逐渐发展而来的。</p>
<p>最早的U-Net的结构如下图所示：</p>
<p><img src="/2024/01/23/20230713-SD3/0-1.jpg" alt></p>
<p>可以看出，U-Net的结构有以下特点：</p>
<ul>
<li>整体上看，U-Net由若干个大层组成。特征在每一大层会被下采样成尺寸更小的特征，再被上采样回原尺寸的特征。整个网络构成一个U形结构。</li>
<li>下采样后，特征的通道数会变多。一般情况下，每次下采样后图像尺寸减半，通道数翻倍。上采样过程则反之。</li>
<li>为了防止信息在下采样的过程中丢失，U-Net每一大层在下采样前的输出会作为额外输入拼接到每一大层上采样前的输入上。这种数据连接方式类似于ResNet中的「短路连接」。</li>
</ul>
<p>DDPM则使用了一种改进版的U-Net。改进主要有两点：</p>
<ul>
<li>原来的卷积层被替换成了ResNet中的残差卷积模块。每一大层有若干个这样的子模块。对于较深的大层，残差卷积模块后面还会接一个自注意力模块。</li>
<li>原来模型每一大层只有一个短路连接。现在每个大层下采样部分的每个子模块的输出都会额外输入到其对称的上采样部分的子模块上。直观上来看，就是短路连接更多了一点，输入信息更不容易在下采样过程中丢失。</li>
</ul>
<p><img src="/2024/01/23/20230713-SD3/0-2.jpg" alt></p>
<p>最后，LDM提出了一种给U-Net添加额外约束信息的方法：把U-Net中的自注意力模块换成交叉注意力模块。具体来说，DDPM的U-Net的自注意力模块被换成了标准的Transformer模块。约束信息$C$可以作为Cross Attention的K, V输入进模块中。</p>
<p>Stable Diffusion的U-Net还在结构上有少许修改，该U-Net的每一大层都有Transformer块，而不是只有较深的大层有。</p>
<p><img src="/2024/01/23/20230713-SD3/0-3.jpg" alt></p>
<p>至此，我们已经学完了Stable Diffusion的采样原理和U-Net结构。接下来我们来看一看它们在不同框架下的代码实现。</p>
<h2 id="Stable-Diffusion-官方-GitHub-仓库"><a href="#Stable-Diffusion-官方-GitHub-仓库" class="headerlink" title="Stable Diffusion 官方 GitHub 仓库"></a>Stable Diffusion 官方 GitHub 仓库</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>克隆仓库后，照着官方Markdown文档安装即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:CompVis/stable-diffusion.git</span><br></pre></td></tr></table></figure>
<p>先用下面的命令创建conda环境，此后<code>ldm</code>环境就是运行Stable Diffusiion的conda环境。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yaml</span><br><span class="line">conda activate ldm</span><br></pre></td></tr></table></figure>
<p>之后去网上下一个Stable Diffusion的模型文件。比较常见一个版本是v1.5，该模型在Hugging Face上：<a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5">https://huggingface.co/runwayml/stable-diffusion-v1-5</a> （推荐下载<code>v1-5-pruned.ckpt</code>）。下载完毕后，把模型软链接到指定位置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p models/ldm/stable-diffusion-v1/</span><br><span class="line">ln -s &lt;path/to/model.ckpt&gt; models/ldm/stable-diffusion-v1/model.ckpt </span><br></pre></td></tr></table></figure>
<p>准备完毕后，只要输入下面的命令，就可以生成实现文生图了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/txt2img.py --prompt &quot;a photograph of an astronaut riding a horse&quot; </span><br></pre></td></tr></table></figure>
<p>在默认的参数下，“一幅骑着马的飞行员的照片”的绘制结果会被保存在<code>outputs/txt2img-samples</code>中。你也可以通过<code>--outdir &lt;dir&gt;</code>参数来指定输出到的文件夹。我得到的一些绘制结果为：</p>
<p><img src="/2024/01/23/20230713-SD3/1.jpg" alt></p>
<blockquote>
<p>如果你在安装时碰到了错误，可以在搜索引擎上或者GitHub的issue里搜索，一般都能搜到其他人遇到的相同错误。</p>
</blockquote>
<h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><p>接下来，我们来探究一下<code>scripts/txt2img.py</code>的执行过程。为了方便阅读，我们可以简化代码中的命令行处理，得到下面这份精简代码。（你可以把这份代码复制到仓库根目录下的一个新Python脚本里并直接运行。别忘了修改代码中的模型路径）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> omegaconf <span class="keyword">import</span> OmegaConf</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm, trange</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"><span class="keyword">from</span> pytorch_lightning <span class="keyword">import</span> seed_everything</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> autocast</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> make_grid</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ldm.util <span class="keyword">import</span> instantiate_from_config</span><br><span class="line"><span class="keyword">from</span> ldm.models.diffusion.ddim <span class="keyword">import</span> DDIMSampler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model_from_config</span>(<span class="params">config, ckpt, verbose=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loading model from <span class="subst">&#123;ckpt&#125;</span>&quot;</span>)</span><br><span class="line">    pl_sd = torch.load(ckpt, map_location=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;global_step&quot;</span> <span class="keyword">in</span> pl_sd:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Global Step: <span class="subst">&#123;pl_sd[<span class="string">&#x27;global_step&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    sd = pl_sd[<span class="string">&quot;state_dict&quot;</span>]</span><br><span class="line">    model = instantiate_from_config(config.model)</span><br><span class="line">    m, u = model.load_state_dict(sd, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(m) &gt; <span class="number">0</span> <span class="keyword">and</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;missing keys:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(m)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(u) &gt; <span class="number">0</span> <span class="keyword">and</span> verbose:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;unexpected keys:&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(u)</span><br><span class="line"></span><br><span class="line">    model.cuda()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    seed = <span class="number">42</span></span><br><span class="line">    config = <span class="string">&#x27;configs/stable-diffusion/v1-inference.yaml&#x27;</span></span><br><span class="line">    ckpt = <span class="string">&#x27;ckpt/v1-5-pruned.ckpt&#x27;</span></span><br><span class="line">    outdir = <span class="string">&#x27;tmp&#x27;</span></span><br><span class="line">    n_samples = batch_size = <span class="number">3</span></span><br><span class="line">    n_rows = batch_size</span><br><span class="line">    n_iter = <span class="number">2</span></span><br><span class="line">    prompt = <span class="string">&#x27;a photograph of an astronaut riding a horse&#x27;</span></span><br><span class="line">    data = [batch_size * [prompt]]</span><br><span class="line">    scale = <span class="number">7.5</span></span><br><span class="line">    C = <span class="number">4</span></span><br><span class="line">    f = <span class="number">8</span></span><br><span class="line">    H = W = <span class="number">512</span></span><br><span class="line">    ddim_steps = <span class="number">50</span></span><br><span class="line">    ddim_eta = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    seed_everything(seed)</span><br><span class="line"></span><br><span class="line">    config = OmegaConf.load(config)</span><br><span class="line">    model = load_model_from_config(config, ckpt)</span><br><span class="line"></span><br><span class="line">    device = torch.device(</span><br><span class="line">        <span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    model = model.to(device)</span><br><span class="line">    sampler = DDIMSampler(model)</span><br><span class="line"></span><br><span class="line">    os.makedirs(outdir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    outpath = outdir</span><br><span class="line"></span><br><span class="line">    sample_path = os.path.join(outpath, <span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    os.makedirs(sample_path, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    grid_count = <span class="built_in">len</span>(os.listdir(outpath)) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    start_code = <span class="literal">None</span></span><br><span class="line">    precision_scope = autocast</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">with</span> precision_scope(<span class="string">&quot;cuda&quot;</span>):</span><br><span class="line">            <span class="keyword">with</span> model.ema_scope():</span><br><span class="line">                all_samples = <span class="built_in">list</span>()</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> trange(n_iter, desc=<span class="string">&quot;Sampling&quot;</span>):</span><br><span class="line">                    <span class="keyword">for</span> prompts <span class="keyword">in</span> tqdm(data, desc=<span class="string">&quot;data&quot;</span>):</span><br><span class="line">                        uc = <span class="literal">None</span></span><br><span class="line">                        <span class="keyword">if</span> scale != <span class="number">1.0</span>:</span><br><span class="line">                            uc = model.get_learned_conditioning(</span><br><span class="line">                                batch_size * [<span class="string">&quot;&quot;</span>])</span><br><span class="line">                        <span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">                            prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">                        c = model.get_learned_conditioning(prompts)</span><br><span class="line">                        shape = [C, H // f, W // f]</span><br><span class="line">                        samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                                         conditioning=c,</span><br><span class="line">                                                         batch_size=n_samples,</span><br><span class="line">                                                         shape=shape,</span><br><span class="line">                                                         verbose=<span class="literal">False</span>,</span><br><span class="line">                                                         unconditional_guidance_scale=scale,</span><br><span class="line">                                                         unconditional_conditioning=uc,</span><br><span class="line">                                                         eta=ddim_eta,</span><br><span class="line">                                                         x_T=start_code)</span><br><span class="line"></span><br><span class="line">                        x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br><span class="line">                        x_samples_ddim = torch.clamp(</span><br><span class="line">                            (x_samples_ddim + <span class="number">1.0</span>) / <span class="number">2.0</span>, <span class="built_in">min</span>=<span class="number">0.0</span>, <span class="built_in">max</span>=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">                        all_samples.append(x_samples_ddim)</span><br><span class="line">                grid = torch.stack(all_samples, <span class="number">0</span>)</span><br><span class="line">                grid = rearrange(grid, <span class="string">&#x27;n b c h w -&gt; (n b) c h w&#x27;</span>)</span><br><span class="line">                grid = make_grid(grid, nrow=n_rows)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># to image</span></span><br><span class="line">                grid = <span class="number">255.</span> * rearrange(grid, <span class="string">&#x27;c h w -&gt; h w c&#x27;</span>).cpu().numpy()</span><br><span class="line">                img = Image.fromarray(grid.astype(np.uint8))</span><br><span class="line">                img.save(os.path.join(outpath, <span class="string">f&#x27;grid-<span class="subst">&#123;grid_count:04&#125;</span>.png&#x27;</span>))</span><br><span class="line">                grid_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Your samples are ready and waiting for you here: \n<span class="subst">&#123;outpath&#125;</span> \n&quot;</span></span><br><span class="line">          <span class="string">f&quot; \nEnjoy.&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p>抛开前面一大堆初始化操作，代码的核心部分只有下面几行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">uc = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> scale != <span class="number">1.0</span>:</span><br><span class="line">    uc = model.get_learned_conditioning(</span><br><span class="line">        batch_size * [<span class="string">&quot;&quot;</span>])</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">    prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">c = model.get_learned_conditioning(prompts)</span><br><span class="line">shape = [C, H // f, W // f]</span><br><span class="line">samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                  conditioning=c,</span><br><span class="line">                                  batch_size=n_samples,</span><br><span class="line">                                  shape=shape,</span><br><span class="line">                                  verbose=<span class="literal">False</span>,</span><br><span class="line">                                  unconditional_guidance_scale=scale,</span><br><span class="line">                                  unconditional_conditioning=uc,</span><br><span class="line">                                  eta=ddim_eta,</span><br><span class="line">                                  x_T=start_code)</span><br><span class="line"></span><br><span class="line">x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br></pre></td></tr></table></figure>
<p>我们来逐行分析一下这段代码。一开始的几行是执行Classifier-Free Guidance (CFG)。<code>uc</code>表示的是CFG中的无约束下的约束张量。<code>scale</code>表示的是执行CFG的程度，<code>scale</code>不等于<code>1.0</code>即表示启用CFG。<code>model.get_learned_conditioning</code>表示用CLIP把文本编码成张量。对于文本约束的模型，无约束其实就是输入文本为空字符串(<code>&quot;&quot;</code>)。因此，在代码中，若启用了CFG，则会用CLIP编码空字符串，编码结果为<code>uc</code>。</p>
<blockquote>
<p>如果你没学过CFG，也不用担心。你可以暂时不要去理解上面这段话。等读完了后文中有关CFG的代码后，你差不多就能理解CFG的用法了。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uc = None</span><br><span class="line">if scale != 1.0:</span><br><span class="line">    uc = model.get_learned_conditioning(</span><br><span class="line">        batch_size * [&quot;&quot;])</span><br></pre></td></tr></table></figure>
<p>之后的几行是在把用户输入的文本编码成张量。同样，<code>model.get_learned_conditioning</code>表示用CLIP把输入文本编码成张量<code>c</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">isinstance</span>(prompts, <span class="built_in">tuple</span>):</span><br><span class="line">    prompts = <span class="built_in">list</span>(prompts)</span><br><span class="line">c = model.get_learned_conditioning(prompts)</span><br></pre></td></tr></table></figure></p>
<p>接着是用扩散模型的采样器生成图片。在这份代码中，<code>sampler</code>是DDIM采样器，<code>sampler.sample</code>函数直接完成了图像生成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">shape = [C, H // f, W // f]</span><br><span class="line">samples_ddim, _ = sampler.sample(S=ddim_steps,</span><br><span class="line">                                  conditioning=c,</span><br><span class="line">                                  batch_size=n_samples,</span><br><span class="line">                                  shape=shape,</span><br><span class="line">                                  verbose=<span class="literal">False</span>,</span><br><span class="line">                                  unconditional_guidance_scale=scale,</span><br><span class="line">                                  unconditional_conditioning=uc,</span><br><span class="line">                                  eta=ddim_eta,</span><br><span class="line">                                  x_T=start_code)</span><br></pre></td></tr></table></figure>
<p>最后，LDM生成的隐空间图片被VAE解码成真实图片。函数<code>model.decode_first_stage</code>负责图片解码。<code>x_samples_ddim</code>在后续的代码中会被后处理成正确格式的RGB图片，并输出至文件里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_samples_ddim = model.decode_first_stage(samples_ddim)</span><br></pre></td></tr></table></figure>
<p>Stable Diffusion 官方实现的主函数主要就做了这些事情。这份实现还是有一些凌乱的。采样算法的一部分内容被扔到了主函数里，另一部分放到了DDIM采样器里。在阅读官方实现的源码时，既要去读主函数里的内容，也要去读采样器里的内容。</p>
<p>接下来，我们来看一看DDIM采样器的部分代码，学完采样算法的剩余部分的实现。</p>
<h3 id="DDIM-采样器"><a href="#DDIM-采样器" class="headerlink" title="DDIM 采样器"></a>DDIM 采样器</h3><p>回头看主函数的前半部分，DDIM采样器是在下面的代码里导入的：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ldm.models.diffusion.ddim <span class="keyword">import</span> DDIMSampler</span><br></pre></td></tr></table></figure><br>跳转到<code>ldm/models/diffusion/ddim.py</code>文件，我们可以找到<code>DDIMSampler</code>类的实现。</p>
<p>先看一下这个类的构造函数。构造函数主要是把U-Net <code>model</code>给存了下来。后文中的<code>self.model</code>都指的是U-Net。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model, schedule=<span class="string">&quot;linear&quot;</span>, **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.model = model</span><br><span class="line">    self.ddpm_num_timesteps = model.num_timesteps</span><br><span class="line">    self.schedule = schedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># in main</span></span><br><span class="line"></span><br><span class="line">config = OmegaConf.load(config)</span><br><span class="line">model = load_model_from_config(config, ckpt)</span><br><span class="line">model = model.to(device)</span><br><span class="line">sampler = DDIMSampler(model)</span><br></pre></td></tr></table></figure></p>
<p>再沿着类的<code>self.sample</code>方法，看一下DDIM采样的实现代码。以下是<code>self.sample</code>方法的主要内容。这个方法其实就执行了一个<code>self.make_schedule</code>，之后把所有参数原封不动地传到了<code>self.ddim_sampling</code>里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">            S,</span></span></span><br><span class="line"><span class="params"><span class="function">            batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">            shape,</span></span></span><br><span class="line"><span class="params"><span class="function">            conditioning=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">            ...</span></span></span><br><span class="line"><span class="params"><span class="function">            </span>):</span></span><br><span class="line">    <span class="keyword">if</span> conditioning <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)</span><br><span class="line">    <span class="comment"># sampling</span></span><br><span class="line">    C, H, W = shape</span><br><span class="line">    size = (batch_size, C, H, W)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Data shape for DDIM sampling is <span class="subst">&#123;size&#125;</span>, eta <span class="subst">&#123;eta&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    samples, intermediates = self.ddim_sampling(...)</span><br></pre></td></tr></table></figure>
<p><code>self.make_schedule</code>用于预处理扩散模型的中间计算参数。它的大部分实现细节可以略过。DDIM用到的有效时间戳列表就是在这个函数里设置的，该列表通过<code>make_ddim_timesteps</code>获取，并保存在<code>self.ddim_timesteps</code>中。此外，由<code>ddim_eta</code>决定的扩散模型的方差也是在这个方法里设置的。大致扫完这个方法后，我们可以直接跳到<code>self.ddim_sampling</code>的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_schedule</span>(<span class="params">self, ddim_num_steps, ddim_discretize=<span class="string">&quot;uniform&quot;</span>, ddim_eta=<span class="number">0.</span>, verbose=<span class="literal">True</span></span>):</span></span><br><span class="line">    self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,</span><br><span class="line">                                              num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>穿越重重的嵌套，我们总算能看到DDIM采样的实现方法<code>self.ddim_sampling</code>了。它的主要内容如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ddim_sampling</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">    device = self.model.betas.device</span><br><span class="line">    b = shape[<span class="number">0</span>]</span><br><span class="line">    img = torch.randn(shape, device=device)</span><br><span class="line">    timesteps = self.ddim_timesteps</span><br><span class="line">    intermediates = ...</span><br><span class="line">    time_range = np.flip(timesteps)</span><br><span class="line">    total_steps = timesteps.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    iterator = tqdm(time_range, desc=<span class="string">&#x27;DDIM Sampler&#x27;</span>, total=total_steps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, step <span class="keyword">in</span> <span class="built_in">enumerate</span>(iterator):</span><br><span class="line">        index = total_steps - i - <span class="number">1</span></span><br><span class="line">        ts = torch.full((b,), step, device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        outs = self.p_sample_ddim(img, cond, ts, ...)</span><br><span class="line">        img, pred_x0 = outs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img, intermediates</span><br></pre></td></tr></table></figure>
<p>这段代码和我们之前自己写的伪代码非常相似。一开始，方法获取了在<code>make_schedule</code>里初始化的DDIM有效时间戳列表<code>self.ddim_timesteps</code>，并预处理成一个<code>iterator</code>。该迭代器用于控制DDIM去噪循环。每一轮循环会根据当前时刻的图像<code>img</code>和时间戳<code>ts</code>计算下一步的图像<code>img</code>。具体来说，代码每次用当前的时间戳<code>step</code>创建一个内容全部为<code>step</code>，形状为<code>(b,)</code>的张量<code>ts</code>。该张量会和当前的隐空间图像<code>img</code>，约束信息张量<code>cond</code>一起传给执行一轮DDIM去噪的<code>p_sample_ddim</code>方法。<code>p_sample_ddim</code>方法会返回下一步的图像<code>img</code>。最后，经过多次去噪后，<code>ddim_sampling</code>方法将去噪后的隐空间图像<code>img</code>返回。</p>
<blockquote>
<p><code>p_sample_ddim</code>里的<code>p_sample</code>看上去似乎意义不明，实际上这个叫法来自于DDPM论文。在DDPM论文中，扩散模型的前向过程用字母$q$表示，反向过程用字母$p$表示。因此，反向过程的一轮去噪在代码里被叫做<code>p_sample</code>。</p>
</blockquote>
<p>最后来看一下<code>p_sample_ddim</code>这个方法，它的主体部分如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">p_sample_ddim</span>(<span class="params">self, x, c, t, ...</span>):</span></span><br><span class="line">    b, *_, device = *x.shape, x.device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">        e_t = self.model.apply_model(x, t, c)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">        t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">        c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">        e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">        e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Prepare variables</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># current prediction for x_0</span></span><br><span class="line">    pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()</span><br><span class="line">    <span class="keyword">if</span> quantize_denoised:</span><br><span class="line">        pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)</span><br><span class="line">    <span class="comment"># direction pointing to x_t</span></span><br><span class="line">    dir_xt = (<span class="number">1.</span> - a_prev - sigma_t**<span class="number">2</span>).sqrt() * e_t</span><br><span class="line">    noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature</span><br><span class="line">    <span class="keyword">if</span> noise_dropout &gt; <span class="number">0.</span>:</span><br><span class="line">        noise = torch.nn.functional.dropout(noise, p=noise_dropout)</span><br><span class="line">    x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise</span><br><span class="line">    <span class="keyword">return</span> x_prev, pred_x0</span><br></pre></td></tr></table></figure>
<p>方法的内容大致可以拆成三段：首先，方法调用U-Net <code>self.model</code>，使用CFG来计算除这一轮该去掉的噪声<code>e_t</code>。然后，方法预处理出DDIM的中间变量。最后，方法根据DDIM的公式，计算出这一轮去噪后的图片<code>x_prev</code>。我们着重看第一部分的代码。</p>
<p>不启用CFG时，方法直接通过<code>self.model.apply_model(x, t, c)</code>调用U-Net，算出这一轮的噪声<code>e_t</code>。而想启用CFG，需要输入空字符串的约束张量<code>unconditional_conditioning</code>，且CFG的强度<code>unconditional_guidance_scale</code>不为1。CFG的执行过程是：对U-Net输入不同的约束<code>c</code>，先用空字符串约束得到一个预测噪声<code>e_t_uncond</code>，再用输入的文本约束得到一个预测噪声<code>e_t</code>。之后令<code>e_t = et_uncond + scale * (e_t - e_t_uncond)</code>。<code>scale</code>大于1，即表明我们希望预测噪声更加靠近有输入文本的那一个。直观上来看，<code>scale</code>越大，最后生成的图片越符合输入文本，越偏离空文本。下面这段代码正是实现了上述这段逻辑，只不过代码使用了一些数据拼接技巧，让空字符串约束下和输入文本约束下的结果在一次U-Net推理中获得。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> unconditional_conditioning <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> unconditional_guidance_scale == <span class="number">1.</span>:</span><br><span class="line">    e_t = self.model.apply_model(x, t, c)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    x_in = torch.cat([x] * <span class="number">2</span>)</span><br><span class="line">    t_in = torch.cat([t] * <span class="number">2</span>)</span><br><span class="line">    c_in = torch.cat([unconditional_conditioning, c])</span><br><span class="line">    e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br><span class="line">    e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br></pre></td></tr></table></figure></p>
<p><code>p_sample_ddim</code> 方法的后续代码都是在实现下面这个DDIM采样公式。代码工工整整地计算了公式中的<code>predicted_x0</code>, <code>dir_xt</code>, <code>noise</code>，非常易懂，没有需要特别注意的地方。</p>
<p><img src="/2024/01/23/20230713-SD3/1-2.jpg" alt></p>
<p>我们已经看完了<code>p_sample_ddim</code>的代码。该方法可以实现一步去噪操作。多次调用该方法去噪后，我们就能得到生成的隐空间图片。该图片会被返回到main函数里，被VAE的解码器解码成普通图片。至此，我们就学完了Stable Diffusion官方仓库的采样代码。</p>
<p>对照下面这份我们之前写的伪代码，我们再来梳理一下Stable Diffusion官方仓库的代码逻辑。官方仓库的采样代码一部分在main函数里，另一部分在<code>ldm/models/diffusion/ddim.py</code>里。main函数主要完成了编码约束文字、解码隐空间图像这两件事。剩下的DDIM采样以及各种Diffusion图像编辑功能都是在<code>ldm/models/diffusion/ddim.py</code>文件中实现的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>)</span></span><br><span class="line"><span class="function">  <span class="title">ddim_scheduler</span> = <span class="title">DDIMScheduler</span>()</span></span><br><span class="line"><span class="function">  <span class="title">vae</span> = <span class="title">VAE</span>()</span></span><br><span class="line"><span class="function">  <span class="title">unet</span> = <span class="title">UNet</span>()</span></span><br><span class="line"><span class="function">  <span class="title">zt</span> = <span class="title">randn</span>(<span class="params">image_shape</span>)</span></span><br><span class="line"><span class="function">  <span class="title">eta</span> = <span class="title">input</span>()</span></span><br><span class="line"><span class="function">  <span class="title">T</span> = 1000</span></span><br><span class="line"><span class="function">  <span class="title">timesteps</span> = <span class="title">ddim_scheduler</span>.<span class="title">get_timesteps</span>(<span class="params">T, ddim_steps</span>) # [1000, 950, 900, ...]</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">text_encoder</span> = <span class="title">CLIP</span>()</span></span><br><span class="line"><span class="function">  <span class="title">c</span> = <span class="title">text_encoder</span>.<span class="title">encode</span>(<span class="params">text</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">for</span> <span class="title">t</span> = <span class="title">timesteps</span>:</span></span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure>
<p>在学习代码时，要着重学习DDIM采样器部分的代码。大部分基于Diffusion的图像编辑技术都是在DDIM采样的中间步骤中做文章，只要学懂了DDIM采样的代码，学相关图像编辑技术就会非常轻松。除此之外，和LDM相关的文字约束编码、隐空间图像编码解码的接口函数也需要熟悉，不少技术会调用到这几项功能。</p>
<p>还有一些Diffusion相关工作会涉及U-Net的修改。接下来，我们就来看Stable Diffusion官方仓库中U-Net的实现。</p>
<h3 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h3><p>我们来回头看一下main函数和DDIM采样中U-Net的调用逻辑。和U-Net有关的代码如下所示。LDM模型类 <code>model</code>在主函数中通过<code>load_model_from_config</code>从配置文件里创建，随后成为了<code>sampler</code>的成员变量。在DDIM去噪循环中，LDM模型里的U-Net会在<code>self.model.apply_model</code>方法里被调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># main.py</span></span><br><span class="line">config = <span class="string">&#x27;configs/stable-diffusion/v1-inference.yaml&#x27;</span></span><br><span class="line">config = OmegaConf.load(config)</span><br><span class="line">model = load_model_from_config(config, ckpt)</span><br><span class="line">sampler = DDIMSampler(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ldm/models/diffusion/ddim.py</span></span><br><span class="line">e_t = self.model.apply_model(x, t, c)</span><br></pre></td></tr></table></figure>
<p>为了知道U-Net是在哪个类里定义的，我们需要打开配置文件 <code>configs/stable-diffusion/v1-inference.yaml</code>。该配置文件有这样一段话：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model:</span></span><br><span class="line">  <span class="attr">target:</span> <span class="string">ldm.models.diffusion.ddpm.LatentDiffusion</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="attr">conditioning_key:</span> <span class="string">crossattn</span></span><br><span class="line">    <span class="attr">unet_config:</span></span><br><span class="line">        <span class="attr">target:</span> <span class="string">ldm.modules.diffusionmodules.openaimodel.UNetModel</span></span><br></pre></td></tr></table></figure></p>
<p>根据这段话，我们知道LDM类定义在<code>ldm/models/diffusion/ddpm.py</code>的<code>LatentDiffusion</code>里，U-Net类定义在<code>ldm/modules/diffusionmodules/openaimodel.py</code>的<code>UNetModel</code>里。一个LDM类有一个U-Net类的实例。我们先简单看一看<code>LatentDiffusion</code>类的实现。</p>
<p><code>ldm/models/diffusion/ddpm.py</code>原本来自DDPM论文的官方仓库，内含<code>DDPM</code>类的实现。<code>DDPM</code>类维护了扩散模型公式里的一些变量，同时维护了U-Net类的实例。LDM的作者基于之前DDPM的代码进行开发，定义了一个继承自<code>DDPM</code>的<code>LatentDiffusion</code>类。除了DDPM本身的功能外，<code>LatentDiffusion</code>还维护了VAE(<code>self.first_stage_model</code>)，CLIP（<code>self.cond_stage_model</code>）。也就是说，<code>LatentDiffusion</code>主要维护了扩散模型中间变量、U-Net、VAE、CLIP这四类信息。这样，所有带参数的模型都在<code>LatentDiffusion</code>里，我们可以从一个checkpoint文件中读取所有的模型的参数。相关代码定义代码如下：</p>
<blockquote>
<p>把所有模型定义在一起有好处也有坏处。好处在于，用户想使用Stable Diffusion时，只需要下载一个checkpoint文件就行了。坏处在于，哪怕用户只改了某个子模型（如U-Net），为了保存整个模型，他还是得把其他子模型一起存下来。这其中存在着信息冗余，十分不灵活。Diffusers框架没有把模型全存在一个文件里，而是放到了一个文件夹里。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPM</span>(<span class="params">pl.LightningModule</span>):</span></span><br><span class="line">    <span class="comment"># classic DDPM with Gaussian diffusion, in image space</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 unet_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ...</span>):</span></span><br><span class="line">        self.model = DiffusionWrapper(unet_config, conditioning_key)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LatentDiffusion</span>(<span class="params">DDPM</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;main class&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 first_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cond_stage_config,</span></span></span><br><span class="line"><span class="params"><span class="function">                 ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.instantiate_first_stage(first_stage_config)</span><br><span class="line">        self.instantiate_cond_stage(cond_stage_config)</span><br></pre></td></tr></table></figure>
<p>我们主要关注<code>LatentDiffusion</code>类的<code>apply_model</code>方法，它用于调用U-Net <code>self.model</code>。<code>apply_model</code>看上去有很长，但略过了我们用不到的一些代码后，整个方法其实非常短。一开始，方法对输入的约束信息编码<code>cond</code>做了一个前处理，判断约束是哪种类型。如论文里所描述的，LDM支持两种约束：将约束与输入拼接、将约束注入到交叉注意力层中。方法会根据<code>self.model.conditioning_key</code>是<code>concat</code>还是<code>crossattn</code>，使用不同的约束方式。Stable Diffusion使用的是后者，即<code>self.model.conditioning_key == crossattn</code>。做完前处理后，方法执行了<code>x_recon = self.model(x_noisy, t, **cond)</code>。接下来的处理交给U-Net <code>self.model</code>来完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_model</span>(<span class="params">self, x_noisy, t, cond, return_ids=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(cond, <span class="built_in">dict</span>):</span><br><span class="line">        <span class="comment"># hybrid case, cond is exptected to be a dict</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(cond, <span class="built_in">list</span>):</span><br><span class="line">            cond = [cond]</span><br><span class="line">        key = <span class="string">&#x27;c_concat&#x27;</span> <span class="keyword">if</span> self.model.conditioning_key == <span class="string">&#x27;concat&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;c_crossattn&#x27;</span></span><br><span class="line">        cond = &#123;key: cond&#125;</span><br><span class="line"></span><br><span class="line">    x_recon = self.model(x_noisy, t, **cond)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(x_recon, <span class="built_in">tuple</span>) <span class="keyword">and</span> <span class="keyword">not</span> return_ids:</span><br><span class="line">        <span class="keyword">return</span> x_recon[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x_recon</span><br></pre></td></tr></table></figure>
<p>现在，我们跳转到<code>ldm/modules/diffusionmodules/openaimodel.py</code>的<code>UNetModel</code>类里。<code>UNetModel</code>只定义了神经网络层的运算，没有多余的功能。我们只需要看它的<code>__init__</code>方法和<code>forward</code>方法。我们先来看较为简短的<code>forward</code>方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = self.time_embed(t_emb)</span><br><span class="line"></span><br><span class="line">    h = x.<span class="built_in">type</span>(self.dtype)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> self.input_blocks:</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">        hs.append(h)</span><br><span class="line">    h = self.middle_block(h, emb, context)</span><br><span class="line">    <span class="keyword">for</span> module <span class="keyword">in</span> self.output_blocks:</span><br><span class="line">        h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">        h = module(h, emb, context)</span><br><span class="line">    h = h.<span class="built_in">type</span>(x.dtype)</span><br><span class="line">    <span class="keyword">return</span> self.out(h)</span><br></pre></td></tr></table></figure>
<p><code>forward</code>方法的输入是<code>x, timesteps, context</code>，分别表示当前去噪时刻的图片、当前时间戳、文本约束编码。根据这些输入，<code>forward</code>会输出当前时刻应去除的噪声<code>eps</code>。一开始，方法会先对<code>timesteps</code>使用Transformer论文中介绍的位置编码<code>timestep_embedding</code>，得到时间戳的编码<code>t_emb</code>。<code>t_emb</code>再经过几个线性层，得到最终的时间戳编码<code>emb</code>。而<code>context</code>已经是CLIP处理过的编码，它不需要做额外的预处理。时间戳编码<code>emb</code>和文本约束编码<code>context</code>随后会注入到U-Net的所有中间模块中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, timesteps=<span class="literal">None</span>, context=<span class="literal">None</span>, y=<span class="literal">None</span>,**kwargs</span>):</span></span><br><span class="line">    hs = []</span><br><span class="line">    t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=<span class="literal">False</span>)</span><br><span class="line">    emb = self.time_embed(t_emb)</span><br></pre></td></tr></table></figure>
<p>经过预处理后，方法开始处理U-Net的计算。中间结果<code>h</code>会经过U-Net的下采样模块<code>input_blocks</code>，每一个子模块的临时输出都会被保存进一个栈<code>hs</code>里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> h = x.<span class="built_in">type</span>(self.dtype)</span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> self.input_blocks:</span><br><span class="line">    h = module(h, emb, context)</span><br><span class="line">    hs.append(h)</span><br></pre></td></tr></table></figure>
<p>接着，<code>h</code>会经过U-Net的中间模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h = self.middle_block(h, emb, context)</span><br></pre></td></tr></table></figure>
<p>随后，<code>h</code>开始经过U-Net的上采样模块<code>output_blocks</code>。此时每一个编码器子模块的临时输出会从栈<code>hs</code>里弹出，作为对应解码器子模块的额外输入。额外输入<code>hs.pop()</code>会与中间结果<code>h</code>拼接到一起输入进子模块里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> self.output_blocks:</span><br><span class="line">    h = th.cat([h, hs.pop()], dim=<span class="number">1</span>)</span><br><span class="line">    h = module(h, emb, context)</span><br><span class="line">h = h.<span class="built_in">type</span>(x.dtype)</span><br></pre></td></tr></table></figure>
<p>最后，<code>h</code>会被输出层转换成一个通道数正确的<code>eps</code>张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> self.out(h)</span><br></pre></td></tr></table></figure>
<p>这段代码的数据连接图如下所示：</p>
<p><img src="/2024/01/23/20230713-SD3/1-3.jpg" alt></p>
<p>在阅读<code>__init__</code>前，我们先看一下待会会用到的另一个模块类<code>TimestepEmbedSequential</code>的定义。在PyTorch中，一系列输入和输出都只有一个变量的模块在串行连接时，可以用串行模块类<code>nn.Sequential</code>来把多个模块合并简化成一个模块。而在扩散模型中，多数模块的输入是<code>x, t, c</code>三个变量，输出是一个变量。为了也能用类似的串行模块类把扩散模型的模块合并在一起，代码中包含了一个<code>TimestepEmbedSequential</code>类。它的行为类似于<code>nn.Sequential</code>，只不过它支持<code>x, t, c</code>的输入。<code>forward</code>中用到的多数模块都是通过<code>TimestepEmbedSequential</code>创建的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimestepEmbedSequential</span>(<span class="params">nn.Sequential, TimestepBlock</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, TimestepBlock):</span><br><span class="line">                x = layer(x, emb)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(layer, SpatialTransformer):</span><br><span class="line">                x = layer(x, context)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>看完了数据的计算过程，我们回头来看各个子模块在<code>__init__</code>方法中是怎么被详细定义的。<code>__init__</code>的主要内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNetModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.time_embed = nn.Sequential(</span><br><span class="line">            linear(model_channels, time_embed_dim),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(time_embed_dim, time_embed_dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.input_blocks = nn.ModuleList(</span><br><span class="line">            [</span><br><span class="line">                TimestepEmbedSequential(</span><br><span class="line">                    conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                )</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">                layers = [</span><br><span class="line">                    ResBlock(...)]</span><br><span class="line">                ch = mult * model_channels</span><br><span class="line">                <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                     layers.append(</span><br><span class="line">                        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...))</span><br><span class="line"></span><br><span class="line">                self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">            <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">                out_ch = ch</span><br><span class="line">                self.input_blocks.append(</span><br><span class="line">                    TimestepEmbedSequential(</span><br><span class="line">                        ResBlock(...)</span><br><span class="line">                        <span class="keyword">if</span> resblock_updown</span><br><span class="line">                        <span class="keyword">else</span> Downsample(...)</span><br><span class="line">                    )</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">        self.middle_block = TimestepEmbedSequential(</span><br><span class="line">            ResBlock(...),</span><br><span class="line">            AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...),</span><br><span class="line">            ResBlock(...),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.output_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">enumerate</span>(channel_mult))[::-<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks + <span class="number">1</span>):</span><br><span class="line">                ich = input_block_chans.pop()</span><br><span class="line">                layers = [</span><br><span class="line">                    ResBlock(...)</span><br><span class="line">                ]</span><br><span class="line">                ch = model_channels * mult</span><br><span class="line">                <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                    layers.append(</span><br><span class="line">                        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...)</span><br><span class="line">                    )</span><br><span class="line">                <span class="keyword">if</span> level <span class="keyword">and</span> i == num_res_blocks:</span><br><span class="line">                    out_ch = ch</span><br><span class="line">                    layers.append(</span><br><span class="line">                        ResBlock(...)</span><br><span class="line">                        <span class="keyword">if</span> resblock_updown</span><br><span class="line">                        <span class="keyword">else</span> Upsample(...)</span><br><span class="line">                    )</span><br><span class="line">                    ds //= <span class="number">2</span></span><br><span class="line">                self.output_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">    self.out = nn.Sequential(</span><br><span class="line">            normalization(ch),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            zero_module(conv_nd(dims, model_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p><code>__init__</code>方法的代码很长。在阅读这样的代码时，我们不需要每一行都去细读，只需要理解代码能拆成几块，每一块在做什么即可。<code>__init__</code>方法其实就是定义了<code>forward</code>中用到的5个模块，我们一个一个看过去即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNetModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line"></span><br><span class="line">        self.time_embed = ...</span><br><span class="line"></span><br><span class="line">        self.input_blocks = nn.ModuleList(...)</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">        self.middle_block = ...</span><br><span class="line"></span><br><span class="line">        self.output_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">enumerate</span>(channel_mult))[::-<span class="number">1</span>]:</span><br><span class="line">            ...</span><br><span class="line">    self.out = ...</span><br></pre></td></tr></table></figure>
<p>先来看<code>time_embed</code>。回忆一下，在<code>forward</code>里，输入的整数时间戳会被正弦编码<code>timestep_embedding</code>（即Transformer中的位置编码）编码成一个张量。之后，时间戳编码处理模块<code>time_embed</code>用于进一步提取时间戳编码的特征。从下面的代码中可知，它本质上就是一个由两个普通线性层构成的模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.time_embed = nn.Sequential(</span><br><span class="line">            linear(model_channels, time_embed_dim),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(time_embed_dim, time_embed_dim),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>再来看U-Net最后面的输出模块<code>out</code>。输出模块的结构也很简单，它主要包含了一个卷积层，用于把中间变量的通道数从<code>dims</code>变成<code>model_channels</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.out = nn.Sequential(</span><br><span class="line">            normalization(ch),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            zero_module(conv_nd(dims, model_channels, out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)),</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>接下来，我们把目光聚焦在U-Net的三个核心模块上：<code>input_blocks</code>, <code>middle_block</code>, <code>output_blocks</code>。这三个模块的组成都很类似，都用到了残差块<code>ResBlock</code>和注意力块。稍有不同的是，<code>input_blocks</code>的每一大层后面都有一个下采样模块，<code>output_blocks</code>的每一大层后面都有一个上采样模块。上下采样模块的结构都很常规，与经典的U-Net无异。我们把学习的重点放在残差块和注意力块上。我们先看这两个模块的内部实现细节，再来看它们是怎么拼接起来的。</p>
<p>Stable Diffusion的U-Net中的<code>ResBlock</code>和原DDPM的U-Net的<code>ResBlock</code>功能完全一样，都是在普通残差块的基础上，支持时间戳编码的额外输入。具体来说，普通的残差块是由两个卷积模块和一条短路连接构成的，即<code>y = x + conv(conv(x))</code>。如果经过两个卷积块后数据的通道数发生了变化，则要在短路连接上加一个转换通道数的卷积，即<code>y = conv(x) + conv(conv(x))</code>。</p>
<p>在这种普通残差块的基础上，扩散模型中的残差块还支持时间戳编码<code>t</code>的输入。为了把<code>t</code>和输入<code>x</code>的信息融合在一起，<code>t</code>会和经过第一个卷积后的中间结果<code>conv(x)</code>加在一起。可是，<code>t</code>的通道数和<code>conv(x)</code>的通道数很可能会不一样。通道数不一样的数据是不能直接加起来的。为此，每一个残差块中都有一个用于转换<code>t</code>通道数的线性层。这样，<code>t</code>和<code>conv(x)</code>就能相加了。整个模块的计算可以表示成<code>y=conv(x) + conv(conv(x) + linear(t))</code>。残差块的示意图和源代码如下：</p>
<p><img src="/2024/01/23/20230713-SD3/1-4.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span>(<span class="params">TimestepBlock</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        self.in_layers = nn.Sequential(</span><br><span class="line">            normalization(channels),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            conv_nd(dims, channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.emb_layers = nn.Sequential(</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            linear(</span><br><span class="line">                emb_channels,</span><br><span class="line">                <span class="number">2</span> * self.out_channels <span class="keyword">if</span> use_scale_shift_norm <span class="keyword">else</span> self.out_channels,</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        self.out_layers = nn.Sequential(</span><br><span class="line">            normalization(self.out_channels),</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            zero_module(</span><br><span class="line">                conv_nd(dims, self.out_channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.out_channels == channels:</span><br><span class="line">            self.skip_connection = nn.Identity()</span><br><span class="line">        <span class="keyword">elif</span> use_conv:</span><br><span class="line">            self.skip_connection = conv_nd(</span><br><span class="line">                dims, channels, self.out_channels, <span class="number">3</span>, padding=<span class="number">1</span></span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.skip_connection = conv_nd(dims, channels, self.out_channels, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb</span>):</span></span><br><span class="line">        h = self.in_layers(x)</span><br><span class="line">        emb_out = self.emb_layers(emb).<span class="built_in">type</span>(h.dtype)</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(emb_out.shape) &lt; <span class="built_in">len</span>(h.shape):</span><br><span class="line">            emb_out = emb_out[..., <span class="literal">None</span>]</span><br><span class="line">        h = h + emb_out</span><br><span class="line">        h = self.out_layers(h)</span><br><span class="line">        <span class="keyword">return</span> self.skip_connection(x) + h</span><br></pre></td></tr></table></figure>
<p>代码中的<code>in_layers</code>是第一个卷积模块，<code>out_layers</code>是第二个卷积模块。<code>skip_connection</code>是用于调整短路连接通道数的模块。若输入输出的通道数相同，则该模块是一个恒等函数，不对数据做任何修改。<code>emb_layers</code>是调整时间戳编码通道数的线性层模块。这些模块的定义都在<code>ResBlock</code>的<code>__init__</code>里。它们的结构都很常规，没有值得注意的地方。我们可以着重阅读模型的<code>forward</code>方法。</p>
<p>如前文所述，在<code>forward</code>中，输入<code>x</code>会先经过第一个卷积模块<code>in_layers</code>，再与经过了<code>emb_layers</code>调整的时间戳编码<code>emb</code>相加后，输入进第二个卷积模块<code>out_layers</code>。最后，做完计算的数据会和经过了短路连接的原输入<code>skip_connection(x)</code>加在一起，作为整个残差块的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, emb</span>):</span></span><br><span class="line">    h = self.in_layers(x)</span><br><span class="line">    emb_out = self.emb_layers(emb).<span class="built_in">type</span>(h.dtype)</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(emb_out.shape) &lt; <span class="built_in">len</span>(h.shape):</span><br><span class="line">        emb_out = emb_out[..., <span class="literal">None</span>]</span><br><span class="line">    h = h + emb_out</span><br><span class="line">    h = self.out_layers(h)</span><br><span class="line">    <span class="keyword">return</span> self.skip_connection(x) + h</span><br></pre></td></tr></table></figure>
<p>这里有一点实现细节需要注意。时间戳编码<code>emb_out</code>的形状是<code>[n, c]</code>。为了把它和形状为<code>[n, c, h, w]</code>的图片加在一起，需要把它的形状变成<code>[n, c, 1, 1]</code>后再相加（形状为<code>[n, c, 1, 1]</code>的数据在与形状为<code>[n, c, h, w]</code>的数据做加法时形状会被自动广播成<code>[n, c, h, w]</code>）。在PyTorch中，<code>x=x[..., None]</code>可以在一个数据最后加一个长度为1的维度。比如对于形状为<code>[n, c]</code>的<code>t</code>，<code>t[..., None]</code>的形状就会是<code>[n, c, 1]</code>。</p>
<p>残差块的内容到此结束。我们接着来看注意力模块。在看模块的具体实现之前，我们先看一下源代码中有哪几种注意力模块。在U-Net的代码中，注意力模型是用以下代码创建的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">    layers.append(</span><br><span class="line">        AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>第一行<code>if ds in attention_resolutions:</code>用于控制在U-Net的哪几个大层。Stable Diffusion每一大层都用了注意力模块，可以忽略这一行。随后，代码根据是否设置<code>use_spatial_transformer</code>来创建<code>AttentionBlock</code>或是<code>SpatialTransformer</code>。<code>AttentionBlock</code>是DDPM中采样的普通自注意力模块，而<code>SpatialTransformer</code>是LDM中提出的支持额外约束的标准Transfomer块。Stable Diffusion使用的是<code>SpatialTransformer</code>。我们就来看一看这个模块的实现细节。</p>
<p>如前所述，<code>SpatialTransformer</code>使用的是<strong>标准</strong>的Transformer块，它和Transformer中的Transformer块完全一致。输入<code>x</code>先经过一个自注意力层，再过一个交叉注意力层。在此期间，约束编码<code>c</code>会作为交叉注意力层的<code>K, V</code>输入进模块。最后，数据经过一个全连接层。每一层的输入都会和输出做一个残差连接。</p>
<p><img src="/2024/01/23/20230713-SD3/1-5.jpg" alt></p>
<p>当然，标准Transformer是针对一维序列数据的。要把Transformer用到图像上，则需要把图像的宽高拼接到同一维，即对张量做形状变换<code>n c h w -&gt; n c (h * w)</code>。做完这个变换后，就可以把数据直接输入进Transformer模块了。<br>这些图像数据与序列数据的适配都是在<code>SpatialTransformer</code>类里完成的。<code>SpatialTransformer</code>类并没有直接实现Transformer块的细节，仅仅是U-Net和Transformer块之间的一个过渡。Transformer块的实现在它的一个子模块里。我们来看它的实现代码。</p>
<p><code>SpatialTransformer</code>有两个卷积层<code>proj_in</code>, <code>proj_out</code>，负责图像通道数与Transformer模块通道数之间的转换。<code>SpatialTransformer</code>的<code>transformer_blocks</code>才是真正的Transformer模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpatialTransformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, n_heads, d_head,</span></span></span><br><span class="line"><span class="params"><span class="function">                 depth=<span class="number">1</span>, dropout=<span class="number">0.</span>, context_dim=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        inner_dim = n_heads * d_head</span><br><span class="line">        self.norm = Normalize(in_channels)</span><br><span class="line"></span><br><span class="line">        self.proj_in = nn.Conv2d(in_channels,</span><br><span class="line">                                 inner_dim,</span><br><span class="line">                                 kernel_size=<span class="number">1</span>,</span><br><span class="line">                                 stride=<span class="number">1</span>,</span><br><span class="line">                                 padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim)</span><br><span class="line">                <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(depth)]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.proj_out = zero_module(nn.Conv2d(inner_dim,</span><br><span class="line">                                              in_channels,</span><br><span class="line">                                              kernel_size=<span class="number">1</span>,</span><br><span class="line">                                              stride=<span class="number">1</span>,</span><br><span class="line">                                              padding=<span class="number">0</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在<code>forward</code>中，图像数据在进出Transformer模块前后都会做形状和通道数上的适配。运算结束后，结果和输入之间还会做一个残差连接。<code>context</code>就是约束信息编码，它会接入到交叉注意力层上。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">    b, c, h, w = x.shape</span><br><span class="line">    x_in = x</span><br><span class="line">    x = self.norm(x)</span><br><span class="line">    x = self.proj_in(x)</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b c h w -&gt; b (h w) c&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> block <span class="keyword">in</span> self.transformer_blocks:</span><br><span class="line">        x = block(x, context=context)</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b (h w) c -&gt; b c h w&#x27;</span>, h=h, w=w)</span><br><span class="line">    x = self.proj_out(x)</span><br><span class="line">    <span class="keyword">return</span> x + x_in</span><br></pre></td></tr></table></figure></p>
<p>每一个Transformer模块的结构完全符合上文的示意图。如果你之前学过Transformer，那这些代码你会十分熟悉。我们快速把这部分代码浏览一遍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicTransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, n_heads, d_head, dropout=<span class="number">0.</span>, context_dim=<span class="literal">None</span>, gated_ff=<span class="literal">True</span>, checkpoint=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)  <span class="comment"># is a self-attention</span></span><br><span class="line">        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)</span><br><span class="line">        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,</span><br><span class="line">                                    heads=n_heads, dim_head=d_head, dropout=dropout)  <span class="comment"># is self-attn if context is none</span></span><br><span class="line">        self.norm1 = nn.LayerNorm(dim)</span><br><span class="line">        self.norm2 = nn.LayerNorm(dim)</span><br><span class="line">        self.norm3 = nn.LayerNorm(dim)</span><br><span class="line">        self.checkpoint = checkpoint</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.attn1(self.norm1(x)) + x</span><br><span class="line">        x = self.attn2(self.norm2(x), context=context) + x</span><br><span class="line">        x = self.ff(self.norm3(x)) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>自注意力层和交叉注意力层都是用<code>CrossAttention</code>类实现的。该模块与Transformer论文中的多头注意力机制完全相同。当<code>forward</code>的参数<code>context=None</code>时，模块其实只是一个提取特征的自注意力模块；而当<code>context</code>为约束文本的编码时，模块就是一个根据文本约束进行运算的交叉注意力模块。该模块用不到<code>mask</code>，相关的代码可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, query_dim, context_dim=<span class="literal">None</span>, heads=<span class="number">8</span>, dim_head=<span class="number">64</span>, dropout=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head * heads</span><br><span class="line">        context_dim = default(context_dim, query_dim)</span><br><span class="line"></span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span></span><br><span class="line">        self.heads = heads</span><br><span class="line"></span><br><span class="line">        self.to_q = nn.Linear(query_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_k = nn.Linear(context_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.to_v = nn.Linear(context_dim, inner_dim, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, query_dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, context=<span class="literal">None</span>, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        h = self.heads</span><br><span class="line"></span><br><span class="line">        q = self.to_q(x)</span><br><span class="line">        context = default(context, x)</span><br><span class="line">        k = self.to_k(context)</span><br><span class="line">        v = self.to_v(context)</span><br><span class="line"></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; (b h) n d&#x27;</span>, h=h), (q, k, v))</span><br><span class="line"></span><br><span class="line">        sim = einsum(<span class="string">&#x27;b i d, b j d -&gt; b i j&#x27;</span>, q, k) * self.scale</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> exists(mask):</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention, what we cannot get enough of</span></span><br><span class="line">        attn = sim.softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        out = einsum(<span class="string">&#x27;b i j, b j d -&gt; b i d&#x27;</span>, attn, v)</span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;(b h) n d -&gt; b n (h d)&#x27;</span>, h=h)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br></pre></td></tr></table></figure>
<p>Transformer块的内容到此结束。看完了<code>SpatialTransformer</code>和<code>ResBlock</code>，我们可以回头去看模块之间是怎么拼接的了。先来看U-Net的中间块。它其实就是一个<code>ResBlock</code>接一个<code>SpatialTransformer</code>再接一个<code>ResBlock</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.middle_block = TimestepEmbedSequential(</span><br><span class="line">    ResBlock(...),</span><br><span class="line">    SpatialTransformer(...),</span><br><span class="line">    ResBlock(...),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下采样块<code>input_blocks</code>和上采样块<code>output_blocks</code>的结构几乎一模一样，区别只在于每一大层最后是做下采样还是上采样。这里我们以下采样块为例来学习一下这两个块的结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">self.input_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        TimestepEmbedSequential(</span><br><span class="line">            conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        layers = [</span><br><span class="line">            ResBlock(...)]</span><br><span class="line">        ch = mult * model_channels</span><br><span class="line">        <span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">                layers.append(</span><br><span class="line">                AttentionBlock(...) <span class="keyword">if</span> <span class="keyword">not</span> use_spatial_transformer <span class="keyword">else</span> SpatialTransformer(...))</span><br><span class="line"></span><br><span class="line">        self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">    <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">        out_ch = ch</span><br><span class="line">        self.input_blocks.append(</span><br><span class="line">            TimestepEmbedSequential(</span><br><span class="line">                ResBlock(...)</span><br><span class="line">                <span class="keyword">if</span> resblock_updown</span><br><span class="line">                <span class="keyword">else</span> Downsample(...)</span><br><span class="line">            )</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>上采样块一开始是一个调整输入图片通道数的卷积层，它的作用和<code>self.out</code>输出层一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.input_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        TimestepEmbedSequential(</span><br><span class="line">            conv_nd(dims, in_channels, model_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>之后正式进行上采样块的构造。此处代码有两层循环，外层循环表示正在构造哪一个大层，内层循环表示正在构造该大层的哪一组模块。也就是说，共有<code>len(channel_mult)</code>个大层，每一大层都有<code>num_res_blocks</code>组相同的模块。在Stable Diffusion中，<code>channel_mult=[1, 2, 4, 4]</code>, <code>num_res_blocks=2</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>每一组模块由一个<code>ResBlock</code>和一个<code>SpatialTransformer</code>构成。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">layers = [</span><br><span class="line">    ResBlock(...)</span><br><span class="line">]</span><br><span class="line">ch = mult * model_channels</span><br><span class="line"><span class="keyword">if</span> ds <span class="keyword">in</span> attention_resolutions:</span><br><span class="line">    ...</span><br><span class="line">    layers.append(</span><br><span class="line">        SpatialTransformer(...)</span><br><span class="line">    )</span><br><span class="line">self.input_blocks.append(TimestepEmbedSequential(*layers))</span><br><span class="line">...</span><br></pre></td></tr></table></figure><br>构造完每一组模块后，若现在还没到最后一个大层，则添加一个下采样模块。Stable Diffusion有4个大层，只有运行到前3个大层时才会添加下采样模块。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> level, mult <span class="keyword">in</span> <span class="built_in">enumerate</span>(channel_mult):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_res_blocks):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">if</span> level != <span class="built_in">len</span>(channel_mult) - <span class="number">1</span>:</span><br><span class="line">        out_ch = ch</span><br><span class="line">        self.input_blocks.append(</span><br><span class="line">            TimestepEmbedSequential(</span><br><span class="line">                ResBlock(...)</span><br><span class="line">                <span class="keyword">if</span> resblock_updown</span><br><span class="line">                <span class="keyword">else</span> Downsample(...)</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        ch = out_ch</span><br><span class="line">        input_block_chans.append(ch)</span><br><span class="line">        ds *= <span class="number">2</span></span><br></pre></td></tr></table></figure><br>至此，我们已经学完了Stable Diffusion的U-Net的主要实现代码。让我们来总结一下。U-Net是一种先对数据做下采样，再做上采样的网络结构。为了防止信息丢失，下采样模块和对应的上采样模块之间有残差连接。下采样块、中间块、上采样块都包含了<code>ResBlock</code>和<code>SpatialTransformer</code>两种模块。<code>ResBlock</code>是图像网络中常使用的残差块，而<code>SpatialTransformer</code>是能够融合图像全局信息并融合不同模态信息的Transformer块。Stable Diffusion的U-Net的输入除了有图像外，还有时间戳<code>t</code>和约束编码<code>c</code>。<code>t</code>会先过几个嵌入层和线性层，再输入进每一个<code>ResBlock</code>中。<code>c</code>会直接输入到所有Transformer块的交叉注意力块中。</p>
<p><img src="/2024/01/23/20230713-SD3/0-3.jpg" alt></p>
<h2 id="Diffusers"><a href="#Diffusers" class="headerlink" title="Diffusers"></a>Diffusers</h2><p>Diffusers是由Hugging Face维护的一套Diffusion框架。这个库的代码被封装进了一个Python模块里，我们可以在安装了Diffusers的Python环境中用<code>import diffusers</code>随时调用该库。相比之下，Diffusers的代码架构更加清楚，且各类Stable Diffusion的新技术都会及时集成进Diffusers库中。</p>
<p>由于我们已经在上文中学过了Stable Diffusion官方源码，在学习Diffusers代码时，我们只会大致过一过每一段代码是在做什么，而不会赘述Stable Diffusion的原理。</p>
<h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>安装该库时，不需要克隆仓库，只需要直接用pip即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade diffusers[torch]</span><br></pre></td></tr></table></figure>
<p>之后，随便在某个地方创建一个Python脚本文件，输入官方的示例项目代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DiffusionPipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">pipeline = DiffusionPipeline.from_pretrained(<span class="string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>, torch_dtype=torch.float16)</span><br><span class="line">pipeline.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline(<span class="string">&quot;An image of a squirrel in Picasso style&quot;</span>).images[<span class="number">0</span>].save(<span class="string">&#x27;output.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>运行代码后，”一幅毕加索风格的松鼠图片”的绘制结果会保存在<code>output.jpg</code>中。我得到的结果如下：</p>
<p><img src="/2024/01/23/20230713-SD3/2-1.jpg" alt></p>
<p>在Diffusers中，<code>from_pretrained</code>函数可以直接从Hugging Face的模型仓库中下载预训练模型。比如，示例代码中<code>from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, ...)</code>指的就是从模型仓库<code>https://huggingface.co/runwayml/stable-diffusion-v1-5</code>中获取模型。</p>
<p>如果在当前网络下无法从命令行中访问Hugging Face，可以先想办法在网页上访问上面的模型仓库，手动下载<code>v1-5-pruned.ckpt</code>。之后，克隆Diffusers的GitHub仓库，再用Diffusers的工具把Stable Diffusion模型文件转换成Diffusers支持的模型格式。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:huggingface/diffusers.git</span><br><span class="line">cd diffusers</span><br><span class="line">python scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path &lt;src&gt; --dump_path &lt;dst&gt;</span><br></pre></td></tr></table></figure>
<p>比如，假设你的模型文件存在<code>ckpt/v1-5-pruned.ckpt</code>，你想把输出的Diffusers的模型文件存在<code>ckpt/sd15</code>，则应该输入：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path ckpt/v1-5-pruned.ckpt --dump_path ckpt/sd15 </span><br></pre></td></tr></table></figure><br>之后修改示例脚本中的路径，就可以成功运行了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> DiffusionPipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">pipeline = DiffusionPipeline.from_pretrained(<span class="string">&quot;ckpt/sd15&quot;</span>, torch_dtype=torch.float16)</span><br><span class="line">pipeline.to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">pipeline(<span class="string">&quot;An image of a squirrel in Picasso style&quot;</span>).images[<span class="number">0</span>].save(<span class="string">&#x27;output.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure><br>对于其他的原版SD checkpoint（比如在civitai上下载的），也可以用同样的方式把它们转换成Diffusers兼容的版本。</p>
<h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>Diffusers使用<code>Pipeline</code>来管理一类图像生成算法。和图像生成相关的模块（如U-Net，DDIM采样器）都是<code>Pipeline</code>的成员变量。打开Diffusers版Stable Diffusion模型的配置文件<code>model_index.json</code>（在 <a target="_blank" rel="noopener" href="https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json">https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/model_index.json</a> 网页上直接访问或者在本地的模型文件夹中找到），我们能看到该模型使用的<code>Pipeline</code>:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;StableDiffusionPipeline&quot;</span>,</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py</code>中，我们能找到<code>StableDiffusionPipeline</code>类的定义。所有<code>Pipeline</code>类的代码都非常长，一般我们可以忽略其他部分，只看运行方法<code>__call__</code>里的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">50</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">List</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    guidance_scale: <span class="built_in">float</span> = <span class="number">7.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_images_per_prompt: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    eta: <span class="built_in">float</span> = <span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 0. Default height and width to unet</span></span><br><span class="line">    height = height <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">    width = width <span class="keyword">or</span> self.unet.config.sample_size * self.vae_scale_factor</span><br><span class="line">    <span class="comment"># to deal with lora scaling and other possible forward hooks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. Check inputs. Raise error if not correct</span></span><br><span class="line">    self.check_inputs(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. Define call parameters</span></span><br><span class="line">    batch_size = ...</span><br><span class="line"></span><br><span class="line">    device = self._execution_device</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. Encode input prompt</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    prompt_embeds, negative_prompt_embeds = self.encode_prompt(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For classifier free guidance, we need to do two forward passes.</span></span><br><span class="line">    <span class="comment"># Here we concatenate the unconditional and text embeddings into a single batch</span></span><br><span class="line">    <span class="comment"># to avoid doing two forward passes</span></span><br><span class="line">    <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">        prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. Prepare timesteps</span></span><br><span class="line">    timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. Prepare latent variables</span></span><br><span class="line">    num_channels_latents = self.unet.config.in_channels</span><br><span class="line">    latents = self.prepare_latents(...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. Prepare extra step kwargs. <span class="doctag">TODO:</span> Logic should ideally just be moved out of the pipeline</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 7. Denoising loop</span></span><br><span class="line">    num_warmup_steps = <span class="built_in">len</span>(timesteps) - num_inference_steps * self.scheduler.order</span><br><span class="line">    self._num_timesteps = <span class="built_in">len</span>(timesteps)</span><br><span class="line">    <span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">        <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br><span class="line">            <span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">            latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># predict the noise residual</span></span><br><span class="line">            noise_pred = self.unet(</span><br><span class="line">                latent_model_input,</span><br><span class="line">                t,</span><br><span class="line">                encoder_hidden_states=prompt_embeds,</span><br><span class="line">                ...</span><br><span class="line">            )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform guidance</span></span><br><span class="line">            <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">                noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">                noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">and</span> self.guidance_rescale &gt; <span class="number">0.0</span>:</span><br><span class="line">                <span class="comment"># Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf</span></span><br><span class="line">                noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># call the callback, if provided</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="built_in">len</span>(timesteps) - <span class="number">1</span> <span class="keyword">or</span> ((i + <span class="number">1</span>) &gt; num_warmup_steps <span class="keyword">and</span> (i + <span class="number">1</span>) % self.scheduler.order == <span class="number">0</span>):</span><br><span class="line">                progress_bar.update()</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> output_type == <span class="string">&quot;latent&quot;</span>:</span><br><span class="line">        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>, generator=generator)[</span><br><span class="line">            <span class="number">0</span></span><br><span class="line">        ]</span><br><span class="line">        image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        image = latents</span><br><span class="line">        has_nsfw_concept = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)</span><br></pre></td></tr></table></figure>
<p>虽然这段代码很长，但代码中的关键内容和我们在本文开头写的伪代码完全一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ldm_text_to_image</span>(<span class="params">image_shape, text, ddim_steps = <span class="number">20</span>, eta = <span class="number">0</span></span>)</span></span><br><span class="line"><span class="function">  <span class="title">ddim_scheduler</span> = <span class="title">DDIMScheduler</span>()</span></span><br><span class="line"><span class="function">  <span class="title">vae</span> = <span class="title">VAE</span>()</span></span><br><span class="line"><span class="function">  <span class="title">unet</span> = <span class="title">UNet</span>()</span></span><br><span class="line"><span class="function">  <span class="title">zt</span> = <span class="title">randn</span>(<span class="params">image_shape</span>)</span></span><br><span class="line"><span class="function">  <span class="title">eta</span> = <span class="title">input</span>()</span></span><br><span class="line"><span class="function">  <span class="title">T</span> = 1000</span></span><br><span class="line"><span class="function">  <span class="title">timesteps</span> = <span class="title">ddim_scheduler</span>.<span class="title">get_timesteps</span>(<span class="params">T, ddim_steps</span>) # [1000, 950, 900, ...]</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">text_encoder</span> = <span class="title">CLIP</span>()</span></span><br><span class="line"><span class="function">  <span class="title">c</span> = <span class="title">text_encoder</span>.<span class="title">encode</span>(<span class="params">text</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">for</span> <span class="title">t</span> = <span class="title">timesteps</span>:</span></span><br><span class="line">    eps = unet(zt, t, c)</span><br><span class="line">    std = ddim_scheduler.get_std(t, eta)</span><br><span class="line">    zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span><br><span class="line">  xt = vae.decoder.decode(zt)</span><br><span class="line">  <span class="keyword">return</span> xt</span><br></pre></td></tr></table></figure>
<p>我们可以对照着上面的伪代码来阅读这个方法。经过Diffusers框架本身的一些前处理后，方法先获取了约束文本的编码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Encode input prompt</span></span><br><span class="line"><span class="comment"># c = text_encoder.encode(text)</span></span><br><span class="line">prompt_embeds, negative_prompt_embeds = self.encode_prompt(...)</span><br></pre></td></tr></table></figure>
<p>方法再从采样器里获取了要用到的时间戳，并随机生成了一个初始噪声。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Prepare timesteps</span></span><br><span class="line"><span class="comment"># timesteps = ddim_scheduler.get_timesteps(T, ddim_steps)</span></span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. Prepare latent variables</span></span><br><span class="line"><span class="comment"># zt = randn(image_shape)</span></span><br><span class="line">num_channels_latents = self.unet.config.in_channels</span><br><span class="line">latents = self.prepare_latents(</span><br><span class="line">    ...</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>做完准备后，方法进入去噪循环。循环一开始是用U-Net算出当前应去除的噪声<code>noise_pred</code>。由于加入了CFG，U-Net计算的前后有一些对数据形状处理的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> self.progress_bar(total=num_inference_steps) <span class="keyword">as</span> progress_bar:</span><br><span class="line">    <span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br><span class="line">        <span class="comment"># eps = unet(zt, t, c)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># expand the latents if we are doing classifier free guidance</span></span><br><span class="line">        latent_model_input = torch.cat([latents] * <span class="number">2</span>) <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">else</span> latents</span><br><span class="line">        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># predict the noise residual</span></span><br><span class="line">        noise_pred = self.unet(</span><br><span class="line">            latent_model_input,</span><br><span class="line">            t,</span><br><span class="line">            encoder_hidden_states=prompt_embeds,</span><br><span class="line">            ...</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># perform guidance</span></span><br><span class="line">        <span class="keyword">if</span> self.do_classifier_free_guidance:</span><br><span class="line">            noise_pred_uncond, noise_pred_text = noise_pred.chunk(<span class="number">2</span>)</span><br><span class="line">            noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.do_classifier_free_guidance <span class="keyword">and</span> self.guidance_rescale &gt; <span class="number">0.0</span>:</span><br><span class="line">            <span class="comment"># Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf</span></span><br><span class="line">            noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)</span><br></pre></td></tr></table></figure>
<p>有了应去除的噪声，方法会调用扩散模型采样器对当前的噪声图片进行更新。Diffusers把采样的逻辑全部封装进了采样器的<code>step</code>方法里。对于包括DDIM在内的所有采样器，都可以调用这个通用的接口，完成一步采样。<code>eta</code>等采样器参数会通过<code>**extra_step_kwargs</code>传入采样器的<code>step</code>方法里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># std = ddim_scheduler.get_std(t, eta)</span></span><br><span class="line"><span class="comment"># zt = ddim_scheduler.get_xt_prev(zt, t, eps, std)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>经过若干次循环后，我们得到了隐空间下的生成图片。我们还需要调用VAE把隐空间图片解码成普通图片。代码中的<code>self.vae.decode(latents / self.vae.config.scaling_factor, ...)</code>用于解码图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> output_type == <span class="string">&quot;latent&quot;</span>:</span><br><span class="line">    image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>, generator=generator)[</span><br><span class="line">        <span class="number">0</span></span><br><span class="line">    ]</span><br><span class="line">    image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    image = latents</span><br><span class="line">    has_nsfw_concept = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)</span><br></pre></td></tr></table></figure>
<p>就这样，我们很快就看完了Diffusers的采样代码。相比之下，Diffusers的封装确实更合理，主要的图像生成逻辑都写在<code>Pipeline</code>类的<code>__call__</code>里，剩余逻辑都封装在VAE、U-Net、采样器等各自的类里。</p>
<h3 id="U-Net-1"><a href="#U-Net-1" class="headerlink" title="U-Net"></a>U-Net</h3><p>接下来我们来看Diffusers中的U-Net实现。还是打开模型配置文件<code>model_index.json</code>，我们可以找到U-Net的类名。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="attr">&quot;unet&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;diffusers&quot;</span>,</span><br><span class="line">    <span class="string">&quot;UNet2DConditionModel&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/models/unet_2d_condition.py</code>文件中，我们可以找到类<code>UNet2DConditionModel</code>。由于Diffusers集成了非常多新特性，整个文件就像一锅大杂烩一样，掺杂着各种功能的实现代码。不过，这份U-Net的实现还是基于原版Stable Diffusion的U-Net进行开发的，原版代码的每一部分都能在这份代码里找到对应。在阅读代码时，我们可以跳过无关的功能，只看我们在Stable Diffusion官方仓库中见过的部分。</p>
<p>先看初始化函数的主要内容。初始化函数依然主要包括<code>time_proj, time_embedding</code>, <code>down_blocks</code>, <code>mid_block</code>, <code>up_blocks</code>, <code>conv_in</code>, <code>conv_out</code>这几个模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UNet2DConditionModel</span>(<span class="params">ModelMixin, ConfigMixin, UNet2DConditionLoadersMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.conv_in = nn.Conv2d(</span><br><span class="line">            in_channels, block_out_channels[<span class="number">0</span>], kernel_size=conv_in_kernel, padding=conv_in_padding</span><br><span class="line">        )</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">elif</span> time_embedding_type == <span class="string">&quot;positional&quot;</span>:</span><br><span class="line">            self.time_proj = Timesteps(block_out_channels[<span class="number">0</span>], flip_sin_to_cos, freq_shift)</span><br><span class="line">        ...</span><br><span class="line">        self.time_embedding = TimestepEmbedding(...)</span><br><span class="line">        self.down_blocks = nn.ModuleList([])</span><br><span class="line">        self.up_blocks = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> i, down_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(down_block_types):</span><br><span class="line">            ...</span><br><span class="line">            down_block = get_down_block(...)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> mid_block_type == ...</span><br><span class="line">            self.mid_block = ...</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, up_block_type <span class="keyword">in</span> <span class="built_in">enumerate</span>(up_block_types):</span><br><span class="line">            up_block = get_up_block(...)</span><br><span class="line"></span><br><span class="line">        self.conv_out = nn.Conv2d(...)</span><br></pre></td></tr></table></figure>
<p>其中，较为重要的<code>down_blocks</code>, <code>mid_block</code>, <code>up_blocks</code>都是根据模块类名称来创建的。我们可以在Diffusers的Stable Diffusion模型文件夹的U-Net的配置文件<code>unet/config.json</code>中找到对应的模块类名称。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">&quot;down_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnDownBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;DownBlock2D&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;mid_block_type&quot;</span>: <span class="string">&quot;UNetMidBlock2DCrossAttn&quot;</span>,</span><br><span class="line">  <span class="string">&quot;up_block_types&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;UpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span>,</span><br><span class="line">    <span class="string">&quot;CrossAttnUpBlock2D&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>diffusers/models/unet_2d_blocks.py</code>中，我们可以找到这几个模块类的定义。和原版代码一样，这几个模块的核心组件都是残差块和Transformer块。在Diffusers中，残差块叫做<code>ResnetBlock2D</code>，Transformer块叫做<code>Transformer2DModel</code>。这几个类的执行逻辑和原版仓库的也几乎一样。比如<code>CrossAttnDownBlock2D</code>的定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossAttnDownBlock2D</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            resnets.append(ResnetBlock2D(...))</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> dual_cross_attention:</span><br><span class="line">                attentions.append(Transformer2DModel(...))</span><br></pre></td></tr></table></figure>
<p>接着我们来看U-Net的<code>forward</code>方法。忽略掉其他功能的实现，该方法的主要内容如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        sample: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        timestep: <span class="type">Union</span>[torch.Tensor, <span class="built_in">float</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">        encoder_hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 0. center input if necessary</span></span><br><span class="line">    <span class="keyword">if</span> self.config.center_input_sample:</span><br><span class="line">        sample = <span class="number">2</span> * sample - <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. time</span></span><br><span class="line">    timesteps = timestep</span><br><span class="line">    t_emb = self.time_proj(timesteps)</span><br><span class="line">    emb = self.time_embedding(t_emb, timestep_cond)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. pre-process</span></span><br><span class="line">    sample = self.conv_in(sample)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. down</span></span><br><span class="line">    down_block_res_samples = (sample,)</span><br><span class="line">    <span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">        sample, res_samples = downsample_block(</span><br><span class="line">            hidden_states=sample,</span><br><span class="line">            temb=emb,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line">        down_block_res_samples += res_samples</span><br><span class="line">    <span class="comment"># 4. mid</span></span><br><span class="line">    sample = self.mid_block(</span><br><span class="line">            sample,</span><br><span class="line">            emb,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. up</span></span><br><span class="line">    <span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">        res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">        down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">        sample = upsample_block(</span><br><span class="line">            hidden_states=sample,</span><br><span class="line">            temb=emb,</span><br><span class="line">            res_hidden_states_tuple=res_samples,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            ...)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># 6. post-process</span></span><br><span class="line">    sample = self.conv_out(sample)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> UNet2DConditionOutput(sample=sample)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>该方法和原版仓库的实现差不多，唯一要注意的是栈相关的实现。在方法的下采样计算中，每个<code>downsample_block</code>会返回多个残差输出的元组<code>res_samples</code>，该元组会拼接到栈<code>down_block_res_samples</code>的栈顶。在上采样计算中，代码会根据当前的模块个数，从栈顶一次取出<code>len(upsample_block.resnets)</code>个残差输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">down_block_res_samples = (sample,)</span><br><span class="line"><span class="keyword">for</span> downsample_block <span class="keyword">in</span> self.down_blocks:</span><br><span class="line">    sample, res_samples = downsample_block(...)</span><br><span class="line">    down_block_res_samples += res_samples</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, upsample_block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.up_blocks):</span><br><span class="line">    res_samples = down_block_res_samples[-<span class="built_in">len</span>(upsample_block.resnets) :]</span><br><span class="line">    down_block_res_samples = down_block_res_samples[: -<span class="built_in">len</span>(upsample_block.resnets)]</span><br><span class="line">    sample = upsample_block(...)</span><br></pre></td></tr></table></figure>
<p>现在，我们已经看完了Diffusers中U-Net的主要内容。可以看出，Diffusers的U-Net包含了很多功能，一般情况下是难以自己更改这些代码的。有没有什么办法能方便地修改U-Net的实现呢？由于很多工作都需要修改U-Net的Attention，Diffusers给U-Net添加了几个方法，用于精确地修改每一个Attention模块的实现。我们来学习一个修改Attention模块的示例。</p>
<p>U-Net类的<code>attn_processors</code>属性会返回一个词典，它的key是每个Attention运算类所在位置，比如<code>down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor</code>，它的value是每个Attention运算类的实例。默认情况下，每个Attention运算类都是<code>AttnProcessor</code>，它的实现在<code>diffusers/models/attention_processor.py</code>文件中。</p>
<p>为了修改Attention运算的实现，我们需要构建一个格式一样的词典<code>attn_processor_dict</code>，再调用<code>unet.set_attn_processor(attn_processor_dict)</code>，取代原来的<code>attn_processors</code>。假如我们自己实现了另一个Attention运算类<code>MyAttnProcessor</code>，我们可以编写下面的代码来修改Attention的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">attn_processor_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> unet.attn_processors.keys():</span><br><span class="line">    <span class="keyword">if</span> we_want_to_modify(k):</span><br><span class="line">        attn_processor_dict[k] = MyAttnProcessor()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_processor_dict[k] = AttnProcessor()</span><br><span class="line"></span><br><span class="line">unet.set_attn_processor(attn_processor_dict)</span><br></pre></td></tr></table></figure>
<p><code>MyAttnProcessor</code>的唯一要求是，它需要实现一个<code>__call__</code>方法，且方法参数与<code>AttnProcessor</code>的一致。除此之外，我们可以自由地实现Attention处理的细节。一般来说，我们可以先把原来<code>AttnProcessor</code>的实现代码复制过去，再对某些细节做修改。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们学习了Stable Diffusion的原版实现和Diffusers实现的主要内容：采样算法和U-Net。具体来说，在原版仓库中，采样的实现一部分在主函数中，一部分在DDIM采样器类中。U-Net由一个简明的PyTorch模块类实现，其中比较重要的子模块是残差块和Transformer块。相比之下，Diffusers实现的封装更好，功能更多。Diffusers用一个Pipeline类来维护采样过程。Diffusers的U-Net实现与原版完全相同，且支持更复杂的功能。此外，Diffusers还给U-Net提供了精确修改Attention计算的接口。</p>
<p>不管是哪个Stable Diffusion的框架，都会提供一些相同的原子操作。各种基于Stable Diffusion的应用都应该基于这些原子操作开发，而无需修改这些操作的细节。在学习时，我们应该注意这些操作在不同的框架下的写法是怎么样的。常用的原子操作包括：</p>
<ul>
<li>VAE的解码和编码</li>
<li>文本编码器（CLIP）的编码</li>
<li>用U-Net预测当前图像应去除的噪声</li>
<li>用采样器计算下一去噪迭代的图像</li>
</ul>
<p>在原版仓库中，相关的实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VAE的解码和编码</span></span><br><span class="line">model.decode_first_stage(...)</span><br><span class="line">model.encode_first_stage(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本编码器（CLIP）的编码</span></span><br><span class="line">model.get_learned_conditioning(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用U-Net预测当前图像应去除的噪声</span></span><br><span class="line">model.apply_model(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用采样器计算下一去噪迭代的图像</span></span><br><span class="line">p_sample_ddim(...)</span><br></pre></td></tr></table></figure>
<p>在Diffusers中，相关的实现代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VAE的解码和编码</span></span><br><span class="line">image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">latents = self.vae.encode(image).latent_dist.sample(generator) * self.vae.config.scaling_factor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本编码器（CLIP）的编码</span></span><br><span class="line">self.encode_prompt(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用U-Net预测当前图像应去除的噪声</span></span><br><span class="line">self.unet(..., return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用采样器计算下一去噪迭代的图像</span></span><br><span class="line">self.scheduler.step(..., return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<p>如今zero-shot（无需训练）的Stable Diffusion编辑技术一般只会修改采样算法和Attention计算，需训练的编辑技术有时会在U-Net里加几个模块。只要我们熟悉了普通的Stable Diffusion是怎么样生成图像的，知道原来U-Net的结构是怎么样的，我们在阅读新论文的源码时就可以把这份代码与原来的代码进行对比，只看那些有修改的部分。相信读完了本文后，我们不仅加深了对Stable Diffusion本身的理解，以后学习各种新出的Stable Diffusion编辑技术时也会更加轻松。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">130</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
