<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/en/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/en/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/en/images/logo.svg" color="#222">

<link rel="stylesheet" href="/en/css/main.css">


<link rel="stylesheet" href="/en/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/en/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Designer, artist, philosopher, researcher.">
<meta property="og:type" content="website">
<meta property="og:title" content="周弈帆的博客">
<meta property="og:url" content="https://zhouyifan.net/en/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="Designer, artist, philosopher, researcher.">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhou Yifan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/en/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/en/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/en/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/en/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/en/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/en/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/en/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-switch_lang">

    <a href="https://zhouyifan.net" rel="section"><i class="fa fa-language fa-fw"></i>简体中文</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/12/15/20241212-Pyramid-Flow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/12/15/20241212-Pyramid-Flow/" class="post-title-link" itemprop="url">论文速览 | Pyramid Flow：以低分辨率的前几帧为约束高效生成视频</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-15 21:55:08" itemprop="dateCreated datePublished" datetime="2024-12-15T21:55:08+08:00">2024-12-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>按分辨率从低到高的顺序生成图像是一种常见思路。此外，Diffusion Forcing 等论文带来了一种新的扩散模型视频生成思路：将视频生成转换为约束于前几帧图像的单张图像自回归生成。Pyramid Flow 工作把两种思路结合起来，提出了一种新的视频生成范式：在自回归生成新帧时，用低分辨率的前几帧图像作为约束。这使得模型能够更高效率地利用历史帧的信息。</p>
<p>论文名：Pyramidal Flow Matching for Efficient Video Generative Modeling</p>
<p>Arxiv： <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.05954">https://arxiv.org/abs/2410.05954</a></p>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/jy0205/Pyramid-Flow">https://github.com/jy0205/Pyramid-Flow</a></p>
<h2 id="以往工作"><a href="#以往工作" class="headerlink" title="以往工作"></a>以往工作</h2><p>将图像生成拆解成从低分辨率到高分辨率是一种很常见的思想。基于扩散模型，有多种方式来应用这种思想。一种比较直接的方式是显式将图像生成分解成生成最低分辨率的图像和多轮超分辨率，代表工作是 <em>Cascaded Diffusion Models for High Fidelity Image Generation</em>；另一种更加巧妙的方式是将图像上采样和扩散模型的去噪同时进行，代表工作是 <em>f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation</em>。本文的多尺度设计和 f-DM 非常相似，我会在文末详细分析二者的区别。</p>
<p>将视频生成转换为约束于之前所有帧的图像生成是一种再简单不过的想法。然而，在使用扩散模型时，选择最佳的约束方式并不是一个简单的问题。比较常见的添加图像约束的方式是与原输入在通道维度上拼接。对于视频自回归生成而言，这个做法的问题是网络对于约束图像和待生成图像的处理不统一。近期，Diffusion Forcing 工作告诉我们，我们可以给视频扩散模型的不同帧添加不同的噪声，并修改注意力机制，从而将其转换成一个约束于之前帧的图像生成模型，并且模型对每一帧的处理在注意力层以外是统一的。</p>
<p>流匹配 (Flow Matching) 可以简单看成一种改进了噪声调度机制的 DDPM。假设时刻 $0$ 表示纯高斯噪声，时刻 $1$ 表示清晰图像，按照最常用的流匹配方法，中间 $t$ 时刻的带噪图像为二者的线性插值。</p>
<script type="math/tex; mode=display">
\mathbf{x}_t = t\mathbf{x}_1 + (1 - t)\mathbf{x}_0</script><p>此外，去噪网络的预测目标也从残余噪声 (epsilon) 改为了速度（velocity）。</p>
<p>相关博文：</p>
<p><a href="https://zhouyifan.net/2024/11/28/20241128-diffusion-forcing/">Diffusion Forcing</a></p>
<h2 id="多尺度的加噪过程"><a href="#多尺度的加噪过程" class="headerlink" title="多尺度的加噪过程"></a>多尺度的加噪过程</h2><p>扩散模型中，我们要把清晰图像中的信息逐渐破坏掉。这样的图像退化方式不只添加高斯噪声一种，我们可以在添加噪声的同时下采样图像，定义出一种新的扩散模型前向过程（退化过程）。</p>
<p>如下图的一维扩散模型所示，Pyramid Flow 在加噪的同时做两次下采样。总时间被拆成了三段，第 $k$ 个阶段的时间范围是 $[s_k, e_k]$。</p>
<p><img src="/2024/12/15/20241212-Pyramid-Flow/1.png" alt></p>
<p>做下采样后，图像中的信息会突然减少。为了让同一个阶段的图像逐渐失去这些信息，而不是在一次下采样中突然失去，我们采用这样的插值策略：设 $\mathbf{x}_t$ 为最大分辨率下 $t$ 时刻的带噪图像（其计算方法由上文的流匹配噪声公式决定），那么在第 $k$ 阶段的分辨率下，$t$ 时刻的带噪图像 $\hat{\mathbf{x}}_t$ 为：</p>
<script type="math/tex; mode=display">
\hat{\mathbf{x}}_{t} = t' Down(\mathbf{x}_{e_k}, 2^k) + (1-t')Up(Down(\mathbf{x}_{s_k}, 2^{k+1}))</script><p>其中，$Down(\mathbf{x}, a)$ 表示把 $\mathbf{x}$ 下采样 $a$ 倍，$Up(\mathbf{x})$ 表示把 $\mathbf{x}$ 上采样 $2$ 倍，$t’=(t-s_k)/(e_k-s_k)$ 表示 $t$ 时刻在第 $k$ 阶段里的插值比例。初看这些公式可能会有点头大，我们可以先看下面的示意图再回头看公式。</p>
<p><img src="/2024/12/15/20241212-Pyramid-Flow/2.png" alt></p>
<p>公式里做插值的两项分别表示当前阶段最清楚、最模糊的图像，插值比例 $t’$ 为 $t$ 在当前阶段的时间窗口里的比例。这种插值不仅让噪声强度渐变，还让因下采样而产生的信息损失渐变，就好像是我们在连续地对图像下采样一样。</p>
<p>修改了退化机制后，除了定义流匹配的输入变量 $\hat{\mathbf{x}}_{t}$ 外，我们还可以定义新的流匹配学习目标。流匹配的学习目标是一个速度，而速度又可以由两个端点决定。我们可以用上面的公式定义每一个阶段的端点 $\hat{\mathbf{x}}_{e_k}, \hat{\mathbf{x}}_{s_k}$，再以速度 $\hat{\mathbf{x}}_{e_k} - \hat{\mathbf{x}}_{s_k}$ 为该阶段所有点的学习目标。</p>
<h2 id="重新设置上采样噪声"><a href="#重新设置上采样噪声" class="headerlink" title="重新设置上采样噪声"></a>重新设置上采样噪声</h2><p>在多尺度生成时，我们必须仔细考虑图像应该怎么跨越两个阶段，也就是图像该怎么上采样。</p>
<p>大多数涉及扩散模型多尺度生成的工作（比如 Laplacian Diffusion Models sampling, f-DM）都会在上采样时考虑调整噪声强度的大小。而 Pyramid Flow 不仅考虑了噪声的强度，还考虑到了噪声的协方差矩阵——原本扩散模型不同位置的噪声是相互独立的，但 2 倍上采样后一个像素会影响到周围 4 个像素，因此噪声之间的关系需要用协方差矩阵表示而不是独立表示。作者根据协方差矩阵必须半正定这一性质推导了一个更好的修改上采样后噪声的方法。</p>
<h2 id="多尺度视频生成"><a href="#多尺度视频生成" class="headerlink" title="多尺度视频生成"></a>多尺度视频生成</h2><p>Diffusion Forcing 工作表明，在训练视频扩散模型时，不同帧的退化程度可以不同。而在 Pyramid Flow 的框架下，图像的退化不仅包括加噪，还包括降采样。因此，用 Pyramid Flow 训练视频扩散模型时，可以让过往帧为低分辨率的，只让最新帧为最高分辨率的。这样做可以大大减少注意力操作的输入量，提升计算效率。此外，还可以参照 Diffusion Forcing 的做法，通过因果（causal）注意力把视频扩散模型转换成约束于前几帧的单张图像自回归生成。</p>
<p><img src="/2024/12/15/20241212-Pyramid-Flow/3.png" alt></p>
<h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>最后，我们来简单看一下 Pyramid Flow 的实现细节。Pyramid Flow 可以基于任何一种 DiT 架构的文生图模型开发，比如 SD3 和 FLUX.1。为了将文生图模型转换成视频模型，本工作做了如下的适配：</p>
<ul>
<li>将图像 VAE 变成能在时间维度压缩视频的 3D VAE。</li>
<li>在 DiT 中用 RoPE 描述时间维度的位置关系。空间维度位置编码保持原模型的设计。</li>
</ul>
<p>另外，为了对齐不同尺度的空间位置编码，本工作参考 CogVideoX，对低分辨率图像使用了位置编码外推。</p>
<p>本工作开源的视频模型需要用 128 块 A100 训练约 1 天，能生成 241 帧总时长 10 秒的视频。相比其他视频模型来说要的资源已经少了很多了。</p>
<h2 id="批判性分析与总结"><a href="#批判性分析与总结" class="headerlink" title="批判性分析与总结"></a>批判性分析与总结</h2><p>Pyramid Flow 在多尺度加噪的设计上和以往工作 f-DM 非常相似。二者都把扩散模型的加噪拆成不同尺度，都在同一尺度内采取线性插值的方式计算带噪图片，都考虑了上采样时噪声的变化。二者的不同之处在于：</p>
<ul>
<li>f-DM 用的是 DDPM 的噪声调度，而 Pyramid Flow 用的是 Rectified Flow 的公式。由于 Rectified Flow 的公式本来就是线性插值，因此它对同尺度带噪图像插值来说兼容性更好一点。</li>
<li>基于上一点，可能是由于同尺度线性插值与 DDPM 不太兼容，f-DM 额外用一个残差项表示下采样导致的误差。Pyramid Flow 没用到这一设计。</li>
<li>f-DM 从局部信噪比的角度推导了上采样时噪声应该发生的变化，而 Pyramid Flow 是从协方差矩阵的角度。</li>
</ul>
<p>Pyramid Flow 没引用 f-DM，作者大概是独立发明了一遍类似的加噪策略。我在 GitHub 上已经告知了作者有 f-DM 这篇类似论文。当然，哪怕是有类似工作在前，Pyramid Flow 在结合流匹配和多尺度生成上还是有一定创新的。</p>
<p>包括这个工作在内，有好几个工作都在用扩散模型做多尺度图像生成。但是，不像自回归中的 VAR，这些工作在纯图像生成任务上并不是很有名。因此，多尺度生成虽然是一个不错的想法，但我们还需要多多思考该怎么在扩散模型里应用它。Pyramid Flow 最大的意义可能不在于其多尺度的设计，而在于将多尺度生成融合进了视频生成中。这样做最直接的好处是减少了历史帧的数据量，提升了模型计算效率。</p>
<p>虽然这个工作试图用同一个去噪模型来处理任意尺度的图像，但实际上它只用了 3 个尺度。只用 3 个尺度并不能说明模型能够处理任意多尺度的图像，这可能仅仅是暴力拟合的结果。因此，我觉得一个比较重要的思考方向是：怎么让去噪扩散模型更好地理解图像尺度这一概念，复用各个尺度的知识，从而实现任意多尺度的图像去噪。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/12/15/20241209-Laplacian-DM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/12/15/20241209-Laplacian-DM/" class="post-title-link" itemprop="url">论文速览 | Laplacian Diffusion Models：将图像拆成不同频率分量并分别生成</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-15 21:55:03" itemprop="dateCreated datePublished" datetime="2024-12-15T21:55:03+08:00">2024-12-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>受到经典图像表示方法拉普拉斯金字塔（Laplacian Pyramid）的启发，英伟达最近公布了一种叫做 Laplacian Diffusion Model （拉普拉斯扩散模型，后文简称 LaDM）的新型<strong>像素空间</strong>扩散模型，并用这种架构实现了文生图、超分辨率、ControlNet 等多种任务。在这篇博文里，我们来着重学习一下这种新型扩散模型的设计思想。</p>
<h2 id="以往工作"><a href="#以往工作" class="headerlink" title="以往工作"></a>以往工作</h2><p>扩散模型奠基之作 DDPM 及其升级版 ADM (<em>Diffusion Models Beat GANs on Image Synthesis</em>) 都是像素空间里的扩散模型。相比 LDM (隐扩散模型，即 Stable Diffusion)，这类扩散模型不需要额外的自编码器来压缩图像，避免了编码解码带来的精度损失。</p>
<p>将图像从分辨率的维度拆解是一种很常见的思想。比如 Cascaded Diffusion Models 就是一种先生成低分辨率图像，再不断超分的扩散模型。今年比较有名的 VAR（<em>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</em>）也是一种按分辨率自回归的生成模型。</p>
<p>和这篇工作非常相关的早期工作是苹果在 2022 发表的 <em>f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation</em>。f-DM 将扩散模型的加噪推广到了降采样、模糊等其他退化策略上。降采样版的 f-DM 有非常多的设计和本工作很像。苹果该团队次年发表的 <em>Matryoshka Diffusion Models</em> 也用到了按分辨率逐次生成的设计。</p>
<h2 id="将拉普拉斯金字塔融入扩散模型"><a href="#将拉普拉斯金字塔融入扩散模型" class="headerlink" title="将拉普拉斯金字塔融入扩散模型"></a>将拉普拉斯金字塔融入扩散模型</h2><p>拉普拉斯金字塔是一种图像表示方法，它把图像按频率成分拆成几张分辨率不同的图像，分辨率越低的图像表示频率越低的图像成分。我们直接通过下面的例子学习它的原理。假如 <code>x</code> 是原图，那么 <code>x(3)=down(down(x))</code>，<code>x(2)=down(x)-up(x(3))</code>, <code>x(1)=x-up(down(x))</code>。对 <code>x(1), x(2), x(3)</code> 求加权和就可以还原输入图像。</p>
<p><img src="https://files.mdnice.com/user/32105/77511764-0811-43a3-bb35-a47167e3c6e8.png" alt></p>
<p>受此启发，LaDM 将扩散模型的训练过程也用类似的方法分解：设 $\mathcal{X}$ 为训练图片集合，$\mathcal{X}^{(1)},\mathcal{X}^{(2)}, \mathcal{X}^{(3)}$ 分别是拉普拉斯金字塔不同成分构成的集合，那么我们在 $\mathcal{X}^{(3)}$, $\mathcal{X}^{(3)} \cup \mathcal{X}^{(2)}$，$\mathcal{X}$ 上分别训练三个去噪模型。也就是说，不同分辨率的模型生成不同层级的拉普拉斯金字塔复原结果。</p>
<p>根据经验，扩散模型早期（加噪后期）生成低频内容，后期（加噪前期）生成高频内容，所以训练时我们让不同分辨率的输入图像随噪声的衰退速度也不同。图像所代表的频率越低，衰减速度越慢，越需要从早期开始去噪。这样，在生成时，我们能生成到中途后再逐渐加上高频细节。</p>
<p><img src="https://files.mdnice.com/user/32105/c52677ed-e078-44ba-94a1-7de05ebc959f.png" alt></p>
<p>在采样过程中，我们按照下图所示的路线从低频到高频生成图像。有了该分辨率的初始图像后，按正常 DDPM 采样的步骤就可以生成当前分辨率的图像了。问题在于某分辨率的初始图像怎么从上一个分辨率过渡而来。</p>
<p>在切换当前带噪图像的分辨率时，我们既要放大其中的清晰图像（信号），也要放大其中的噪声。观察上一张图和下面的图，在分辨率切换时，新的高频成分（上图中的$\mathbf{x}^{(2)}$在时刻 3 及 $\mathbf{x}^{(2)}$ 在时刻 2）是一张纯黑图，新信号为零，所以对于信号的部分我们可以直接放大。而放大噪声时，我们要做一些噪声强度上的修改，保证放大后信噪比不变。这部分的细节详见论文。</p>
<p><img src="https://files.mdnice.com/user/32105/dcba9b54-b812-4060-98ea-080dabb8ec1c.png" alt></p>
<h2 id="1K-图像生成"><a href="#1K-图像生成" class="headerlink" title="1K 图像生成"></a>1K 图像生成</h2><p>为了生成 $1024 \times 1024$ 分辨率的图像，LaDM 采用了两阶段 Cascaded Diffusion Model 的设计，让生成高分辨率的图像约束于低分辨率图像。另外，由于注意力操作的时间复杂度很高，一般的像素扩散模型只能做到 $256 \times 256$ 大小。为了解决此问题，LaDM 依然用一个 $256 \times 256$ 的去噪模型来生成 1K 图片，但输入前后用小波变换来压缩/复原图像。</p>
<p><img src="https://files.mdnice.com/user/32105/acdf00af-0ff3-406f-a182-97a03c888469.png" alt></p>
<h2 id="批判性分析与总结"><a href="#批判性分析与总结" class="headerlink" title="批判性分析与总结"></a>批判性分析与总结</h2><p>这篇文章是一篇由公司发表的技术报告，展示了很多可视化结果，却没有任何定量结果，代码也没有开源，不知道它的生成能力和其他模型比起来如何。</p>
<p>这篇文章提出的模型虽然是像素空间扩散模型，但是其拉普拉斯金字塔的设计与模型是像素空间模型还是隐空间模型无关。我们完全可以把这套设计搬到隐空间上。VAR 已经向我们证明了对隐空间图像做拉普拉斯分解是可行的。另外，这篇文章的主干网络是 U-Net 而不是 DiT。想对这个工作做一点简单的改进的话，可以弄一个 LDM + DiT 版本的。</p>
<p>LaDM 设计最巧妙的点是其加噪过程，频率越高的成分越早变成纯噪声。这样的话我们可以在图像生成到一半的时候再直接把高频成分加上。如果高频成分一直在的话，我们还需要额外的设计在切换分辨率时把缺少的高频加上。</p>
<p>有工作证明神经网络不擅长拟合高频信息。因此，在图像任务中，手动将输入图像拆成不同频率成分可能有助于网络的学习。我们可能可以沿着这个思路去改进之前多种图像任务的输入。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/12/09/20241208-Context-Window-Extension/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/12/09/20241208-Context-Window-Extension/" class="post-title-link" itemprop="url">让预训练 Transformer 生成更长的文本/图像：位置编码长度外推技术</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-09 02:15:32" itemprop="dateCreated datePublished" datetime="2024-12-09T02:15:32+08:00">2024-12-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>随着视觉主干模型不断向 Transformer 靠拢，和 Transformer 配套的一些技术也从 NLP 社区涌入了 CV 社区。比如 Stable Diffusion 3 还在用标准 Transformer 那一套正弦位置编码，而其升级版 FLUX.1 就用上了旋转位置编码 (RoPE) , Lumina-T2X 模型甚至把 RoPE 的长度外推技术也从 NLP 社区搬了过来。在这篇博文中，我将站在一个对 NLP 技术了解不深的 CV 研究者的视角，较为详细地介绍一下 NLP 中 RoPE 相关的位置编码知识、RoPE 长度外推技术以及它们在 CV 里的应用。</p>
<p>长度外推，指的是使用在短序列上预训练的 Transformer 模型直接生成超出训练长度的长序列。类比到图像生成中，长度外推可以看成对模型所建模的图像分布做了一次超分辨率：比如模型训练时只见过 $256 \times 256$ 的图像，我们想直接用它生成 $512 \times 512$ 且同样清晰的图像。</p>
<p>推荐大家在阅读本文前先熟悉位置编码的基本原理，强烈推荐阅读 RoPE 提出者苏剑林的系列文章。</p>
<p>位置编码设计原则与 RoPE 的首次提出：<a target="_blank" rel="noopener" href="https://kexue.fm/archives/8130">https://kexue.fm/archives/8130</a></p>
<p>详细介绍 RoPE：<a target="_blank" rel="noopener" href="https://kexue.fm/archives/8265">https://kexue.fm/archives/8265</a></p>
<p>介绍长度外推的一项关键改进 (NTK-aware)：<a target="_blank" rel="noopener" href="https://kexue.fm/archives/9675">https://kexue.fm/archives/9675</a> <a target="_blank" rel="noopener" href="https://kexue.fm/archives/9706">https://kexue.fm/archives/9706</a> <a target="_blank" rel="noopener" href="https://kexue.fm/archives/9948">https://kexue.fm/archives/9948</a></p>
<p>和这篇博文相关的两篇学术论文是：</p>
<p>YaRN，一种公认效果较好的长度外推技术：<em>YaRN: Efficient Context Window Extension of Large Language Models</em> (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.00071">https://arxiv.org/abs/2309.00071</a>)</p>
<p>Lumina-Next，前沿扩散 Transformer (Diffusion Transformer, DIT) 模型，采用了长度外推技术：<em>Lumina-Next : Making Lumina-T2X Stronger and Faster with Next-DiT</em> (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.18583">https://arxiv.org/abs/2406.18583</a>)</p>
<h2 id="位置编码知识回顾"><a href="#位置编码知识回顾" class="headerlink" title="位置编码知识回顾"></a>位置编码知识回顾</h2><h3 id="Transformer-中的位置编码"><a href="#Transformer-中的位置编码" class="headerlink" title="Transformer 中的位置编码"></a>Transformer 中的位置编码</h3><p>相比于此前流行的 CNN、RNN 模型，Transformer 的一大特点是其输出与输入次序无关。比如我们用 Transformer 建模文本的概率，那么模型会把「上海」和「海上」当成概率一样的词语。这也就是说 Transformer 无法从输入词元 (token) 的位置关系中获取信息。</p>
<blockquote>
<p>如果让 Transformer 不输出信息聚合后的概率，还是保留输入词元的结构的话，那么打乱输入词元顺序就会同样地打乱输出词元顺序。模型依然无法获取输入词元间的位置关系。</p>
</blockquote>
<p>为了把 <code>1, 2, 3, 4</code> 这样的位置信息输入进模型，标准 Tranformer 的做法是给不同位置的输入加上不同的位置编码。假设模型的中间变量都是二维向量，那么在句子中位置为 $k$ 的词元的位置编码是：</p>
<script type="math/tex; mode=display">
[\sin(k), \cos(k)]</script><p>如果模型的中间变量都是 $d$ 维向量 （为了方便不妨认为 $d$ 是偶数），我们只需要把 $d$ 拆成 $d/2$ 组，每组用不同频率的三角函数即可。这样，长度为 $d$ 的词元在位置 $k$ 的位置编码是：</p>
<script type="math/tex; mode=display">
[\sin(k/10000^{2\cdot 0/d}), \cos(k/10000^{2\cdot 0/d}), ..., \sin(k/10000^{2\cdot i/d}), \cos(k/10000^{2\cdot i/d}) , ...]</script><p>直观上来看，随着中间变量的维度越来越长，位置编码中对应的三角函数的频率不断变低，从一开始的 $1$ 逐渐靠向 $1/10000$。</p>
<blockquote>
<p>上述公式来自论文，代码实现时要注意更多细节。比如有些代码中 $i$ 是从 $0$ 开始计数的。由于指数的分子的范围是从$0$到$d-2$，代码会把指数的分母也改成 $d-2$，保证最后一组三角函数的频率是 $1/10000$。</p>
</blockquote>
<p>算出一个和输入词元向量等长的位置编码后，该编码会直接加到输入向量上。由于这种编码用了正弦函数，所以它被后续工作称为正弦位置编码。</p>
<h3 id="相对位置编码与-RoPE"><a href="#相对位置编码与-RoPE" class="headerlink" title="相对位置编码与 RoPE"></a>相对位置编码与 RoPE</h3><p>在设计位置编码时，最好能让编码传达词元的<strong>绝对位置</strong>和<strong>相对位置</strong>信息。比如句号会出现在文本结尾而不是文本开头，这一规律来自绝对位置信息；而每几个词元会组成固定的词组，与它们在整段文本中的位置无关，这反映了相对位置信息的意义。</p>
<p>正弦位置编码同时满足了这两个性质。首先，正弦位置编码的输入只有绝对位置，它本质上就是一种绝对编码。另外，根据三角函数和角公式，假设偏移 $\Delta k$ 是常数，那么$sin(\alpha-\Delta k)$可以由含 $\alpha$ 的三角函数的线性组合表示。这说明模型能够从正弦编码中部分了解到一些相对位置的信息。</p>
<p>作为绝对位置编码，正弦编码虽然能够表达一些相对信息，但是这些信息太隐晦了。并且，该编码只在输入时加入，可能在网络运算中途这些信息就消失了。我们能不能更加显式地用某种绝对位置编码建模相对位置关系呢？</p>
<p>在 Transformer 中，不同位置的词元仅会在注意力操作时做信息交互。观察下面的注意力计算公式，更具体一点来说，信息交互发生在注意力的 QK 内积时。我们可以在每次注意力操作前都给 $Q$, $K$ 里各个向量加上位置编码，保证相对位置信息能反映在注意力计算里。</p>
<script type="math/tex; mode=display">
Attn(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d}})V</script><p>苏剑林设计了一种新的位置编码：考虑 $Q$ 里位置为 $m$ 的向量 $\mathbf{q}_m$ 和 $K$ 里位置为 $n$ 的向量 $\mathbf{k}_n$，给位置为 $j$ 的向量右乘上复数 $e^{ij\theta}$，其中 $i$ 是虚数单位， $\theta$ 是一个角度。这样，复数下的 QK 内积结果为：</p>
<script type="math/tex; mode=display">
\langle \mathbf{q}_me^{im\theta}, \mathbf{k}_ne^{in\theta} \rangle=Re[\mathbf{q}_m\mathbf{k}_n^*e^{i(m-n)\theta}]</script><p>其中，$Re[]$是取实部，$*$ 为共轭复数。可以发现，内积结果也出现了位置编码 $e^{i(m-n)\theta}$，且编码的值仅取决于相对位置 $(m-n)$。因此，这种编码能够更加显式地在注意力运算里建模相对位置。</p>
<p>由于最终结果取了实部，所以上述所有运算都可以转换成实数域的操作。假设 $\mathbf{q}_m = (q_0, q_1)$ 只是一个二维向量，那么上述右乘位置编码的操作可以写成：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
& \cos m \theta & -\sin m \theta & \\
& \sin m \theta &  \cos m \theta & \\
\end{bmatrix}
\begin{bmatrix}
\begin{aligned}
q_0 \\
q_1 \\
\end{aligned}
\end{bmatrix}</script><p>从几何意义上讲，这个操作其实是二维向量旋转。因此，这种位置编码被称为<strong>旋转位置编码</strong>(RoPE)。</p>
<p>和正弦编码类似，要把二维中间变量拓展成 $d$ 维时，只要分组讨论，改变每组的频率（这里的频率是角度 $\theta$）就行了。</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
& \cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0  \\
& \sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0\\
& 0 & 0 & \cos m\theta_1 & -\sin m\theta_1 & \cdots & 0 & 0\\
& 0 & 0 & \sin m\theta_1 & \cos m\theta_1 & \cdots & 0 & 0 \\
&\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
& 0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2-1} & -\sin m\theta_{d/2-1} \\
& 0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2-1} & \cos m\theta_{d/2-1}
\end{bmatrix}
\begin{bmatrix}
q_0 \\
q_1 \\
q_2 \\
q_3 \\
\vdots \\
q_{d-2} \\
q_{d-1}
\end{bmatrix}</script><p>角度 $\theta$ 的设计可以参考 Transformer 的正弦编码：对于第 $i$ 组，我们令 $\theta_i=1/10000^{2i/d}$</p>
<p>RoPE 和正弦编码有同有异。相同之处在于：</p>
<ul>
<li>二者都是绝对位置编码，并通过编码公式的某些设计间接传递了相对位置信息。</li>
<li>二者用了同样的正弦编码方式：随着变量通道数的增大，对应位置编码的正弦函数的频率不断指数衰减。</li>
</ul>
<p>不同之处在于：</p>
<ul>
<li>正弦编码仅在模型输入时施加一次，RoPE 在所有自注意力计算时都施加。</li>
<li>正弦编码会生成一组编码向量，加到输入上。而 RoPE 是一种操作，它的几何意义是把注意力输入向量旋转一个角度。</li>
</ul>
<h2 id="用-RoPE-实现长度外推"><a href="#用-RoPE-实现长度外推" class="headerlink" title="用 RoPE 实现长度外推"></a>用 RoPE 实现长度外推</h2><p>现在，我们正式进入本文的正题：长度外推。长度外推严格来说是一类任务，并不一定要用外推的做法。它似乎最早出自论文 ALiBi (<em>Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</em>)。正如论文标题所示，该任务的目的就是「短训练，长推理」：在短序列上训练后，不经额外训练或只需少量微调，让模型生成长文本。后来这种任务也被称为「上下文窗口拓展」(Context Window Extension)，目的依旧是用已经训好的模型来生成更大的文本，只是不强调方法是外推。为了称呼方便，我们在这篇博文里将该任务统称为「长度外推」。</p>
<p>我们想一想，假设模型训练时最大文本长度是 $L$，现在要生成长度为 $L’$ 的句子 ($L’&gt;L$)，我们需要做什么呢？其实我们只要把代码写好，除了生成长度以外啥也不改就行了。</p>
<p>这样的话，模型在运行时究竟哪里发生了变化呢？根据我们之前的分析，Transformer 是不知道位置信息的，只有位置编码传递了位置信息。因此，增加了生成句子长度后，原本只见过位置在 $L$ 之内的位置编码，现在要尝试解读位置为 $L, L+1, …, L’ - 1$ 的位置编码。因此，如果除了修改生成长度外什么也不做，其实就是让模型把学到的位置编码知识外推。但很可惜，由于没学过这些训练集之外的位置关系，这种外推法效果很差。</p>
<p>我们在接下来的几节里会讨论一些更加强大的长度外推策略。这里先补充介绍一点东西。看了对长度外推任务的基本介绍，读者或许会疑惑：长度外推似乎只要考虑位置编号就行了，不是非得和 RoPE 绑定起来？其实，长度外推真正要考虑的是位置编码的形式而不是只考虑编号。我们稍后的分析其实对所有类正弦编码都有效。但现在大家都是基于 RoPE 讨论，用基于 RoPE 的模型做实验，可能是因为 RoPE 更加直接、全面地建模了词元间的交互关系，只要调整了 RoPE 的公式，其效果立刻就能反映出来。相比之下，正弦位置编码只是在输入时提供了位置信息，修改位置编码的细节不能全面地影响模型的输出。</p>
<h2 id="位置内插"><a href="#位置内插" class="headerlink" title="位置内插"></a>位置内插</h2><p>既然超出 $L$ 的位置没有被训练过，那么在 $L$ 之内多选一些位置为分数的点不就行了？位置内插（Positional Interpolation, PI）就是这样的一种长度外推方法，它把长度为 $0$~$L’$ 的位置线性压缩到 $0$~$L$ 内。也就是说，对于位置$m$，将其的位置编号修改为：</p>
<script type="math/tex; mode=display">
m := \frac{L}{L'}m</script><p>由于位置编号会被送进正弦函数里，所以编号哪怕是分数也没关系。通过这种简单的线性内插方法，我们就能在已经学好的编号范围内多选一些位置，实现长度外推。</p>
<p>内插确实比外推的效果要好得多。后续所有长度外推方法实际上都是在研究如何更好的求插值位置编码。很快，有人就从频率分析的角度提出了线性内插的一个改进。</p>
<h2 id="改变正弦函数基础频率：NTK-aware-Scaled-RoPE"><a href="#改变正弦函数基础频率：NTK-aware-Scaled-RoPE" class="headerlink" title="改变正弦函数基础频率：NTK-aware Scaled RoPE"></a>改变正弦函数基础频率：NTK-aware Scaled RoPE</h2><h3 id="直观认识"><a href="#直观认识" class="headerlink" title="直观认识"></a>直观认识</h3><p>就在位置内插提出不久，就有研究者在社区 (<a target="_blank" rel="noopener" href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/</a>) 提出了一种效果更好，完全不需要微调的长度外推技术：NTK-aware Scaled RoPE （后文简称为”NTK-aware RoPE”）。该研究者后续将此方法进一步整理优化，发表了论文 <em>YaRN: Efficient Context Window Extension of Large Language Models</em>。我们先看一下 NTK-aware RoPE。</p>
<p>NTK-aware RoPE 的改动非常简洁，但它改动的地方却很出人意料：原来，总长度为 $d$ 的向量的第 $i$ 组位置编码的频率为：</p>
<script type="math/tex; mode=display">
\theta_i=1/10000^{2i/d}</script><p>现在，我们把 $10000$ 改掉，公式变为：</p>
<script type="math/tex; mode=display">
\theta_i^{NTK-aware}=(10000 \cdot (\frac{L'}{L})^{d/(d-2)} )^{-2i/d}</script><p>使用这种新长度外推方法，在上下文窗口大小为 2048 的 LLaMA 模型上，模型生成长文本的误差远低于之前的方法。</p>
<p><img src="https://files.mdnice.com/user/32105/57c1010e-f74d-440c-9eb7-2fdf0f851bff.png" alt></p>
<p>为什么这么简洁而奇怪的修改这么有效呢？在深入理解其原理之前，我们先直观地看一看这个方法具体修改了公式里的哪些参数。</p>
<p>先看新位置编码向量 $\sin m\theta_i^{NTK-aware}$ 的第二组（假设 $i$ 从 $0$ 开始计数），也就是含参频率最大的这一组。它现在是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sin m\theta_i^{NTK-aware} &= \sin(m \cdot (10000 \cdot (\frac{L'}{L})^{d/(d-2)} )^{-2/d}) \\
&=\sin(m \cdot 10000^{-2/d} \cdot (\frac{L'}{L})^{-2/(d-2)} )
\end{aligned}</script><p>$\frac{L’}{L}$ 表示新长度是训练长度的几倍，它是一个大于 $1$ 的数。$d$ 在 LLaMA 里是 $128$，所以我们可以认为 $(d-2)$ 远大于 $2$。所以，$(\frac{L’}{L})^{(-2)/(d-2)}$ 这一项略小于 $1$。整体上看，这一改动差不多就是给三角函数的频率乘上了一个略小于 $1$ 的常数，几乎没变。</p>
<p>再看频率最小的 $i=d/2-1$ 项。它现在是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\sin m\theta_i^{NTK-aware} &= \sin(m \cdot (10000 \cdot (\frac{L'}{L})^{d/(d-2)} )^{-(d-2)/d}) \\
&=\sin(m \cdot 10000^{-2 (d/2-1)/d} \cdot \frac{L}{L'} )
\end{aligned}</script><p>而在线性内插中，我们直接把所有 $m$ 替换成了 $\frac{L}{L’}m$。所以，频率最小的项的公式和线性内插时的公式完全相同。</p>
<blockquote>
<p>这里要澄清一下「外推」和「内插」的概念，这两个词的意义在很多博客和论文里并没有讲清楚。「内插」指的是通过像前面的线性位置内插一样，修改位置编号，使其<strong>恰好</strong>落在训练长度内。然而，一旦这个内插不够彻底，那么新位置编号就可能会超出训练长度，形成位置「外推」。我们本文讨论的所有方案，都是让不同频率的项在完全内插（恰好长度适合）和完全外推直接找一个平衡。一旦内插不彻底，就可以称为外推。所以很多文章里的「外推」，有的时候指的是不完全的内插。根据这样的术语定义，NTK-aware RoPE 的行为可以称为：最低频内插，其他频率外推。</p>
</blockquote>
<p>从上面的分析可以看出，NTK-aware RoPE 还是沿用位置线性内插的思路，但是它对 RoPE 的影响更加平滑：对于位置编码高频项，公式几乎不变；对于最低频项，公式完全等于线性内插时的公式。</p>
<p>那么，NTK-aware RoPE 为什么有效呢？它又是怎么被想出来的呢？说起来，这个一直出现的 “NTK” 又是什么意思？NTK 其实是和神经网络相关的一种理论。NTK-aware RoPE 的提出者在构思这些公式时受到了 NTK 的启发，但他后续在论文里解释此方法时完全没有从严谨的理论入手，而只是讲了一些直觉的观察。在之后的两小节中，我将先从 NTK 理论的角度试图还原提出者的心路历程，再从一个广为人知、更易理解的角度来介绍 NTK-aware RoPE。</p>
<h3 id="从-NTK-角度的解释"><a href="#从-NTK-角度的解释" class="headerlink" title="从 NTK 角度的解释"></a>从 NTK 角度的解释</h3><p>近几年和 NTK 理论比较相关的论文叫做 <em>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</em>。这篇论文用 NTK 理论分析了 NeRF 这种以位置坐标为输入的 MLP 需要位置编码的原因，并将这类位置编码归纳为「傅里叶特征」。</p>
<p><img src="https://files.mdnice.com/user/32105/522a0160-e00f-4988-9dc2-54d1fd28d96d.png" alt></p>
<p>这篇论文最大的一个发现是：在形式为 $[\sin \theta_i x, \cos \theta_i x]$ 这样的傅里叶特征中，最重要的是决定最大频率。最大频率越大，MLP 拟合高频信息的能力越强。</p>
<p>由于 RoPE 的公式来自于正弦位置编码，而正弦编码又可以看成一种特殊的傅里叶特征，所以 NTK-aware RoPE 的提出者也试图将傅里叶特征中的规律套用在 RoPE 上。他可能观察到了应用线性内插后 RoPE 公式（正弦编码公式）的频率变化。原来编码第 $i$ 项为：</p>
<script type="math/tex; mode=display">
\sin(m\theta_i)</script><p>应用线性内插后，公式为：</p>
<script type="math/tex; mode=display">
\sin(\frac{L}{L'} \cdot m\theta_i)</script><p>这里 $\frac{L}{L’}$ 是一个小于 $1$ 的数。所以，加上线性内插后，所有项的频率都变小了。自然，公式能表达的最大频率也变小了，拟合高频信息的能力下降了。</p>
<p>我们可以把线性内插类比到 NeRF 这类任务中。如果我们增加输入坐标的密度，确实可以让图片/3D 模型的输出分辨率变大。但是，根据信号处理的知识，这种分辨率变大并不能超出原有的频率，所以变大后的图片/3D 模型会看起来很模糊。「模糊」在文本任务中的体现可能就是误差指标上升。</p>
<p>出于这些原因，NTK-aware RoPE 的策略是尽可能不动高频项的频率，仅动低频项的频率。当然，按照这种设计思路，我们其实可以提出各种各样的方案。NTK-aware RoPE 选了实现起来最方便的一种：修改频率基底，让它在最低频时和线性内插对齐（读者感兴趣可以设方程自行推导频率基底的修改值，把我们刚刚有关最低频项的分析倒过来）。这样，自然就有高频项几乎不变，低频项向线性内插靠拢，也就是我们在上一小节中的观察。</p>
<p>根据我的理解，傅里叶特征本身就只是稍微用到 NTK 相关的理论（参见我有关<a href="https://zhouyifan.net/2024/12/05/20241202-fourier-feature/">傅里叶特征的博文</a>）。而 NTK-aware RoPE 的作者貌似仅是受到了傅里叶特征的某些启发，完全没有严谨地用 NTK 理论来推导 NTK-aware RoPE 的形式。所以，我认为，要学习 NTK-aware RoPE，完全不用学习 NTK 理论。</p>
<p>NTK-aware RoPE 的提出者在互联网上和 YaRN 论文中用了一些更好理解的方式解释 NTK-aware RoPE。类似地，从进制转换的角度，苏剑林也发表了两篇一针见血的解读博文：<a target="_blank" rel="noopener" href="https://kexue.fm/archives/9675">https://kexue.fm/archives/9675</a> <a target="_blank" rel="noopener" href="https://kexue.fm/archives/9706">https://kexue.fm/archives/9706</a> 。我建议从这些角度来学习 NTK-aware RoPE，然后忘掉 NTK 这个词。我们在下一节里就从这个角度重新认识一遍位置编码。</p>
<h3 id="从进制的角度解释"><a href="#从进制的角度解释" class="headerlink" title="从进制的角度解释"></a>从进制的角度解释</h3><p>其实几乎每个人都理解位置编码。</p>
<p>不信？我来问个问题：看到 $1234$ 后，你看到了几个数？</p>
<p>确实，这只是一个数。但是，我们人在看到这个数的时候，其实是看到了 $4$ 个十进制数字。通过把不同位置的数字组合，我们才理解了这个数究竟是多少。真正的数是一个概念，我们可以把两个东西这一概念，表示成汉字「二」，阿拉伯数字「2」，或者是二进制下的 $10$。我们常见的十进制只是表达数的一种方式。</p>
<p>而进制表示其实就是一种表达数的位置编码。想象一个十进制计时器，它的数字从 $0, 1, …$ 开始不断增长。每隔 1 次，个位变一次；每隔 10 次，十位变一次；每隔 100 次，百位变一次……。也就是说，个位是频率最高的，位数越高频率越低。是不是这和正弦位置编码很像？正弦位置编码和进制表示的区别在于，进制用求余体现周期性，正弦位置编码用正弦函数体现周期性。</p>
<p>长度外推，就好像一个只见过 0-999 的模型，突然要处理 1000 以上的数一样。为了只用三位数来表达更大的数，一种简单的做法是进制转换。比如我们直接把十进制变成十六进制，那么可以表达的数就从 $10^3$ 变成了 $16^3$。</p>
<p>回到正弦编码的公式里，进制这个概念体现在哪呢？进制的底数又是什么呢？</p>
<script type="math/tex; mode=display">
\theta_i=1/10000^{2i/d}</script><p>在十进制里，不同位表示十、百、千……每算一个更高的位的值，就要多除以一次 $10$。所以，在正弦编码里，我们需要关注哪个被除以的量在做指数运算。通过观察发现，正弦编码的底数是 $10000^{2/d}$。</p>
<p>知道了我们想把句子长度拓展几倍，我们就可以精确地算出新底数。通过这种方式，我们就能推导出 NTK-aware RoPE。也就是说，NTK-aware RoPE 修改频率基底其实就是对正弦函数做进制转换。这部分推荐大家去阅读前面提到的苏剑林的博文。</p>
<p>基于数字进制，我们可以把位置编码类比成表示时间的时钟，便于后续概念的理解。这是因为：</p>
<ul>
<li>正弦函数本身就可以用周期旋转来解释。</li>
<li>相比数字的进制，时间的进制的底数是不同的：1 天有 24 个小时，而一小时有 60 分钟。这提示我们：我们不一定要对每种频率做同样的处理。</li>
</ul>
<p>利用这个时钟的比喻，NTK-aware RoPE 的提出者在社区解释了不应该像线性内插一样修改最高频率的原因：就像我们用秒针来区分最精确的时间一样，神经网络用最高频的正弦编码区分相对位置关系，且只能看清 1 秒以上的偏差。使用线性内插后，最小的时间偏差是 0.5 秒，神经网络就不能很好地处理最高频的那块信息了。而 NTK-aware RoPE 不会修改一秒的定义，只会在分钟、小时等更低频的分量上多插值一点，神经网络依然能区分最精细的时间。</p>
<h2 id="改进-NTK-aware-RoPE：分部-NTK"><a href="#改进-NTK-aware-RoPE：分部-NTK" class="headerlink" title="改进 NTK-aware RoPE：分部 NTK"></a>改进 NTK-aware RoPE：分部 NTK</h2><p>我们在上一节中学到，NTK-aware RoPE 的设计思想是高频不动（或理解成高频外推），只对低频内插。只改频率基底虽然做法简洁，但不见得是最优的做法。高频不动这部分应该没什么问题，我们把目光放在 RoPE 的低频分量上。</p>
<p>还是从十进制的角度看待位置编码。假设训练集的位置只有 $0$~$2800$，那么在千位上，模型只见过 $0, 1, 2$ 三个数字。由于在千位上模型没有完整见过 $0$~$9$ 的循环，模型不能推测出其他几个数字的意义。因此，在千位上做长度外推时，一定要用内插把位置编号正确缩放到已学习的范围内。</p>
<p>这套分析怎么迁移到正弦编码上呢？对于十进制数字，我们能很快判断出某一位是否走完了一个周期。比如要把千位上的 $0$~$9$ 都走一遍，就至少得要一万个数。怎么找出正弦编码每个频率走一个周期需要的距离呢？</p>
<p>在正弦函数中，我们可以用 $2 \pi$ 除以频率，得到波长。正弦位置编码某一项的波长表示当训练上下文长度至少为多少时，这一项会「转」完一个周期。比如时钟上，秒针 60 秒转一圈，分针 3600 秒转一圈。 </p>
<blockquote>
<p>$2 \pi$ 除以频率明明算出的是周期，周期乘上速度才是波长。但 YaRN 的作者就是在论文里把这个量定义成了周长。可能他们认为波长的单位是长度，上下文窗口大小也是长度，两个单位是匹配的。我认为这个名字取得很糟糕，就应该叫做周期的，只不过周期的单位也是长度而已。</p>
</blockquote>
<p>根据这个定义出来的波长，我们可以对正弦位置编码的不同位置分类讨论：</p>
<ul>
<li>如果波长过大，大于了训练时的文本长度，那么就用普通的线性内插，保证不在这些维度上外推。设它们的内插程度为 $1$。相比之下， NTK-aware RoPE 只对最低频项做了完整内插，而没有考虑其他波长过大的项也应该完整内插。</li>
<li>如果波长过小，说明频率很高，不应该做任何修改。设它们的内插程度为 $0$。</li>
<li>对于其他位置，根据它们的波长，线性选择内插程度。</li>
</ul>
<p>这里波长过大、过小的阈值用超参数来决定，每个模型都需要手动调整。</p>
<p>总之，NTK-aware RoPE 只是模糊地定义了高频分量应该尽可能不变，低频分量应该尽可能像线性内插。而分部 NTK 则允许我们显式对各个频率分量做分类讨论。最终的位置编码方案 YaRN 在分部 NTK 的基础上还做了少许修改，对此感兴趣的读者可以去阅读论文。</p>
<h2 id="图像生成中的-RoPE-与长度外推"><a href="#图像生成中的-RoPE-与长度外推" class="headerlink" title="图像生成中的 RoPE 与长度外推"></a>图像生成中的 RoPE 与长度外推</h2><p>了解了近年来 NLP 社区的位置编码技术，我们来以 Lumina-T2X 为例，再看一下这些技术是怎么用到视觉生成任务上的。</p>
<h3 id="多维-RoPE"><a href="#多维-RoPE" class="headerlink" title="多维 RoPE"></a>多维 RoPE</h3><p>RoPE 本来是设计给 1D 的文本数据的。而在视觉任务中，图像是二维的，视频是三维的，我们需要设计更高维的位置编码。</p>
<p>回顾 RoPE 的形式：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
& \cos m\theta_0 & -\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0  \\
& \sin m\theta_0 & \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0\\
& 0 & 0 & \cos m\theta_1 & -\sin m\theta_1 & \cdots & 0 & 0\\
& 0 & 0 & \sin m\theta_1 & \cos m\theta_1 & \cdots & 0 & 0 \\
&\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
& 0 & 0 & 0 & 0 & \cdots & \cos m\theta_{d/2-1} & -\sin m\theta_{d/2-1} \\
& 0 & 0 & 0 & 0 & \cdots & \sin m\theta_{d/2-1} & \cos m\theta_{d/2-1}
\end{bmatrix}
\begin{bmatrix}
q_0 \\
q_1 \\
q_2 \\
q_3 \\
\vdots \\
q_{d-2} \\
q_{d-1}
\end{bmatrix}</script><p>要把它拓展成高维很简单。比如要拓展成 3D RoPE，只要把上面的公式复制两份，放到原公式的下面就行。也就是说，我们把向量拆成三份分别处理，每一部分和 1D RoPE 一样。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\begin{bmatrix}
& \cos m\theta_0 & -\sin m\theta_0 \\
& \sin m\theta_0 & \cos m\theta_0
\end{bmatrix}
\begin{bmatrix}
q_0 \\
q_1 \\
\end{bmatrix} \\
\begin{bmatrix}
& \cos m\theta_0 & -\sin m\theta_0 \\
& \sin m\theta_0 & \cos m\theta_0
\end{bmatrix}
\begin{bmatrix}
q_2 \\
q_3 \\
\end{bmatrix} \\
\begin{bmatrix}
& \cos m\theta_0 & -\sin m\theta_0 \\
& \sin m\theta_0 & \cos m\theta_0
\end{bmatrix}
\begin{bmatrix}
q_4 \\
q_5 \\
\end{bmatrix}
\end{aligned}</script><p>在这种设计下，模型所有中间向量的不同维度有了不同的意义，它们可能负责了视频宽度、高度或长度上的信息处理。我们也可以根据实际需要，让负责不同视频维度的向量长度不同。</p>
<h3 id="视觉扩散模型中-RoPE-的长度外推设计"><a href="#视觉扩散模型中-RoPE-的长度外推设计" class="headerlink" title="视觉扩散模型中 RoPE 的长度外推设计"></a>视觉扩散模型中 RoPE 的长度外推设计</h3><p>为了生成比训练分辨率更大的图像，Lumina-T2X 也参考了 NTK-aware RoPE，提出了一些和图像相关的 RoPE 改进策略。</p>
<p>首先，和分布 NTK 策略一样，Lumina-T2X 提出了频率感知 RoPE。在这种策略下，波长大于等于训练长度的位置编码项完全使用线性内推，剩下的项使用 NTK-aware RoPE。</p>
<p>另外，Lumina-T2X 还提出了时刻感知 RoPE。这个「时刻」指的是扩散模型里的加噪/去噪时刻。根据实验结果，Lumina-T2X 的作者发现线性内插会保持图像整体结构，但是图像局部质量下降；NTK-aware 策略提升了局部质量，却会出现内容重复现象，也就是全局关系不合理。能不能在某一方面结合二者呢？根据之前使用扩散模型的经验，扩散模型在去噪初期只生成低频信息，也就是全局信息，后期才会生成高频细节。受此启发，Lumina-T2X 提出了时刻感知 RoPE，该策略会在去噪早期仅使用线性内插，后续慢慢过渡到频率感知 RoPE。</p>
<p>以下是论文展示的在各种长度外推策略下生成 2K 图片的效果图。最左侧的 1K 图片供参考。</p>
<p><img src="https://files.mdnice.com/user/32105/adf72101-248f-4967-b46a-efbada316b30.png" alt></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>长度外推是生成任务中的一项重要技术，它让我们在不大规模重新训练模型的前提下提升输出内容的长度/大小。而 Transformer 本身是一种无法获取输入元素位置信息的生成模型，需要靠额外的位置编码来感知位置。那么正好，只要我们能够适当地修改位置编码的推理行为，就能想办法让模型生成更长的内容。目前长度外推的方案都和修改 RoPE——一种给 Transformer 显式提供相对位置信息的位置编码——有关。我们主要学习了 NTK-aware RoPE 的设计原理，并通过深入的分析学习了其改进版分部 NTK RoPE。基于这些知识，我们简单认识了 RoPE 长度外推在视觉生成中的应用，其中比较有趣的一项设计是做长度外推时考虑扩散模型的去噪时刻。</p>
<p>说白了，本文所有长度外推设计都是在从两个维度上排列组合：RoPE 可以看成是由多个频率项组成的正弦编码；外推方案可以从位置编号线性内插过渡到位置编号不变（即位置外推）。一般的设计策略是：对于没有学满一个完整周期的频率项，采用完全线性内插；对于其余频率项，按一定比例执行线性内插。加上了扩散模型的去噪时刻这一设计维度后，我们可以按同样的设计思路，早期更关注低频，晚期更关注高频。</p>
<p>我觉得长度外推技术的能力是有上限的。我们完全可以从信号处理或者信息论的角度来思考这一问题，因为它的本质和从频域对图像做超分辨率很像。在较短的序列中，模型只能学到这种长度的序列所能表示的最大频率的信息。强行用它来生成更长的序列，只会出现两种情况：要么序列局部不够清晰，要么每个局部很清晰但是没有很好的全局依赖关系。根据信息论，模型就是不能从短序列中学到长序列蕴含的一些规律。从 Lumina-T2X 展示的结果里，我感觉 NTK-aware RoPE 的做法某种程度上就像是把全图做超分辨率变成拆成几个小图，每个小图在原来的训练长度上分别做超分辨率。这样最后图像每一块都很清晰，但合起来看就有问题。可能对于一些文本任务来说，只要局部质量高就行了，长距离依赖没那么重要。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/12/05/20241202-fourier-feature/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/12/05/20241202-fourier-feature/" class="post-title-link" itemprop="url">位置编码背后的理论解释——傅里叶特征 (Fourier Feature）与核回归</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-12-05 11:34:08" itemprop="dateCreated datePublished" datetime="2024-12-05T11:34:08+08:00">2024-12-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近我在看位置编码最新技术时，看到了一个叫做 “NTK-aware” 的词。我想：「”NTK”是什么？Next ToKen （下一个词元）吗？为什么要用这么时髦的缩写？」看着看着，我才发现不对劲。原来，NTK 是神经网络理论里的一个概念，它从 kernel regression 的角度解释了神经网络的学习方法。基于 NTK 理论，有人解释了位置编码的理论原理并将其归纳为一种特殊的 Fourier Feature （傅里叶特征）。这么多专有名词一下就把我绕晕了，我花了几天才把它们之间的关系搞懂。</p>
<p>在这篇文章里，我主要基于论文 <em>Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains</em> （后文简称为「傅里叶特征论文」），介绍傅里叶特征这一概念。为了讲清这些理论的发展脉络，我会稍微讲一下 NTK 等理论概念。介绍完傅里叶特征后，我还会讲解它在其他方法中的应用。希望读完本文后，读者能够以这篇论文为基点，建立一个有关位置编码原理的知识网络，以从更深的层次来思考新的科研方向。</p>
<h2 id="用-MLP-表示连续数据"><a href="#用-MLP-表示连续数据" class="headerlink" title="用 MLP 表示连续数据"></a>用 MLP 表示连续数据</h2><p>我们先从一个具体的任务入手，直观体会傅里叶特征能够做些什么事。</p>
<p>我们知道，神经网络，哪怕是最简单的多层感知机（MLP），都有着很强的泛化能力：训练完毕后，对于训练集里完全没见过的输入，网络也能给出很正确的输出。特别地，如果新输入恰好和训练集的某个输入很近，那么它的输出也会和对应的训练集输出很近；随着新输出与训练集输入的距离不断增加，新输出也会逐渐变得不同。这反映了神经网络的连续性：如果输入的变化是连续的，那么输出的变化也是连续的。</p>
<p>基于神经网络的这一特性，有人想到：我们能不能用神经网络来表示<strong>连续数据</strong>呢？比如我想表达一张处处连续的图像，于是我令神经网络的输入是 <code>(x, y)</code> 表示的二维坐标，输出是 RGB 颜色。之后，我在单张图像上过拟合这个 MLP。这样，学会表示这张图像后，哪怕输入坐标是分数而不是整数，神经网络也能给出一个颜色输出。</p>
<p>这种连续数据有什么好处呢？我们知道，计算机都是以离散的形式来存储数据的。比如，我们会把图像拆成一个个像素，每个像素存在一块内存里。对于图像这种二维数据，计算机的存储空间还勉强够用。而如果想用密集的离散数据表达更复杂的数据，比如 3D 物体，计算机的容量就捉襟见肘了。但如果用一个 MLP 来表达 3D 物体的话，我们只需要存储 MLP 的参数，就能获取 3D 物体在任何位置的信息了。</p>
<p>这就是经典工作神经辐射场 (Neural Radiance Field, NeRF) 的设计初衷。NeRF 用一个 MLP 拟合 3D 物体的属性，其输入输出如下图所示。我们可以用 MLP 学习每个 3D 坐标的每个 2D 视角处的属性（这篇文章用的属性是颜色和密度）。根据这些信息，利用某些渲染算法，我们就能重建完整的 3D 物体。</p>
<p><img src="/2024/12/05/20241202-fourier-feature/1.png" alt></p>
<p>上述过程看起来好像很简单直接。但在 NeRF 中，有一个重要的实现细节：必须给输入加上位置编码，MLP 才能很好地过拟合连续数据。这是为什么呢？让我们先用实验复现一下这个现象。</p>
<h2 id="MLP-拟合连续图像实验"><a href="#MLP-拟合连续图像实验" class="headerlink" title="MLP 拟合连续图像实验"></a>MLP 拟合连续图像实验</h2><p>为了快速复现和位置编码相关的问题，我们简单地用一个 MLP 来表示图像：MLP 的输入是 2D 坐标，输出是此处的三通道 RGB 颜色。我为这篇博文创建一个 GitHub 文件夹 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/FourierFeature">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/FourierFeature</a> ，该实验的 Notebook 代码在文件夹的 <code>image_mlp.ipynb</code> 中，欢迎大家 clone 项目并动手尝试。</p>
<p><img src="/2024/12/05/20241202-fourier-feature/0.png" alt></p>
<p>一开始，我们先导入库并可视化要拟合的图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image, ImageReadMode</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms.functional <span class="keyword">import</span> to_pil_image</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viz_image</span>(<span class="params">pt_img: torch.Tensor</span>):</span></span><br><span class="line">    pil_img = to_pil_image(pt_img)</span><br><span class="line">    display(pil_img)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">input_image = read_image(<span class="string">&#x27;misuzu.png&#x27;</span>, ImageReadMode.RGB)</span><br><span class="line">input_image = input_image.to(torch.float32) / <span class="number">255</span></span><br><span class="line">input_image = input_image.unsqueeze(<span class="number">0</span>)</span><br><span class="line">input_image = F.interpolate(input_image, (<span class="number">256</span>, <span class="number">256</span>), mode=<span class="string">&#x27;bilinear&#x27;</span>)</span><br><span class="line">viz_image(input_image[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2024/12/05/20241202-fourier-feature/2.png" alt></p>
<p>我们再定义一个 MLP 类。稍后我们会并行地传入二维坐标。具体来说，我们会将输入定义为一个 <code>[1, 2, H, W]</code> 形状的数据，其中通道数 2 表示 <code>(i, j)</code> 格式的坐标。由于输入是以图像的形式并行输入的，我们可以用 $1 \times 1$ 的 2D 卷积来表示二维数据上的并行 MLP。所以在下面这个 MLP 里，我们只用到 $1 \times 1$ 卷积、激活函数、归一化三种层。按照傅里叶特征论文的官方示例，网络最后要用一个 Sigmoid 激活函数调整输出的范围。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_c, out_c=<span class="number">3</span>, hiden_states=<span class="number">256</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.mlp = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_c, hiden_states, <span class="number">1</span>), nn.ReLU(), nn.BatchNorm2d(hiden_states),</span><br><span class="line">            nn.Conv2d(hiden_states, hiden_states, <span class="number">1</span>), nn.ReLU(), nn.BatchNorm2d(hiden_states),</span><br><span class="line">            nn.Conv2d(hiden_states, hiden_states, <span class="number">1</span>), nn.ReLU(), nn.BatchNorm2d(hiden_states),</span><br><span class="line">            nn.Conv2d(hiden_states, out_c, <span class="number">1</span>), nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.mlp(x)</span><br></pre></td></tr></table></figure>
<p>之后我们来定义训练数据。在一般的任务中，输入输出都是从训练集获取的。而在这个任务中，输入是二维坐标，输出是图像的颜色值。输出图像 <code>input_image</code> 我们刚刚已经读取完毕了，现在只需要构建输入坐标即可。我们可以用下面的代码构建一个 <code>[1, 2, H, W]</code> 形状的二维网格，<code>grid[0, :, i, j]</code> 处的数据是其坐标 <code>(i, j)</code> 本身。当然，由于神经网络的输入一般要做归一化，所以我们会把原本 <code>0~H</code> 和 <code>0~W</code> 里的高宽坐标缩放都到 <code>0~1</code>。最终 <code>grid[0, :, i, j]==(i/H, j/W)</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">H, W = input_image.shape[<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line">h_coord = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, H)</span><br><span class="line">w_coord = torch.linspace(<span class="number">0</span>, <span class="number">1</span>, W)</span><br><span class="line">grid = torch.stack(torch.meshgrid([h_coord, w_coord]), -<span class="number">1</span>).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>准备好一切后，我们就可以开始训练了。我们初始化模型 <code>model</code> 和优化器 <code>optimizer</code>，和往常一样训练这个 MLP。如前所述，这个任务的输入输出非常直接，输入就是坐标网格 <code>grid</code>，目标输出就是图片 <code>input_image</code>。每训练一段时间，我们就把当前 MLP 拟合出的图片和误差打印出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = MLP(<span class="number">2</span>).to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line">n_loops = <span class="number">400</span></span><br><span class="line">input_image = input_image.to(device)</span><br><span class="line">grid = grid.to(device)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_loops)):</span><br><span class="line">    output = model(grid)</span><br><span class="line">    loss = F.l1_loss(output, input_image)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> epoch == n_loops - <span class="number">1</span>:</span><br><span class="line">        viz_image(output[<span class="number">0</span>])</span><br><span class="line">        <span class="built_in">print</span>(loss.item())</span><br></pre></td></tr></table></figure>
<p>运行代码，大致能得到如下输出。可以看到，从一开始，图像就非常模糊。</p>
<p><img src="/2024/12/05/20241202-fourier-feature/3.png" alt></p>
<p>不过，如果我们在把坐标输入进网络前先将其转换成位置编码——一种特殊的傅里叶特征，那么 MLP 就能清晰地拟合出原图片。这里我们暂时不去关注这段代码的实现细节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FourierFeature</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_c, out_c, scale</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        fourier_basis = torch.randn(in_c, out_c // <span class="number">2</span>) * scale</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;_fourier_basis&#x27;</span>, fourier_basis)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        N, C, H, W = x.shape</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;n c h w -&gt; (n h w) c&#x27;</span>)</span><br><span class="line">        x = x @ self._fourier_basis</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;(n h w) c -&gt; n c h w&#x27;</span>, h = H, w = W)</span><br><span class="line">            </span><br><span class="line">        x = <span class="number">2</span> * torch.pi * x</span><br><span class="line">        x = torch.cat([torch.sin(x), torch.cos(x)], dim=<span class="number">1</span>) </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">feature_length = <span class="number">256</span></span><br><span class="line">model = MLP(feature_length).to(device)</span><br><span class="line">fourier_feature = FourierFeature(<span class="number">2</span>, feature_length, <span class="number">10</span>).to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line">n_loops = <span class="number">400</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_loops)):</span><br><span class="line">    x = fourier_feature(grid)</span><br><span class="line">    output = model(x)</span><br><span class="line">    loss = F.l1_loss(output, input_image)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> epoch == n_loops - <span class="number">1</span>:</span><br><span class="line">        viz_image(output[<span class="number">0</span>])</span><br><span class="line">        <span class="built_in">print</span>(loss.item())</span><br><span class="line">prev_output = output</span><br></pre></td></tr></table></figure>
<p><img src="/2024/12/05/20241202-fourier-feature/4.png" alt></p>
<p>简单地对比一下，此前方法的主要问题是 MLP 无法拟合高频的信息（如图块边缘），只能生成模糊的图像。而使用位置编码后，MLP 从一开始就能较好地表示高频信息。可见，问题的关键在于如何让 MLP 更好地拟合数据的高频信息。</p>
<p><img src="/2024/12/05/20241202-fourier-feature/5.png" alt></p>
<p>接下来，我们来从一个比较偏理论的角度看一看论文是怎么分析位置编码在拟合高频信息中的作用的。</p>
<h2 id="核回归"><a href="#核回归" class="headerlink" title="核回归"></a>核回归</h2><p>傅里叶特征论文使用了神经正切核（Nerual Tangent Kernel, NTK）来分析 MLP 的学习规律，而 NTK 又是一种特殊的核回归 (Kernel Regression) 方法。在这一节里，我会通过代码来较为仔细地介绍核回归。下一节我会简单介绍 NTK。</p>
<p>和神经网络类似，核回归也是一种数学模型。给定训练集里的输入和输出，我们建立这样一个模型，用来拟合训练集表示的未知函数。相比之下，核回归的形式更加简单，我们有更多的数学工具来分析其性质。</p>
<p>核回归的设计思想来源于我们对于待拟合函数性质的观察：正如我们在前文的分析一样，要用模型拟合一个函数时，该模型在训练数据附近最好是连续变化的。离训练集输入越近，输出就要和其对应输出越近。基于这种想法，核回归直接利用和所有数据的相似度来建立模型：假设训练数据为 $(x_i, y_i), i \in [1, n]$，我们定义了一个计算两个输入相似度指标 $K(x_1, x_2)$，那么任意输入 $x$ 的输出为：</p>
<script type="math/tex; mode=display">
w_i = \frac{K(x_i, x)}{\sum_{i=1}^{n}K(x_i, x)} \\
f(x) = \sum_{i=1}^{n}w_iy_i</script><p>也就是说，对于一个新输入 $x$，我们算它和所有输入 $x_i$ 的相似度 $w_i$，并把相似度归一化。最后的输出 $f(x)$ 是现有 $y_i$ 的相似度加权和。</p>
<p>这样看来，只要有了相似度指标，最终模型的形式也就决定下来了。我们把这个相似度指标称为「核」。至于为什么要把它叫做核，是因为这个相似度指标必须满足一些性质，比如非负、对称。但我们这里不用管那么多，只需要知道核是一种衡量距离的指标，决定了核就决定了核回归的形式。</p>
<p>我们来通过一个简单的一维函数拟合实验来进一步熟悉核回归。该实验代码在项目文件夹下的 <code>kernel_regression.ipynb</code> 中。</p>
<p>先导入库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<p>再创建一个简单的非线性函数，做为我们的拟合目标。这个函数就是一个简单的周期为 $2$ 的正弦函数乘上线性函数 $(1-x)$。我们可以简单可视化一下函数在 $[-1, 1]$ 之间的图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.sin(np.pi * x) * (<span class="number">1</span> - x)</span><br><span class="line"></span><br><span class="line">xs = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">ys = func(xs)</span><br><span class="line">plt.plot(xs, ys)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2024/12/05/20241202-fourier-feature/6.png" alt></p>
<p>基于这个函数，我们等间距地选一些点做为训练数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sample_x = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">sample_y = func(sample_x)</span><br><span class="line">plt.scatter(sample_x, sample_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2024/12/05/20241202-fourier-feature/7.png" alt></p>
<p>有了数据后，我们来用核回归根据数据拟合这个函数。在决定核回归时，最重要的是决定核的形式。这里我们用正态分布的概率密度函数来表示核，该核唯一的超参数是标准差，需要我们根据拟合结果手动调整。标准差为 <code>1</code> 的标准正态分布核的图像如下所示。由于最后要做归一化，正态分布密度函数的系数被省略掉了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_func</span>(<span class="params">x_ref, x_input, sigma=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(-(x_input-x_ref)**<span class="number">2</span> / (<span class="number">2</span> * sigma**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">xs = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">ys = kernel_func(<span class="number">0</span>, xs)</span><br><span class="line">plt.plot(xs, ys)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2024/12/05/20241202-fourier-feature/8.png" alt></p>
<p>可以从图像中看出，离某输入越近（假设该输入是 <code>0</code>），那么相似度就越高。这符合我们对于相似度函数的要求。</p>
<p>有了核函数后，我们就直接得到了模型。根据核回归模型计算结果的函数为 <code>kernel_regression</code>。函数参数 <code>xs, ys</code> 表示训练数据，<code>x_input</code> 表示测试时用的输入坐标，<code>sigma</code> 是核回归的超参数。</p>
<p>假设有 <code>n</code> 个训练样本，有 <code>m</code> 个测试输入，那么我们要计算每个测试输入对每个训练输入的 <code>n * m</code> 个相似度，这些相似度会存到矩阵 <code>weight</code> 里。为此，我们需要对 <code>xs</code> 和 <code>x_input</code> 做一些形状变换，再用上面定义的核函数 <code>kernel_func</code> 求出每对相似度。有了相似度后，我们根据公式计算点乘结果 <code>weight_dot</code> 及归一化系数 <code>weight_sum</code>，并最终计算出核回归的结果 <code>res</code>。</p>
<p>基于这个函数，我们可以将测试输入定义成 <code>[-1, 1]</code> 上一些更密集的坐标，并用上面定义好的 10 个样本做为训练集，得到核回归的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_regression</span>(<span class="params">xs, ys, x_input, sigma=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="comment"># xs: [n, ]</span></span><br><span class="line">    <span class="comment"># ys: [n, ]</span></span><br><span class="line">    <span class="comment"># x_input: [m, ]</span></span><br><span class="line">    N = xs.shape[<span class="number">0</span>]</span><br><span class="line">    xs = np.expand_dims(xs, <span class="number">1</span>)</span><br><span class="line">    ys = np.expand_dims(ys, <span class="number">1</span>)</span><br><span class="line">    x_input = np.expand_dims(x_input, <span class="number">0</span>)</span><br><span class="line">    x_input = np.repeat(x_input, N, <span class="number">0</span>)</span><br><span class="line">    weight = kernel_func(xs, x_input, sigma) <span class="comment"># [n, m]</span></span><br><span class="line">    weight_sum = np.<span class="built_in">sum</span>(weight, <span class="number">0</span>)</span><br><span class="line">    weight_dot = weight.T @ ys</span><br><span class="line">    weight_dot = np.squeeze(weight_dot, <span class="number">1</span>)</span><br><span class="line">    res = weight_dot / weight_sum</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">sigma = <span class="number">1</span></span><br><span class="line">xs = np.linspace(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">ys = kernel_regression(sample_x, sample_y, xs, sigma)</span><br><span class="line">plt.title(<span class="string">f&#x27;sigma = <span class="subst">&#123;sigma&#125;</span>&#x27;</span>)</span><br><span class="line">plt.plot(xs, ys)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>我们可以通过修改 <code>sigma</code> 来得到不同的拟合效果。以下是我的一些结果：</p>
<p><img src="/2024/12/05/20241202-fourier-feature/9.png" alt></p>
<p>可以看出，标准差越小，模型倾向于过拟合；随着标准差变大，曲线会逐渐平缓。我们需要不断调整超参数，在过拟合和欠拟合之间找到一个平衡。这种现象很容易解释：正态分布核函数的标准差越小，意味着每个训练数据的影响范围较小，那么测试样本更容易受到少数样本的影响；标准差增大之后，各个训练样本的影响开始共同起作用，我们拟合出的函数也越来越靠近正确的函数；但如果标准差过大，每个训练样本的影响都差不多，那么模型就什么都拟合不了了。</p>
<p>从实验结果中，我们能大致感受到核回归和低通滤波很像，都是将已知数据的平均效果施加在未知数据上。因此，在分析核回归的时候，往往会从频域分析核函数。如果核函数所代表低通滤波器的带宽 （bandwidth）越大，那么剩下的高频信息就更多，核回归也更容易拟合高频信息较多的数据。</p>
<h2 id="神经正切核"><a href="#神经正切核" class="headerlink" title="神经正切核"></a>神经正切核</h2><p>那么，核回归是怎么和神经网络关联起来的呢？有研究表明，在一些特殊条件下，MLP 的最终优化结果可以用一个简单的核回归来表示。这不仅意味着我们可以神奇地提前预测梯度下降的结果，还可以根据核回归的性质来分析神经网络的部分原理。这种能表示神经网络学习结果的核被称为神经正切核（NTK）。</p>
<blockquote>
<p>这些特殊条件包括 MLP 无限宽、SGD 学习率的学习率趋近 0 等。由于这些条件和实际神经网络的配置相差较远，我们难以直接用核回归预测复杂神经网络的结果。不过，我们依然可以基于这些理论来分析和神经网络相关的问题。傅里叶特征的分析就是建立在 NTK 上的。</p>
</blockquote>
<p>NTK 的形式为</p>
<script type="math/tex; mode=display">
k_{NTK}(\mathbf{x_i}, \mathbf{x_j}) = \mathbb{E}_{\theta\sim\mathcal{N}} \langle\frac{\partial f(\mathbf{x_i}; \theta)}{\partial \theta}, \frac{\partial f(\mathbf{x_j}; \theta)}{\partial \theta} \rangle，</script><p>其中，$f$ 是参数为 $\theta$ 的神经网络，$\langle\cdot,\cdot \rangle$为内积运算。简单来看，这个式子是说神经网络的核回归中，任意两个向量间的相似度等于网络对参数的偏导的内积的期望。基于 NTK，我们可以分析出很多神经网络的性质，比如出乎意料地，神经网络的结果和随机初始化的参数无关，仅和网络结构和训练数据有关。</p>
<p>在学习傅里叶特征时，我们不需要仔细研究这些这些理论，而只需要知道一个结论：一般上述 NTK 可以写成标量函数 $h_{NTK}(\mathbf{x}_i^T\mathbf{x}_j)$，也就是可以先算内积再求偏导。这意味用核回归表示神经网络时，真正要关心的是输入间的内积。别看 NTK 看起来那么复杂，傅里叶特征论文其实主要就用到了这一个性质。</p>
<p>为了从理论上讲清为什么 MLP 难以拟合高频，作者还提及了很多有关 NTK 的分析，包括一种叫做谱偏差（spectral bias）的现象：神经网络更容易学习到数据中的低频特征。可能作者默认读者已经熟悉了相关的理论背景，这部分论述经常会出现逻辑跳跃，很难读懂。当然，不懂这些理论不影响理解傅里叶特征。我建议不要去仔细阅读这篇文章有关谱偏差的那一部分。</p>
<blockquote>
<p>正如我们在前文的核回归实验里观察到的，核回归模型能否学到高频取决于核函数的频域特征。因此，这部分分析和 NTK 的频域有关。对这部分内容感兴趣的话可以去阅读之前有关谱偏差的论文。</p>
</blockquote>
<h2 id="傅里叶特征的平移不变性"><a href="#傅里叶特征的平移不变性" class="headerlink" title="傅里叶特征的平移不变性"></a>傅里叶特征的平移不变性</h2><p>在上两节中，我们花了不少功夫去认识谱回归和 NTK。总结下来，其实我们只需要搞懂两件事：</p>
<ul>
<li>神经网络最终的收敛效果可以由简单的核回归决定。而核回归重点是定义两个输入之间的相似度指标（核函数）。</li>
<li>表示神经网络的核回归相似度指标是 NTK，它其实又只取决于两个输入的内积 $\mathbf{x}_i^T\mathbf{x}_j$。</li>
</ul>
<p>根据这一性质，我们可以部分解释为什么在文章开头那个 MLP 拟合连续图像的实验中，位置编码可以提升 MLP 拟合高频信息的能力了。这和位置输入的特性有关。</p>
<p>当 MLP 的输入表示位置时，我们希望模型对输入位置具有<strong>平移不变性</strong>。比如我们现在有一条三个样本组成的句子 $(1, A), (2, B), (3, C)$。当我们同时改变句子的位置信息时，比如将句子的位置改成 $(11, A), (12, B), (13, C)$时，网络能学出完全一样的东西。但显然不对输入位置做任何处理的话， $(1, 2, 3)$ 和 $(11, 12, 13)$ 对神经网络来说是完全不同的意思。</p>
<p>而使用位置编码的话，情况就完全不同了。假如输入数据是二维坐标 $\mathbf{v}\in [0, 1)^d$，我们可以用下面的式子建立一个维度为 $2m$ 的位置编码：</p>
<p><img src="/2024/12/05/20241202-fourier-feature/10.png" alt></p>
<p>其中 $a_i$ 是系数， $b \in \mathbb{R}^{m \times 2}$ 是一个投影矩阵，用于把原来 2D 的位置变成一个更长的位置编码。当然，由于位置编码中既要有 $\sin$ 也要有 $\cos$，所以最终的位置编码长度为 $2m$。</p>
<p>根据我们之前的分析，NTK 只取决于输入间的内积。算上位置编码后，一对输入位置 $\mathbf{v}_1, \mathbf{v}_2$ 的内积为：</p>
<script type="math/tex; mode=display">
\gamma(\mathbf{v_1})^T\gamma(\mathbf{v_2}) = [a_1^2 \cos(2\pi \mathbf{b_1}^T\mathbf{v_1})\cos(2\pi \mathbf{b_1}^T\mathbf{v_2})+a_1^2 \sin(2\pi \mathbf{b_1}^T\mathbf{v_1})\sin(2\pi \mathbf{b_1}^T\mathbf{v_2}) + ...]</script><p>而根据三角函数和角公式可知：</p>
<script type="math/tex; mode=display">
\cos(\alpha-\beta)=\cos\alpha \cos\beta + \sin\alpha \sin\beta</script><p>这样，上面那个内积恰好可以写成：</p>
<script type="math/tex; mode=display">
\gamma(\mathbf{v_1})^T\gamma(\mathbf{v_2}) = [a_1^2\cos(2\pi \mathbf{b_1}^T (\mathbf{v_1}-\mathbf{v_2})) + ...]</script><p>上式完全由位置间的相对距离 $\mathbf{v_1}-\mathbf{v_2}$ 决定。上式决定了 NTK，NTK 又决定了神经网络的学习结果。所以，神经网络的收敛结果其实完全取决于输入间的相对距离，而不取决于它们的绝对距离。也因此，位置编码使得 MLP 对于输入位置有了平移不变性。</p>
<p>加入位置编码后，虽然 MLP 满足了平移不变性，但这并不代表 MLP 学习高频信息的能力就变强了。平移不变性能给我们带来什么好处呢？作者指出，当满足了平移不变性后，我们就能手动调整 NTK 的带宽了。回想一下我们上面做的核回归实验，如果我们能够调整核的带宽，就能决定函数是更加高频（尖锐）还是更加低频（平滑）。这里也是同理，如果我们能够调大 NTK 的带宽，让它保留更多高频信息，那么 MLP 也就能学到更多的高频信息。</p>
<blockquote>
<p>作者在此处用信号处理的知识来分析平移不变性的好处，比如讲了新的 NTK 就像一个重建卷积核 （reconstruction filter），整个 MLP 就像是在做卷积。还是由于作者省略了很多推导细节，这部分逻辑很难读懂。我建议大家直接记住推理的结论：平移不变性使得我们能够调整 NTK 的带宽，从而调整 MLP 学习高频的能力。</p>
</blockquote>
<p>那我们该怎么调整 NTK 的带宽呢？现在的新 NTK 由下面的式子决定：</p>
<script type="math/tex; mode=display">
\gamma(\mathbf{v_1})^T\gamma(\mathbf{v_2}) = \sum_{j=1}^m a_j^2\cos(2\pi \mathbf{b_j}^T (\mathbf{v_1}-\mathbf{v_2}))</script><p>为了方便分析，我们假设$\mathbf{v}$和$\mathbf{b_j}$都是一维实数。那么，如果我们令$b_j=j$的话：</p>
<script type="math/tex; mode=display">
\gamma(\mathbf{v_1})^T\gamma(\mathbf{v_2}) = \sum_{j=1}^m a_j^2\cos(2\pi j (\mathbf{v_1}-\mathbf{v_2}))</script><p>这个式子能令你想到什么？没错，就是傅里叶变换。$j$ 较大的项就表示 NTK 的高频分量。我们可以通过修改前面的系数 $a_j$ 来手动调整 NTK 的频域特征。我们能看到，位置编码其实就是在模拟傅里叶变换，所以作者把位置编码总结为傅里叶特征。</p>
<p>作者通过实验证明我们可以手动修改 NTK 的频谱。实验中，作者令 $b_j=j, a_j=1/j^p$。$p=\infty$ 表示位置编码只有第一项：$\gamma(v)=[\cos 2\pi v, \sin 2\pi v]^T$。不同 $p$ 时 NTK 的空域和频域示意图如下所示。可以看出，令 $p=0$ 时，即傅里叶特征所有项的系数都为 $1$ 时，NTK 的高频分量不会衰减。这也意味着 MLP 学高频信息和低频信息的能力差不多。</p>
<p><img src="/2024/12/05/20241202-fourier-feature/11.png" alt></p>
<h2 id="随机傅里叶特征"><a href="#随机傅里叶特征" class="headerlink" title="随机傅里叶特征"></a>随机傅里叶特征</h2><p>现在我们已经知道傅里叶特征的公式是什么，并知道如何设置其中的参数 $a_j$, $\mathbf{b}_j$ 了。现在，还有一件事我们没有决定：该如何设置傅里叶特征的长度 $m$ 呢？</p>
<p>既然我们说傅里叶特征就是把输入的位置做了一次傅里叶变换，那么一般来讲，傅里叶特征的长度应该和原图像的像素数一样。比如我们要表示一个 $256 \times 256$ 的图像，那么我们就需要令 $m = 256 \times 256 / 2$ ，$\mathbf{b}$ 表示不同方向上的频率：$[(1, 1), (1, 2), …, (128, 256)]$。但这样的话，神经网络的参数就太多了。可不可以令 $m$ 更小一点呢？</p>
<p>根据之前的研究 <em>Random features for large-scale kernel machines</em> 表明，我们不需要密集地采样傅里叶特征，只需要稀疏地采样就行了。具体来说，我们可以从某个分布随机采样 $m$ 个频率 $\mathbf{b_j}$ 来，这样的学习结果和密集采样差不多。当然，根据前面的分析，我们还是令所有系数 $a_j=1$。在实验中，作者发现，$\mathbf{b_j}$ 从哪种分布里采样都无所谓，关键是 $\mathbf{b_j}$ 的采样分布的标准差，因为这个标准差决定了傅里叶特征的带宽，也决定了网络拟合高频信息的能力。实验的结果如下：</p>
<p><img src="/2024/12/05/20241202-fourier-feature/12.png" alt></p>
<p>我们可以不管图片里 $1/f^x$ 是啥意思，只需要知道 a, b, c 是三组不同的实验就行。虚线是密集采样傅里叶特征的误差，它的结果反映了一个「较好」的误差值。令人惊讶的是，不管从哪种分布里采样 $\mathbf{b_j}$，最后学出来的网络误差都差不多。问题的关键在于采样分布的标准差。把标准差调得够好的话，模型的误差甚至低于密集采样的误差。</p>
<p>也就是说，虽然我们花半天分析了位置编码和傅里叶变换的关系，但我们没必要照着傅里叶变换那样密集地采样频率，只需要随机选一些频率即可。当然，这个结论只对 MLP 拟合连续数据的任务有效，和 Transformer 里的位置编码无关。</p>
<h2 id="代码实现随机傅里叶特征"><a href="#代码实现随机傅里叶特征" class="headerlink" title="代码实现随机傅里叶特征"></a>代码实现随机傅里叶特征</h2><p>现在，我们可以回到博文开头的代码，看一下随机傅里叶特征是怎么实现的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FourierFeature</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_c, out_c, scale</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        fourier_basis = torch.randn(in_c, out_c // <span class="number">2</span>) * scale</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;_fourier_basis&#x27;</span>, fourier_basis)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        N, C, H, W = x.shape</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;n c h w -&gt; (n h w) c&#x27;</span>)</span><br><span class="line">        x = x @ self._fourier_basis</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;(n h w) c -&gt; n c h w&#x27;</span>, h = H, w = W)</span><br><span class="line">            </span><br><span class="line">        x = <span class="number">2</span> * torch.pi * x</span><br><span class="line">        x = torch.cat([torch.sin(x), torch.cos(x)], dim=<span class="number">1</span>) </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">feature_length = <span class="number">256</span></span><br><span class="line">model = MLP(feature_length).to(device)</span><br><span class="line">fourier_feature = FourierFeature(<span class="number">2</span>, feature_length, <span class="number">10</span>).to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line">n_loops = <span class="number">400</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_loops)):</span><br><span class="line">    x = fourier_feature(grid)</span><br><span class="line">    output = model(x)</span><br><span class="line">    loss = F.l1_loss(output, input_image)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> epoch == n_loops - <span class="number">1</span>:</span><br><span class="line">        viz_image(output[<span class="number">0</span>])</span><br><span class="line">        <span class="built_in">print</span>(loss.item())</span><br><span class="line">prev_output = output</span><br></pre></td></tr></table></figure>
<p>傅里叶特征通过类 <code>FourierFeature</code> 实现。其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FourierFeature</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_c, out_c, scale</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        fourier_basis = torch.randn(in_c, out_c // <span class="number">2</span>) * scale</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;_fourier_basis&#x27;</span>, fourier_basis)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        N, C, H, W = x.shape</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;n c h w -&gt; (n h w) c&#x27;</span>)</span><br><span class="line">        x = x @ self._fourier_basis</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;(n h w) c -&gt; n c h w&#x27;</span>, h = H, w = W)</span><br><span class="line">            </span><br><span class="line">        x = <span class="number">2</span> * torch.pi * x</span><br><span class="line">        x = torch.cat([torch.sin(x), torch.cos(x)], dim=<span class="number">1</span>) </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>构造函数里的 <code>fourier_basis</code> 表示随机傅里叶特征的频率，对应论文公式里的$\mathbf{b}$，<code>scale</code> 表示采样的标准差。初始化好了随机频率后，对于输入位置 <code>x</code>，只要按照公式将其投影到长度为 <code>out_c / 2</code> 的向量上，再对向量的每一个分量求 <code>sin, cos</code> 即可。按照之前的分析，我们令所有系数 $a$ 为 $1$，所以不需要对输出向量乘系数。</p>
<h2 id="傅里叶特征在-StyleGAN3-里的应用"><a href="#傅里叶特征在-StyleGAN3-里的应用" class="headerlink" title="傅里叶特征在 StyleGAN3 里的应用"></a>傅里叶特征在 StyleGAN3 里的应用</h2><p>傅里叶特征最经典的应用就是 NeRF 这类过拟合连续数据任务。除此之外，傅里叶特征另一次大展身手是在 StyleGAN3 中。</p>
<p>StyleGAN3 希望通过平滑地移动生成网络的输入来使输出图片也发生对应的移动。为此，StyleGAN3 将生成网络的输入定义为频域上的一个有限带宽图像信号：根据信号处理知识，我们能够将有限带宽信号转换成空域上无限连续的信号。也就是说，不管输入的分辨率（采样率）多低，我们都能够平滑地移动输入图片。StyleGAN3 借助随机傅里叶特征来实现这样一个频域图像。</p>
<p>以下代码选自 StyleGAN3 中傅里叶特征的构造函数。这个函数的关键是随机生成一些频率固定，但方向可以不同的傅里叶频率。函数先随机采样了一些频率，再将它们归一化，最后乘上指定的带宽 <code>bandwidth</code>，保证所有频率大小相等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SynthesisInput</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">        w_dim,          <span class="comment"># Intermediate latent (W) dimensionality.</span></span></span></span><br><span class="line"><span class="params"><span class="function">        channels,       <span class="comment"># Number of output channels.</span></span></span></span><br><span class="line"><span class="params"><span class="function">        size,           <span class="comment"># Output spatial size: int or [width, height].</span></span></span></span><br><span class="line"><span class="params"><span class="function">        sampling_rate,  <span class="comment"># Output sampling rate.</span></span></span></span><br><span class="line"><span class="params"><span class="function">        bandwidth,      <span class="comment"># Output bandwidth.</span></span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.w_dim = w_dim</span><br><span class="line">        self.channels = channels</span><br><span class="line">        self.size = np.broadcast_to(np.asarray(size), [<span class="number">2</span>])</span><br><span class="line">        self.sampling_rate = sampling_rate</span><br><span class="line">        self.bandwidth = bandwidth</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Draw random frequencies from uniform 2D disc.</span></span><br><span class="line">        freqs = torch.randn([self.channels, <span class="number">2</span>])</span><br><span class="line">        radii = freqs.square().<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>).sqrt()</span><br><span class="line">        freqs /= radii * radii.square().exp().<span class="built_in">pow</span>(<span class="number">0.25</span>)</span><br><span class="line">        freqs *= bandwidth</span><br><span class="line">        phases = torch.rand([self.channels]) - <span class="number">0.5</span></span><br></pre></td></tr></table></figure>
<p>而在使用这个类获取网络输入时，和刚刚的 MLP 实现一样，我们会先生成一个二维坐标表格 <code>grid</code> 用于查询连续图片每一处的颜色值，再将其投影到各个频率上，并计算新向量的正弦函数。</p>
<p>这段代码中，有两块和我们自己的实现不太一样。第一，StyleGAN3 允许对输入坐标做仿射变换（平移和旋转）。仿射变换对坐标的影响最终会转化成对三角函数相位 <code>phases</code> 和频率 <code>freqs</code> 的影响。第二，在计算三角函数时，StyleGAN3 只用了正弦函数，没有用余弦函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, ...</span>):</span></span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Transform frequencies.</span></span><br><span class="line">    phases = ...</span><br><span class="line">    freqs = ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Construct sampling grid.</span></span><br><span class="line">    theta = torch.eye(<span class="number">2</span>, <span class="number">3</span>, device=w.device)</span><br><span class="line">    theta[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">0.5</span> * self.size[<span class="number">0</span>] / self.sampling_rate</span><br><span class="line">    theta[<span class="number">1</span>, <span class="number">1</span>] = <span class="number">0.5</span> * self.size[<span class="number">1</span>] / self.sampling_rate</span><br><span class="line">    grids = torch.nn.functional.affine_grid(theta.unsqueeze(<span class="number">0</span>), [<span class="number">1</span>, <span class="number">1</span>, self.size[<span class="number">1</span>], self.size[<span class="number">0</span>]], align_corners=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Fourier features.</span></span><br><span class="line">    x = (grids.unsqueeze(<span class="number">3</span>) @ freqs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)).squeeze(<span class="number">3</span>) <span class="comment"># [batch, height, width, channel]</span></span><br><span class="line">    x = x + phases.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">    x = torch.sin(x * (np.pi * <span class="number">2</span>))</span><br><span class="line">    x = x * amplitudes.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Ensure correct shape.</span></span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>) <span class="comment"># [batch, channel, height, width]</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>我们在 MLP 拟合连续图像的实验里复现一下这两个改动。首先是二维仿射变换。给定旋转角 <code>theta</code> 和两个方向的平移 <code>tx, ty</code>，我们能够构造出一个 $3 \times 3$ 的仿射变换矩阵。把它乘上坐标 <code>[x, y, 1]</code> 后，就能得到仿射变换的输出。我们对输入坐标 <code>grid</code> 做仿射变换后得到 <code>grid_ext</code>，再用 <code>grid_ext</code> 跑一遍傅里叶特征和 MLP。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">N, C, H, W = grid.shape</span><br><span class="line">tx = <span class="number">50</span> / H</span><br><span class="line">ty = <span class="number">0</span></span><br><span class="line">theta = torch.tensor(torch.pi * <span class="number">1</span> / <span class="number">8</span>)</span><br><span class="line">affine_matrix = torch.tensor([</span><br><span class="line">    [torch.cos(theta), -torch.sin(theta), tx],</span><br><span class="line">    [torch.sin(theta), torch.cos(theta), ty],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">]</span><br><span class="line">).to(device)</span><br><span class="line">grid_ext = torch.ones(N, <span class="number">3</span>, H, W).to(device)</span><br><span class="line">grid_ext[:, :<span class="number">2</span>] = grid.clone()</span><br><span class="line">grid_ext = grid_ext.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">grid_ext = (grid_ext @ affine_matrix.T)</span><br><span class="line">grid_ext = grid_ext.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)[:, :<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">x = fourier_feature(grid_ext)</span><br><span class="line">output = model(x)</span><br><span class="line">viz_image(output[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>在示例代码中，我们可以得到旋转 45 度并向下平移 50 个像素的图片。可以看到，变换成功了。这体现了连续数据的好处：我们可以在任意位置对数据采样。当然，由于这种连续数据是通过过拟合实现的，在训练集没有覆盖的坐标处无法得到有意义的颜色值。</p>
<p><img src="/2024/12/05/20241202-fourier-feature/13.png" alt></p>
<p>之后，我们来尝试在傅里叶特征中只用正弦函数。我们将投影矩阵的输出通道数从 <code>out_c / 2</code> 变成 <code>out_c</code>，再在 <code>forward</code> 里只用 <code>sin</code> 而不是同时用 <code>sin, cos</code>。经实验，这样改了后完全不影响重建质量，甚至由于通道数更多了，重建效果更好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FourierFeature</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_c, out_c, scale</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        fourier_basis = torch.randn(in_c, out_c) * scale</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;_fourier_basis&#x27;</span>, fourier_basis)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        N, C, H, W = x.shape</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;n c h w -&gt; (n h w) c&#x27;</span>)</span><br><span class="line">        x = x @ self._fourier_basis</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;(n h w) c -&gt; n c h w&#x27;</span>, h = H, w = W)</span><br><span class="line">            </span><br><span class="line">        x = <span class="number">2</span> * torch.pi * x</span><br><span class="line">        x = torch.sin(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>StyleGAN3 论文并没有讲为什么只用 <code>sin</code>，网上也很少有人讨论傅里叶特征的实现细节。我猜傅里叶特征并不是非得和傅里叶变换完全对应，毕竟它只是用来给神经网络提供更多信息，而没有什么严格的意义。只要把输入坐标分解成不同频率后，神经网络就能很好地学习了。</p>
<p>只用 <code>sin</code> 而不是同时用 <code>sin, cos</code> 后，似乎我们之前对 NTK 平移不变的推导完全失效了。但是，根据三角函数的周期性可知，只要是把输入映射到三角函数上后，网络主要是从位置间的相对关系学东西。绝对位置对网络来说没有那么重要，不同的绝对位置只是让所有三角函数差了一个相位而已。只用 <code>sin</code> 的神经网络似乎也对绝对位置不敏感。为了证明这一点，我把原来位于 <code>[0, 1]</code> 间的坐标做了一个幅度为 <code>10</code> 的平移。结果网络的误差几乎没变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(n_loops)):</span><br><span class="line">    x = fourier_feature(grid + <span class="number">10</span>)</span><br><span class="line">    output = model2(x)</span><br><span class="line">    loss = F.l1_loss(output, input_image)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>根据这些实验结果，我感觉是不是从 NTK 的角度来分析傅里叶特征完全没有必要？是不是只要从直觉上理解傅里叶特征的作用就行了？按我的理解，傅里叶特征在真正意义在于显式把网络对于不同频率的关注度建模出来，从而辅助网络学习高频细节。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇博文中，我们学习了傅里叶特征及其应用，并顺带了解其背后有关核回归、NTK 的有关理论知识。这些知识很杂乱，我来按逻辑顺序把它们整理一下。</p>
<p>为了解释为什么 NeRF 中的位置编码有效，傅里叶特征论文研究了用 MLP 拟合连续数据这一类任务中如何让 MLP 更好地学到高频信息。论文有两大主要结论：</p>
<ul>
<li>通过从 NTK 理论的分析，位置编码其实是一种特殊的傅里叶特征。这种特征具有平移不变性。因此，神经网络就像是在对某个输入信号做卷积。而我们可以通过调整傅里叶特征的参数来调整卷积的带宽，也就是调整网络对于不同频率的关注程度，从而使得网络不会忽略高频信息。</li>
<li>傅里叶特征的频率不需要密集采样，只需要从任意一个分布随机稀疏采样。影响效果的关键是采样分布的标准差，它决定了傅里叶特征的带宽，也就决定了网络是否能关注到高频信息。</li>
</ul>
<p>由于这些结论比较抽象，我们可以通过一个简单的二维图像拟合实验来验证论文的结论。实验表明直接将坐标输入给 MLP 不太行，必须将输入转换成傅里叶特征才能有效让网络学到高频信息。这个傅里叶特征可以是随机、稀疏的。</p>
<p>除了过拟合连续数据外，傅里叶特征的另一个作用是直接表示带宽有限信号，以实现在空域上的连续采样。StyleGAN3 在用傅里叶特征时，允许对输入坐标进行仿射变换，并且计算特征时只用了正弦函数而不是同时用正弦、余弦函数。这表明有关 NTK 的理论分析可能是没有必要的，主要说明问题的还是实验结果。</p>
<p>傅里叶特征论文仅研究了拟合连续数据这一类问题，没有讨论 Transformer 中位置编码的作用。论文中的一些结论可能无法适用。比如在大模型的位置编码中，我们还是得用密集的 <code>sin, cos</code> 变换来表示位置编码。不过，我们可以依然借助该论文中提到的理论分析工具，来尝试分析所有位置编码的行为。</p>
<p>只通过文字理解可能还不太够，欢迎大家尝试我为这篇博客写的 Notebook，通过动手做实验来加深理解。 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/FourierFeature">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/FourierFeature</a></p>
<p>推荐参考资料：</p>
<p>用代码示例理解核回归：<a target="_blank" rel="noopener" href="https://towardsdatascience.com/kernel-regression-made-easy-to-understand-86caf2d2b844">https://towardsdatascience.com/kernel-regression-made-easy-to-understand-86caf2d2b844</a></p>
<p>直观理解 NTK: <a target="_blank" rel="noopener" href="https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/">https://www.inference.vc/neural-tangent-kernels-some-intuition-for-kernel-gradient-descent/</a></p>
<p>傅里叶特征官方示例 PyTorch 实现: <a target="_blank" rel="noopener" href="https://github.com/ndahlquist/pytorch-fourier-feature-networks/tree/master">https://github.com/ndahlquist/pytorch-fourier-feature-networks/tree/master</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/11/28/20241128-diffusion-forcing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/11/28/20241128-diffusion-forcing/" class="post-title-link" itemprop="url">论文速览 | Diffusion Forcing：给视频扩散模型的每一帧添加不同强度的噪声</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-11-28 22:58:06" itemprop="dateCreated datePublished" datetime="2024-11-28T22:58:06+08:00">2024-11-28</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="论文速览"><a href="#论文速览" class="headerlink" title="论文速览"></a>论文速览</h2><p>平时我自己读新论文时，往往简单看一下摘要和图表就能差不多明白文章的大意。而要用一篇博文来详细介绍新论文的设计动机、实现方法，需要花多得多的时间来写作。为了能分享更多的新论文，并且让我自己能够更好地整理论文间的关系，从这篇文章开始，我会不定期发表「论文速览」类文章，以简明扼要的文字来帮助计算机视觉的同行们快速了解新论文。</p>
<p>和发文频繁的自媒体相比，我的论文速览文章几乎不会展示论文的结果，仅会从科研角度介绍文章的贡献及方法，并给出我的批判性分析。和我之前的论文详解文章相比，这个新系列的文章会用更少的文字介绍背景，几乎不介绍方法细节，力求用更少的文字来表达关键信息，提升文字沟通效率。不过另一方面，我会花更多心思在介绍新论文与过往论文的关系上，为不熟悉某些背景知识的读者提供学习途径，以填补背景介绍上的空缺。</p>
<h2 id="Diffusion-Forcing"><a href="#Diffusion-Forcing" class="headerlink" title="Diffusion Forcing"></a>Diffusion Forcing</h2><p>视频扩散模型普遍存在视频质量随着视频长度增加不断下降的问题。为此，论文作者提出了 Diffusion Forcing 这一建模任何<strong>序列生成</strong>问题的新范式：在训练该扩散模型时，序列中的每个元素会<strong>独立地</strong>添加不同强度的噪声。作者在简单视频生成、决策 (decision-making) 任务上验证了这种生成范式的有效性。我将主要从视频生成的角度介绍本工作。</p>
<h2 id="以往工作"><a href="#以往工作" class="headerlink" title="以往工作"></a>以往工作</h2><p>稍后我们会了解到，Diffusion Forcing 与此前两种主流序列生成范式密切相关：自回归生成与全序列扩散模型。</p>
<p>在序列的自回归生成中，模型会不断根据序列前 $n-1$ 个元素，预测下一个元素（第 $n$ 个元素）。自回归生成在 NLP 中最为常见，RNN、Transformer 用的都是这种生成方法。</p>
<p>扩散模型可以直接生成任何形状的数据。如果我们不把视频视为一种由图像组成的序列数据，而是将其视为一种「三维图像」，那么我们可以直接将 2D 图像扩散模型简单地拓展成 3D 视频扩散模型。这种做法在这篇论文中被称为「全序列扩散模型」。使用这一方法的早期工作有 DDPM 的作者提出的 <em>Video Diffusion Models</em>。Stable Diffusion 的作者也基于 LDM 提出类似的 <em>Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</em>（Video LDM）工作。</p>
<p>全序列扩散模型仅能生成固定长度的视频。为了将其拓展到长视频生成，还是得将其和自回归结合起来。但是，自回归视频生成存在着生成质量与训练集质量不匹配的问题。Stable Video Diffusion 等工作参考 Cascaded Diffusion Models 的做法，通过给约束图像/视频帧加噪声缓解此问题。</p>
<p><em>AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation</em> 进一步探讨了自回归生成与全序列扩散模型的结合方法：在生成文本时，不同时刻的文本噪声不同，越早的文本上的噪声越少。无独有偶，<em>FIFO-Diffusion: Generating Infinite Videos from Text without Training</em> 提到了如何在预训练视频扩散模型上，以不同的噪声强度来生成不同的视频帧。或许是受到这些工作的启发，Diffusion Forcing 系统探讨了如何在训练时独立地给序列元素添加噪声。</p>
<h2 id="科研动机"><a href="#科研动机" class="headerlink" title="科研动机"></a>科研动机</h2><p>本工作的作者发现了自回归与全序列扩散模型的不足之处，并认为这两种生成范式是互补的：</p>
<ul>
<li>自回归不能在推理时加入新的优化目标，且存在着前述的训练采样质量不匹配导致的降质问题。</li>
<li>全序列扩散模型不能生成变长的序列。</li>
</ul>
<p>反过来讲：</p>
<ul>
<li>全序列扩散模型可以在推理加入 Classifier-guidance，且在固定序列长度内几乎不存在降质。</li>
<li>自回归可以生成变长序列。</li>
</ul>
<p>那能不能把二者结合起来呢？在推理时，我们希望序列是按时间顺序先后生成的。同时，从噪声强度这一维度上，我们希望每个元素能够从完全带噪到完全清晰逐渐生成。结合二者的方法是：在采样时，较早的元素有较少的噪声，较新的元素有较多的噪声，不同噪声强度的元素在序列中同时生成。比如采样分3步完成，共生成3帧，那么一步去噪会让$[x_1^{1/3\cdot T}, x_2^{2/3\cdot T}, x_3^{T}]$变成$[x_1^0, x_2^{1/3\cdot T}, x_3^{2/3\cdot T}]$。</p>
<p>为了实现这样的采样，我们在训练时就要让每一元素能够在其他元素和自己的噪声强度不同时也能顺利去噪。和以往的工作不同，本工作的作者发现，训练时我们不必和推理时一样固定每一元素的噪声强度，而是可以<strong>独立地</strong>对每帧随机采样噪声强度。</p>
<h2 id="简单视频生成模型"><a href="#简单视频生成模型" class="headerlink" title="简单视频生成模型"></a>简单视频生成模型</h2><p>本文的想法十分简洁，只需要在 DDPM 序列生成模型上把训练和推理时的噪声强度换掉即可。为了进一步了解本工作的方法，我们来看一下论文中有关视频模型的方法与实验。</p>
<p>整体上看，训练时，方法和 $\epsilon$ 预测的普通 DDPM 一样，只是不同帧噪声强度不一样。</p>
<p><img src="/2024/11/28/20241128-diffusion-forcing/1.png" alt></p>
<p>在帧间关系上，Diffusion Forcing 用因果（causal）关系建模，即当前帧只能看到之前帧的信息。</p>
<p><img src="/2024/11/28/20241128-diffusion-forcing/2.png" alt></p>
<p>具体来说，本工作用 RNN （准确来说是 GRU）的隐变量来建模之前帧传过来的信息。加入了 RNN 后，论文把本来很简单的公式变得无比复杂，不建议读者深究论文中有关 RNN 的内容。</p>
<p><img src="/2024/11/28/20241128-diffusion-forcing/3.png" alt></p>
<p>在采样时，由于不同帧的噪声强度不同，现在我们需要定义一个二维的噪声强度表，表示每一帧在不同位置及不同去噪时刻的噪声强度。为了让每一时刻的噪声强度不同，一开始较新帧的去噪时刻会停留在原地。作者在附录中介绍了同时刻去噪的详细设计。</p>
<p><img src="/2024/11/28/20241128-diffusion-forcing/4.png" alt></p>
<p>作者发现，Diffusion Forcing 的这种序列生成方式可以自然地推广到无限长度的视频生成上：在生成下一个片段时，<strong>不用滑动窗口</strong>，直接把 RNN 的初始隐变量初始化为上一个片段的隐变量输出。</p>
<p>作者用同样的 RNN 结构训练了两个基准模型：一个自回归模型，一个全序列<strong>因果</strong>扩散模型。定性结果表明，不管是在预定视频长度内，还是超出原本长度的长视频生成，Diffusion Forcing 的结果均好于基准方法。结果可以在官方项目网站上查看：</p>
<p><a target="_blank" rel="noopener" href="https://boyuan.space/diffusion-forcing/">https://boyuan.space/diffusion-forcing/</a></p>
<h2 id="批判性分析"><a href="#批判性分析" class="headerlink" title="批判性分析"></a>批判性分析</h2><p>论文开头说自回归模型在推理时缺少添加约束的方法。但这对视频生成来说并不致命，因为一般可以在训练时加入约束，推理时用 Classifier-free Guidance 就行了。</p>
<p>作者说出于简洁，他们在论文中用 RNN 实现了 Diffusion Forcing。但很明显 3D U-Net 才应该是直观上最简单实用的实现方法，毕竟最早期的视频扩散模型就是拿 3D U-Net 做的。在官方仓库中，有本科生帮他们实现了一个 3D U-Net 加时间注意力的模型，比原来视频模型效果要好。</p>
<p>我认为本文的视频生成基准方法设置过低。对于自回归视频生成/图生成视频，现在大家都会参照 Cascaded Diffusion, 对约束图像加噪并把噪声强度做为额外约束传入当前帧的生成模型。这种设计和 Diffusion Forcing 原理相似。为了体现新方法的好处，有必要跟这个更加强大的基准自回归方法做对比。</p>
<p>作者对于全序列视频扩散模型的设计也很奇怪。全序列视频扩散模型的初衷就是把视频当成 3D 图像来看待，允许帧间两两交换信息，只保证预定长度内的视频是连贯的。作者现在用 RNN 实现了一个因果版全序列视频模型，这个模型的表现肯定是不如非因果版的。虽然作者说 Diffusion Forcing 在因果视频生成上总是比全序列扩散模型更加连贯，我很怀疑去掉了因果这个条件后 Diffusion Forcing 还能否比得过。</p>
<p>Diffusion Forcing 在视频生成的主要好处应该体现在超出预定长度的长视频生成。因此，哪怕在预定长度内比不过全序列扩散模型，也没有关系。作者应该比较结合自回归和全序列扩散模型的方法，比如用 Stable Video Diffusion 这种图生视频模型，把上一个视频的末帧当作下一个视频的首帧约束，证明 Diffusion Forcing 在长视频生成上的优越性。</p>
<p>综上，我认为作者在视频生成任务上的实验是不够充分的。也的确，这篇论文有一半篇幅是在决策任务上，没有只讲视频生成任务。我相信 Diffusion Forcing 的设计会在长视频生成上缓解降质问题，这需要后续大公司的工作来跟进。但是，长视频的根本问题是记忆缺失，这一本质问题是 Diffusion Forcing 这种方法难以做好的。</p>
<p>这篇工作对我最大的启发是，我们一直把视频当成完整的 3D 数据来看待，却忘了视频可以被看成是图像序列。如果把视频当成 3D 数据的话，不同帧只能通过时序注意力看到其他帧在当前去噪时刻的信息；而对于序列数据，我们可以在不同帧的依赖关系上做更多设计，比如这篇工作的不同去噪强度。我很早前就在构思一种依赖更加强的序列生成范式：我们可不可以把序列中其他元素的<strong>所有去噪时刻</strong>的<strong>所有信息</strong>（包括中间去噪结果及去噪网络的中间变量）做为当前元素的约束呢？这种强约束序列模型可能对多视角生成、视频片段生成等任务的一致性有很大帮助。由于生成是约束于另一个去噪过程的，我们对此去噪过程做的任何编辑，都可以自然地传播到当前元素上。比如在视频生成中，如果整个视频约束于首帧的去噪过程，那么我们用任意基于扩散模型的图像编辑方法来编辑首帧，都可以自然地修改后续帧。当然，我只是提供一个大致的想法，暂时没有考虑细节，欢迎大家往这个方向思考。</p>
<p>有人肯定也会想到能否把 Diffusion Forcing 拓展到图像像素间关系上。我认为要实现训练是完全没有问题的，问题出在推理上：Diffusion Forcing 在推理时需要预定义不同元素间的去噪强度。对于视频这种有先后顺序的数据，我们很自然地可以让越早的帧噪声强度越低。但是，对于图像来说，如何定义不同像素在不同去噪步数时的去噪强度并不是一个易解的问题。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/09/13/20240825-trans-diff/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/09/13/20240825-trans-diff/" class="post-title-link" itemprop="url">速览多模态模型 Transfusion 和 Show-o：用 Transformer + 扩散模型同时处理文本和图像</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-09-13 13:48:27" itemprop="dateCreated datePublished" datetime="2024-09-13T13:48:27+08:00">2024-09-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>近期，有两个大型多模态模型于同期公布：一个是来自 Meta 的 Transfusion，另一个是来自 Show Lab 和字节跳动的 Show-o 。好巧不巧，二者都宣称自己的模型是几乎最早将多模态任务用一个 Transformer 完成的，不需要借助额外的文本编码器实现图像生成，同时结合了自回归生成和扩散模型。我很好奇这两篇工作究竟有多少创新，于是快速扫完了这两篇论文，并简单给大家分享一下它们的核心内容。</p>
<p>在这篇文章中，我会快速介绍两篇工作的核心模型架构与部分实验结果。由于我仅对视觉任务比较熟悉，对语言和多模态没有那么了解，我的分析将主要围绕视觉任务。</p>
<p>论文 Arxiv 链接：</p>
<p>Transfusion: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.11039">https://arxiv.org/pdf/2408.11039</a></p>
<p>Show-o: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.12528">https://arxiv.org/pdf/2408.12528</a></p>
<h2 id="读前准备"><a href="#读前准备" class="headerlink" title="读前准备"></a>读前准备</h2><p>在阅读这两篇新工作时，建议大家先熟悉以 Transformer 为代表的自回归生成、以 DDPM、LDM、DiT 为代表的扩散模型、以 MaskGIT (Masked Generative Image Transformer), MAR (Masked autoregressive models, 于 <em>Autoregressive Image Generation without Vector Quantization</em> 论文中提出) 为代表的掩码自回归图像生成这三类生成模型，并简单了解此前较为先进的 Chameleon (<em>Chameleon: Mixed-Modal Early-Fusion Foundation<br>Models</em>) 多模态模型。本文不会对这些知识做深入回顾，如果读者遇到了不懂的旧概念，请先回顾有关论文后再来看这两篇新文章。</p>
<h3 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h3><p>自回归模型用于生成形如 $\mathbf{x}=\{x_1, x_2, …, x_n\}$ 这样的<strong>有序</strong>序列。自回归算法会逐个生成序列中的元素。假设正在生成第 $i$ 个元素，则算法 $F$ 会参考之前所有的信息 $\{x_j \mid j &lt; i\}$，得到 $x_i = F(\{x_j \mid j &lt; i\})$。比如：</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_1 &= F(\emptyset) \\
    x_2 &= F(\{x_1\}) \\
    x_3 &= F(\{x_1, x_2\}) \\
\end{aligned}</script><p>自回归任务最常见的应用场景是文本生成。给定第一个词，生成第二个词；给定前两个词，生成第三个词……。</p>
<p>为了训练实现这一算法的模型，一般我们需要假设每个元素的取值是有限的。比如我们要建模一个生成单词的模型，每个元素是小写字母，那么元素的取值只有 <code>a, b, c, ..., z</code>。满足这个假设后，我们就可以像分类任务一样，用神经网络模型预测的类别分布建模已知之前所有元素时，下一个元素的分布，并用交叉熵损失函数去优化模型。这种训练任务被称为下一个词元预测 (next token prediction, NTP)。</p>
<p>用自回归生成建模某类数据时，最重要的是定义好每个元素的先后顺序。对于文本数据，我们只需要把每个词元 (token) 按它们在句子里的先后顺序编号即可。而对于图像中的像素，则有多种编号方式了。最简单的一种方式是按从左到右、从上到下的顺序给像素编号。</p>
<h3 id="掩码自回归模型"><a href="#掩码自回归模型" class="headerlink" title="掩码自回归模型"></a>掩码自回归模型</h3><p>由于图像的像素数很多，用自回归模型一个一个去生成像素是很慢的；另外，按从左到右、从上到下的顺序给像素编号显然不会是最合理的。为了提升自回归模型的速度和表现，研究者提出了掩码自回归模型。它做了两个改进：</p>
<p>1） 相比按序号一个一个生成元素的经典自回归模型，这种模型在每轮生成时可以生成多个像素（下图中的橙色像素）。</p>
<p>2） 相比从左到右、从上到下的固定顺序，像素的先后顺序完全随机（下图中的 (b) 和 (c) ）。</p>
<p>由于这种方式下必须一次给模型输入所有像素，并用掩码剔除未使用的像素，所以这种自回归被叫做掩码自回归。</p>
<p><img src="/2024/09/13/20240825-trans-diff/1.jpg" alt></p>
<h3 id="扩散模型"><a href="#扩散模型" class="headerlink" title="扩散模型"></a>扩散模型</h3><p>扩散模型将图像生成表示成一个噪声图像 $\mathbf{x}_T$ 从时刻 $T$ 开始随时间变化 $\mathbf{x}_t = F(\mathbf{x}_{t+1})$，最后得到目标图像 $\mathbf{x}_0$ 的过程。和输入输出为部分像素的自回归模型不同，扩散模型的输入输出自始至终都是完整图像。</p>
<p>为了减少扩散模型的计算量，参考 Latent Diffusion Model (LDM) 的做法，我们一般会先用一个自编码器压缩图像，再用扩散模型生成压缩过的小图像。正如 NLP 中将文本拆成单词、标点的「词元化」(tokenize) 操作一样，这一步操作可以被称为「图块化」(patchify)。当然，有些时候大家也会把图块叫做词元，把图块化叫做图像词元化。</p>
<blockquote>
<p>严格来说，本文讲到的「像素」其实是代表一个图块的图像词元。用「像素」是为了强调图像元素的二维空间信息，用「图像词元」是强调图像元素在自回归模型中是以一维序列的形式处理的。</p>
</blockquote>
<p>有人认为，掩码自回归模型是一种逐渐把纯掩码图像变成有意义图像的模型，它和逐渐把纯噪声图像变成有意义图像的扩散模型原理类似。因此，他们把掩码自回归模型称为<strong>离散扩散模型</strong>。还有人认为扩散模型也算一种更合理的自回归，每轮输入一个高噪声图像，输出一个噪声更少的图像。但这些观点仅仅是从称呼上统一两种模型，两种模型在实现上还是有不少差别的。</p>
<h3 id="Chameleon"><a href="#Chameleon" class="headerlink" title="Chameleon"></a>Chameleon</h3><p>Chameleon 似乎是此前最为先进的多模态模型，它是这两篇新工作的主要比较对象。在语言模型的基础上，Chameleon 并没有对图像的处理多加设计，只是以离散自编码器（如 VQGAN）的编码器为图像词元化工具，以其解码器为图像词元化还原工具，让被词元化后的图像词元以同样的方式与文本词元混在一起处理。 </p>
<p><img src="/2024/09/13/20240825-trans-diff/2.jpg" alt></p>
<h2 id="功能与效果"><a href="#功能与效果" class="headerlink" title="功能与效果"></a>功能与效果</h2><p>看方法前，我们先明确一下两个多模态模型能做的任务，及各任务的输入输出。</p>
<p>Transfusion 是一个标准多模态模型，也就是一个输入输出可以有图像词元的语言模型。它输入已知文本和图像，输出后续文本和图像。</p>
<p><img src="/2024/09/13/20240825-trans-diff/3.jpg" alt></p>
<p>基于这个多模态模型，可以做文生图任务。</p>
<p><img src="/2024/09/13/20240825-trans-diff/4.jpg" alt></p>
<p>这个模型似乎没有为特定任务设置特殊词元，所有图像功能完全靠文本指定。因此，要做图像编辑任务的话，需要在带文本标注的图像编辑数据集上微调。文章指出，只需要在一个仅有 8000 项数据的数据集上微调就能让模型具有一定的编辑能力。</p>
<p><img src="/2024/09/13/20240825-trans-diff/5.jpg" alt></p>
<p>相比之下，Show-o 可以在序列前多输入一个区分任务的特殊词元。因此，Show-o 可以完成多模态理解（输入多模态，输出文本描述）、图像生成（根据文字生成图像或填补图像）、多模态生成（输入输出都包含图片、文本）等丰富的任务。似乎特殊词元仅有多模态理解 (MMU, Multi-modal Understanding MMU)、文生图 (T2I, Text to Image) 这两种。</p>
<p><img src="/2024/09/13/20240825-trans-diff/6.jpg" alt></p>
<p>Transfusion 的基础模型在微调后才能做根据文本提示来编辑图像的任务，而 Show-o 的基础模型默认是在此类带有文本提示的图像编辑数据集上微调的。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>对于熟悉此前图像生成模型、语言模型的研究者来说，这两篇工作都仅用了现有技术，核心方法非常易懂。这两篇工作并不是试图开发一种新的图像生成技术，而是在考虑如何更好地将现有图像模型融入多模态模型。</p>
<p>在读新技术之前，我们先以 Chameleon 为例，看一下之前的多模态模型是怎么做生成的。在我看来，之前的多模态模型不应该叫「多模态模型」，而应该叫「强行把图像当成词元的语言模型」。语言模型在处理文本时，文本中的词语、标点会根据规则被拆成「词元」，成为模型的最小处理单位。然而，要用同样的方式处理图像，就要定义什么是「图像词元」。受到之前图像离散压缩模型（以 VQGAN 为代表）的启发，大家用一个编码器把图像切成图块，每个图块可以用一个 1, 2, 3 这样的整数序号表示，再用一个解码器把用序号表示的图块翻译回真实图像。这里的带序号图块就和文本里的单词一样，可以用「词元」来表示。文本、图像都被统一成词元后，就能用标准 Transformer 的下一个词元预测任务来训练多模态模型。</p>
<p>如下图所示，训练时，文本基于程序规则变成词元，而图像经过一个编码器模型变成词元。两类词元被当成一类数据，以同样的方式按下一个词元预测任务训练。生成时，多模态模型自回归地生成所有词元，随后文本词元基于程序规则恢复成文本，而图像词元通过解码器模型恢复成图像。</p>
<p><img src="/2024/09/13/20240825-trans-diff/2.jpg" alt></p>
<p>这种多模态模型最大的问题是没有充分设计图像词元生成，还是暴力地用 Transformer 自回归那一套。虽然有些模型会多加入一些图像生成上的设计，比如 LaVIT (<em>Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization</em>) 用扩散模型来做图像词元的解码，但核心的图像词元生成还是离不开标准自回归。</p>
<p>Transfusion 和 Show-o 的设计初衷都是引入更先进的图像生成技术来改进图像词元生成。先看 Show-o。要改进标准的一个一个按顺序生成图像词元的图像自回归模型，最容易想到的做法就是按照 MaskGIT, MAR 那一套，将标准自回归换成掩码自回归。在做掩码自回归时，像素的先后顺序完全随机，且一次可以生成多个像素。另外，图像词元之间可以两两互相做交叉注意力，而不用像文本词元一样只能后面的对前面的做交叉注意力。</p>
<p><img src="/2024/09/13/20240825-trans-diff/7.jpg" alt></p>
<blockquote>
<p>Show-o 莫名其妙地把自己的图像生成模型称为离散扩散模型。如前文所述，叫离散扩散模型还是掩码自回归，只是一个称呼上的问题。由于问题建模上的重大差异，大家一般还是会把扩散模型和掩码自回归看成两类模型。</p>
</blockquote>
<p>而 Transfusion 更加激进地在革新了多模态模型中的图像生成方式。现在最好的图像生成技术不是扩散模型吗？我们干脆直接把整个扩散模型搬过来。于是，在 Transfusion 生成多模态内容时，程序会交替执行两种模式：在语言模型模式下，程序按标准自回归逐个生成文本词元。一旦生成了特殊词元 BOI (begin of image)，就切换到扩散模式。在扩散模式下，程序按 DiT, SD 3 (Stable Diffusion 3) 那种标准扩散模型的方式，一次性生成所有图像词元。结束此模式后，程序往词元序列里填一个 EOI (end of image)，重返语言模型模式。</p>
<p><img src="/2024/09/13/20240825-trans-diff/3.jpg" alt></p>
<p>同理，在训练时，两种模态也用不同的任务来训练。语言模型老老实实地按下一个词元预测训练，而扩散模型那部分就按照训练扩散模型的标准方式，先给所有图像词元加噪，再预测噪声。因此，只看图像生成任务的话，Transfusion 更像 SD 3 这种文生图模型，而不像此前的基于语言模型的多模态模型。</p>
<p>Transfusion 和 SD 3 之间的最大区别在于，文本词元还是按照语言模型那一套，只能在交叉注意力看到之前的文本词元。而图像词元之间两两都能看到。这种交叉注意力的设计和 Show-o 是一模一样的。当然，由于现在文本也会在同一个 Transformer 里处理，所以 Transfusion 自己就扮演了解读文本的工作，而不像 SD 3 那样还需要单独的文本编码器。</p>
<p><img src="/2024/09/13/20240825-trans-diff/8.jpg" alt></p>
<h2 id="定量评测结果"><a href="#定量评测结果" class="headerlink" title="定量评测结果"></a>定量评测结果</h2><p>我们最后来看一下两篇文章展示的定量评测结果。</p>
<p>Transfusion 用了许多篇幅来展示它与 Chameleon 之间的对比。从数值指标上看，Transfusion 全面领先 Chameleon。明明没有对文本任务做特别的优化，Transfusion 却在文本任务超越了 Chameleon，这挺令人惊讶的。为了探究这一现象的原因，作者从同一个预训练语言模型开始，以同样的配置训练了 Transfusion, Chameleon。结果显示，相比加入图像扩散模型，加入 Chameleon 那种离散图像词元对文本指标的损害更大。作者猜测，这是因为扩散模型更加高效，模型能够把更多精力放在文本任务上。</p>
<p>而从图像生成模型的对比上看，Transfusion 比之前多数文生图模型都要好，只是比顶尖文生图模型 SD 3 要差一点。</p>
<p><img src="/2024/09/13/20240825-trans-diff/9.jpg" alt></p>
<p>再来看 Show-o 的评测结果。Show-o 在部分文本指标上超过了之前的语言模型。作者也坦言，这些指标仅表明 Show-o 有潜力在文本任务上做到顶尖。</p>
<p><img src="/2024/09/13/20240825-trans-diff/10.jpg" alt></p>
<p>Show-o 也展示了图像任务的指标。和 Transfusion 一样，Show-o 展示了表示图像质量的 COCO FID 指标以及评价文本图像匹配度的 GenEval 指标。Show-o 在图像指标上超越了此前多数多模态模型，且超越了 Stable Diffusion 2.1 等图像生成模型。但是其图像指标比 Transfusion 还是差了不少。Show-o 的最大优点是需要的图像训练数据远远少于其他模型。</p>
<p><img src="/2024/09/13/20240825-trans-diff/11.jpg" alt></p>
<h2 id="总结与讨论"><a href="#总结与讨论" class="headerlink" title="总结与讨论"></a>总结与讨论</h2><p>此前多模态模型都只是强行把图像变成离散图像词元，再用标准自回归来生成图像词元。为了改进这些多模态模型，无独有偶，Transfusion 和 Show-o 都用到了更先进的图像生成技术。Show-o 将标准自回归改成了更强大的掩码自回归，而 Transfusion 激进地引入了完整的图像扩散模型，并把文本生成和图像生成当成两个相对独立的任务。二者的相同之处如下：</p>
<ul>
<li>两个多模态模型都用同一个 Transformer 来处理文本和图像。</li>
<li>两个模型的 Transformer 都使用了同样的交叉注意力机制。文本词元只能看到此前的图像、文本词元，而图像词元可以看到此前的文本词元和当前所有图像词元。</li>
</ul>
<p>二者的不同之处在于：</p>
<ul>
<li>Transfusion 使用标准扩散模型实现图像生成，而 Show-o 使用掩码自回归实现图像生成。Show-o 强行将自己的图像生成模型称为「离散扩散模型」，有借扩散模型的名头宣传之嫌。</li>
<li>Transfusion 没有用特殊词元来区分不同任务。要用 Transfusion 编辑图像，需要在基础模型上用图像编辑数据集微调。Show-o 用特殊词元来区分不同任务，默认支持文本理解、图像生成、图像编辑、多模态生成等多种任务。</li>
<li>二者在文本、图像指标上都超越了之前的多模态模型。但二者相互对比之下，Transfusion 的表现更好，而 Show-o 需要的训练资源少得多。</li>
</ul>
<p>我再来谈一下我看完这两篇文章后的一些感想。此前我对多模态模型很不感兴趣，觉得它们就是在语言模型的基础上，强行加入图像信息，然后加数据、加显卡，大火乱炖，没有太大的创新。而 Transfusion 和 Show-o 的设计令我眼前一亮。我觉得这两篇文章的结果再次表明，图像和文本的处理方法不应该是一致的，不能强行把文本自回归那套方法直接搬到图像任务上。不管是换成 MaskGIT 掩码自回归那一套，还是完全用扩散模型，都比之前的方法要好。</p>
<p>而究竟是掩码自回归好还是扩散模型更好呢？在我看来，扩散模型是更好的。文本是离散的，而图像是连续的。这种连续性不仅体现在图像的颜色值上，还体现在图像像素间的位置关系上。强行用 Transformer 自回归生成图像，一下子把这两种连续信息都破坏了。而何恺明团队近期的 MAR 工作则试图找回图像在颜色值上的连续性，但依然无法充分利用空间上的连续性。相比之下，扩散模型每步迭代都是对整幅图像做操作，不会破坏图像的连续性。这两个多模态工作也反映了这一点，Transfusion 的表现要比 Show-o 好很多。</p>
<p>在生成图像时，Transfusion 的行为几乎和 SD 3, FLUX.1 这些文生图模型一样了。两类模型的区别在于，SD 3 它们得用一个预训练的语言处理模型。而 Transfusion 用同一个 Transformer 来处理文本、图像信息。尽管现在 Transfusion 还没有超过 SD 3，但我认为文生图任务本质上是一个多模态任务，这两类模型的后续发展路线很可能会交汇到一起。文生图也应该从多模态中汲取经验。比如我们在 SD 3 的基础上，加入一些语言任务上的学习，说不定能进一步提升文生图时的图文匹配度。</p>
<p>当然，仅根据我读完这两篇文章后有限的认知，我认为多模态并不是一个值得广大科研人员投入的研究方向，而只适合有足够资源的大公司来做。其根本原因是验证多模态设计的代价太大了。在图像生成领域，要验证一个生成模型好不好，我们拿 ImageNet，甚至拿只有几万张图像的人类数据集训练及评测，都很有说服力。而多模态任务必须在大量文本、图像数据集上训练，评测文本、图像上的多种指标，小一点的团队根本做不来。这样，各种小创新都难以验证。而没有各种各样的小创新，领域也就很难快速发展。所以多模态只能不断从纯语言生成、纯图像生成方向找灵感，很难有仅属于多模态任务的创新。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/09/03/20240829-GameNGen/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/09/03/20240829-GameNGen/" class="post-title-link" itemprop="url">锐评能模拟射击游戏的扩散模型 GameNGen</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-09-03 16:33:35" itemprop="dateCreated datePublished" datetime="2024-09-03T16:33:35+08:00">2024-09-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E6%9D%82%E8%B0%88/%E8%AE%AE%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">议论文</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天看到一则新闻，说 Google DeepMind 发布了一个叫做 GameNGen （英文读音 “game engine”，游戏引擎）的扩散模型，它可以仅用神经网络的生成结果，模拟经典射击游戏 DOOM。</p>
<p>项目网站：<a target="_blank" rel="noopener" href="https://gamengen.github.io/">https://gamengen.github.io/</a></p>
<p>视频链接：<a target="_blank" rel="noopener" href="https://gamengen.github.io/static/videos/e1m1_t.mp4">https://gamengen.github.io/static/videos/e1m1_t.mp4</a></p>
<p>作为一名未来的游戏设计师，每次看到这类「今天 AI 又取代了创作者」的新闻，我的第一反应总会是愤怒：创作是人类智慧的最高结晶，能做到这种程度的 AI 必然是强人工智能。但显然现在 AI 的水平没有那么高，那么这类宣传完全是无稽之谈。我带着不满看完了论文，果然这个工作并没有在技术上有革命性的突破。不过，这篇论文还是提出了一个比较新颖的科研任务并漂亮地将其解决了的，算是一篇优秀的工作。除了不要脸地将自己的模型称为「游戏引擎」外，这篇工作在宣传时还算克制，对模型的能力没有太多言过其实的描述。</p>
<p>如果不懂相关技术的话，外行人很容易对这篇工作的应用前景产生一些不切实际的幻想。在这篇文章中，我将完整而清晰地介绍这篇工作的内容，再给出我从游戏开发方面和科研方面对这篇工作的评价。</p>
<h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>看完了这篇工作的展示视频，大家第一个想问的问题一定是：这个模型的输入是什么？是只能随机生成游戏视频，还是能够根据用户的输入来生成后续内容？</p>
<p>答案是，模型可以根据之前的游戏进度及当前的用户输入，输出下一帧的游戏图片。我们来详细看一下该工作对于「游戏」的定义。</p>
<p>论文的第二章详细定义了该工作要完成的任务。作者认为，一个游戏可以由<strong>游戏状态</strong>、<strong>游戏画面</strong>、<strong>玩家操作集</strong>这三类信息组成。游戏状态包括角色血量、装备、地图、敌人等所有影响游戏进程的信息；游戏画面就是游戏屏幕上显示的二维图片；玩家操作集就是移动、射击等玩家所有可能进行的操作。此外，为了让游戏运行，还需要两类游戏机制：如何根据游戏状态生成当前游戏画面的<strong>渲染机制</strong>；如何根据游戏状态和当前玩家操作更新下一时刻游戏状态的<strong>逻辑机制</strong>。</p>
<p>而用一个生成模型来模拟游戏时，我们不需要让模型学会游戏机制、状态，只需要让模型<strong>根据之前所有的画面和玩家操作，以及当前时刻的玩家操作，输出当前时刻的画面</strong>。这样，我们给第一个初始帧和操作，模型就能输出第二帧；给前两帧和之前及现在的操作，模型就能输出第三帧……。也就是说，模型以自回归的方式模拟游戏画面。</p>
<blockquote>
<p>给想读这篇论文的读者一点阅读上的提示。论文以「交互环境」$\mathcal{E}$来指代我上文中的「游戏」，以 「交互世界模拟」$q$ 来指代生成模型。在定义 $\mathcal{E}$ 用到的几个字母对应我前文的粗体名词。</p>
</blockquote>
<p>在我看来，这篇工作的主要贡献，就是把游戏模拟任务以如此简明的形式清楚定义了出来。怎么把任务做好，纯粹只是工程实现问题。明确了任务后，相关领域的科研人员基本上能猜出这篇工作是怎么实现的了。下面，我们就来看一下论文中分享的实现过程。</p>
<h2 id="GameNGen"><a href="#GameNGen" class="headerlink" title="GameNGen"></a>GameNGen</h2><h3 id="用强化学习造数据"><a href="#用强化学习造数据" class="headerlink" title="用强化学习造数据"></a>用强化学习造数据</h3><p>本工作的任务是根据某些信息生成下一帧的画面，这种生成某类图像的任务用日益成熟的图像扩散模型技术就能解决了。但是，为了训练扩散模型，本工作有一道跨不过的坎——缺乏游戏画面数据。</p>
<p>为了生成足够多的图片，作者利用强化学习训练了一个玩游戏的 AI。在这一块，作者用了一个非常巧妙的设计：和其他强化学习任务不同，这个玩游戏的 AI 并不是为了将游戏漂亮地通关，而是造出尽可能多样的数据。因此，该强化学习的奖励函数包括了击中敌人、使用武器、探索地图等丰富内容，鼓励 AI 制造出不同的游戏画面。</p>
<h3 id="带约束图像扩散模型"><a href="#带约束图像扩散模型" class="headerlink" title="带约束图像扩散模型"></a>带约束图像扩散模型</h3><p>有了数据后，问题进一步缩小：现在该怎么用 Stable Diffussion 这个比较成熟的图像生成模型来根据<strong>之前的图片</strong>和<strong>之前及当前的操作</strong>来生成图片。</p>
<p>「之前的图片」和「之前及现在的操作」是输入给图像生成模型的两类额外信息。用专业术语来说，它们是给一个随机生成图像的模型的<strong>约束</strong>条件，用于让模型的输出不那么随机。而带约束图像生成也是一个被研究得比较透的任务了。</p>
<blockquote>
<p>以下内容是写给相关科研者看的，看不懂可以跳过。</p>
</blockquote>
<p>先谈怎么让模型约束于之前的图片。本文参考了经典的 Cascaded Diffusion 实现图像约束：将之前的图片过 VAE 编码器，与扩散模型原来的噪声输入拼接。然而，这种约束方式存在分布不匹配的问题：训练时，图像约束来自训练集；推理时，图像约束来自于模型之前自回归的生成结果。为了填平两类图像在分布上的差异，我们需要给所有约束图像加噪，并把加噪程度当成额外约束输入进模型。这种图像约束方法和图生视频的 Stable Video Diffusion 是一样的。</p>
<p>再看模型怎么约束于操作。每个操作都有独特的含义，它和语言中的单词是类似的。因此，我们可以把离散的操作变成嵌入向量，用 Stable Diffusion 处理单词的机制来处理操作。所以，在 GameNGen 的扩散模型中，文本约束没有了，被操作约束取代了。顺带一提，我们要输入之前及当前的操作，它们构成了一个操作序列。我们只要把这个操作序列当成由单词构成的文本，还是用原来 Stable Diffusion 那套处理文本的机制就行了。</p>
<p>此外，该工作还微调了 Stable Diffusion 的 VAE 的解码器，用以提升其在特定数据上的重建效果。这种操作也是比较常见的，比如 Stable Video Diffusion 将图像解码器微调成了视频解码器。</p>
<h2 id="实验及结果"><a href="#实验及结果" class="headerlink" title="实验及结果"></a>实验及结果</h2><p>本工作仅对图像约束用了强度为 1.5 的 Classifier-Free Guidance (CFG)，没有对操作约束加 CFG。采样用的是 DDIM 采样器，实验表明 4 步采样的结果就足够好了。在单个 TPU-v5 上，模拟模型每秒能渲染 20 帧画面。</p>
<p>扩散模型基于 Stable Diffusion V1.4 训练。输入图像分辨率为 $320 \times 256$。之前图像和操作的上下文窗口长度为 $64$。训练集包含 900M 张图片，要用 128 块 TPU-v5e 训练 700,000 步。</p>
<p>论文还评价了模型的生成质量。由于训练集能够提供当前要预测的帧的真值，因此我们可以用重建误差来反映模型的质量。具体来说，论文展示了表示图像相似度的 PSNR 和图像感知误差的 LPIPS。 重建质量通过两个任务来反映：</p>
<ul>
<li>图像质量：输入之前图像、操作均为数据集（按理说是测试集而不是训练集）里的真值，仅评测当前帧。此时重建质量较好，平均 PSNR 为 29.43，和一般图像经 JPEG 压缩产生的损耗相近。 </li>
<li>视频质量：给定初始图像和数据集里的之前操作，让模型自回归地生成一段视频。这种情况下每一帧的质量会逐渐降低，如下所示。</li>
</ul>
<p><img src="/2024/09/03/20240829-GameNGen/1.jpg" alt></p>
<p>除此之外，论文还展示了人类的评估结果：给出两段长度为 1.6 秒或 3.2 秒的游戏视频，分别来自真实游戏和 AI 生成，请人类分辨哪段视频是 AI 生成的。对于 1.6 秒的视频，正确率为 <code>58%</code>；对于 3.2 秒的视频，正确率为 <code>60%</code>。</p>
<blockquote>
<p>此类评测的最低（最优）正确率是 <code>50%</code>，因为我们总是可以随便猜一个。</p>
</blockquote>
<h2 id="论文总结与评价"><a href="#论文总结与评价" class="headerlink" title="论文总结与评价"></a>论文总结与评价</h2><h3 id="信息整理"><a href="#信息整理" class="headerlink" title="信息整理"></a>信息整理</h3><p>到目前为止，我已经客观介绍了论文中展示的内容。让我用外行人也能看懂的语言总结一下：</p>
<p>本工作提出的 GameNGen 模型可以根据之前的游戏画面、用户的历史操作、当前操作，在完全不了解游戏机制的前提下，生成包含游戏逻辑（血量、弹药）的当前帧画面。生成画面的模型是一个深度学习模型。因此，需要一个包含了过往操作、真实游戏画面的大型数据集来训练该模型。本工作利用强化学习制造了大量数据。训练用了 128 张 TPU-v5e 计算卡。该生成模型能在单张 TPU-v5 (专门用来做深度学习的计算「显卡」，只租不卖，比价值 25000 美元的 H100 显卡要好) 上能以 20 帧每秒的速度生成画面。模型最多能记住 64 帧，即 3.2 秒内的游戏信息。若将 3.2 秒的生成视频与真实游戏视频比较，人类仅有 60% 的概率分辨出 AI 生成的视频。当然，利用自回归技术反复输入之前生成的画面，模型也能够生成更长的视频，正如本文开头所展示的那个视频。</p>
<h3 id="论文中提及的缺陷"><a href="#论文中提及的缺陷" class="headerlink" title="论文中提及的缺陷"></a>论文中提及的缺陷</h3><p>哪怕不懂深度学习，大家也可以根据上述信息，提出自己的看法。当然，在那之前，我们先看一下作者在论文里是怎么描述模型的缺陷的。</p>
<p>作者讲到，GameNGen 受制于有限的记忆。模型仅能获取三秒之短的历史信息，却能在极其长的时间里保持游戏逻辑稳定，这很了不起（怎么讲着讲着又开始夸起自己了？）。但由于模型学到了太多东西，模型会创造短期记忆处理不了的场景，比如角色在某处打倒了敌人，数分钟后又返回打倒了敌人的地方（文章讲了半天讲不出个东西，我替他们总结了一下）。对于这些问题，以当前的模型架构，再怎么加大记忆窗口也无济于事。</p>
<h3 id="面向交互视频游戏的新范式"><a href="#面向交互视频游戏的新范式" class="headerlink" title="面向交互视频游戏的新范式"></a>面向交互视频游戏的新范式</h3><p>基于这篇工作的成果，作者还展望了未来：（以下是我对原文的精心翻译，欢迎对比）</p>
<p>如今，视频游戏是靠人类编程实现的。而 GameNGen 表明了以神经网络的权重来描述游戏这种新范式的部分可行性。GameNGen 展示了，在现有硬件上用神经网络模型来高效地交互运行一个复杂的游戏 （DOOM）不是奢望，这样的模型架构与模型权重是存在的。尽管还有很多重要问题要解决，我们希望这种范式能够造福众生。比如说，用这种新范式开发视频游戏的代价可能更小，门槛更低，借此，我们或许能仅通过编辑文本描述或示例图片来开发游戏。这个愿景的一小部分，即在现有游戏上略作调整或创造新行为，也许会在不久的将来就能实现。比如即使拿不到作者的源代码，我们也可能可以把几段游戏截图变成一个可玩的游戏关卡或者基于现有图像创建新角色。这个范式还能带来其他好处，比如严格控制帧率、内存占用不变。路漫漫其修远兮，尽管我们目前还没有探究这些方向，我们还是乐于尝试！希望我们这一小步，会在未来的某天，化为人们享受视频游戏的美好瞬间；甚至更进一步，化为人们使用交互软件系统的点点日常。</p>
<p>作为一个对这段抒情深有共鸣的人，在看论文前，我有千言万语想喷，却找不到切入点。看了这段话，我总算知道该怎么针对性地发表意见了。</p>
<h3 id="锐评"><a href="#锐评" class="headerlink" title="锐评"></a>锐评</h3><p>总有人说，AI 要取代人类了。</p>
<p>之前是说 AI 可以代替画师，又是说 AI 能代替人写小说。现在，来说 AI 能够完全模拟游戏了。</p>
<p>我真的很不解：人类的作品怎么会沦落到和现有 AI 的对比了？</p>
<p>我想了很久，为什么我无法容忍「现有 AI 技术能代替人类创作」这种观点。我的核心论点是：1）现有基于深度学习的 AI 无法达到和人同等的智力水平；2）达到人类同等的智力水平，意味着能够理解人类的行为，进而在创作、编程、教学、心理咨询等所有现在看来比较困难的领域看齐甚至超越人类。</p>
<p>详细对深度学习了解了一段时间后，绝大多数人都能推理出深度学习的上限。通过对深度学习应用的种种观察，我们能够用我们脑中的那个「神经网络」，那种基于数据推理现实的能力，预知深度学习的能力上限。深度学习适用于一些数据定义良好，目标定义良好的任务。只要给了数据，给了目标，网络就能学习，甚至涌现出一些意想不到的强大理解、生成能力。</p>
<p>但是，数据不是全部，永远有从大量数据学不到的东西。</p>
<p>那就是人心。</p>
<p>就和无数探讨机器人的作品所展示的一样。</p>
<p>正因为我们是人，所以我们能感知我们生活在这个世界上。我们感受痛苦，所以我们思考，并追逐美好。</p>
<p>无论是穷尽多少数据，复读多少经书也体会不到的；无论是洞察多少规律，拟合多少逻辑也推理不出的，就是人心。</p>
<p>和人心等价的一切任务，是现在的深度学习 AI 做不到的。</p>
<p>有关深度学习是否能达到人类水平，那是技术讨论，我们在别处再谈。如果最终能认同目前深度学习无法达到人类水平，但目前认为「现有 AI 技术能代替人类创作」的话，简单来看有两种可能：</p>
<ol>
<li><p>不懂深度学习</p>
</li>
<li><p>不懂创作的难度</p>
</li>
</ol>
<p>而对于一个相关专业人士来说，只能是第二种可能了。所以听到懂深度学习的人讲出「AI 要代替人类」时，我都会下意识地认为他在贬低人类创作的含金量，自然是怒火中烧。不懂创作，就不要妄加评论。</p>
<p>除此之外，还有人明明知道当前 AI 的实力，还要违心地宣传 AI 如何如何，宣传自己的垃圾工作多么有价值。这种人只会为自己的利益考虑，纯纯的坏而已。这种无可救药的坏连讨论的价值也没有。</p>
<p>最后还有一种可能，很多人做研究时，并不会像我这样想这么多。在他们看来，科研就是从现有知识出发，朝外迈一小步。不管这一步是否方向正确，不管这一步有多么小，只要是拓宽人类的知识边界，那就是好的。他们的研究是纯洁的、无私的，不在意有生之年能否看到自己的成果被用上，甚至不在意自己的研究是否真的会被用到，只是为了科研增砖添瓦而已。我不得不承认，这是真正的、高尚的科研。</p>
<p>他们是幸福的，可以不在意眼前的得失。所以，他们可以望着远处高峰，轻松而豪放地说出：「希望我的研究，能化为人们享受视频游戏的美好瞬间」。</p>
<p>然而，这句话，对我而言，是沉重的。如果这种话是我说出来的，那么它不会是期盼，而是矢志不渝的誓言。不是拿着望远镜向远处眺望，不是用手指着地图挥斥方遒，而是用我的脚，一步，一步，踏出来的。</p>
<p>我是各种作品的鼓舞下走过来的，优秀的作品对我而言是神圣的。所以，我希望立刻，亲眼见到更多的好作品。没有对好作品急功近利的渴望，也就说明他们生活中有更加便捷的能量来源。所以我说，真正能以纯洁的心做科研的人，是幸福的人。</p>
<p>把我这些话总结一下，能认为深度学习能代替人类创作游戏，要么是深度学习的信徒，要么是不懂深度学习，要么是不懂创作，要么是坏，要么就是没想那么多觉得有新科研工作就是好事。</p>
<p>有人可能还会说：「我也同意深度学习代替不了人类，但也不能说这些技术就完全没用」。这我非常同意，我就认为大家应该把现在的 AI 当成一种全新的工具。基于这些新工具，我们把创新的重点放在如何适配这些工具上，辅助以前的应用，或者开发一些新的应用，而不是非得一步到位直接妄想着把人类取代了。比如，根据简笔画生成图片就是一个很好的新应用啊。</p>
<p>回到这篇文章的锐评上来。一上来，标题就写着《扩散模型是实时游戏引擎》。其实这是一个在顶级计算卡上每秒生成 20 张低分辨率图片的模型，这真的是我们认为的实时吗？作为一个游戏引擎，你能修改游戏机制吗？哦，我对引擎的理解有误，这不是游戏开发引擎，而是一个运行游戏的引擎。那踏踏实实叫做「游戏模拟器」不好吗？标题取得夸张一点，想吸引大家注意，能够理解，不多讲了。</p>
<p>文章主体部分都是客观陈述，写得非常清晰，我读起来也很舒服。本来都准备把「锐评」改成「简述」的，看完作者最后那段对未来的畅想后，一阵无名火在我心中燃起。通过「我们或许能仅通过编辑文本描述或示例图片来开发游戏」这段话，我感觉这是作者是几个对游戏制作质量没有那么高要求的人。可是，就是这样的人，却能写出「路漫漫其修远兮，尽管我们目前还没有探究这些方向，我们还是乐于尝试！」这说明他们可能是真心热爱游戏的玩家。那为什么，为什么只做出了这种程度的工作呢？900M 张图片，128 块卡，别说爱好者，就是一般的大学实验室，都难以跟进这篇工作，这是想要给设计师、爱好者开发新工具的态度吗？没有其他更加贴近用户的项目了吗？好，你说你们以长期的科研为主，这只是这个方向的初步尝试，你们重心在科研上而不是提供游戏开发工具上。那你们是抱着多大的觉悟说出「希望我们这一小步，会在未来的某天，化为人们享受视频游戏的美好瞬间」的？给人的感觉就是一群深居象牙塔的人，一辈子也不去了解业界真的需要什么，只是「正确地」做着科研而已。</p>
<p>非常抱歉，以上都是我的主观评价，请恕我对作者的想法妄加猜测。看完文章最后那段话后，我就有了这样一种矛盾的愤怒感。我喷了这么多，其实也不是想喷这篇工作的作者，更想批判的，是我长年以来在生活中的见闻。把气撒到这篇工作上，可能只是我嫉妒他们，没有 128 块卡去做想做的事情而已。</p>
<p>但我毫不怀疑地相信，我要有的资源最后都会有的。「游戏开发的新范式」、「造福众生」、「今天的一小步」……，如果有一天我说出了这些话，那必然不是在论文里，而是在我的产品得到了用户的充分肯定后，向世界吹嘘的胜利宣言吧。</p>
<h2 id="新科研方向的讨论"><a href="#新科研方向的讨论" class="headerlink" title="新科研方向的讨论"></a>新科研方向的讨论</h2><p>先谈一下这篇工作在科研上给我们的启发。我认为有三点：</p>
<ol>
<li>对于深度学习应用来说，不要去在意功能有多么异想天开，只要把问题定义好，数据准备好，问题就可解。</li>
<li>可以以用户操作为约束，用生成模型建模一个可交互世界。这个「用户操作」不一定局限于游戏玩家的操作。</li>
<li>强化学习可以用来造大批数据。并且，我们需要精心设计模型的学习目标，使其造出多样的数据。</li>
</ol>
<p>再来看顺着这篇文章的结果，我们能有怎样的新思考。拿图像生成模型这种结果极不稳定的东西做要求输入输出可控的游戏是不可能的。但是，我们应该把思路逆转过来：哪些任务可以以不确定的图像为输入？最容易想到的是缺数据的自动驾驶任务。也就是说，这篇工作实际上是提出了一种带交互的图像数据生成器。</p>
<p>这篇工作用图像模型学习了 3D 场景在移动后的变化。也就是说，模型「理解」了 3D 场景。那么，有没有办法从模型中抽取出相关的知识呢？按理说，能理解 3D，就能理解物体是有远近的。那么，深度估计、语义分割这种任务是不是可以直接用这种模型来做呢？以交互为约束的图像生成模型可能蕴含了比文生图模型更加丰富的图像知识。很可惜，不知道这篇工作最后会不会开源。</p>
<p>如果一个模型能够建模一个简单的世界，我们下一步要思考的是怎么编辑这个世界。就像有了图像生成模型，我们要给它加上文本约束一样。最容易想到的是以二维图片为约束，生成一个世界里的三维物体。但目前这个模型要的训练量太大了，做这种新实验的代价根本不敢想。</p>
<p>当前这个模型的结果还是比较弱的。别看这里用了扩散模型，模型的训练目标实际上是一个重建任务而非生成任务，最后的评测指标也是重建误差而没有考虑 FID 等图像生成质量指标。有没有办法让模型设法输出有多样性的新内容？还有，这个模型在设计上应该是一个自回归模型，只不过下一张图片是用扩散模型隐式建立了概率分布。但由于用户操作是在线而不是离线一次性给出，这种任务在时序上没办法用视频扩散模型建模。所以，从本质上看，要设计带用户操作的生成模型，其实是要一种时序信息在线输入的生成模型。除了自回归模型外，能不能用一种全新的生成范式呢？一旦有了这样一种新式模型，训练交互世界神经网络的代价将大幅降低。</p>
<p>除了时序信息不好输入外，这个任务还面临另一个问题：随着时间不断推移，模型会忘记以前的内容。NLP 领域通过 Transformer 全局信息交互暴力地解决了这类问题，但在长视频生成中，这种问题还是没能很好解决。或许需要其他领域为长时序建模提供了更好的工具后，才能考虑长时间的世界交互。但我们也可以从另一个角度思考：把要模拟的世界简化，不要去做模拟游戏那么难的任务，做一个时序依赖少的模拟任务。</p>
<p>结束科研新方向的思考前，我再对这个模型的训练量喷两句。搞大批数据，搞大模型，一堆 GPU 狂训，强行拟合一个任务，在我看来是非常不优雅的做法。得不到学术界广泛跟进的工作，是发展不下去的。在思考这种世界模拟模型的种种应用前，最重要的还是把训练量降下去。我认为优化在线时序约束生成模型是最有价值的方向。</p>
<p>最后我再从游戏开发的角度讲一下这篇工作的启发。我们只讨论如何用生成模型减少美术工作量，不去探究怎么让模型学习设计复杂的游戏机制。</p>
<p>真的想要为游戏开发提供工具的话，应该从 2D 游戏而不是 3D 游戏入手。拟合 2D 游戏画面的训练代价较小，且 2D 游戏对开发者来说更容易做。仅基于现在的 AI 技术，我们就已经能够做出一些简单的美术生成应用了。比如前段比较火的植物大战僵尸杂交版，我们完全可以想办法定义一个科研任务，训练一个融合两种植物的生成模型。明明有更切实际的 2D 不做去做 3D，这也是我为什么觉得这篇工作在为游戏开发提供工具这个层面上显得非常没有诚意。这篇文章建模的 3D 世界模拟器其实更适合前面提到的自动驾驶等真实场景，反而不是很适合游戏开发。</p>
<p>从这篇工作出发，或许可以联想出非常多游戏开发应用。这里我随便举一个 2D 网格地图生成任务。用同一种地板拼接地形时，根据地板的连通情况，游戏开发引擎会自动生成完整地形。但是，这种生成是基于写死的规则的，且最终每种地形还是以正方形网格为单位呈现。能不能用生成模型让这种地形生成更加多样呢？</p>
<p><img src="/2024/09/03/20240829-GameNGen/2.jpg" alt></p>
<p>从本文的启发 2 出发，我们或许可以有一些新想法：可以以操作为约束，用生成模型建模一个可交互世界。那么，我们将「操作」定义为设计师向地图上铺上一块地板，而不是定义为玩家的操作。这样，模型就能站在设计师的角度的学习生成地图了。</p>
<p>如果数据足够的话，这种定义方式一定能让神经网络学会地形生成的。问题的关键就在于如何获取数据。根据玩家的操作，我们能够自然生成大量游戏数据。而根据设计师的每一步想法，每一步都绘制合理且高质量的图片，可能要花费大量资源。因此，如果是从这个角度出发，需要大量美术、游戏设计、深度学习的相关人员参与进来。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>GameNGen 将模拟 3D 可交互场景的任务定义为根据历史画面、历史及当前操作生成当前画面的带约束图像生成任务。该工作用强化学习巧妙造出大量数据，用扩散模型实现带约束图像生成。结果表明，该模型不仅能自回归地生成连贯的游戏画面，还能学会子弹、血量等复杂交互信息。然而，受制于硬件及模型架构限制，模型要求的训练资源极大，且一次只能看到 3.2 秒内的信息。这种大量数据驱动的做法难以在学校级实验室里复刻，也不能够归纳至更一般的 3D 世界模拟任务上。</p>
<p>我个人认为，从科研的角度来看，这篇工作最大的贡献是提出了一种用带约束图像生成来描述 3D 世界模拟任务的问题建模方式。其次的贡献是确确实实通过长期的工程努力把这个想法做成功了，非常不容易。但从游戏开发的角度来看，这个工作现阶段没什么用处。</p>
<p>从科研启发的角度思考，这篇工作告诉我们，定义好交互世界里的操作，我们就能部分地用图像生成模型建模一个交互世界。从本质上来看，这是一个每一个时刻的约束都在线给出的视频生成任务。针对这个任务，我们既可以去思考能否用自回归以外的更高效的方式来实现它，也可以去思考是否可以修改对于「操作」的定义来实现模拟玩家操作以外的世界模拟任务。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/09/03/20240809-flux1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/09/03/20240809-flux1/" class="post-title-link" itemprop="url">Stable Diffusion 3「精神续作」FLUX.1 源码深度前瞻解读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-09-03 14:41:19" itemprop="dateCreated datePublished" datetime="2024-09-03T14:41:19+08:00">2024-09-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">知识整理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>前几个月，推出了著名文生图模型 Stable Diffusion 的 Stability AI 公司曝出了核心团队集体离职的消息。一时间，AI 从业者们议论纷纷，不知道这究竟是团队出现了矛盾，还是这些员工觉得文生图模型做下去没有前途了。而近期，该核心团队重新组建的创业团队 Black Forest Labs（黑森林实验室）带着名为 FLUX.1 的文生图模型「复仇归来」。FLUX.1 受到了用户的广泛好评，让人们期盼更强开源文生图模型的热情得以延续。</p>
<p>Black Forest Labs 的成员基本上都是 Stable Diffusion 3 的作者，其中三名元老级成员还是 Stable Diffusion 论文的作者。同时，FLUX.1 也是一个在 Stable Diffusion 3 架构上做改进的模型。不管从哪个角度，FLUX.1 都称得上是Stable Diffusion 3 的「精神续作」。秉承着此前的开源精神，FLUX.1 也在上线之始就为社区开放了源代码和模型权重。不过，配套的技术论文并没能及时发布，想要了解 FLUX.1 技术细节的用户恐怕还得等上一阵子。为了尽快搞清楚 FLUX.1 相较 Stable Diffusion 3 做了哪些改进，我直接去细读了 FLUX.1 的源码。在这篇文章中，按照惯例，我将主要从源码层面上分享 FLUX.1 中已知的科研创新，做一个官方论文发布前的前瞻解读，而不会评测 FLUX.1 的图像生成效果。</p>
<p>具体来说，我会介绍 FLUX.1 中的以下改动：</p>
<ul>
<li>略微变动的图块化策略</li>
<li>不使用 Classifier-Free Guidance 的指引蒸馏</li>
<li>为不同分辨率图像调整流匹配噪声调度</li>
<li>用二维旋转式位置编码 (RoPE) 代替二维正弦位置编码</li>
<li>在原 Stable Diffusion 3 双流 Transformer 块后添加并行单流 Transformer 块</li>
</ul>
<p>我会先简单介绍 FLUX.1 的官方公告及 Diffusers 版使用示例，再按照我读代码的逻辑，从熟悉整个代码框架，到深究每一处创新的代码细节，最后分享我对于 FLUX.1 科研改进上的分析。对源码不感兴趣的读者，可以跳过通读代码框架章节，或者直接阅读感兴趣的那部分改动。想看省流版文章的读者，可以直接跳到结尾看总结。</p>
<p>建议读者在学习 Flux.1 前熟悉 Stable Diffusion 3。欢迎参考我之前写的文章：Stable Diffusion 3 论文及源码概览。</p>
<h2 id="模型简介与-Diffusers-示例脚本"><a href="#模型简介与-Diffusers-示例脚本" class="headerlink" title="模型简介与 Diffusers 示例脚本"></a>模型简介与 Diffusers 示例脚本</h2><p>在正式阅读源码前，我们先来看一下官方推文（<a target="_blank" rel="noopener" href="https://blackforestlabs.ai/announcing-black-forest-labs/">https://blackforestlabs.ai/announcing-black-forest-labs/</a> ）中有关 FLUX.1 的简介，并在 Diffusers 中跑通 FLUX.1 的图像生成示例脚本。</p>
<p>据官方介绍，FLUX.1 是一套文生图模型。它有三个变体（variant，可以理解成结构相似或相同，但权重不同的几个模型）：</p>
<ul>
<li>FLUX.1 [pro]: FLUX.1 系列的最强模型，只能通过付费的 API 或者在线平台使用。</li>
<li>FLUX.1 [dev]：FLUX.1 [pro] 的指引蒸馏（guidance-distilled）模型，质量与文本匹配度与原模型相近，运行时更高效。</li>
<li>FLUX.1 [schnell]：为本地开发和个人使用而裁剪过的本系列最快模型。据 Diffusers 中的文档介绍，这是一个 Timestep-distilled（时间戳蒸馏）的模型，因此仅需 1~4 步就可以完成生成。无法设置指引强度。</li>
</ul>
<p>官方对这些模型的详细介绍少之又少。FLUX.1 [dev] 用到的指引蒸馏技术似乎来自论文 <em>On Distillation of Guided Diffusion Models</em>，其目标是让模型直接学习 Classifier-Free Guidance (CFG) 的生成结果，使得模型一次输出之前要运行两次才能得到的指引生成结果，节约一半的运行时间。官方也没有讲 FLUX.1 [schnell] 的蒸馏细节，似乎它是从 FLUX.1 [dev] 中用扩散模型加速蒸馏手段得到的模型。因此，FLUX.1 [schnell] 不仅能一次输出有指引的生成结果，还能在极少的采样步数里完成生成。</p>
<p>官方推文中还说，FLUX.1 的生成神经网络基于 Stable Diffusion 3 的 MMDiT 架构和并行的 DiT 块，参数量扩大至 120 亿。生成模型是根据流匹配（flow matching）推导的扩散模型。为了提升性能与效率，模型新引入了旋转式位置编码 (RoPE) 和并行注意力层。</p>
<p>这段话这么长，还把并行注意力说了两遍，其实没有多少新东西。说白了，FLUX.1 就是在 Stable Diffusion 3 的基础上，加了 RoPE 和并行注意力层。官方推文到这里就没有其他有关模型细节的介绍了。FLUX.1 具体做了哪些改动，我们直接去源码里看。</p>
<p>FLUX.1 的官方仓库是 <a target="_blank" rel="noopener" href="https://github.com/black-forest-labs/flux">https://github.com/black-forest-labs/flux</a> 。相比 Stable Diffusion 那个臃肿杂乱的 generative-models 仓库，这个仓库的代码要简洁很多。不过，我还是推荐使用 Diffusers 框架来运行 FLUX.1。</p>
<p>Diffusers 中运行 FLUX.1 的官方文档为 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux">https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux</a> 。目前（2024 年 8 月 11 日），相关代码还在 Diffusers 的在线主分支里进行开发，并没有集成进 pip 版的 Diffusers 里。因此，要在 Diffusers 中使用 FLUX，必须要从源码安装 Diffusers：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/huggingface/diffusers.git</span><br><span class="line">cd diffusers</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure>
<p>安装完毕后，我们可以随便新建一个 python 脚本，填入以下的官方示例代码。在能够连通 Hugging Face 的环境中运行此脚本的话，脚本会自动下载模型并把生成结果保存在 <code>image.png</code> 中。注意，FLUX.1 的神经网络很大，显存占用极高，可能至少需要在 RTX 3090 同等级的显卡上运行。在示例代码中，我还改了一行，使用 <code>pipe.enable_sequential_cpu_offload()</code> 让模型把更多参数临时放到 CPU 上，避免显存不够。经测试，改了这一行后，FLUX.1 才勉强能在显存为 24G 的 RTX 3090 上运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> diffusers <span class="keyword">import</span> FluxPipeline</span><br><span class="line"></span><br><span class="line">pipe = FluxPipeline.from_pretrained(<span class="string">&quot;black-forest-labs/FLUX.1-schnell&quot;</span>, torch_dtype=torch.bfloat16)</span><br><span class="line"><span class="comment"># pipe.enable_model_cpu_offload()</span></span><br><span class="line">pipe.enable_sequential_cpu_offload()</span><br><span class="line"></span><br><span class="line">prompt = <span class="string">&quot;A cat holding a sign that says hello world&quot;</span></span><br><span class="line">image = pipe(</span><br><span class="line">    prompt,</span><br><span class="line">    guidance_scale=<span class="number">0.0</span>,</span><br><span class="line">    num_inference_steps=<span class="number">4</span>,</span><br><span class="line">    max_sequence_length=<span class="number">256</span>,</span><br><span class="line">    height=<span class="number">1024</span>,</span><br><span class="line">    width=<span class="number">1024</span>,</span><br><span class="line">    generator=torch.Generator(<span class="string">&quot;cpu&quot;</span>).manual_seed(<span class="number">0</span>)</span><br><span class="line">).images[<span class="number">0</span>]</span><br><span class="line">image.save(<span class="string">&quot;image.png&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>由于随机数是固定的，运行后，我们应该总能得到这样的图片：</p>
<p><img src="/2024/09/03/20240809-flux1/1.jpg" alt></p>
<h2 id="通读代码框架"><a href="#通读代码框架" class="headerlink" title="通读代码框架"></a>通读代码框架</h2><p>由于开发还没有结束，在当前 Diffusers 的 FLUX.1 源码中，我们能看到各种潦草的写法及残缺不全的文档，这让读源码变成了一项颇具趣味的挑战性任务。让我们先看一下代码的整体框架，找出 FLUX.1 相较 Stable Diffusioni 3 在代码上的改动，再来详细分析这些创新。</p>
<p>和 Diffusers 中的其他生成模型一样，FLUX.1 的采样算法写在一个采样流水线类里。我们可以通过示例脚本里的 <code>FluxPipeline</code> 类跳转到定义该类的文件 <code>diffusers/pipelines/flux/pipeline_flux.py</code> 里。这个文件是从 Stable Diffusion 3 的采样流水线文件 <code>diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py</code> 改过来的，大部分文档都没有更新。我们可以用肉眼对比两份文件的区别。</p>
<p>先看构造函数。Stable Diffusion 3 用了三个文本编码器，<code>clip-vit-large-patch14</code>, <code>CLIP-ViT-bigG-14-laion2B-39B-b160k</code>, <code>t5-v1_1-xxl</code>，而 FLUX.1 没有用第二个 CLIP 编码器，只用了另外两个文本编码器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StableDiffusion3Pipeline</span>(<span class="params">DiffusionPipeline, SD3LoraLoaderMixin, FromSingleFileMixin</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        transformer: SD3Transformer2DModel,</span></span></span><br><span class="line"><span class="params"><span class="function">        scheduler: FlowMatchEulerDiscreteScheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">        vae: AutoencoderKL,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder: CLIPTextModelWithProjection,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder_2: CLIPTextModelWithProjection,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer_2: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder_3: T5EncoderModel,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer_3: T5TokenizerFast,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FluxPipeline</span>(<span class="params">DiffusionPipeline, FluxLoraLoaderMixin</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        scheduler: FlowMatchEulerDiscreteScheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">        vae: AutoencoderKL,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder: CLIPTextModel,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer: CLIPTokenizer,</span></span></span><br><span class="line"><span class="params"><span class="function">        text_encoder_2: T5EncoderModel,</span></span></span><br><span class="line"><span class="params"><span class="function">        tokenizer_2: T5TokenizerFast,</span></span></span><br><span class="line"><span class="params"><span class="function">        transformer: FluxTransformer2DModel,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br></pre></td></tr></table></figure>
<p>再往下翻，我们能用火眼金睛发现 FLUX.1 的 VAE 压缩比是 16，是所有版本的 Stable Diffusion VAE 压缩比的两倍。这是为什么呢？不是增加压缩比会让 VAE 重建效果下降吗？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SD3</span></span><br><span class="line">self.vae_scale_factor = (</span><br><span class="line">    <span class="number">2</span> ** (<span class="built_in">len</span>(self.vae.config.block_out_channels) - <span class="number">1</span>) </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;vae&quot;</span>) <span class="keyword">and</span> self.vae <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">8</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FLUX.1</span></span><br><span class="line">self.vae_scale_factor = (</span><br><span class="line">    <span class="number">2</span> ** (<span class="built_in">len</span>(self.vae.config.block_out_channels)) </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;vae&quot;</span>) <span class="keyword">and</span> self.vae <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="number">16</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>查看周围其他代码，我们能找到 <code>_pack_latents</code>，<code>_unpack_latents</code> 这两个方法。<code>_pack_latents</code> 其实就是一个图块化操作，它能把 $2 \times 2$ 个像素在通道维度上拼接到一起，而 <code>_unpack_latents</code> 是该操作的逆操作。原来，代码把图块化的两倍压缩比也算进 VAE 里了。这里直接把 <code>vae_scale_factor</code> 乘个 2 是一种非常差，歧义性极强的写法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_pack_latents</span>(<span class="params">latents, batch_size, num_channels_latents, height, width</span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_unpack_latents</span>(<span class="params">latents, height, width, vae_scale_factor</span>):</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>相比 SD3, FLUX.1 将图块化操作写在了去噪网络外面。因此，SD3 的去噪网络的输入通道数是 16，和 VAE 的隐空间通道数相同；而 FLUX.1 由于把 $2 \times 2$ 个像素在通道上拼接到了一起，其去噪网络的输入通道数是 64。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;SD3Transformer2DModel&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;in_channels&quot;</span>: <span class="number">16</span>,</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;_class_name&quot;</span>: <span class="string">&quot;FluxTransformer2DModel&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;in_channels&quot;</span>: <span class="number">64</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>再来看采样主方法 <code>__call__</code>。先看一下它的主要参数。相比之下，FLUX.1 少了一组提示词，且没有负面提示词。少一组提示词是因为少用了一个文本编码器。而没有负面提示词是因为该模型是指引蒸馏过的，在文本指引上没那么灵活。我们稍后还会看到 FLUX.1 具体是怎么利用文本指引的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SD3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt_2: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt_3: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">28</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">List</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    guidance_scale: <span class="built_in">float</span> = <span class="number">7.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt_2: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    negative_prompt_3: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"># <span class="title">FLUX</span>.1</span></span><br><span class="line"><span class="function"><span class="title">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    prompt_2: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">str</span>]]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    height: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    width: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="number">28</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">List</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    guidance_scale: <span class="built_in">float</span> = <span class="number">7.0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>)</span></span><br></pre></td></tr></table></figure>
<p>之后的内容都与其他扩散模型流水线一样，代码会判断输入是否合法、给输入文本编码、随机生成初始化噪声。值得关注的是初始化噪声采样器前的一段新内容：代码会算一个 <code>mu</code>，并传进 <code>retrieve_timesteps</code> 里。这个变量最后会传到流匹配采样算法里。我们先把该改动记在心里，不看细节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mu = calculate_shift(</span><br><span class="line">    image_seq_len,</span><br><span class="line">    self.scheduler.config.base_image_seq_len,</span><br><span class="line">    self.scheduler.config.max_image_seq_len,</span><br><span class="line">    self.scheduler.config.base_shift,</span><br><span class="line">    self.scheduler.config.max_shift,</span><br><span class="line">)</span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(</span><br><span class="line">    self.scheduler,</span><br><span class="line">    num_inference_steps,</span><br><span class="line">    device,</span><br><span class="line">    timesteps,</span><br><span class="line">    sigmas,</span><br><span class="line">    mu=mu,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在去噪循环部分，FLUX.1 没有做 Classifier-Free Guidance (CFG)，而是把指引强度 <code>guidance</code> 当成了一个和时刻 <code>t</code> 一样的约束信息，传入去噪模型 <code>transformer</code> 中。CFG 的本意是过两遍去噪模型，一次输入为空文本，另一次输入为给定文本，让模型的输出远离空文本，靠近给定文本。而负面提示词只是一种基于 CFG 的技巧。把 CFG 里的空文本换成负面文本，就能让结果背离负面文本。但现在这个模型是一个指引蒸馏模型，指引强度会作为一个变量输入模型，固定地表示输入文本和空文本间的差距。因此，我们就不能在这个模型里把空文本换成负面文本了。</p>
<p>除了指引方式上的变动外，FLUX.1 的去噪网络还多了 <code>txt_ids</code> 和 <code>img_ids</code> 这两个输入。我们待会来看它们的细节。</p>
<p>FLUX.1 的去噪网络和 SD3 的一样，除了输入完整文本嵌入 <code>prompt_embeds</code> 外，依然会将池化过的短文本嵌入 <code>pooled_prompt_embeds</code> 输入进模型。我们现在可以猜测，FLUX.1 使用了和 SD3 类似的文本约束机制，输入了两类文本约束信息。</p>
<p>代码里的 <code>/1000</code> 是临时代码。之后所有涉及乘除 1000 的代码全可以忽略。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, t <span class="keyword">in</span> <span class="built_in">enumerate</span>(timesteps):</span><br><span class="line">    timestep = t.expand(latents.shape[<span class="number">0</span>]).to(latents.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># handle guidance</span></span><br><span class="line">    <span class="keyword">if</span> self.transformer.config.guidance_embeds:</span><br><span class="line">        guidance = torch.tensor([guidance_scale], device=device)</span><br><span class="line">        guidance = guidance.expand(latents.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        guidance = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    noise_pred = self.transformer(</span><br><span class="line">        hidden_states=latents,</span><br><span class="line">        timestep=timestep / <span class="number">1000</span>,</span><br><span class="line">        guidance=guidance,</span><br><span class="line">        pooled_projections=pooled_prompt_embeds,</span><br><span class="line">        encoder_hidden_states=prompt_embeds,</span><br><span class="line">        txt_ids=text_ids,</span><br><span class="line">        img_ids=latent_image_ids,</span><br><span class="line">        joint_attention_kwargs=self.joint_attention_kwargs,</span><br><span class="line">        return_dict=<span class="literal">False</span>,</span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the previous noisy sample x_t -&gt; x_t-1</span></span><br><span class="line">    latents_dtype = latents.dtype</span><br><span class="line">    latents = self.scheduler.step(noise_pred, t, latents, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>采样流水线最后会将隐空间图片解码。如前所述，由于现在图块化和反图块化是在去噪网络外面做的，这里隐空间图片在过 VAE 解码之前做了一次反图块化操作 <code>_unpack_latents</code>。对应的图块化操作是在之前随机生成初始噪声的 <code>prepare_latents</code> 方法里做的，为了节约时间我们就不去看了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> output_type == <span class="string">&quot;latent&quot;</span>:</span><br><span class="line">    image = latents</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)</span><br><span class="line">    latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor</span><br><span class="line">    image = self.vae.decode(latents, return_dict=<span class="literal">False</span>)[<span class="number">0</span>]</span><br><span class="line">    image = self.image_processor.postprocess(image, output_type=output_type)</span><br></pre></td></tr></table></figure>
<p>接下来，我们再简单看一下去噪网络的结构。在采样流水线里找到对应类 <code>FluxTransformer2DModel</code>，我们能用代码跳转功能定位到文件 <code>diffusers/models/transformers/transformer_flux.py</code>。SD3 去噪网络类是 <code>SD3Transformer2DModel</code>，它位于文件 <code>diffusers/models/transformers/transformer_sd3.py</code>。</p>
<p>同样，我们先对比类的构造函数。构造函数的新参数我们暂时读不懂，所以直接跳到构造函数内部。</p>
<p>在使用位置编码时，SD3 用了二维位置编码类 <code>PatchEmbed</code>。该类会先对图像做图块化，再设置位置编码。 而 FLUX.1 的位置编码类叫 <code>EmbedND</code>。从官方简介以及参数里的单词 <code>rope</code> 中，我们能猜出这是一个旋转式位置编码 (RoPE)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SD3</span></span><br><span class="line">self.pos_embed = PatchEmbed(</span><br><span class="line">    height=self.config.sample_size,</span><br><span class="line">    width=self.config.sample_size,</span><br><span class="line">    patch_size=self.config.patch_size,</span><br><span class="line">    in_channels=self.config.in_channels,</span><br><span class="line">    embed_dim=self.inner_dim,</span><br><span class="line">    pos_embed_max_size=pos_embed_max_size,  <span class="comment"># hard-code for now.</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FLUX.1</span></span><br><span class="line">self.pos_embed = EmbedND(dim=self.inner_dim, theta=<span class="number">10000</span>, axes_dim=axes_dims_rope)</span><br></pre></td></tr></table></figure>
<p>再往下看，FLUX.1 的文本嵌入类有两种选择。不设置 <code>guidance_embeds</code> 的话，这个类就是 <code>CombinedTimestepTextProjEmbeddings</code>，和 SD3 的一样。这说明正如我们前面猜想的，FLUX.1 用了和 SD3 一样的额外文本约束机制，将一个池化过的文本嵌入约束加到了文本嵌入上。</p>
<p>设置 <code>guidance_embeds</code> 的话，<code>CombinedTimestepGuidanceTextProjEmbeddings</code> 类应该就得额外处理指引强度了。我们待会来看这个类是怎么工作的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">text_time_guidance_cls = (</span><br><span class="line">    CombinedTimestepGuidanceTextProjEmbeddings <span class="keyword">if</span> guidance_embeds <span class="keyword">else</span> CombinedTimestepTextProjEmbeddings</span><br><span class="line">)</span><br><span class="line">self.time_text_embed = text_time_guidance_cls(</span><br><span class="line">    embedding_dim=self.inner_dim, pooled_projection_dim=self.config.pooled_projection_dim</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>之后函数定义了两个线性层。<code>context_embedder</code> 在 SD3 里也有，是用来处理文本嵌入的。但神秘的 <code>x_embedder</code> 又是什么呢？可能得在其他函数里才能知道了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.context_embedder = nn.Linear(self.config.joint_attention_dim, self.inner_dim)</span><br><span class="line">self.x_embedder = torch.nn.Linear(self.config.in_channels, self.inner_dim)</span><br></pre></td></tr></table></figure>
<p>函数的末尾定义了两个模块列表。相比只有一种 Transformer 块的 SD3，FLUX.1 用了两种结构不同的 Transformer 块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">self.transformer_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        FluxTransformerBlock(...)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.num_layers)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.single_transformer_blocks = nn.ModuleList(</span><br><span class="line">    [</span><br><span class="line">        FluxSingleTransformerBlock(...)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.config.num_single_layers)</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们再来看 <code>forward</code> 方法，看看之前看构造函数时留下的问题能不能得到解答。</p>
<p><code>forward</code> 里首先是用 <code>x_embedder</code> 处理了一下输入。原本在 SD3 中，输入图像会在 <code>pos_embed</code> 里过一个下采样两倍的卷积层，同时完成图块化和修改通道数两件事。而现在 FLUX.1 的图块化写在外面了，所以这里只需要用一个普通线性层 <code>x_embedder</code> 处理一下输入通道数就行了。这样说来，变量名有个 <code>x</code> 估计是因为神经网络的输入名通常叫做 <code>x</code>。既然这样，把它叫做 <code>input_embedder</code> 不好吗？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SD3</span></span><br><span class="line">hidden_states = self.pos_embed(hidden_states)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FLUX.1</span></span><br><span class="line">hidden_states = self.x_embedder(hidden_states)</span><br></pre></td></tr></table></figure>
<p>下一步是求时刻编码。这段逻辑是说，如果模型输入了指引强度，就把指引强度当成一个额外的实数约束，将其编码加到时刻编码上。具体细节都在 <code>time_text_embed</code> 的类里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">timestep = timestep.to(hidden_states.dtype) * <span class="number">1000</span></span><br><span class="line"><span class="keyword">if</span> guidance <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    guidance = guidance.to(hidden_states.dtype) * <span class="number">1000</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    guidance = <span class="literal">None</span></span><br><span class="line">temb = (</span><br><span class="line">    self.time_text_embed(timestep, pooled_projections)</span><br><span class="line">    <span class="keyword">if</span> guidance <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span> self.time_text_embed(timestep, guidance, pooled_projections)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>下一行是常规的修改约束文本嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder_hidden_states = self.context_embedder(encoder_hidden_states)</span><br></pre></td></tr></table></figure>
<p>再之后的两行出现了一个新操作。输入的 <code>txt_ids</code> 和 <code>img_ids</code> 拼接到了一起，构成了 <code>ids</code>，作为旋转式位置编码的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">image_rotary_emb = self.pos_embed(ids)</span><br></pre></td></tr></table></figure>
<p>此后图像信息 <code>hidden_states</code> 和文本信息 <code>encoder_hidden_states</code> 会反复输入进第一类 Transformer 块里。和之前相比，模块多了一个旋转式位置编码输入 <code>image_rotary_emb</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoder_hidden_states, hidden_states = block(</span><br><span class="line">    hidden_states=hidden_states,</span><br><span class="line">    encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">    temb=temb,</span><br><span class="line">    image_rotary_emb=image_rotary_emb,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>本来过了这些块后，SD3 会直接会直接返回 <code>hidden_states</code> 经后处理后的信息。而 FLUX.1 在过完第一类 Transformer 块后，将图像和文本信息拼接，又输入了第二类 Transformer 块中。第二类 Transformer 块的输出才是最终输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span>:</span><br><span class="line">    hidden_states = block(</span><br><span class="line">        hidden_states=hidden_states,</span><br><span class="line">        temb=temb,</span><br><span class="line">        image_rotary_emb=image_rotary_emb,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">hidden_states = hidden_states[:, encoder_hidden_states.shape[<span class="number">1</span>] :, ...]</span><br></pre></td></tr></table></figure>
<p>到这里，我们就把 FLUX.1 的代码结构过了一遍。我们发现，FLUX.1 是一个基于 SD3 开发的模型。它在图块化策略、噪声调度器输入、位置编码类型、Transformer 块类型上略有改动。且由于开源的 FLUX.1 是指引蒸馏过的，该模型无法使用 CFG。[dev] 版可以以实数约束的方式设置指引强度，而 [schnell] 版无法设置指引强度。</p>
<p>在这次阅读中，我们已经弄懂了以下细节：</p>
<ul>
<li>采样流水线会在去噪网络外面以通道堆叠的方式实现图块化。</li>
<li>指引强度不是以 CFG 的形式写在流水线里，而是以约束的形式输入进了去噪网络。</li>
</ul>
<p>我们还留下了一些未解之谜：</p>
<ul>
<li>输入进噪声采样器的 <code>mu</code> 是什么？</li>
<li>决定旋转式位置编码的 <code>txt_ids</code> 和 <code>img_ids</code> 是什么？</li>
<li>旋转式位置编码在网络里的实现细节是什么？</li>
<li>新的那种 Transformer 块的结构是怎么样的？</li>
</ul>
<p>针对这些问题，我们来细读代码。</p>
<h2 id="调整流匹配标准差"><a href="#调整流匹配标准差" class="headerlink" title="调整流匹配标准差"></a>调整流匹配标准差</h2><p>在采样流水线里，我们见到了这样一个神秘变量 <code>mu</code>。从名字中，我们猜测这是一个表示正态分布均值的变量，用来平移 (shift) 某些量的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mu = calculate_shift(...)</span><br><span class="line">timesteps, num_inference_steps = retrieve_timesteps(</span><br><span class="line">    ...</span><br><span class="line">    mu=mu,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们先看 <code>calculate_shift</code> 做了什么。第一个参数 <code>image_seq_len</code> 表示图像 token 数，可以认为是函数的自变量 <code>x</code>。后面四个参数其实定义了一条直线。我们可以认为 <code>base_seq_len</code> 是 <code>x1</code>, <code>max_seq_len</code> 是 <code>x2</code>，<code>base_shift</code> 是 <code>y1</code>，<code>max_shift</code> 是 <code>y2</code>。根据这两个点的坐标就可以解出一条直线方程出来。也就是说，<code>calculate_shift</code> 会根据模型允许的最大 token 数 4096 ($64 \times 64$) 和最小 token 数 256 ($16 \times 16$)，把当前的输入 token 数线性映射到 0.5 ~ 1.16 之间。但我们暂时不知道输出 <code>mu</code> 的意义是什么。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_shift</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    image_seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">    base_seq_len: <span class="built_in">int</span> = <span class="number">256</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_seq_len: <span class="built_in">int</span> = <span class="number">4096</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    base_shift: <span class="built_in">float</span> = <span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_shift: <span class="built_in">float</span> = <span class="number">1.16</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)</span><br><span class="line">    b = base_shift - m * base_seq_len</span><br><span class="line">    mu = image_seq_len * m + b</span><br><span class="line">    <span class="keyword">return</span> mu</span><br></pre></td></tr></table></figure>
<p>再追踪进调用了 <code>mu</code> 的 <code>retrieve_timesteps</code> 函数里，我们发现 <code>mu</code> 并不在参数表中，而是在 <code>kwargs</code> 里被传递给了噪声迭代器的 <code>set_timesteps</code> 方法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieve_timesteps</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    scheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    device: <span class="type">Optional</span>[<span class="type">Union</span>[<span class="built_in">str</span>, torch.device]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    timesteps: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">int</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    sigmas: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">float</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    **kwargs,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">...</span><br><span class="line">scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)</span><br></pre></td></tr></table></figure></p>
<p>根据流水线构造函数里的类名，我们能找到位于 <code>diffusers/schedulers/scheduling_flow_match_euler_discrete.py</code> 调度器类 <code>FlowMatchEulerDiscreteScheduler</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    scheduler: FlowMatchEulerDiscreteScheduler,</span></span></span><br><span class="line"><span class="params"><span class="function">    ...</span>)</span></span><br></pre></td></tr></table></figure></p>
<p>再找到类的 <code>set_timesteps</code> 方法。<code>set_timesteps</code> 一般是用来设置推理步数 <code>num_inference_steps</code> 的。有些调度器还会在总推理步数确定后，初始化一些其他变量。比如这里的流匹配调度器，会在这个方法里初始化变量 <code>sigmas</code>。我们可以忽略这背后的原理，仅从代码上看，输入 <code>mu</code> 会通过 <code>time_shift</code> 修改 <code>sigmas</code> 的值。</p>
<blockquote>
<p>这里的变量命名又乱七八糟，输入 <code>time_shift</code> 的 <code>sigmas</code> 是第三个参数，而在 <code>time_shift</code> 里的 <code>sigmas</code> 是除了 <code>self</code> 以外的第二个参数。这是因为 Diffusers 在移植官方代码时没有取好变量名。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_shift</span>(<span class="params">self, mu: <span class="built_in">float</span>, sigma: <span class="built_in">float</span>, t: torch.Tensor</span>):</span></span><br><span class="line">    <span class="keyword">return</span> math.exp(mu) / (math.exp(mu) + (<span class="number">1</span> / t - <span class="number">1</span>) ** sigma)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_timesteps</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    num_inference_steps: <span class="built_in">int</span> = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    device: <span class="type">Union</span>[<span class="built_in">str</span>, torch.device] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    sigmas: <span class="type">Optional</span>[<span class="type">List</span>[<span class="built_in">float</span>]] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    mu: <span class="type">Optional</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    <span class="keyword">if</span> self.config.use_dynamic_shifting:</span><br><span class="line">        sigmas = self.time_shift(mu, <span class="number">1.0</span>, sigmas)</span><br></pre></td></tr></table></figure>
<p>我们再跑出去看一下流水线里输入的 <code>sigmas</code> 是什么。假设总采样步数为 $T$，则 <code>sigmas</code> 是 $1$ 到 $\frac{1}{T}$ 间均匀采样的 $T$ 个实数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sigmas = np.linspace(<span class="number">1.0</span>, <span class="number">1</span> / num_inference_steps, num_inference_steps)</span><br></pre></td></tr></table></figure>
<p>现在要解读 <code>mu</code> 的作用就很容易了。假设 <code>sigmas</code> 是下标和值构成的点，我们可以测试 <code>mu</code> 不同的情况下， <code>sigmas</code> 经过 <code>time_shift</code> 函数形成的曲线图。 </p>
<p><img src="/2024/09/03/20240809-flux1/2.jpg" alt></p>
<p>可以看出，<code>mu=0</code>则不修改曲线。随着 <code>mu</code> 增大，曲线逐渐上凸。</p>
<p>我对流匹配的具体细节不是很懂，只能大概猜测 <code>mu</code> 的作用。流匹配中，图像沿着某条路线从纯噪声运动到训练集中，标准差 sigma 用于控制不同时刻图像的不确定性。时刻为 0 时，图像为纯噪声，标准差为 1； 时刻为 1 时，图像为生成集合中的图像，标准差要尽可能趋于 0。对于中间时刻，标准差默认按照时刻线性变化。而 <code>mu</code> 是一个 0.5 ~ 1.16 之间的数，可能控制的是中间时刻的噪声均值。图像分辨率越大，token 越多，<code>mu</code> 越大，要加的噪声越重。这也符合之前 Stable Diffusion 3 论文在 <em>Resolution-dependent shifting of timestep schedules</em> 小节里的设计，对于分辨率越高的图像，需要加更多噪声来摧毁原图像的信号。总之，这个 <code>mu</code> 可能是训练的时候加的，用于给高分辨率图像加更多噪声，推理时也不得不带上这个变量。</p>
<p>FLUX.1 官方仓库对应部分是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_lin_function</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    x1: <span class="built_in">float</span> = <span class="number">256</span>, y1: <span class="built_in">float</span> = <span class="number">0.5</span>, x2: <span class="built_in">float</span> = <span class="number">4096</span>, y2: <span class="built_in">float</span> = <span class="number">1.15</span></span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="type">Callable</span>[[<span class="built_in">float</span>], <span class="built_in">float</span>]:</span></span><br><span class="line">    m = (y2 - y1) / (x2 - x1)</span><br><span class="line">    b = y1 - m * x1</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> x: m * x + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_schedule</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    num_steps: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    image_seq_len: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    base_shift: <span class="built_in">float</span> = <span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    max_shift: <span class="built_in">float</span> = <span class="number">1.15</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    shift: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">float</span>]:</span></span><br><span class="line">    <span class="comment"># extra step for zero</span></span><br><span class="line">    timesteps = torch.linspace(<span class="number">1</span>, <span class="number">0</span>, num_steps + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># shifting the schedule to favor high timesteps for higher signal images</span></span><br><span class="line">    <span class="keyword">if</span> shift:</span><br><span class="line">        <span class="comment"># eastimate mu based on linear estimation between two points</span></span><br><span class="line">        mu = get_lin_function(y1=base_shift, y2=max_shift)(image_seq_len)</span><br><span class="line">        timesteps = time_shift(mu, <span class="number">1.0</span>, timesteps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> timesteps.tolist()</span><br></pre></td></tr></table></figure>
<p><code>mu</code> 的作用确实和高信号图片有关。但他们的设计初衷是偏移时间戳，而不是根据某种公式修改 sigma。比如原来去噪迭代 0~500 步就表示 t=0 到 t=0.5，偏移时间戳后，0~500 步就变成了 t=0 到 t=0.3。偏移时间戳使得模型能够把更多精力学习对如何对高噪声的图像去噪。</p>
<h2 id="使用单流并行注意力层的-Transformer-架构"><a href="#使用单流并行注意力层的-Transformer-架构" class="headerlink" title="使用单流并行注意力层的 Transformer 架构"></a>使用单流并行注意力层的 Transformer 架构</h2><p>接下来的问题都和 FLUX.1 的新 Transformer 架构相关。我们先把整个网络架构弄懂，再去看旋转式位置编码的细节。</p>
<p>为了理清网络架构，我们来根据已知信息，逐步完善网络的模块图。首先，我们先粗略地画一个 Transformer 结构，定义好输入输出。相比 SD3，FLUX.1 多了指引强度和编号集 <code>txt_ids</code>, <code>img_ids</code>这两类输入。</p>
<p><img src="/2024/09/03/20240809-flux1/3.jpg" alt></p>
<p>接下来，我们把和 SD3 相似的结构画进来。所有 Transformer 块都是那种同时处理两类 token 的双流注意力块。输入文本的 T5 嵌入会作为文本支流进入主模型。输入文本的 CLIP 嵌入会经池化与MLP，与经过了位置编码和 MLP 的时刻编码加到一起。时刻编码会以 AdaLayerNorm 的方式修改所有层的数据规模，以及数据在输出前的尺寸与均值。</p>
<p><img src="/2024/09/03/20240809-flux1/4.jpg" alt></p>
<p>在 <code>CombinedTimestepGuidanceTextProjEmbeddings</code> 类中，我们能知道小文本嵌入、时刻嵌入、指引嵌入是怎么加到一起的。我们主要关心指引嵌入的有关操作。由于指引强度 <code>guidance</code> 和时刻 <code>timestep</code> 都是实数，所以 <code>guidance_emb</code> 的处理方式与 <code>timesteps_emb</code> 一模一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CombinedTimestepGuidanceTextProjEmbeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim, pooled_projection_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.time_proj = Timesteps(num_channels=<span class="number">256</span>, flip_sin_to_cos=<span class="literal">True</span>, downscale_freq_shift=<span class="number">0</span>)</span><br><span class="line">        self.timestep_embedder = TimestepEmbedding(in_channels=<span class="number">256</span>, time_embed_dim=embedding_dim)</span><br><span class="line">        self.guidance_embedder = TimestepEmbedding(in_channels=<span class="number">256</span>, time_embed_dim=embedding_dim)</span><br><span class="line">        self.text_embedder = PixArtAlphaTextProjection(pooled_projection_dim, embedding_dim, act_fn=<span class="string">&quot;silu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, timestep, guidance, pooled_projection</span>):</span></span><br><span class="line">        timesteps_proj = self.time_proj(timestep)</span><br><span class="line">        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=pooled_projection.dtype))  <span class="comment"># (N, D)</span></span><br><span class="line"></span><br><span class="line">        guidance_proj = self.time_proj(guidance)</span><br><span class="line">        guidance_emb = self.guidance_embedder(guidance_proj.to(dtype=pooled_projection.dtype))  <span class="comment"># (N, D)</span></span><br><span class="line"></span><br><span class="line">        time_guidance_emb = timesteps_emb + guidance_emb</span><br><span class="line"></span><br><span class="line">        pooled_projections = self.text_embedder(pooled_projection)</span><br><span class="line">        conditioning = time_guidance_emb + pooled_projections</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> conditioning</span><br></pre></td></tr></table></figure>
<p><img src="/2024/09/03/20240809-flux1/5.jpg" alt></p>
<p>在去噪模型 <code>FluxTransformer2DModel</code> 的 <code>forward</code> 方法中，原先的图块化及二维位置编码模块被一个简单的线性层 <code>x_embedder</code> 取代了，现在的位置编码 <code>image_rotary_emb</code> 会输入进所有层中，而不是一开始和输入加在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">hidden_states, ...</span>):</span></span><br><span class="line">    hidden_states = self.x_embedder(hidden_states)</span><br><span class="line">    ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">    image_rotary_emb = self.pos_embed(ids)</span><br></pre></td></tr></table></figure>
<p><img src="/2024/09/03/20240809-flux1/6.jpg" alt></p>
<p>之后，除了过 MM-DiT 块以外，文本信息还会和图像信息融合在一起，过若干个单流 Transformer 块。过了这些模块后，原来文本 token 那部分会被丢弃。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.transformer_blocks):</span><br><span class="line">    encoder_hidden_states, hidden_states = block(</span><br><span class="line">        hidden_states=hidden_states,</span><br><span class="line">        encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">        temb=temb,</span><br><span class="line">        image_rotary_emb=image_rotary_emb,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.single_transformer_blocks):</span><br><span class="line">    hidden_states = block(</span><br><span class="line">        hidden_states=hidden_states,</span><br><span class="line">        temb=temb,</span><br><span class="line">        image_rotary_emb=image_rotary_emb,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">hidden_states = hidden_states[:, encoder_hidden_states.shape[<span class="number">1</span>] :, ...]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2024/09/03/20240809-flux1/7.jpg" alt></p>
<p>我们已经画完了去噪模型的结构，最后把 VAE 部分加上就比较完美了。</p>
<p><img src="/2024/09/03/20240809-flux1/8.jpg" alt></p>
<p>多数模块的细节都可以在 SD3 的论文里找到，除了 RoPE 和单流 DiT 块。我们在这一节里再仔细学习一下单流 DiT 块的结构。</p>
<p>根据官方介绍，FLUX.1 的 Transformer 里用到了并行 Transformer。准确来说，FLUX.1 仅在最后的单流 DiT 块里用到了并行注意力层。并行注意力层是在文章 <em>Scaling Vision Transformers to 22 Billion Parameters</em> 中提出的。如下图所示，这项技术很好理解，只不过是把注意力和线性层之间的串联结构变成并联结构。这样的好处是，由于数据在过注意力层前后本身就要各过一次线性层，在并联后，这些线性层和 MLP 可以融合。这样的话，由于计算的并行度更高，模型的运行效率会高上一些。</p>
<p>顺带一提，在 Q, K 后做归一化以提升训练稳定性也是在这篇文章里提出的。SD3 和 FLUX.1 同样用了这一设计，但用的是 RMSNorm 而不是 LayerNorm。</p>
<p><img src="/2024/09/03/20240809-flux1/9.jpg" alt></p>
<p>我们可以在 <code>FluxSingleTransformerBlock</code> 类里找到相关实现。代码不长，我们可以一次性读完。相比上面的示意图，Q, K, V 的投影操作被单独放进了 <code>Attention</code> 类里，并没有和第一个线性层融合。而做了注意力操作后，Att-out 和 MLP-out 确实是放在一起做的。<code>attn_output</code> 和 <code>mlp_hidden_states</code> 拼接了起来，一起过了 <code>proj_out</code>。此外，这里的归一化层还是 DiT 里的 AdaLN，模块能接收时刻编码的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FluxSingleTransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, num_attention_heads, attention_head_dim, mlp_ratio=<span class="number">4.0</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line"></span><br><span class="line">        self.norm = AdaLayerNormZeroSingle(dim)</span><br><span class="line">        self.proj_mlp = nn.Linear(dim, self.mlp_hidden_dim)</span><br><span class="line">        self.act_mlp = nn.GELU(approximate=<span class="string">&quot;tanh&quot;</span>)</span><br><span class="line">        self.proj_out = nn.Linear(dim + self.mlp_hidden_dim, dim)</span><br><span class="line"></span><br><span class="line">        processor = FluxSingleAttnProcessor2_0()</span><br><span class="line">        self.attn = Attention(...)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        hidden_states: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        temb: torch.FloatTensor,</span></span></span><br><span class="line"><span class="params"><span class="function">        image_rotary_emb=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        residual = hidden_states</span><br><span class="line">        norm_hidden_states, gate = self.norm(hidden_states, emb=temb)</span><br><span class="line">        mlp_hidden_states = self.act_mlp(self.proj_mlp(norm_hidden_states))</span><br><span class="line"></span><br><span class="line">        attn_output = self.attn(</span><br><span class="line">            hidden_states=norm_hidden_states,</span><br><span class="line">            image_rotary_emb=image_rotary_emb,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        hidden_states = torch.cat([attn_output, mlp_hidden_states], dim=<span class="number">2</span>)</span><br><span class="line">        gate = gate.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        hidden_states = gate * self.proj_out(hidden_states)</span><br><span class="line">        hidden_states = residual + hidden_states</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<p>此处具体的注意力运算写在 <code>FluxSingleAttnProcessor2_0</code> 类里。跳过前面繁杂的形状变换操作，我们来看该注意力运算的关键部分。在做完了标准注意力运算 <code>scaled_dot_product_attention</code> 后，一般要调用 <code>attn.to_out[0](hidden_states)</code> 对数据做一次投影变换。但是，在这个注意力运算中，并没有对应的操作。这表明该模块确实是照着并行注意力层设计的，离开注意力的投影与 MLP 的第二个线性层融合到了一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    self,</span></span></span><br><span class="line"><span class="params"><span class="function">    attn: Attention,</span></span></span><br><span class="line"><span class="params"><span class="function">    hidden_states: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">    encoder_hidden_states: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    attention_mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    image_rotary_emb: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function"></span>) -&gt; torch.Tensor:</span></span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> image_rotary_emb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        query, key = apply_rope(query, key, image_rotary_emb)</span><br><span class="line"></span><br><span class="line">    hidden_states = F.scaled_dot_product_attention(query, key, value, dropout_p=<span class="number">0.0</span>, is_causal=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    hidden_states = hidden_states.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, attn.heads * head_dim)</span><br><span class="line">    hidden_states = hidden_states.to(query.dtype)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> hidden_states</span><br></pre></td></tr></table></figure>
<h2 id="旋转式位置编码思想及-FLUX-1-实现"><a href="#旋转式位置编码思想及-FLUX-1-实现" class="headerlink" title="旋转式位置编码思想及 FLUX.1 实现"></a>旋转式位置编码思想及 FLUX.1 实现</h2><p>旋转式位置编码是苏剑林在 <em>RoFormer: Enhanced Transformer with Rotary Position Embedding</em> 中提出的一种专门为注意力计算设计的位置编码。在这篇文章中，我们来简单地了解一下旋转式位置编码的设计思想，为学习 FLUX.1 的结构做准备。</p>
<blockquote>
<p>想深究旋转式位置编码的读者可以去阅读苏剑林的博文，先阅读《让研究人员绞尽脑汁的Transformer位置编码》（<a target="_blank" rel="noopener" href="https://kexue.fm/archives/8130）">https://kexue.fm/archives/8130）</a> 了解该怎么设计位置编码，再阅读《Transformer升级之路：2、博采众长的旋转式式位置编码》（<a target="_blank" rel="noopener" href="https://kexue.fm/archives/8265）">https://kexue.fm/archives/8265）</a> 了解旋转式位置编码的细节。</p>
</blockquote>
<p>Transformer 仅包括注意力和全连接两种运算，这两种运算都是和位置无关的。为了让 Transformer 知道词语的前后关系，或者像素间的空间关系，就要给 Transformer 中的 token 注入某种位置信息。然而，仅仅告诉每个 token 它的<strong>绝对位置</strong>是不够好的，这样做最明显的缺点是模型无法处理训练时没有见过的长序列。比如训练集里最长的句子是 512 个 token，如果输入 600 个 token，由于模型没有见过编号超过 512 的位置编码，就不能很好地处理 512 号以后的 token。因此，我们不仅希望每个 token 知道自己的绝对位置，还希望 token 能从位置编码里知道<strong>相对位置</strong>的信息。</p>
<p>在提出 Transfomer 的论文中，作者给出了如下的一套正弦位置编码方案。这也是多数工作默认使用的位置编码方式。为了简化表示，我们假设输入 token 是一个二维向量，这样，每个 token 需要的位置编码也是一个二维向量。</p>
<script type="math/tex; mode=display">
PE(k) = (cos(\frac{k}{10000}), sin(\frac{k}{10000}))</script><p>其中，$k$ 表示第 $k$ 个 token。这样做的好处是，根据三角函数和角公式，位置编码之间可以用线性组合来表示，这种编码蕴含了一定的相对位置信息。</p>
<script type="math/tex; mode=display">
\begin{aligned}
sin(a+b) = sina \cdot cosb + cosa \cdot sinb \\
cos(a+b) = cosa \cdot cosb - sina \cdot sinb
\end{aligned}</script><p>当我们要把二维向量拓展成 $d$ 维向量时，只需要把 $d$ 维两两打包成一组，每组用不同周期的正弦函数即可。因此，在后文中，我们也不讨论 $d$ 维的 token，只需要搞明白二维的 token 该怎么编码就行。</p>
<script type="math/tex; mode=display">
PE(k, 2i) = sin(\frac{k}{10000^{2i/d}}) \\
PE(k, 2i+1) = cos(\frac{k}{10000^{2i/d}})</script><p>尽管正弦编码能表示一定的相对信息，但是，由于位置编码之间是线性关系，经过了 Transformer 中最重要的操作——注意力操作后，这种相对位置信息几乎就消失了。有没有一种位置编码方式能够让注意力计算也能知道 token 间的相对位置关系呢？</p>
<p>经苏剑林设计，假设每个 token 的二维位置编码是一个复数，如果用以下的公式来定义绝对位置编码，那么经过注意力计算里的求内积操作后，结果里恰好会出现相对位置关系。设两个 token 分别位于位置 $m$ 和 $n$，令给位置为 $j$ 的注意力输入 Q, K $q_j, k_j$ 右乘上 $e^{ij/10000}$的位置编码，则求 Q, K 内积的结果为：</p>
<script type="math/tex; mode=display">
\langle q_me^{im/10000}, k_ne^{in/10000} \rangle=Re[q_mk_n^*e^{i(m-n)/10000}]</script><p>其中，$i$ 为虚数单位，$*$ 为共轭复数，$Re$ 为取复数实部。只是为了理解方法的思想的话，我们不需要仔细研究这个公式，只需要注意到输入的 Q, K 位置编码分别由位置 $m$, $n$ 决定，而输出的位置编码由相对位置 $m-n$ 决定。这种位置编码既能给输入提供绝对位置关系，又能让注意力输出有相对位置关系，非常巧妙。</p>
<p>根据欧拉公式，我们可以把 $e^i$ 用一个含 $sin$ 和 $cos$ 的向量表示。由于该变换对应向量的旋转，所以这种位置编码被称为「旋转式位置编码」。在实际实现时，我们不需要复数库，只需要用两个分别含 $sin$ 和 $cos$ 的数来表示一个位置编码。也就是说，原来正弦位置编码中每个位置的编码只有一个实数，现在需要两个实数，或者说要一个二维向量。</p>
<p>总结一下用旋转式位置编码替换正弦位置编码后，我们在实现时应该做的改动。现在，我们不是提前算好位置编码，再加到输入上，而是先预处理好位置编码，在每次注意力 Q，K 求内积前给输入乘上。和正弦编码一样，我们会把特征长度为 $d$ 的 token 向量的分量两两分组，分别维护位置关系。但是，现在每个分量的编码由两个而不是一个实数表示。所以，在之后的代码中，我们会看到生成位置编码时，会先把 token 特征向量长度除以二，再给每组 token 生成 $2 \times 2$ 个编码，对应每组两个编码，每个编码长度为二。</p>
<p>我们来看一下 FLUX.1 的 Transformer 是怎么处理位置编码的。在 <code>FluxTransformer2DModel</code> 的 <code>forward</code> 方法中，我们能看到输入的 <code>0, 1, 2, 3</code> 这样的整数位置编码 <code>ids</code> 被传入了位置编码层 <code>pos_embed</code> 中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">image_rotary_emb = self.pos_embed(ids)</span><br></pre></td></tr></table></figure>
<p>位置编码层类 <code>EmbedND</code> 定义了位置编码的具体计算方式。这个类的逻辑我们暂时跳过，直接看最后真正在算旋转式位置编码的 <code>rope</code> 函数。函数中，输入参数 <code>pos</code> 是一个 <code>0, 1, 2, 3</code> 这样的整数序号张量，<code>dim</code> 表示希望生成多长的位置编码，其值应该等于 token 的特征长度，<code>theta</code> 用来控制三角函数的周期，一般都是取常数 <code>10000</code>。我们能看到，<code>rope</code> 计算了输入的三角函数值，并把长度为 <code>dim</code> 的编码两两分组，每组有 <code>(2, 2)</code> 个位置编码值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rope</span>(<span class="params">pos: torch.Tensor, dim: <span class="built_in">int</span>, theta: <span class="built_in">int</span></span>) -&gt; torch.Tensor:</span></span><br><span class="line">    <span class="keyword">assert</span> dim % <span class="number">2</span> == <span class="number">0</span>, <span class="string">&quot;The dimension must be even.&quot;</span></span><br><span class="line"></span><br><span class="line">    scale = torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>, dtype=torch.float64, device=pos.device) / dim</span><br><span class="line">    omega = <span class="number">1.0</span> / (theta**scale)</span><br><span class="line"></span><br><span class="line">    batch_size, seq_length = pos.shape</span><br><span class="line">    out = torch.einsum(<span class="string">&quot;...n,d-&gt;...nd&quot;</span>, pos, omega)</span><br><span class="line">    cos_out = torch.cos(out)</span><br><span class="line">    sin_out = torch.sin(out)</span><br><span class="line"></span><br><span class="line">    stacked_out = torch.stack([cos_out, -sin_out, sin_out, cos_out], dim=-<span class="number">1</span>)</span><br><span class="line">    out = stacked_out.view(batch_size, -<span class="number">1</span>, dim // <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> out.<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbedND</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, theta: <span class="built_in">int</span>, axes_dim: <span class="type">List</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.theta = theta</span><br><span class="line">        self.axes_dim = axes_dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, ids: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        n_axes = ids.shape[-<span class="number">1</span>]</span><br><span class="line">        emb = torch.cat(</span><br><span class="line">            [rope(ids[..., i], self.axes_dim[i], self.theta) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_axes)],</span><br><span class="line">            dim=-<span class="number">3</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> emb.unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>我们来看一下位置编码是怎么传入 Transformer 块的注意力计算的。在预处理完位置编码后，<code>image_rotary_emb</code> 会作为输入参数传入所有 Transformer 块，包括前面的双流块和后面的单流块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">...</span>):</span></span><br><span class="line">    ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">    image_rotary_emb = self.pos_embed(ids)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.transformer_blocks):</span><br><span class="line">        encoder_hidden_states, hidden_states = block(</span><br><span class="line">                        ...</span><br><span class="line">                        image_rotary_emb=image_rotary_emb,</span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index_block, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.single_transformer_blocks):</span><br><span class="line">        hidden_states = block(</span><br><span class="line">            ...</span><br><span class="line">            image_rotary_emb=image_rotary_emb,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>位置编码 <code>image_rotary_emb</code> 最后会传入双流注意力计算类 <code>FluxAttnProcessor2_0</code> 和单流注意力计算类 <code>FluxSingleAttnProcessor2_0</code>。由于位置编码在这两个类中的用法都相同，我们就找 <code>FluxSingleAttnProcessor2_0</code> 的代码来看一看。在其 <code>__call__</code> 方法中，可以看到，在做完了 Q, K 的投影变换、形状变换、归一化后，方法调用了 <code>apply_rope</code> 来执行旋转式位置编码的计算。而 <code>apply_rope</code> 会把 Q, K 特征向量的分量两两分组，根据之前的公式，模拟与位置编码的复数乘法运算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FluxSingleAttnProcessor2_0</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        ...</span></span></span><br><span class="line"><span class="params"><span class="function">        image_rotary_emb: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">        query = attn.to_q(hidden_states)</span><br><span class="line">        <span class="keyword">if</span> encoder_hidden_states <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            encoder_hidden_states = hidden_states</span><br><span class="line">        key = attn.to_k(encoder_hidden_states)</span><br><span class="line">        value = attn.to_v(encoder_hidden_states)</span><br><span class="line"></span><br><span class="line">        query = query.view(batch_size, -<span class="number">1</span>, attn.heads, head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        key = key.view(batch_size, -<span class="number">1</span>, attn.heads, head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        value = value.view(batch_size, -<span class="number">1</span>, attn.heads, head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> attn.norm_q <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            query = attn.norm_q(query)</span><br><span class="line">        <span class="keyword">if</span> attn.norm_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            key = attn.norm_k(key)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> image_rotary_emb <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            query, key = apply_rope(query, key, image_rotary_emb)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_rope</span>(<span class="params">xq, xk, freqs_cis</span>):</span></span><br><span class="line">    xq_ = xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    xk_ = xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    xq_out = freqs_cis[..., <span class="number">0</span>] * xq_[..., <span class="number">0</span>] + freqs_cis[..., <span class="number">1</span>] * xq_[..., <span class="number">1</span>]</span><br><span class="line">    xk_out = freqs_cis[..., <span class="number">0</span>] * xk_[..., <span class="number">0</span>] + freqs_cis[..., <span class="number">1</span>] * xk_[..., <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> xq_out.reshape(*xq.shape).type_as(xq), xk_out.reshape(*xk.shape).type_as(xk)</span><br></pre></td></tr></table></figure>
<p>这样，我们就看完了旋转式位置编码在 FLUX.1 里的实现。但是，我们还遗留了一个重要问题：在 NLP 中，句子天然有前后关系，我们按照 <code>0, 1, 2, 3</code> 给 token 编号就行了。而在这个模型中，既有图像 token，又有文本 token，该怎么给 token 编号呢？</p>
<h2 id="图像及文本-token-的位置编号"><a href="#图像及文本-token-的位置编号" class="headerlink" title="图像及文本 token 的位置编号"></a>图像及文本 token 的位置编号</h2><p>现在，我们把目光倒回到流水线类。输入给去噪模型的序号变量有两个：<code>text_ids</code>，<code>latent_image_ids</code>。它们是怎么得到的？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">noise_pred = self.transformer(</span><br><span class="line">    ...</span><br><span class="line">    txt_ids=text_ids,</span><br><span class="line">    img_ids=latent_image_ids,</span><br><span class="line">    ...</span><br><span class="line">)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>在文本编码方法中，我们看到，<code>text_ids</code> 竟然只是一个全零张量。它的第一维表示 batch 大小，第二维序列长度等于文本编码 <code>prompt_embeds</code> 的长度，第三维序号长度为 3。也就是说，对于每一个文本 token 的每一个位置，都用 <code>(0, 0, 0)</code> 来表示它的位置编号。这也暗示在 FLUX.1 中，token 的位置是三维的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_prompt</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">    ...</span></span></span><br><span class="line"><span class="params"><span class="function"></span>):</span></span><br><span class="line">    ...</span><br><span class="line">    text_ids = torch.zeros(batch_size, prompt_embeds.shape[<span class="number">1</span>], <span class="number">3</span>).to(device=device, dtype=dtype)</span><br><span class="line">    text_ids = text_ids.repeat(num_images_per_prompt, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> prompt_embeds, pooled_prompt_embeds, text_ids</span><br></pre></td></tr></table></figure>
<p>而 <code>latent_image_ids</code> 主要是在 <code>_prepare_latent_image_ids</code> 函数里生成的。这个函数的主要输入参数是图像的高宽。根据高宽，函数会生成 <code>(0, 0) ~ (height, width)</code> 的二维位置坐标表格，作为位置坐标 <code>latent_image_ids</code> 的第二、第三维。而位置坐标的第一维全是 0。也就是说，位置为 <code>(i, j)</code> 的像素的位置编号为 <code>(0, i, j)</code>。代码里给高宽除以 2 是因为输入没有考虑 2 倍的图块化，这写得真够乱的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prepare_latent_image_ids</span>(<span class="params">batch_size, height, width, device, dtype</span>):</span></span><br><span class="line">    latent_image_ids = torch.zeros(height // <span class="number">2</span>, width // <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    latent_image_ids[..., <span class="number">1</span>] = latent_image_ids[..., <span class="number">1</span>] + torch.arange(height // <span class="number">2</span>)[:, <span class="literal">None</span>]</span><br><span class="line">    latent_image_ids[..., <span class="number">2</span>] = latent_image_ids[..., <span class="number">2</span>] + torch.arange(width // <span class="number">2</span>)[<span class="literal">None</span>, :]</span><br><span class="line"></span><br><span class="line">    latent_image_id_height, latent_image_id_width, latent_image_id_channels = latent_image_ids.shape</span><br><span class="line"></span><br><span class="line">    latent_image_ids = latent_image_ids[<span class="literal">None</span>, :].repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    latent_image_ids = latent_image_ids.reshape(</span><br><span class="line">        batch_size, latent_image_id_height * latent_image_id_width, latent_image_id_channels</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latent_image_ids.to(device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_latents</span>(<span class="params">...</span>):</span></span><br><span class="line">    height = <span class="number">2</span> * (<span class="built_in">int</span>(height) // self.vae_scale_factor)</span><br><span class="line">    width = <span class="number">2</span> * (<span class="built_in">int</span>(width) // self.vae_scale_factor)</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    latent_image_ids = self._prepare_latent_image_ids(batch_size, height, width, device, dtype)</span><br><span class="line">    <span class="keyword">return</span> latents, latent_image_ids</span><br></pre></td></tr></table></figure>
<p>文本位置编号 <code>txt_ids</code> 和 <code>img_ids</code> 会在第二维，也就是序列长度那一维拼接成 <code>ids</code>。<code>ids</code> 会输入给 <code>EmbedND</code> 类的实例 <code>pos_embed</code>。<code>EmbedND</code> 的构造函数参数中，<code>dim</code> 完全没有被用到，<code>theta</code> 控制编码的三角函数周期，<code>axes_dim</code> 表示位置坐标每一维的编码长度。比如 FLUX.1 的位置坐标是三维的， <code>axes_dim</code> 是 <code>[16, 56, 56]</code>，那么它就表示第一个维度用长度 <code>16</code> 的位置编码，后两维用长度 <code>56</code> 的位置编码。位置编号经 <code>rope</code> 函数计算得到旋转式位置编码后，会拼接到一起，最后形成 <code>128</code> 维的位置编码。注意，所有 Transformer 块每个头的特征数 <code>attention_head_dim</code> 也是 <code>128</code>。这两个值必须相等。</p>
<blockquote>
<p>「头」指的是「多头注意力」里的「头」。头数乘上每次参与注意力运算的特征长度才等于总特征长度。由于位置编码是给 Q, K 准备的，所以位置编码的长度应该与参与注意力运算的特征长度相同。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FluxTransformer2DModel</span>():</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        self,</span></span></span><br><span class="line"><span class="params"><span class="function">        patch_size: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        in_channels: <span class="built_in">int</span> = <span class="number">64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_layers: <span class="built_in">int</span> = <span class="number">19</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_single_layers: <span class="built_in">int</span> = <span class="number">38</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        attention_head_dim: <span class="built_in">int</span> = <span class="number">128</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        num_attention_heads: <span class="built_in">int</span> = <span class="number">24</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        joint_attention_dim: <span class="built_in">int</span> = <span class="number">4096</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        pooled_projection_dim: <span class="built_in">int</span> = <span class="number">768</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        guidance_embeds: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        axes_dims_rope: <span class="type">List</span>[<span class="built_in">int</span>] = [<span class="number">16</span>, <span class="number">56</span>, <span class="number">56</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">    </span>):</span></span><br><span class="line">    self.pos_embed = EmbedND(dim=self.inner_dim, theta=<span class="number">10000</span>, axes_dim=axes_dims_rope)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">...</span>):</span></span><br><span class="line">    ids = torch.cat((txt_ids, img_ids), dim=<span class="number">1</span>)</span><br><span class="line">    image_rotary_emb = self.pos_embed(ids)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbedND</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, theta: <span class="built_in">int</span>, axes_dim: <span class="type">List</span>[<span class="built_in">int</span>]</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.theta = theta</span><br><span class="line">        self.axes_dim = axes_dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, ids: torch.Tensor</span>) -&gt; torch.Tensor:</span></span><br><span class="line">        n_axes = ids.shape[-<span class="number">1</span>]</span><br><span class="line">        emb = torch.cat(</span><br><span class="line">            [rope(ids[..., i], self.axes_dim[i], self.theta) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_axes)],</span><br><span class="line">            dim=-<span class="number">3</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> emb.unsqueeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>我们来整理一下 FLUX.1 的位置编码机制。每个文本 token 的位置编号都是 <code>(0, 0, 0)</code>。位于 <code>(i, j)</code> 的像素的位置编号是 <code>(0, i, j)</code>。它们会生成 <code>128</code> 维的位置编码。编码前 <code>16</code> 个通道是第一维位置编号的位置编码，后面两组 <code>56</code> 个通道分别是第二维、第三位位置编号的位置编码。也就是说，在每个头做多头注意力运算时，特征的前 <code>16</code> 个通道不知道位置信息，中间 <code>56</code> 个通道知道垂直的位置信息，最后 <code>56</code> 个通道知道水平的位置信息。</p>
<p>乍看下来，这种位置编号方式还是非常奇怪的。所有 token 的第一维位置编号都是 0，这一维岂不是什么用都没有？</p>
<h2 id="FLUX-1-旋转式位置编码原理猜测与实验"><a href="#FLUX-1-旋转式位置编码原理猜测与实验" class="headerlink" title="FLUX.1 旋转式位置编码原理猜测与实验"></a>FLUX.1 旋转式位置编码原理猜测与实验</h2><p>在这一节中，我将主观分析 FLUX.1 的现有源码，猜测 FLUX.1 未开源的 [pro] 版本中旋转式位置编码是怎么设置的。此外，我还会分享一些简单的相关实验结果。</p>
<p>已开源的 FLUX.1 为什么会出现 <code>(0, 0, 0)</code>, <code>(0, i, j)</code> 这样奇怪的位置编号呢？由于现在已开源的两版模型是在 FLUX.1 [pro] 上指引蒸馏的结果，很可能原模型在指引机制，也就是和文本相关的处理机制上与现有模型不同。因此，我使用我独创的代码心理学，对现有源码进行了分析。</p>
<p>首先，令我感到疑惑的是采样流水线里生成位置编号的代码。<code>latent_image_ids</code> 一开始是一个全零张量，你写它加一个数，和直接赋值的结果不是一样的吗？为什么要浪费时间多写一个加法呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">latent_image_ids = torch.zeros(height // <span class="number">2</span>, width // <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">latent_image_ids[..., <span class="number">1</span>] = latent_image_ids[..., <span class="number">1</span>] + torch.arange(height // <span class="number">2</span>)[:, <span class="literal">None</span>]</span><br><span class="line">latent_image_ids[..., <span class="number">2</span>] = latent_image_ids[..., <span class="number">2</span>] + torch.arange(width // <span class="number">2</span>)[<span class="literal">None</span>, :]</span><br></pre></td></tr></table></figure>
<p>为了确认这段代码不是 Diffusers 的开发者写的，我去看了 FLUX.1 的官方代码，发现他们的写法是一样的。在看 Diffusers 源码时，我们还看到了其他一些写得很差的代码，这些代码其实也都是从官方仓库里搬过来的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare</span>(<span class="params">t5: HFEmbedder, clip: HFEmbedder, img: Tensor, prompt: <span class="built_in">str</span> | <span class="built_in">list</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="built_in">dict</span>[<span class="built_in">str</span>, Tensor]:</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    img_ids = torch.zeros(h // <span class="number">2</span>, w // <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    img_ids[..., <span class="number">1</span>] = img_ids[..., <span class="number">1</span>] + torch.arange(h // <span class="number">2</span>)[:, <span class="literal">None</span>]</span><br><span class="line">    img_ids[..., <span class="number">2</span>] = img_ids[..., <span class="number">2</span>] + torch.arange(w // <span class="number">2</span>)[<span class="literal">None</span>, :]</span><br><span class="line">    img_ids = repeat(img_ids, <span class="string">&quot;h w c -&gt; b (h w) c&quot;</span>, b=bs)</span><br></pre></td></tr></table></figure>
<p>从这些代码中，我们不难猜出开发者的心理。FLUX.1 的开发者想，我们要赶快搞一个大新闻，论文也不写了，直接加班加点准备开源。Diffusers 的开发者一看，你们这么急，我们也得搞快一点。于是他们先把 SD3 的代码复制了一遍，然后又照搬了 FLUX.1 官方仓库里的一些逻辑，直接修改 SD3 的代码。</p>
<p>相信大家都有这样的代码重构经历：把自己写的个人开发代码，急忙删删改改，变成能给别人看的代码。能少改一点，就少改一点。上面的代码用加法而不是赋值，就是重构的时候代码没删干净的痕迹。这说明，一开始的 <code>img_ids</code> 很可能不是一个全零张量，而是写了一些东西在里面。</p>
<p>而另一边，设置文本位置编号的官方源码里，非常干脆地写着一个全零向量。我倾向于这部分代码没有在开源时改过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">txt_ids = torch.zeros(bs, txt.shape[<span class="number">1</span>], <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>那么，问题就来了，这个看似全零的图像位置编号一开始是什么？它对整个位置编码的设计有什么影响？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_ids = torch.zeros(h // <span class="number">2</span>, w // <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>我猜开发者设置这个变量的目的是为了区分文本和图像 token。目前，所有文本 token 的位置编号是 <code>(0, 0, 0)</code>，这其实不太合理，因为这种做法实际上是把所有文本 token 都默认当成位置为 <code>(0, 0)</code> 图像 token。为了区分文本和图像 token，应该还有其他设计。我猜最简单的方法是在第一维上做一些改动，比如令所有图像 token 的第一维都是 1。但看起来更合理的做法是对三个维度的编号都一些更改，比如给所有图像位置编号都加上一个常量 <code>(a, b, c)</code>。这样，图像 token 间的相对位置并不会变，而图像和文本 token 的相对位置就不同了，文本就不会默认在图像 <code>(0, 0)</code> 处了。从代码里的加法来看，我更倾向于认为 <code>img_ids</code> 原来是一个三个维度都有值的常量，且这个量或许是可以学习的。而在指引蒸馏时，位置编号的设计被简化了。</p>
<blockquote>
<p>网上有人说文本位置编码全零是因为 t5 编码器自带位置编码。而在我看来，过了一个文本编码器后，文本的每个 token 已经包含所有文本的全局信息，文本 token 之间的位置编码在这里已经不重要了。重要的是文本 token 和图像 token 之间的「位置」关系，这并不能通过 t5 的位置编码来反映。</p>
</blockquote>
<p>为了验证位置编码的作用，我尝试修改了图像位置编号的定义，还是跑本文开头那个测试示例。</p>
<p>如果把图像位置编号全置零，会得到下面的结果。这说明位置编码对结果的影响还是很大的，模型只能从位置编码处获取 token 间的相对关系。</p>
<p><img src="/2024/09/03/20240809-flux1/10.jpg" alt></p>
<p>如果把位置编号除以二，会得到下面的结果。我们能发现，图像好像变模糊了一点，且像素有锯齿化的倾向。这非常合理，因为位置编号除以二后，模型实际上被要求生成分辨率低一倍的结果。但突然又多了一些距离为 0.5 的像素，模型突然就不知道怎么处理了，最终勉强生成了这种略显模糊，锯齿现象明显的图片。注意哦，这里虽然像素间的关系不对，但图中的文字很努力地想要变得正常一点。</p>
<p><img src="/2024/09/03/20240809-flux1/11.jpg" alt></p>
<p>位置编号乘二的结果如下所示。可能模型并没有见过没有距离为 1 的图像 token 的情况，结果全乱套了。但尽管是这样，我们依然能看到图中的 “Hello World”。结合上面的结果，这说明文本指引对结果的影响还是很大的，正常的文本 token 在努力矫正图像生成结果。</p>
<p><img src="/2024/09/03/20240809-flux1/12.jpg" alt></p>
<p>位置编号乘 1.2 的结果如下所示。图像的结果还是比较正常的。这说明这套位置编码允许位置编号发生小的扰动，且模型能认识非整数的位置编号，即在模型看来，位置编号是连续的。</p>
<p><img src="/2024/09/03/20240809-flux1/13.jpg" alt></p>
<p>原图片和将位置编号第一维全置 1 的结果如下所示。如我所料，位置编号的第一维几乎没什么作用。图片只是某些地方发生了更改，整体的画面结构没有变化。</p>
<p><img src="/2024/09/03/20240809-flux1/14.jpg" alt></p>
<p>目前看下来，由于现在我们有了显式定义 token 相对位置关系的方法，要在 FLUX.1 上做一些图像编辑任务的科研，最容易想到地方就是位置编码这一块。我目前随便能想到的做法有两个：</p>
<ul>
<li>直接基于位置编号做超分辨率。想办法修改位置编码的机制，使得所有图像 token 距离 2 个单位时也能某种程度上正常输出图片。以此配置反演一张低分辨率图片，得到纯噪声，重新以图像 token 距离 1 单位的正常配置来生成图片，但旧像素不对新像素做注意力，再想一些办法控制文本那部分，尽量保持旧像素输出不变，最后就能得到两倍超分辨率的结果了。inpainting 似乎也能拿类似的思路来做。</li>
<li>目前所有文本 token 的位置默认是 <code>(0, 0)</code>，改变文本 token 的位置编号或许能让我们精确控制文本指定的生成区域。当然，这个任务在之前的 Stable Diffusion 里好像已经被做滥了。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们围绕 FLUX.1 相对 Stable Diffusion 3 的改动，仔细阅读了 FLUX.1 在 Diffusers 中的源码。这些改动具体总结如下：</p>
<ul>
<li>SD3 是在去噪网络里用下采样 2 倍的卷积实现图块化，而 FLUX.1 通过把 $2 \times 2$ 个图像 token 在通道上堆叠直接实现图块化。</li>
<li>FLUX.1 目前公布的两个模型都是指引蒸馏过的。我们无需使用 Classifier-Free Guidance，只要把指引强度当成一个约束条件输出进模型，就能在一次推理中得到带指定指引强度的输出。</li>
<li>FLUX.1 遵照 Stable Diffusion 3 的噪声调度机制，对于分辨率越高的图像，把越多的去噪迭代放在了高噪声的时刻上。但相较 Stable Diffusion 3，似乎不仅训练时有这种设计，采样时也需要用到这种设计。</li>
<li>FLUX.1 将文本的位置编号设为 <code>(0, 0, 0)</code>，图像的位置编号设为 <code>(0, i, j)</code>，之后用标准的旋转式位置编码对三个维度的编号编码，再把三组编码拼接。这种看似不太合理的位置编号设计方式或许是指引蒸馏导致的，目前从源代码中看不出原 FLUX.1 模型的位置编号设计方式。</li>
<li>在原 Stable Diffusion 的 MM-DiT 块之后，FLUX.1 将文本和图像 token 拼接，输入进了一个单流的 Transformer 块。该 Transformer 块遵照之前并行注意力层的设计，注意力层和 MLP 并联执行，在执行速度上有所提升。</li>
</ul>
<p>FLUX.1 的总模型结构图如下所示。</p>
<p><img src="/2024/09/03/20240809-flux1/8.jpg" alt></p>
<p>作为最强开源 DiT 文生图模型，FLUX.1 狠狠打脸了拖拖拉拉刚开源没多久的 Stable Diffusion 3。可以预见，之后大家会把开发图像编辑工作的基础模型从 U-Net 版 Stable Diffusion 逐渐换成 FLUX.1。这方面的研究目前还是蓝海，值得大家投入精力研究。</p>
<p>FLUX.1 还是在科研上能给我们一些启示的。RoPE 都是 NLP 那边已经出了很久的工作了，直到现在才搬到图像生成这边来。我们或许能够把 NLP 或者其他视觉任务中使用的神经网络技术搬到图像生成这边来，不费什么力气地改进现有的图像生成模型。</p>
<p>但是，在搬运 NLP 技术中，我们也要思考如何更合理地在视觉应用中使用这些技术。文本和图像存在本质上的区别：文本是离散的，而图像是连续的。这种连续性不仅体现在图像的颜色值上，还体现在图像像素间的位置关系上。就以这里的旋转式位置编码为例，NLP 中，token 间的距离就得是整数。而在 CV 中，如果我们认为图像是一种连续信号，那么非整数的 token 距离或许也是有意义的。从文本和图像的本质区别出发，我们或许能够把 NLP 的技术更好地适配到 CV 上，而不是把 Transformer 搬过来，然后加数据一把梭。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/07/27/20240717-ar-wo-vq/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/07/27/20240717-ar-wo-vq/" class="post-title-link" itemprop="url">解读何恺明新作：不用向量离散化的自回归图像生成（Autoregressive Image Generation without Vector Quantization）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-27 20:32:45" itemprop="dateCreated datePublished" datetime="2024-07-27T20:32:45+08:00">2024-07-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>自回归是一种根据之前已生成内容，不断递归预测下一项要生成的内容的生成模型。这种生成方式十分易懂，符合我们对生活的观察。比如我们希望模型生成一句话，第一个是「今」字，那么第二个字很可能就是「天」字。如果前三个字是「今天早」，那么第四个字就很可能是「上」。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（空）  -&gt; 今</span><br><span class="line">今      -&gt; 天</span><br><span class="line">今天    -&gt; 早</span><br><span class="line">今天早  -&gt; 上</span><br></pre></td></tr></table></figure><br>为这种自回归模型的而设计的 Transformer 网络在自然语言处理（NLP）中取得了极大的成功。然而，尽管许多人也尝试用它生成图像，自回归模型却一直没有成为最强大、最受欢迎的图像生成模型。</p>
<p>为了解决此问题，何恺明团队公布了论文 <em>Autoregressive Image Generation without Vector Quantization</em>。作者分析了目前最常见的自回归图像生成模型后，发现模型中的<strong>向量离散化 (Vector Quantization, VQ)</strong> 是拖累模型能力的罪魁祸首。作者用一些巧妙的方法绕过了 VQ，最终设计出了一种新式自回归模型。该模型在图像生成任务上表现出色，在 ImageNet 图像生成指标上不逊于最先进的图像扩散模型。在这篇博文中，我们就来学习一下这种新颖的无 VQ 自回归图像生成模型。</p>
<p>建议读者在阅读本文前熟悉 VQ-VAE、Transformer、DDPM 等经典工作，了解 NLP 和图像生成中连续值和离散值的概念。可以参考我之前写的文章：</p>
<p>轻松理解 VQ-VAE：首个提出 codebook 机制的生成模型</p>
<p>VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型</p>
<p>Transformer 论文精读</p>
<p>扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现</p>
<p>Stable Diffusion 解读（一）：回顾早期工作</p>
<h2 id="知识回顾"><a href="#知识回顾" class="headerlink" title="知识回顾"></a>知识回顾</h2><h3 id="连续值与离散值"><a href="#连续值与离散值" class="headerlink" title="连续值与离散值"></a>连续值与离散值</h3><p>在计算机科学中，我们既会用到连续值，也会用到离散值。比如颜色就是一个常见的连续值，我们用 0~1 之间的实数表示灰度从全黑到全白。而词元 (token) 需要用离散值表示，比如我们用 “0” 表示字母 “A”，”1” 表示 “B”, “2” 表示 “C”，并不代表 “B” 是 「’A’ 和 ‘C’ 的平均值」。离散值的数值只是用来区分不同概念的。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/1.jpg" alt></p>
<p>神经网络默认输入是连续变化的。因此，一个连续值可以直接输入进网络。而代表离散值的整数不能直接输入网络，需要先过一个嵌入层，再正常输入进网络。</p>
<h3 id="自回归与类别分布"><a href="#自回归与类别分布" class="headerlink" title="自回归与类别分布"></a>自回归与类别分布</h3><p>在自回归文本生成模型中，为了不断预测下一个词元，通常的做法是用一个神经网络建模下一个词元的类别分布（categorical distribution）。如下面的例子所示，所谓类别分布，就是下一步选择每一个词元的概率。有了概率分布后，我们就能用采样算法采样出下一个词元。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/2.jpg" alt></p>
<p>要训练这个预测模型也很简单。每次预测下一个词元的类别分布，其实就是一个分类任务。我们直接照着分类任务的做法，以数据集里现有句子为真值，用交叉熵损失函数就能训练这个预测模型了。</p>
<h3 id="自回归图像生成"><a href="#自回归图像生成" class="headerlink" title="自回归图像生成"></a>自回归图像生成</h3><p>由于 Transformer 在 NLP 中的成功，大家也想用 Transformer 做图像生成。在用自回归模型生成图像时，需要考虑图像和文本的两个区别：</p>
<ol>
<li>文本是一维的，天然有先后顺序以供自回归生成。而图像是二维的，没有先后顺序。</li>
<li>图像的颜色值是连续而非离散的。而只有离散值才能用类别分布表示。</li>
</ol>
<p>解决问题 1 的方法很简单：没有先后顺序，我们就人工定义一个先后顺序就好了，比如从左上到右下给图像编号。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/3.jpg" alt></p>
<p>而对于问题 2，一种最简单的方式是把连续的颜色值离散化。比如将原来 0 ~ 1 的灰度值转换为「0 号灰度」、「1 号灰度」、…… 「7号灰度」。神经网络像对待词元一样对待这些灰度值，不知道它们之间的大小关系，只知道生成图像的颜色只能由这 8 种「颜色词语」构成。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/4.jpg" alt></p>
<h3 id="向量离散化"><a href="#向量离散化" class="headerlink" title="向量离散化"></a>向量离散化</h3><p>把颜色值离散化后，我们的确可以用自回归做图像生成了。但是，由于图像的像素数比文章的词元数要多很多，这种逐像素生成方式会非常慢。为了加速自回归生成，VQ-VAE, VQGAN 等工作借由向量离散化自编码器（VQ 自编码器）实现了一个两阶段的图像生成方法：</p>
<ul>
<li>训练时，先训练一个包括编码器 (encoder) 和解码器 (decoder) 两个子模型的 VQ 自编码器，再训练一个生成压缩图像的自回归模型。</li>
<li>生成时，先用自回归模型生成出一个压缩图像，再用 VQ 自编码器将其复原成真实图像。</li>
</ul>
<p>相比普通的自编码器，VQ 自编码器有一项特点：它生成的压缩图像仅由离散值组成。这样，它就同时完成了两项任务，使得自回归模型能够高效地实现图像生成：1）将连续图像变成离散图像；2）减少要生成的像素数。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/5.jpg" alt></p>
<blockquote>
<p>如果你还是不太理解 VQ 的作用，请先回顾 VQ-VAE 工作，再来学习这篇工作。</p>
</blockquote>
<h2 id="抛弃-VQ，拥抱扩散模型"><a href="#抛弃-VQ，拥抱扩散模型" class="headerlink" title="抛弃 VQ，拥抱扩散模型"></a>抛弃 VQ，拥抱扩散模型</h2><p>我们来总结一下为什么要使用基于 VQ 的自回归图像生成：大家想用基于 Transformer 的自回归模型做图像生成。自回归模型在预测下一个词元/像素时，<strong>通常</strong>会用一个类别分布来建模下一项数据。由于类别分布只能描述离散数据，而图像又是连续数据，我们需要把连续像素值变成离散值。一种<strong>常用</strong>的将连续图像变成离散图像的方法是 VQ 自编码器，它既能减少图像尺寸以提高生成效率，又能将连续图像变成离散图像。</p>
<p>但相比普通的自编码器，如 VAE，VQ 自编码器有着一些缺点：</p>
<ul>
<li>VQ 自编码器很难训练</li>
<li>VQ 自编码器的重建效果没有 VAE 好。比如在 Stable Diffusion 中，开发者选择了用 VAE 而不是 VQ-VAE 作为自编码器</li>
</ul>
<p>出于抛弃 VQ 的想法，论文的作者发问道：「自回归图像生成真的需要和 VQ 绑定起来吗？」注意到，在我们刚刚阐述使用 VQ 自回归生成的动机时，用了几个「通常」、「常用」这样的非肯定词。这表明我们的这条推理链不是必然的。要取代 VQ，我们可以从两个方面入手：</p>
<ol>
<li>换一种更强力的把连续图像变成离散图像的方法</li>
<li>从更根本处入手，不用类别分布来建模下一项数据</li>
</ol>
<p>论文的作者选择了第二种做法：不就是建模一个像素值的分布吗？我们为什么要用死板的类别分布呢？既然扩散模型如此强大，能够拟合复杂的图像分布，那用它来拟合一个像素值的分布还不是轻轻松松？论文的核心思想也就呼之欲出了：<strong>用扩散模型而不是类别分布来建模自回归模型中下一个像素值的分布，从而抛弃自编码器里的 VQ 操作，提升模型能力。</strong></p>
<p>可能读者第一次看到这个想法时会有些疑惑：扩散模型不是用来生成一整张图像的吗？它怎么建模一个像素值的分布？它和自回归模型又有什么关系？我们来多花点时间深入理解这个想法。</p>
<p>在文本自回归生成中，输入是已生成文本，输出是下一个词元的类别分布。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/6.jpg" alt></p>
<p>而在图像自回归生成中，输入是已生成像素，输出是下一个像素的类别分布。现在，我们希望不用类别分布，而用另一种方式，<strong>根据之前的像素生成出下一个像素</strong>。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/7.jpg" alt></p>
<p>论文作者从扩散模型中获取了灵感。扩散模型是一种强力的生成模型，它可以不根据任何信息，或根据类别、文本等信息，隐式建模训练集的图像分布，从而生成符合训练集分布的图像。既然扩散模型能够建模复杂的图像分布，那它也可以根据之前像素的信息，建模下一个像素的分布。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/8.jpg" alt></p>
<p>那么，在这种新式自回归模型里，我们可以用约束于 Transformer 输出的上下文信息的扩散模型来建模下一个像素的分布，尽管现在我们并不知道每种颜色出现的概率。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/9.jpg" alt></p>
<p>这样做的好处是，以前我们只能用离散的有限类型的颜色（准确来说是图像词元）来表示图像，现在我们能够用连续值来表示图像。模型能够更加轻松地生成内容丰富的图像。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/10.jpg" alt></p>
<p>当然，抛弃了 VQ 后，自回归模型确实不需要 VQ 自编码器来把连续图像变成离散图像了。但是，我们依然需要用自编码器来压缩图像，减少要生成的像素数。本工作依然采取了 VQ-VAE、VQGAN 那种两阶段的生成方式，只不过把 VQ 自编码器换成了用 KL loss 约束的 VAE。</p>
<p>训练这种扩散模型的方法很简单。在每一步训练时，我们知道上下文像素是什么，也知道当前像素的真值是什么。那么，只要以上下文像素为约束，用当前像素的真值去训练一个带约束扩散模型就行了。作者把训练这种隐式描述下一个像素值分布的误差函数称为 Diffusion Loss。</p>
<p>具体来说，本工作使用了最基础的带约束 DDPM 扩散模型。它和标准 DDPM 的唯一区别在于误差函数多了一个约束信息 $z$，该信息是上下文像素过 Transformer 的输出。</p>
<script type="math/tex; mode=display">
L(z, x) = \mathbb{E}_{\epsilon}[||\epsilon-\epsilon_{\theta}(x_t|t, z)||^2]</script><p>$t$ 时刻的噪声图像 $x_t$也是由 DDPM 加噪公式得来的。</p>
<script type="math/tex; mode=display">
x_t = \sqrt{\bar{\alpha_t}} x + \sqrt{1 - \bar{\alpha_t}} \epsilon</script><p>Diffusion Loss 不仅可以用来训练表示分布的扩散模型，还可以训练前面提取上下文信息的 Transformer。由于约束信息 $z$ 来自 Transformer，可以把 Diffusion Loss 的梯度通过 $z$ 回传到 Transformer 的参数里。</p>
<p>扩散模型的采样公式也和 DDPM 的一样，这里不再赘述。特别地，以前的自回归模型在使用类别分布时，会用温度来控制采样的多样性。为了在扩散模型中也加入类似的温度参数，本工作参考了 <em>Diffusion models beat GANs on image synthesis</em> 论文的有关设计。</p>
<p>在具体模型超参数上，本工作的 DDPM 训练时有 1000 步，采样时有 100 步。乍看之下，DDPM 会为整个生成模型增加许多计算量，但由于只需要建模一个像素的分布，这套模型的 DDPM 可以用非常轻量级的结构。默认配置下，这套模型的 DDPM 的去噪模型是一个由 3 个残差块组成小型 MLP。每个残差块由 LayerNorm、线性层、SiLU、线性层组成。约束信息 $z$ 会和时刻 $t$ 的编码加在一起，用 DiT (<em>Scalable diffusion models with Transformers</em>) 里的 AdaLN 约束机制输入进 LayerNorm 层里。</p>
<h2 id="套用更先进的自回归模型"><a href="#套用更先进的自回归模型" class="headerlink" title="套用更先进的自回归模型"></a>套用更先进的自回归模型</h2><p>仅是去掉 VQ，把 Diffusion Loss 加进标准自回归模型，并不能得到一个很好的图像生成模型。于是，作者用更加先进的一些自回归模型（掩码生成模型 Masked Gernerative Models，如 <em>MaskGIT: Masked generative image Transformer</em>、<em>MAGE: Masked generative encoder to unify representation learning and image synthesis</em>）代替标准自回归模型，极大提升了模型的生成能力。</p>
<h3 id="双向注意力"><a href="#双向注意力" class="headerlink" title="双向注意力"></a>双向注意力</h3><p>在标准 Transformer 中（如下图 (a) causal 所示），每一个词元只能看到自己及之前词元的信息。这样做的好处是模型能够并行训练，串行推理。训练和推理的速度都会比较快。但是，由于每个词元看不到后面词元的信息，Transformer 提取整个句子（图像）特征的能力会下降。</p>
<p>而 MAE (<em>Masked autoencoders are scalable vision learners</em>) 论文提出了一种双向注意力机制，它可以让词元两两之间都传递信息。但是，这样模型就不能用同一个句子并行训练了，也失去了 KV cache 加速推理的手段。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/11.jpg" alt></p>
<blockquote>
<p>如果你不太了解 Transformer 为什么是并行训练，请仔细回顾 Transformer 论文中有关自回归机制的描述。</p>
</blockquote>
<h3 id="广义自回归模型"><a href="#广义自回归模型" class="headerlink" title="广义自回归模型"></a>广义自回归模型</h3><p>除了双向注意力外，作者还将一些掩码生成模型的设计融合进标准自回归模型。这种广义上的自回归模型效果更好，且能缓解双向注意力导致的推理速度慢的问题。</p>
<p>一般来说，用图像自回归模型时，我们都是按从左到右，从上到下的顺序生成词元，如下图 (a) 所示。但是，这种顺序不一定是最合理的。</p>
<p>按理来说，模型应该可以通过任何顺序生成词元，这样模型学到的生成方式更加多样。更合理的生成方式应该如下图 (b) 所示，不是从左到右，从上到下给词元编号，而是随机选择一个排列给图像编号。这样就能按照随机的顺序生成图像的词元了。</p>
<p>而在掩码自回归生成中，模型可以一次性生成任意一个集合的词元。因此，为了加速 (b) 模型，我们可以如下图 (c) 所示，在随机给词元编号后一次生成多个词元。(b) 可以看成是 (c) 一次只预测下一个词元的特例。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/12.jpg" alt></p>
<h2 id="Transformer-模型配置"><a href="#Transformer-模型配置" class="headerlink" title="Transformer 模型配置"></a>Transformer 模型配置</h2><p>本工作并没有给 Transformer 加入新设计，我们来确认一遍论文中介绍的 Transformer 配置。</p>
<p>本工作依然采取了两阶段的生成方法。第一个阶段的自编码器（又可以理解成 NLP 中的 tokenizer）来自 LDM 工作官方仓库的 VQ-16 和 KL-16 模型。前者是 VQ 自编码器（VQGAN），后者是一个加强版的 VAE。</p>
<p>本工作用的 Transformer 和 ViT 一样。得到图像词元后，词元会加上位置编码，且词元序列开头会附加一个 <code>[cls]</code> 词元，用以在类别约束生成任务里输入类别。</p>
<p>基于这个类别词元，本工作使用了一种特别的 Classifier-free guidance (CFG) 机制：模型用一个假类别词元来表示「类别不明」。训练时，10% 的正确类别词元被替换成了假类别词元。这样，在用扩散模型时，就可以根据标准 CFG 的做法，用正确类别和假类别实现 CFG。详情请参见论文附录 B。</p>
<p>在训练掩码自回归模型时，70%~100% 的词元是未知的。由于采样序列可能会很短，作者在输入序列前附加了 64 个 <code>[cls]</code> 词元。掩码自回归模型的其他主要设计都与 MAE 相同。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>本工作面向的是图像生成任务，主要评估 ImageNet 数据集上按类别生成的 FID 和 IS 指标。FID 越低越好，IS 越高越好。这篇工作的实验结果中有许多信息，让我们来仔细看一看这份结果。</p>
<h3 id="Diffusion-Loss-与广义自回归模型"><a href="#Diffusion-Loss-与广义自回归模型" class="headerlink" title="Diffusion Loss 与广义自回归模型"></a>Diffusion Loss 与广义自回归模型</h3><p>论文首先展示了 Diffusion Loss、广义自回归模型这两项主要设计的优越性，如下表所示。由于图像是按类别生成的，可以用 CFG 提升模型的生成效果。为了公平比较，模型使用的 VQ 自编码器和 KL 自编码器都来自 LDM 仓库。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/13.jpg" alt></p>
<p>表格的 4 大行展示了改进自回归模型的影响，每一大行里不同 loss 的对比体现了 Diffusioin Loss 的影响。</p>
<p>从第一大行可以看出，Diffusion Loss 似乎对标准自回归的改进不是很明显，且这一套方法的生成能力并不出色。只有把自回归模型逐渐改进后，Diffusion Loss 的效果才能逐渐体现出来。在后几行掩码自回归模型中，Diffusion Loss 的作用还是很大的。</p>
<p>而对比前三大行，我们可以发现自回归模型的架构极大地提升了生成效果，且似乎将 Transformer 由 causal 改成 bidirect 的提升更加显著。</p>
<p>第四大行相比第三大行，提升了每次预测的词元数，主要是为了加速。这两行的对比结果表明，做了这个加速操作后，模型生成能力并没有下降多少。后续实验都是基于第四行的配置。</p>
<h3 id="Diffusion-Loss-适配不同的自编码器"><a href="#Diffusion-Loss-适配不同的自编码器" class="headerlink" title="Diffusion Loss 适配不同的自编码器"></a>Diffusion Loss 适配不同的自编码器</h3><p>相比原来类别分布，用 Diffusion Loss 解除了自编码器必须输出离散图像的限制。因此，目前的模型能够适配多种自编码器，如下表所示。图中 rFID 指的是图像重建任务的 FID，越低越好。这里的 VQ-16 指的是将 VQGAN 的 VQ 层当作解码器的一部分，这样 VQGAN 的编码器输出也可以看成是连续图像，和 LDM 里的做法一样。最后一行的 KL-16 是作者重新重新在 ImageNet 上训练的 VAE，而前两行的 VQ-16 和 KL-16 是在 OpenImages 上训练的。由于后文的实验都基于 ImageNet，所以后文都会用第五行那个 VAE。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/14.jpg" alt></p>
<p>首先对比一下这里 VQ-16 w/o CFG 的 FID 和上表里最后一大行 CrossEnt 的 FID。这两组实验的自编码器相同，仅有误差函数不同。将误差函数从交叉熵换成了 Diffusion Loss 后，FID 从 8.79 变成了 7.82。这一项直接对比的实验证明了不考虑自编码器的改进时，Diffusion Loss 本身的优越性。</p>
<p>再对比前两行，KL 的自编码器无论是图像恢复指标还是最后的生成指标都优于 VQ 的自编码器。这印证了论文开头想要抛弃 VQ 自编码器的动机：VQ 自编码器逊于 KL 自编码器。</p>
<p>第三、第四行展示了方法也可以兼容下采样 8 倍的自编码器。本来测试用的 ImageNet 是 $256 \times 256$ 大小的，按照一开始下采样 16 倍的配置，能得到 $16 \times 16$ 的压缩图像，即输入 Transformer 的词元序列长度为 $16 \times 16$。现在改成了下采样 8 倍后，为了兼容之前 $16 \times 16$ 的序列长度，作者把 $2 \times 2$ 个像素打包成一个词元。论文里没讲是怎么打包的，我猜测是在通道上拼接。Consistency 是另一套自编码器，作者展示这个估计是为了说明这套方法兼容性很强。</p>
<h3 id="和-SOTA-图像生成模型对比"><a href="#和-SOTA-图像生成模型对比" class="headerlink" title="和 SOTA 图像生成模型对比"></a>和 SOTA 图像生成模型对比</h3><p>为了证明方法的优越性，论文还展示了本工作与其他 SOTA 工作在 ImageNet 图像生成任务上的定量对比结果。下表是 ImageNet $256 \times 256$ 的结果。为了方便对比，我还贴出了 DiT 论文里展示的表格（左表）。本文的模型在表里被称作 MAR。 </p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/15.jpg" alt></p>
<p>下表是 ImageNet $512 \times 512$ 的结果。左边那张表是 EDM2 展示的结果。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/16.jpg" alt></p>
<p>从表里可以看出，本工作在 ImageNet 图像生成任务上表现很不错，超越了绝大多数模型。</p>
<h3 id="图像生成速度对比"><a href="#图像生成速度对比" class="headerlink" title="图像生成速度对比"></a>图像生成速度对比</h3><p>下面是不同生成模型的速度对比结果。第一张图是本论文展示的和 DiT 的对比结果。DIT 采用的扩散模型采样步数是 (50 ,75, 150, 250)。由于本工作的性能瓶颈在自回归模型而不在扩散模型上，所以本工作展示的不同采样步数由自回归步数决定。图中的自回归步数是 (8, 16, 32, 64, 128)。中间的图是 LDM 的结果，同模型不同点表示的是采样步数为 (10, 20, 50, 100, 200) 的结果。右边的表是 EDM2 的采样速度等指标。左边两张图是 ImageNet $256 \times 256$ 上的，最右边的表是 ImageNet $512 \times 512$ 上的。</p>
<p><img src="/2024/07/27/20240717-ar-wo-vq/17.jpg" alt></p>
<p>由于不同图表的采样速度指标不太一样，我们将指标统一成每秒生成的图像。从第一张图的对比可以看出，DiT 最快也是一秒 2.5 张图像左右，而 MAR 又快又好，默认（自回归步数 64）一秒生成 3 张图左右。同时，通过 MAR 和有 kv cache 加速的标准 AR 的对比，我们能发现 MAR 在默认自回归步数下还是比标准 AR 慢了不少。</p>
<p>我们再看中间 LDM 的速度。我们观察一下最常使用的 LDM-8。如果是令 DDIM 步数为 20 （第二快的结果）的话，LDM-8 的生成速度在一秒 16 张图像左右，还是比 MAR 快很多。DDIM 步数取 50 时也会比 MAR 快一些。</p>
<p>最后看右边较新的图像扩散模型 EDM2 的速度。由于这个是在 $512 \times 512$ 的图片上测试的，和前面的速度相比时大概要乘个 4。哪怕是最大的 XXL 模型，在有 guidance 时，生成速度也是 2 张图片每秒。换算到 $256 \times 256$ 上约 8 张图片每秒，还是比 MAR 快。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>自回归图像生成中的向量离散化和类别分布必须同时使用。为了去除表现较差的向量离散化操作，本工作的作者重新用扩散模型建模了自回归中下一个图像词元的分布，从而提升了模型的生成能力。由于标准自回归模型生成能力有限，为了进一步提升模型，作者又引入了最新的掩码自回归模型。最终的模型在 ImageNet 图像生成指标上取得了几乎最顶尖的结果。</p>
<p>以上是论文的叙述逻辑。但掩码自回归那一块应该是之前工作的研究成果，这篇文章实际上就是把新提出的 diffusion loss 用到了掩码自回归上，把本来在 ImageNet 上生成能力尚可的掩码自回归推到了最前列。</p>
<p>这篇文章在科研上的最大创新是打破了大家在图像自回归上的固有思维，认为必须用离散词元，必须用类别分布。但仔细一想，建模一个分布的方法其实许许多多。随便把另一种生成完整图像的模型用到生成一个像素上，就能取代之前的类别分布，得到更好的图像生成结果。这篇文章用简单的 DDPM 只是为了验证这个想法的可行性，用更复杂的模型或许能有更好的结果，但用 DDPM 做验证就足够了。之后肯定会有各种后续工作，研究如何用更好的模型来建模本框架中一个像素值的分布。</p>
<p>反过来想，这篇文章也在提醒我们，扩散模型并不只是可以用来生成图像，它的本质是建模一个分布。如果某个模型中间需要建模一个简单的分布的话，都可以尝试用 DDPM。</p>
<p>相比其科研创新，这篇文章在 ImageNet 图像生成指标的成就反而没有那么耀眼了。本工作在 ImageNet 的 FID 等指标上取得了几乎最优的结果，战胜了多数最强的扩散模型，有望将大家的科研眼光从扩散模型移到自回归上。但由于自回归本身步数较多，且每一步要在 Transformer 里做完整的注意力操作，这种方法的速度还是比扩散模型要慢一点。</p>
<p>目前 GitHub 上已有本工作的复现：<a target="_blank" rel="noopener" href="https://github.com/lucidrains/autoregressive-diffusion-pytorch">https://github.com/lucidrains/autoregressive-diffusion-pytorch</a> 。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/en/2024/07/27/20240605-diffusers-training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/en/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="Designer, artist, philosopher, researcher.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/en/2024/07/27/20240605-diffusers-training/" class="post-title-link" itemprop="url">定制适合自己的 Diffusers 扩散模型训练脚本</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-07-27 20:31:57" itemprop="dateCreated datePublished" datetime="2024-07-27T20:31:57+08:00">2024-07-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%88%9B%E4%BD%9C/" itemprop="url" rel="index"><span itemprop="name">创作</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/en/categories/%E5%88%9B%E4%BD%9C/%E7%BC%96%E7%A8%8B%E9%A1%B9%E7%9B%AE/" itemprop="url" rel="index"><span itemprop="name">编程项目</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Diffusers 库为社区用户提供了多种扩散模型任务的训练脚本。每个脚本都平铺直叙，没有多余的封装，把训练的绝大多数细节都写在了一个脚本里。这种设计既能让入门用户在不阅读源码的前提下直接用脚本训练，又方便高级用户直接修改脚本。</p>
<p>可是，这种设计就是最好的吗？关于训练脚本的最佳设计风格，社区用户们往往各执一词。有人更喜欢更贴近 PyTorch 官方示例的写法，而有人会喜欢用 PyTorch Lightning 等封装度高、重复代码少的库。而在我看来，选择哪种风格的训练脚本，确实是个人喜好问题。但是，在开始使用训练脚本之前，我们要从细节入手，理解训练脚本到底要做哪些事。学懂了之后，不管是用别人的训练库，还是定制适合自己的训练脚本，都是很轻松的。不管怎么说，Diffusers 的这种训练脚本是一份很好的学习素材。</p>
<p>当然，我在用 Diffusers 的训练脚本时，发现一旦涉及多类任务的训练，比如既要能训练 Stable Diffusion，又要能训练 VAE，那么这份脚本就会用起来比较困难，而写两份训练脚本又会有很大的冗余。Diffusers 的训练脚本依然有改进的空间。</p>
<p>在这篇文章中，我会主要面向想系统性学习扩散模型训练框架的读者，先详细介绍 Diffusers 官方训练脚本，再分享我重构训练脚本的过程，使得脚本能够更好地兼容多类模型的训练。文章的末尾，我会展示几个简单的扩散模型训练实例。</p>
<p>在阅读本文时，建议大家用电脑端，一边看源代码一边读文章。「官方训练脚本细读」一节细节较多，初次阅读时可以快速浏览，看完「训练脚本内容总结」中的流程图，再回头仔细看一遍。</p>
<h2 id="准备源代码"><a href="#准备源代码" class="headerlink" title="准备源代码"></a>准备源代码</h2><p>我们将以最简单的 DDPM 官方训练脚本 <code>examples/unconditional_image_generation/train_unconditional.py</code> 为例，学习训练脚本的通用写法。<code>examples</code> 文件夹在位于 Diffusers 官方 GitHub 仓库中，用 pip 安装的 Diffusers 可能没有这个文件夹，最好是手动 clone 官方仓库，再在本地查看这个文件夹。使用 Diffusers 训练时，可能还要安装其他库。官方在不同的训练教程里给了不同的安装指令，建议大家都安装上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd examples/text_to_image</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line">pip install diffusers[training]</span><br></pre></td></tr></table></figure>
<p>我为本教程准备的脚本在仓库 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample">https://github.com/SingleZombie/DiffusersExample</a> 中。请 clone 这个仓库，再切换到 <code>TrainingScript</code> 目录下。<code>train_official.py</code> 是原官方训练脚本 <code>train_unconditional.py</code>，<code>train_0.py</code> 是第一次修改后的训练脚本<br>，<code>train_1.py</code> 是第二次修改后的训练脚本。</p>
<h2 id="官方训练脚本细读"><a href="#官方训练脚本细读" class="headerlink" title="官方训练脚本细读"></a>官方训练脚本细读</h2><p>先拉到文件的最底部，我们能在这找到程序的入口。在 <code>parse_args</code> 函数中，脚本会用 <code>argparse</code> 库解析命令行参数，并将所有参数保存在 <code>args</code> 里。<code>args</code> 会传进 <code>main</code> 函数里。稍后我们看到所有 <code>args.</code> 打头的变量调用，都表明该变量来自于命令行参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    args = parse_args()</span><br><span class="line">    main(args)</span><br></pre></td></tr></table></figure>
<p>接着，我们正式开始学习训练主函数。一开始，函数会配置 accelerate 库及日志记录器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">logging_dir = os.path.join(args.output_dir, args.logging_dir)</span><br><span class="line">accelerator_project_config = ProjectConfiguration(</span><br><span class="line">    project_dir=args.output_dir, logging_dir=logging_dir)</span><br><span class="line"></span><br><span class="line"><span class="comment"># a big number for high resolution or big dataset</span></span><br><span class="line">kwargs = InitProcessGroupKwargs(timeout=timedelta(seconds=<span class="number">7200</span>))</span><br><span class="line">accelerator = Accelerator(...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_tensorboard_available():</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">elif</span> args.logger == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_wandb_available():</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">import</span> wandb</span><br></pre></td></tr></table></figure>
<p>在配置日志的中途，函数插入了一段修改模型存取逻辑的代码。为了让我们阅读代码的顺序与实际运行顺序一致，我们等待会用到了这段代码时再回头来读。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># `accelerate` 0.16.0 will have better support for customized saving</span></span><br><span class="line"><span class="keyword">if</span> version.parse(accelerate.__version__) &gt;= version.parse(<span class="string">&quot;0.16.0&quot;</span>):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">models, weights, output_dir</span>):</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">models, input_dir</span>):</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>跳过上面的代码，还是日志配置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make one log on every process with the configuration for debugging.</span></span><br><span class="line">logging.basicConfig(</span><br><span class="line">    <span class="built_in">format</span>=<span class="string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,</span><br><span class="line">    datefmt=<span class="string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,</span><br><span class="line">    level=logging.INFO,</span><br><span class="line">)</span><br><span class="line">logger.info(accelerator.state, main_process_only=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">if</span> accelerator.is_local_main_process:</span><br><span class="line">    datasets.utils.logging.set_verbosity_warning()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_info()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    datasets.utils.logging.set_verbosity_error()</span><br><span class="line">    diffusers.utils.logging.set_verbosity_error()</span><br></pre></td></tr></table></figure>
<p>之后其他版本的训练脚本会有一段设置随机种子的代码，我们给这份脚本补上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If passed along, set the training seed now.</span></span><br><span class="line"><span class="keyword">if</span> args.seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    set_seed(args.seed)</span><br></pre></td></tr></table></figure>
<p>接着，函数会创建输出文件夹。如果我们想把模型推送到在线仓库上，函数还会创建一个仓库。</p>
<p>这段代码还出现了一行比较重要的判断语句：<code>if accelerator.is_main_process:</code>。在多卡训练时，只有主进程会执行这个条件语句块里的内容。该判断在并行编程中十分重要。很多时候，比如在输出、存取模型时，我们只需要让一个进程执行操作就行了。这个时候就要用到这行判断语句。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Handle the repository creation</span></span><br><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> args.output_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        os.makedirs(args.output_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        repo_id = create_repo(...).repo_id</span><br></pre></td></tr></table></figure>
<p>准备完辅助工具后，函数开始准备模型。输入参数里的 <code>model_config_name_or_path</code> 表示预定义的模型配置文件。如果该配置文件不存在，则函数会用默认的配置创建一个 DDPM 的 U-Net 模型。在写我们自己的训练脚本时，我们需要在这个地方初始化我们需要的所有模型。比如训练 Stable Diffusion 时，除了 U-Net，需要在此处准备 VAE、CLIP 文本编码器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the model</span></span><br><span class="line"><span class="keyword">if</span> args.model_config_name_or_path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    model = UNet2DModel(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    config = UNet2DModel.load_config(args.model_config_name_or_path)</span><br><span class="line">    model = UNet2DModel.from_config(config)</span><br></pre></td></tr></table></figure>
<p>这份脚本还帮我们写好了维护 EMA（指数移动平均）模型的功能。EMA 模型用于存储模型可学习的参数的局部平均值。有时 EMA 模型的效果会比原模型要好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create EMA for the model.</span></span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    ema_model = EMAModel(</span><br><span class="line">        model.parameters(),</span><br><span class="line">        model_cls=UNet2DModel,</span><br><span class="line">        model_config=model.config,</span><br><span class="line">        ...)</span><br></pre></td></tr></table></figure>
<p>此处函数还会根据 accelerate 配置自动设置模型的精度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">weight_dtype = torch.float32</span><br><span class="line"><span class="keyword">if</span> accelerator.mixed_precision == <span class="string">&quot;fp16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.float16</span><br><span class="line">    args.mixed_precision = accelerator.mixed_precision</span><br><span class="line"><span class="keyword">elif</span> accelerator.mixed_precision == <span class="string">&quot;bf16&quot;</span>:</span><br><span class="line">    weight_dtype = torch.bfloat16</span><br><span class="line">    args.mixed_precision = accelerator.mixed_precision</span><br></pre></td></tr></table></figure>
<p>函数还会尝试启用 <code>xformers</code> 来提升 Attention 的效率。PyTorch 在 2.0 版本也加入了类似的 Attention 优化技术。如果你的显卡性能有限，且 PyTorch 版本小于 2.0，可以考虑使用 <code>xformers</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.enable_xformers_memory_efficient_attention:</span><br><span class="line">    <span class="keyword">if</span> is_xformers_available():</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>准备了 U-Net 后，函数会准备噪声调度器，即定义扩散模型的细节。</p>
<blockquote>
<p>注意，扩散模型不是一个神经网络，而是一套定义了加噪、去噪公式的模型。扩散模型中需要一个去噪模型来去噪，去噪模型一般是一个神经网络。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the scheduler</span></span><br><span class="line">accepts_prediction_type = <span class="string">&quot;prediction_type&quot;</span> <span class="keyword">in</span> <span class="built_in">set</span>(</span><br><span class="line">    inspect.signature(DDPMScheduler.__init__).parameters.keys())</span><br><span class="line"><span class="keyword">if</span> accepts_prediction_type:</span><br><span class="line">    noise_scheduler = DDPMScheduler(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    noise_scheduler = DDPMScheduler(...)</span><br></pre></td></tr></table></figure>
<p>准备完所有扩散模型组件后，函数开始准备其他和训练相关的模块。其他版本的训练脚本会在这个地方加一段缓存梯度和自动放缩学习率的代码，我们给这份脚本补上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.gradient_checkpointing:</span><br><span class="line">    unet.enable_gradient_checkpointing()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.scale_lr:</span><br><span class="line">    args.learning_rate = (</span><br><span class="line">        args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>函数先准备的训练模块是优化器。这里默认使用的优化器是 <code>AdamW</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=args.learning_rate,</span><br><span class="line">    betas=(args.adam_beta1, args.adam_beta2),</span><br><span class="line">    weight_decay=args.adam_weight_decay,</span><br><span class="line">    eps=args.adam_epsilon,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>函数随后会准备训练集。这个脚本用 HuggingFace 的 datasets 库来管理数据集。我们既可以读取在线数据集，也可以读取本地的图片文件夹数据集。自定义数据集的方法可以参考 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder">https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</a> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.dataset_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        args.dataset_name,</span><br><span class="line">        args.dataset_config_name,</span><br><span class="line">        cache_dir=args.cache_dir,</span><br><span class="line">        split=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dataset = load_dataset(</span><br><span class="line">        <span class="string">&quot;imagefolder&quot;</span>, data_dir=args.train_data_dir, cache_dir=args.cache_dir, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    <span class="comment"># See more about loading custom images at</span></span><br><span class="line">    <span class="comment"># https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder</span></span><br></pre></td></tr></table></figure>
<p>有了数据集后，函数会继续准备 PyTorch 的 DataLoader。在这一步中，除了定义 DataLoader 外，我们还要编写数据预处理的方法。下面这段代码的编写顺序和执行顺序不同，我们按执行顺序来整理一遍下面的代码：</p>
<ol>
<li>将预定义的预处理函数传给数据集对象 <code>dataset.set_transform(transform_images)</code>。在使用数据集里的数据时，才会调用这个函数预处理图像。</li>
<li>使用 PyTorch API 定义 DataLoader。<code>train_dataloader = ...</code></li>
<li>每次用 DataLoader 获取数据时，一个数据词典 <code>examples</code> 会被传入预处理函数 <code>transform_images</code>。<code>examples</code> 里既包含了图像数据，也包含了数据的各种标签。而对于无约束图像生成任务，我们只需要图像数据，因此可以直接通过词典的 <code>&quot;image&quot;</code> 键得到 PIL 格式的图像数据。用 <code>convert(&quot;RGB&quot;)</code> 把图像转成三通道后，该 PIL 图像会被传入预处理流水线。</li>
<li>图像预处理流水线 <code>augmentations</code> 是用 Torchvision 里的 <code>transform</code> API 定义的。默认的流水线包括短边缩放至指定分辨率、按分辨率裁剪、随机反转、归一化。</li>
<li>处理过的数据会被存到词典的 <code>&quot;input&quot;</code> 键里。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocessing the datasets and DataLoaders creation.</span></span><br><span class="line">augmentations = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize(</span><br><span class="line">            args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),</span><br><span class="line">        transforms.CenterCrop(</span><br><span class="line">            args.resolution) <span class="keyword">if</span> args.center_crop <span class="keyword">else</span> transforms.RandomCrop(args.resolution),</span><br><span class="line">        transforms.RandomHorizontalFlip() <span class="keyword">if</span> args.random_flip <span class="keyword">else</span> transforms.Lambda(<span class="keyword">lambda</span> x: x),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_images</span>(<span class="params">examples</span>):</span></span><br><span class="line">    images = [augmentations(image.convert(<span class="string">&quot;RGB&quot;</span>))</span><br><span class="line">                <span class="keyword">for</span> image <span class="keyword">in</span> examples[<span class="string">&quot;image&quot;</span>]]</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;input&quot;</span>: images&#125;</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">f&quot;Dataset size: <span class="subst">&#123;<span class="built_in">len</span>(dataset)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">dataset.set_transform(transform_images)</span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=args.train_batch_size, shuffle=<span class="literal">True</span>, num_workers=args.dataloader_num_workers</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>在准备工作的最后，函数会准备学习率调度器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize the learning rate scheduler</span></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    args.lr_scheduler,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,</span><br><span class="line">    num_training_steps=(<span class="built_in">len</span>(train_dataloader) * args.num_epochs),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>准备完了所有模块，函数会调用 accelerate 库来把所有模块变成适合并行训练的模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(</span><br><span class="line">    model, optimizer, train_dataloader, lr_scheduler</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    ema_model.to(accelerator.device)</span><br></pre></td></tr></table></figure>
<p>之后函数还会用 accelerate 库配置训练日志。默认情况下日志名 <code>run</code> 由当前脚本名决定。如果不想让之前的日志被覆盖的话，可以让日志名 <code>run</code> 由当前的时间决定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    run = os.path.split(__file__)[-<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    accelerator.init_trackers(run)</span><br></pre></td></tr></table></figure>
<p>马上就要开始训练了。在此之前，函数会准备全局变量并记录日志。注意，这里函数会算一次总的 batch 数，它由输入 batch 数、进程数（显卡数）、梯度累计步数共同决定。梯度累计是一种用较少的显存实现大 batch 训练的技术。使用这项技术时，训练梯度不会每步优化，而是累计了若干步后再优化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">total_batch_size = args.train_batch_size * \</span><br><span class="line">    accelerator.num_processes * args.gradient_accumulation_steps</span><br><span class="line">num_update_steps_per_epoch = math.ceil(</span><br><span class="line">    <span class="built_in">len</span>(train_dataloader) / args.gradient_accumulation_steps)</span><br><span class="line">max_train_steps = args.num_epochs * num_update_steps_per_epoch</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&quot;***** Running training *****&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Num examples = <span class="subst">&#123;<span class="built_in">len</span>(dataset)&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Num Epochs = <span class="subst">&#123;args.num_epochs&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Instantaneous batch size per device = <span class="subst">&#123;args.train_batch_size&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Total train batch size (w. parallel, distributed &amp; accumulation) = <span class="subst">&#123;total_batch_size&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(</span><br><span class="line">    <span class="string">f&quot;  Gradient Accumulation steps = <span class="subst">&#123;args.gradient_accumulation_steps&#125;</span>&quot;</span>)</span><br><span class="line">logger.info(<span class="string">f&quot;  Total optimization steps = <span class="subst">&#123;max_train_steps&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">global_step = <span class="number">0</span></span><br><span class="line">first_epoch = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>在开始训练前，如果设置了 <code>args.resume_from_checkpoint</code>，则函数会读取之前训练过的权重。负责读取训练权重的函数是 <code>load_state</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint:</span><br><span class="line">    <span class="keyword">if</span> args.resume_from_checkpoint != <span class="string">&quot;latest&quot;</span>:</span><br><span class="line">        path = ..</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Get the most recent checkpoint</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> path <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        accelerator.load_state(os.path.join(args.output_dir, path))</span><br><span class="line">        accelerator.<span class="built_in">print</span>(<span class="string">f&quot;Resuming from checkpoint <span class="subst">&#123;path&#125;</span>&quot;</span>)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在每个 epoch 中，函数会重置进度条。接着，函数会进入每一个 batch 的训练迭代。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train!</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(first_epoch, args.num_epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    progress_bar = tqdm(total=num_update_steps_per_epoch,</span><br><span class="line">                        disable=<span class="keyword">not</span> accelerator.is_local_main_process)</span><br><span class="line">    progress_bar.set_description(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br></pre></td></tr></table></figure>
<p>如果是继续训练的话，训练开始之前会更新当前的步数 <code>step</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Skip steps until we reach the resumed step</span></span><br><span class="line"><span class="keyword">if</span> args.resume_from_checkpoint <span class="keyword">and</span> epoch == first_epoch <span class="keyword">and</span> step &lt; resume_step:</span><br><span class="line">    <span class="keyword">if</span> step % args.gradient_accumulation_steps == <span class="number">0</span>:</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<p>训练的一开始，函数会从数据的 <code>&quot;input&quot;</code> 键里取出图像数据。此处的键名是我们之前在数据预处理函数 <code>transform_images</code> 里写的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clean_images = batch[<span class="string">&quot;input&quot;</span>].to(weight_dtype)</span><br></pre></td></tr></table></figure>
<p>之后函数会设置扩散模型训练中的其他变量，包含随机噪声、时刻。由于本文的重点并不是介绍扩散模型的原理，这段代码我们就快速略过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">noise = torch.randn(...)</span><br><span class="line">timesteps =...</span><br><span class="line">noisy_images = noise_scheduler.add_noise(</span><br><span class="line">    clean_images, noise, timesteps)</span><br></pre></td></tr></table></figure>
<p>接下来，函数会用去噪网络做前向传播。为了让模型能正确累计梯度，我们要用 <code>with accelerator.accumulate(model):</code> 把模型调用与反向传播的逻辑包起来。在这段代码中，我们会先得到模型的输出 <code>model_output</code>，再根据扩散模型得到损失函数 <code>loss</code>，最后用 accelerate 库的 API <code>accelerator</code> 代替原来 PyTorch API 来完成反向传播、梯度裁剪，并完成参数更新、学习率调度器更新、优化器更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> accelerator.accumulate(model):</span><br><span class="line">    <span class="comment"># Predict the noise residual</span></span><br><span class="line">    model_output = model(noisy_images, timesteps).sample</span><br><span class="line"></span><br><span class="line">    loss = ...</span><br><span class="line"></span><br><span class="line">    accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">        accelerator.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    lr_scheduler.step()</span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>确保一步训练结束后，函数会更新和步数相关的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.sync_gradients:</span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.step(model.parameters())</span><br><span class="line">    progress_bar.update(<span class="number">1</span>)</span><br><span class="line">    global_step += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>在这个地方，函数还会尝试保存模型。默认情况下，每 <code>args.checkpointing_steps</code> 步保存一次中间结果。确认要保存后，函数会算出当前的保存点名称，并根据最大保存点数 <code>checkpoints_total_limit</code> 决定是否要删除以前的保存点。做完准备后，函数会调用 <code>save_state</code> 保存当前训练时的所有中间变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">f accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> global_step % args.checkpointing_steps == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> args.checkpoints_total_limit <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            checkpoints = os.listdir(args.output_dir)</span><br><span class="line">            checkpoints = [</span><br><span class="line">                d <span class="keyword">for</span> d <span class="keyword">in</span> checkpoints <span class="keyword">if</span> d.startswith(<span class="string">&quot;checkpoint&quot;</span>)]</span><br><span class="line">            checkpoints = <span class="built_in">sorted</span>(</span><br><span class="line">                checkpoints, key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x.split(<span class="string">&quot;-&quot;</span>)[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(checkpoints) &gt;= args.checkpoints_total_limit:</span><br><span class="line">                ...</span><br><span class="line"></span><br><span class="line">            save_path = os.path.join(</span><br><span class="line">            args.output_dir, <span class="string">f&quot;checkpoint-<span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">            accelerator.save_state(save_path)</span><br><span class="line">            logger.info(<span class="string">f&quot;Saved state to <span class="subst">&#123;save_path&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个地方，主函数开头设置的存取模型回调函数终于派上用场了。在调用 <code>save_state</code> 时，会自动触发下面的回调函数来保存模型。如果不加下面的代码，所有模型默认会以 <code>.safetensor</code> 的形式存下来。而用了下面的代码后，模型能够被 <code>save_pretrained</code> 存进一个文件夹里，就像其他标准 Diffusers 模型一样。</p>
<blockquote>
<p>这里的输入参数 <code>models</code> 来自于之前的 <code>accelerator.prepare</code>，感兴趣可以去阅读文档或源码。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">models, weights, output_dir</span>):</span></span><br><span class="line">    <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">        <span class="keyword">if</span> args.use_ema:</span><br><span class="line">            ema_model.save_pretrained(</span><br><span class="line">                os.path.join(output_dir, <span class="string">&quot;unet_ema&quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, model <span class="keyword">in</span> <span class="built_in">enumerate</span>(models):</span><br><span class="line">            model.save_pretrained(os.path.join(output_dir, <span class="string">&quot;unet&quot;</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># make sure to pop weight so that corresponding model is not saved again</span></span><br><span class="line">            weights.pop()</span><br></pre></td></tr></table></figure>
<p>与上面的这段代码对应，脚本还提供了读取文件的回调函数。它会在继续中断的训练后调用 <code>load_state</code> 时被调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">models, input_dir</span>):</span></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        load_model = EMAModel.from_pretrained(</span><br><span class="line">            os.path.join(input_dir, <span class="string">&quot;unet_ema&quot;</span>), UNet2DModel)</span><br><span class="line">        ema_model.load_state_dict(load_model.state_dict())</span><br><span class="line">        ema_model.to(accelerator.device)</span><br><span class="line">        <span class="keyword">del</span> load_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(models)):</span><br><span class="line">        <span class="comment"># pop models so that they are not loaded again</span></span><br><span class="line">        model = models.pop()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># load diffusers style into model</span></span><br><span class="line">        load_model = UNet2DModel.from_pretrained(</span><br><span class="line">            input_dir, subfolder=<span class="string">&quot;unet&quot;</span>)</span><br><span class="line">        model.register_to_config(**load_model.config)</span><br><span class="line"></span><br><span class="line">        model.load_state_dict(load_model.state_dict())</span><br><span class="line">        <span class="keyword">del</span> load_model</span><br></pre></td></tr></table></figure>
<p>两个回调函数需要用下面的代码来设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">accelerator.register_save_state_pre_hook(save_model_hook)</span><br><span class="line">accelerator.register_load_state_pre_hook(load_model_hook)</span><br></pre></td></tr></table></figure>
<p>回到最新的代码处。训练迭代的末尾，脚本会记录当前步的日志。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">logs = &#123;<span class="string">&quot;loss&quot;</span>: loss.detach().item(), <span class="string">&quot;lr&quot;</span>: lr_scheduler.get_last_lr()[<span class="number">0</span>], <span class="string">&quot;step&quot;</span>: global_step&#125;</span><br><span class="line"><span class="keyword">if</span> args.use_ema:</span><br><span class="line">    logs[<span class="string">&quot;ema_decay&quot;</span>] = ema_model.cur_decay_value</span><br><span class="line">progress_bar.set_postfix(**logs)</span><br><span class="line">accelerator.log(logs, step=global_step)</span><br></pre></td></tr></table></figure>
<p>执行完了一个 epoch 后，脚本调用 accelerate API 保证所有进程均训练完毕。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">progress_bar.close()</span><br><span class="line">accelerator.wait_for_everyone()</span><br></pre></td></tr></table></figure>
<p>此处脚本可能会在主进程中验证模型或保存模型。如果当前是最后一个 epoch，或者达到了配置指定的验证/保存时刻，脚本就会执行验证/保存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">    <span class="keyword">if</span> epoch % args.save_images_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % args.save_model_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>脚本默认的验证方法是随机生成图片，并用日志库保存图片。生成图片的方法是使用标准 Diffusers 采样流水线 <code>DDPMPipeline</code>。由于此时模型 <code>model</code> 可能被包裹成了一个用于多卡训练的 PyTorch 模块，需要用相关 API 把 <code>model</code> 解包成普通 PyTorch 模块 <code>unet</code>。如果使用了 EMA 模型，为了避免对 EMA 模型的干扰，此处需要先保存 EMA 模型参数，采样结束再还原参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % args.save_images_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">    unet = accelerator.unwrap_model(model)</span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.store(unet.parameters())</span><br><span class="line">        ema_model.copy_to(unet.parameters())</span><br><span class="line"></span><br><span class="line">    pipeline = DDPMPipeline(</span><br><span class="line">        unet=unet,</span><br><span class="line">        scheduler=noise_scheduler,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    generator = torch.Generator(device=pipeline.device).manual_seed(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># run pipeline in inference (sample random noise and denoise)</span></span><br><span class="line">    images = pipeline(...).images</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.restore(unet.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># denormalize the images and save to tensorboard</span></span><br><span class="line">    images_processed = (images * <span class="number">255</span>).<span class="built_in">round</span>().astype(<span class="string">&quot;uint8&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">elif</span> args.logger == <span class="string">&quot;wandb&quot;</span>:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>在保存模型时，脚本同样会先用去噪模型 <code>model</code> 构建一个流水线，再调用流水线的保存方法 <code>save_pretrained</code> 将扩散模型的所有组件（去噪模型、噪声调度器）保存下来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> epoch % args.save_model_epochs == <span class="number">0</span> <span class="keyword">or</span> epoch == args.num_epochs - <span class="number">1</span>:</span><br><span class="line">    <span class="comment"># save the model</span></span><br><span class="line">    unet = accelerator.unwrap_model(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.store(unet.parameters())</span><br><span class="line">        ema_model.copy_to(unet.parameters())</span><br><span class="line"></span><br><span class="line">    pipeline = DDPMPipeline(</span><br><span class="line">        unet=unet,</span><br><span class="line">        scheduler=noise_scheduler,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    pipeline.save_pretrained(args.output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.use_ema:</span><br><span class="line">        ema_model.restore(unet.parameters())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.push_to_hub:</span><br><span class="line">        upload_folder(...)</span><br></pre></td></tr></table></figure>
<p>一个 epoch 训练的代码就到此结束了。所有 epoch 的训练结束后，脚本调用 API 结束训练。这个 API 会自动关闭所有的日志库。训练代码到这里也就结束了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accelerator.end_training()</span><br></pre></td></tr></table></figure>
<h2 id="训练脚本内容总结"><a href="#训练脚本内容总结" class="headerlink" title="训练脚本内容总结"></a>训练脚本内容总结</h2><p>大概熟悉了一遍这份训练脚本后，我们可以用下面的流程图概括训练脚本的执行顺序和主要内容。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/1.jpg" alt></p>
<h2 id="去掉命令行参数"><a href="#去掉命令行参数" class="headerlink" title="去掉命令行参数"></a>去掉命令行参数</h2><p>我不喜欢用命令行参数传训练参数，而喜欢把训练参数写进配置文件里，理由有：</p>
<ul>
<li>我一般会直接在命令行里手敲命令。如果命令行参数过多，我则会把要运行的命令及其参数保存在某文件里。这样还不如把参数写在另外的文件里。</li>
<li>将大量参数藏在一个词典 <code>args</code> 里，而不是把所有需用的参数在某处定义好，是一种很差的编程方式。各个参数将难以追踪。</li>
</ul>
<p>在正式重构脚本之前，我做的第一步是去掉脚本中原来的命令行参数，将所有参数先塞进一个数据类里面。脚本将只留一个命令行参数，表示参数配置文件的路径。具体做法如下：</p>
<p>先编写一个存命令行参数的数据类。这个类是一个 Python 的 <code>dataclass</code>。Python 中 <code>dataclass</code> 是一种专门用来放数据的类。定义数据类时，我们只需要定义类中所有数据的类型及默认值，不需要编写任何方法。初始化数据类时，我们只需要传一个词典或列表。一个示例如下（示例来源 <a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/understanding-python-dataclasses/）：">https://www.geeksforgeeks.org/understanding-python-dataclasses/）：</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"> </span><br><span class="line"><span class="comment"># A class for holding an employees content</span></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">employee</span>:</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Attributes Declaration</span></span><br><span class="line">    <span class="comment"># using Type Hints</span></span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    emp_id: <span class="built_in">str</span></span><br><span class="line">    age: <span class="built_in">int</span></span><br><span class="line">    city: <span class="built_in">str</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">emp1 = employee(<span class="string">&quot;Satyam&quot;</span>, <span class="string">&quot;ksatyam858&quot;</span>, <span class="number">21</span>, <span class="string">&#x27;Patna&#x27;</span>)</span><br><span class="line">emp2 = employee(<span class="string">&quot;Anurag&quot;</span>, <span class="string">&quot;au23&quot;</span>, <span class="number">28</span>, <span class="string">&#x27;Delhi&#x27;</span>)</span><br><span class="line">emp3 = employee(&#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;Satyam&quot;</span>, </span><br><span class="line">   <span class="string">&quot;emp_id&quot;</span>: <span class="string">&quot;ksatyam858&quot;</span>, </span><br><span class="line">   <span class="string">&quot;age&quot;</span>: <span class="number">21</span>, </span><br><span class="line">   <span class="string">&quot;city&quot;</span>: <span class="string">&#x27;Patna&#x27;</span>&#125;)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;employee object are :&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(emp1)</span><br><span class="line"><span class="built_in">print</span>(emp2)</span><br><span class="line"><span class="built_in">print</span>(emp3)</span><br></pre></td></tr></table></figure></p>
<p>我们可以用 <code>dataclass</code> 编写一个存储所有命令行参数的数据类，该类开头内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseTrainingConfig</span>:</span></span><br><span class="line">    <span class="comment"># Dir</span></span><br><span class="line">    logging_dir: <span class="built_in">str</span></span><br><span class="line">    output_dir: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Logger and checkpoint</span></span><br><span class="line">    logger: <span class="built_in">str</span> = <span class="string">&#x27;tensorboard&#x27;</span></span><br><span class="line">    checkpointing_steps: <span class="built_in">int</span> = <span class="number">500</span></span><br><span class="line">    checkpoints_total_limit: <span class="built_in">int</span> = <span class="number">20</span></span><br><span class="line">    valid_epochs: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    valid_batch_size: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">    save_model_epochs: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    resume_from_checkpoint: <span class="built_in">str</span> = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>之后在训练脚本里，我们可以把旧的命令行参数全删了，再加一个命令行参数 <code>cfg</code>，表示训练配置文件的路径。我们可以用 <code>omegaconf</code> 打开这个配置文件，得到一个词典 <code>data_dict</code>，再用这个词典构建配置文件 <code>cfg</code>。接下来，只需要把原来代码里所有 <code>args.</code> 改成 <code>cfg.</code> 就行了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> omegaconf <span class="keyword">import</span> OmegaConf</span><br><span class="line"><span class="keyword">from</span> training_cfg_0 <span class="keyword">import</span> BaseTrainingConfig</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&#x27;cfg&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">data_dict = OmegaConf.load(args.cfg)</span><br><span class="line">cfg = BaseTrainingConfig(**data_dict)</span><br></pre></td></tr></table></figure>
<p>第一次修改过的训练脚本为 <code>train_0.py</code>，配置文件类在 <code>training_cfg_0.py</code> 里，示例配置文件为 <code>cfg_0.json</code>，一个简单 DDPM 模型配置写在 <code>unet_cfg</code> 目录里。可以直接运行下面的命令测试此训练脚本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_0.py cfg_0.json</span><br></pre></td></tr></table></figure>
<p>在配置文件里，我们只需要改少量的训练参数就行了。如果想知道还有哪些参数可以改，可以去查看 <code>training_cfg_0.py</code> 文件。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;logging_dir&quot;</span>: <span class="string">&quot;logs&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;models/ddpm_0&quot;</span>,</span><br><span class="line"></span><br><span class="line">    <span class="attr">&quot;model_config&quot;</span>: <span class="string">&quot;unet_cfg&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;num_epochs&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">64</span>,</span><br><span class="line">    <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">5000</span>,</span><br><span class="line">    <span class="attr">&quot;valid_epochs&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;valid_batch_size&quot;</span>: <span class="number">4</span>,</span><br><span class="line">    <span class="attr">&quot;dataset_name&quot;</span>: <span class="string">&quot;ylecun/mnist&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;resolution&quot;</span>: <span class="number">32</span>,</span><br><span class="line">    <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>读者感兴趣的话也可以尝试这样改一遍代码。这样做会强迫自己读一遍训练脚本，让自己更熟悉这份代码。</p>
<h2 id="适配多种任务的训练脚本"><a href="#适配多种任务的训练脚本" class="headerlink" title="适配多种任务的训练脚本"></a>适配多种任务的训练脚本</h2><p>如果只是训练一种任务，Diffusers 的这种训练脚本还算好用。但如果我们想用完全相同的训练流程训练多种任务，这种脚本的弊端就暴露出来了：</p>
<ul>
<li>各任务的官方示例脚本本身就不完全统一。比如有的训练脚本支持设置随机种子，有的不支持。</li>
<li>一旦想修改训练过程，就得同时修改所有任务的脚本。这不符合编程中「代码复用」的思想。</li>
</ul>
<p>为此，我想重构一下官方训练脚本，将训练流程和每种任务的具体训练过程解耦开，让一份训练脚本能够被多种任务使用。于是，我又从头过了一遍训练脚本，将代码分成两类：所有任务都会用到的代码、仅 DDPM 训练会用到的代码。如下图所示，我用红字表示了训练脚本中应该由具体任务决定的部分。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/2.jpg" alt></p>
<p>根据这个划分规则，我将仅和 DDPM 相关的代码剥离出来，并用一个描述某具体任务的训练器接口类的方法调用代替原有代码。这样，每次换一个训练任务，只需要重新实现一个训练器类就行了。如下图所示，原流程图中所有红字的内容都可以由接口类的方法代替。对于不同任务，我们需要实现不同的训练器类。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/3.jpg" alt></p>
<p>具体在代码中，我写了一个接口类 <code>Trainer</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>(<span class="params">metaclass=ABCMeta</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, logger, cfg</span>):</span></span><br><span class="line">        self.weight_dtype = weight_dtype</span><br><span class="line">        self.accelerator = accelerator</span><br><span class="line">        self.logger = logger</span><br><span class="line">        self.cfg = cfg</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_modules</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                     enable_xformer: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                     gradient_checkpointing: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_optimizers</span>(<span class="params">self, train_batch_size</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_lr_schedulers</span>(<span class="params">self, gradient_accumulation_steps, num_epochs</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_dataset</span>(<span class="params">self, dataset, train_dataloader</span>):</span></span><br><span class="line">        self.dataset = dataset</span><br><span class="line">        self.train_dataloader = train_dataloader</span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_modules</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">models_to_train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_step</span>(<span class="params">self, global_step, batch</span>) -&gt; <span class="built_in">dict</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, global_step</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_pipeline</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_model_hook</span>(<span class="params">self, models, weights, output_dir</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_model_hook</span>(<span class="params">self, models, input_dir</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>根据类型名和初始化参数可以创建具体的训练器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_trainer</span>(<span class="params"><span class="built_in">type</span>, weight_dtype, accelerator, logger, cfg_dict</span>) -&gt; Trainer:</span></span><br><span class="line">    <span class="keyword">from</span> ddpm_trainer <span class="keyword">import</span> DDPMTrainer</span><br><span class="line">    <span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainer</span><br><span class="line"></span><br><span class="line">    __TYPE_CLS_DICT = &#123;</span><br><span class="line">        <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainer,</span><br><span class="line">        <span class="string">&#x27;lora&#x27;</span>: LoraTrainer</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> __TYPE_CLS_DICT[<span class="built_in">type</span>](weight_dtype, accelerator, logger, cfg_dict)</span><br></pre></td></tr></table></figure>
<p>原来训练脚本里的具体训练逻辑被接口类方法调用代替。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># old</span></span><br><span class="line"><span class="keyword">if</span> cfg.model_config <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    model = UNet2DModel(...)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    config = UNet2DModel.load_config(cfg.model_config)</span><br><span class="line">    model = UNet2DModel.from_config(config)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create EMA for the model.</span></span><br><span class="line"><span class="keyword">if</span> cfg.use_ema:</span><br><span class="line">    ema_model = EMAModel(...)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># new</span></span><br><span class="line">trainer.init_modules(enable_xformers, cfg.gradient_checkpointing)</span><br></pre></td></tr></table></figure>
<p>原来仅和 DDPM 训练相关的代码全被我搬到了 <code>DDPMTrainer</code> 类中。与之对应，除了代码需要搬走外，原配置文件里的数据也需要搬走。我在 <code>DDPMTrainer</code> 类里加了一个 <code>DDPMTrainingConfig</code> 数据类，用来存对应的配置数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DDPMTrainingConfig</span>:</span></span><br><span class="line">    <span class="comment"># Diffuion Models</span></span><br><span class="line">    model_config: <span class="built_in">str</span></span><br><span class="line">    ddpm_num_steps: <span class="built_in">int</span> = <span class="number">1000</span></span><br><span class="line">    ddpm_beta_schedule: <span class="built_in">str</span> = <span class="string">&#x27;linear&#x27;</span></span><br><span class="line">    prediction_type: <span class="built_in">str</span> = <span class="string">&#x27;epsilon&#x27;</span></span><br><span class="line">    ddpm_num_inference_steps: <span class="built_in">int</span> = <span class="number">100</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>因此，我们需要用稍微复杂一点的方式来创建配置文件。现在全局训练配置和任务配置放在两组配置里。配置文件最外层除 <code>&quot;base&quot;</code> 外的那个键表明了训练器的类型。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;base&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;logging_dir&quot;</span>: <span class="string">&quot;logs&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;output_dir&quot;</span>: <span class="string">&quot;models/ddpm_1&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;checkpointing_steps&quot;</span>: <span class="number">5000</span>,</span><br><span class="line">        <span class="attr">&quot;valid_epochs&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="attr">&quot;dataset_name&quot;</span>: <span class="string">&quot;ylecun/mnist&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;resolution&quot;</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="attr">&quot;train_batch_size&quot;</span>: <span class="number">64</span>,</span><br><span class="line">        <span class="attr">&quot;num_epochs&quot;</span>: <span class="number">10</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;ddpm&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;model_config&quot;</span>: <span class="string">&quot;unet_cfg&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;learning_rate&quot;</span>: <span class="number">1e-4</span>,</span><br><span class="line">        <span class="attr">&quot;valid_batch_size&quot;</span>: <span class="number">4</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">__TYPE_CLS_DICT = &#123;</span><br><span class="line">    <span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;lora&#x27;</span>: LoraTrainingConfig</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_training_config</span>(<span class="params">config_path: <span class="built_in">str</span></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, BaseTrainingConfig]:</span></span><br><span class="line">    data_dict = OmegaConf.load(config_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The config must have a &quot;base&quot; key</span></span><br><span class="line">    base_cfg_dict = data_dict.pop(<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The config must have one another model config</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(data_dict) == <span class="number">1</span></span><br><span class="line">    model_key = <span class="built_in">next</span>(<span class="built_in">iter</span>(data_dict))</span><br><span class="line">    model_cfg_dict = data_dict[model_key]</span><br><span class="line">    model_cfg_cls = __TYPE_CLS_DICT[model_key]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig(**base_cfg_dict),</span><br><span class="line">            model_key: model_cfg_cls(**model_cfg_dict)&#125;</span><br></pre></td></tr></table></figure>
<p>这样改完过后，训练脚本开头也需要稍作更改，其他地方保持不变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> training_cfg_1 <span class="keyword">import</span> BaseTrainingConfig, load_training_config</span><br><span class="line"><span class="keyword">from</span> trainer <span class="keyword">import</span> Trainer, create_trainer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;cfg&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    cfgs = load_training_config(args.cfg)</span><br><span class="line">    cfg: BaseTrainingConfig = cfgs.pop(<span class="string">&#x27;base&#x27;</span>)</span><br><span class="line">    trainer_type = <span class="built_in">next</span>(<span class="built_in">iter</span>(cfgs))</span><br><span class="line">    trainer_cfg_dict = cfgs[trainer_type]</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    trainer: Trainer = create_trainer(</span><br><span class="line">        trainer_type, weight_dtype, accelerator, cfg.logger, trainer_cfg_dict)</span><br></pre></td></tr></table></figure>
<p>这次修改过的训练脚本为 <code>train_1.py</code>，配置文件类在 <code>training_cfg_1.py</code> 里，DDPM 训练器在 <code>TrainingScript/ddpm_trainer.py</code> 里,示例配置文件为 <code>cfg_1.json</code>。可以直接运行下面的命令测试此训练脚本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_1.py cfg_1.json</span><br></pre></td></tr></table></figure>
<p>运行这一版或者上一版的训练脚本后，我们都能很快训练完一个 MNIST 上的 DDPM 模型。从训练可视化结果可以看出，代码重构大概是没有出错，模型能正确生成图片。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/4.jpg" alt></p>
<blockquote>
<p>对训练器类的程序设计思路感兴趣的话，欢迎阅读附录。</p>
</blockquote>
<h2 id="添加新的训练任务"><a href="#添加新的训练任务" class="headerlink" title="添加新的训练任务"></a>添加新的训练任务</h2><p>为了验证这套新代码的可拓展性，我仿照 Diffusers 官方 SD LoRA 训练脚本 <code>examples/text_to_image/train_text_to_image_lora.py</code>，快速实现了一个 SD LoRA 训练器类。这个类在 <code>sd_lora_trainer.py</code> 文件里。</p>
<p>我来简单介绍添加新训练任务的过程。要添加新训练任务，要修改三处：</p>
<ol>
<li>创建新文件，在文件里定义配置数据类及实现训练器类。</li>
<li>在 <code>trainer.py</code> 里导入新训练器类。</li>
<li>在 <code>training_cfg_1.py</code> 里导入新配置数据类。</li>
</ol>
<p>先来看较简单的第二处和第三处修改。导入新训练器类只需要加一行 import 和一条词典项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_trainer</span>(<span class="params"><span class="built_in">type</span>, weight_dtype, accelerator, logger, cfg_dict</span>) -&gt; Trainer:</span></span><br><span class="line">    <span class="keyword">from</span> ddpm_trainer <span class="keyword">import</span> DDPMTrainer</span><br><span class="line">    <span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainer</span><br><span class="line"></span><br><span class="line">    __TYPE_CLS_DICT = &#123;</span><br><span class="line">        <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainer,</span><br><span class="line">        <span class="string">&#x27;lora&#x27;</span>: LoraTrainer</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> __TYPE_CLS_DICT[<span class="built_in">type</span>](weight_dtype, accelerator, logger, cfg_dict)</span><br></pre></td></tr></table></figure>
<p>导入新配置数据类也一样，一行 import 和一项词典项。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sd_lora_trainer <span class="keyword">import</span> LoraTrainingConfig</span><br><span class="line"></span><br><span class="line">__TYPE_CLS_DICT = &#123;</span><br><span class="line">    <span class="string">&#x27;base&#x27;</span>: BaseTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;ddpm&#x27;</span>: DDPMTrainingConfig,</span><br><span class="line">    <span class="string">&#x27;lora&#x27;</span>: LoraTrainingConfig</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而实现一个训练器类会比较繁琐。我是先把 DDPM 训练器类复制了过来，在此基础上进行修改。由于 SD LoRA 训练器有官方训练脚本作为参考，我还是和之前实现 DDPM 训练器一样，从官方训练脚本里抠出对应代码，将其填入训练器类方法里。比如在初始化模块时，我们不仅需要初始化 U-Net，还有 VAE 等模块。在初始化优化器时，应该只优化 LoRA 参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoraTrainer</span>(<span class="params">Trainer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_modules</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                     enable_xformer=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                     gradient_checkpointing=<span class="literal">False</span></span>):</span></span><br><span class="line">        cfg = self.cfg</span><br><span class="line">        <span class="comment"># Load scheduler, tokenizer and models.</span></span><br><span class="line">        self.noise_scheduler = DDPMScheduler...</span><br><span class="line">        self.tokenizer = CLIPTokenizer...</span><br><span class="line">        self.text_encoder = CLIPTextModel... </span><br><span class="line">        self.vae = AutoencoderKL...</span><br><span class="line">        self.unet = UNet2DConditionModel...</span><br><span class="line">        <span class="comment"># freeze parameters of models to save more memory</span></span><br><span class="line">        self.unet.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        self.vae.requires_grad_(<span class="literal">False</span>)</span><br><span class="line">        self.text_encoder.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.unet.parameters():</span><br><span class="line">            param.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        unet_lora_config = LoraConfig(...)</span><br><span class="line">        self.lora_layers = <span class="built_in">filter</span>(</span><br><span class="line">                <span class="keyword">lambda</span> p: p.requires_grad, self.unet.parameters())</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_optimizers</span>(<span class="params">self, train_batch_size</span>):</span></span><br><span class="line">        ...</span><br><span class="line">        self.optimizer = torch.optim.AdamW(</span><br><span class="line">            self.lora_layers,</span><br><span class="line">            ...)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<p>SD LoRA 训练器类在 <code>sd_lora_trainer.py</code> 文件里，对应配置文件为 <code>cfg_lora.json</code>。用下面的代码即可尝试 LoRA 训练。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_1.py cfg_lora.json</span><br></pre></td></tr></table></figure>
<p>可能是 MNIST 数据集的图片太小了，而 SD 又是为较大的图片设计的，又或是 LoRA 的拟合能力有限，生成的效果不是很好。但可以看出，SD LoRA 学到了 MNIST 的图片风格。</p>
<p><img src="/2024/07/27/20240605-diffusers-training/5.jpg" alt></p>
<p>就我自己使用下来，添加一个新的训练任务还是非常轻松的。我可以只关心初始化模型、训练、验证等实现细节，而不用关心那些通用的训练代码。当然，这份通用训练脚本还不够强大，还不能处理更复杂的数据集。SD LoRA 其实需要一个带文本标注的数据集，但由于我只是想测试添加新训练器的难度，就没有去改数据集，只是默认用了空文本来训练 LoRA。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我自己在使用 Diffusers 训练脚本时，发现这种训练脚本难以适配多任务训练，于是重构了一份拓展性更强的训练脚本。在这篇文章中，我先是介绍了 Diffusers 训练脚本的通用框架，再分享了我改写脚本的过程。相信读者在读完本文后，不仅能够熟悉 Diffusers 训练脚本的具体原理，还能够动手修改它，或者基于我的这一版改进脚本，编写一份适合自己的训练脚本。</p>
<p>我重构的这套训练器也没有太多封装，在维持 Diffusers 那种平铺直叙风格的同时，将每种训练任务独有的代码、数据搬了出来，让开发者专注于编写新的逻辑。我没怎么用过别的训练框架，不太好直接对比。但至少相比于 PyTorch Lightning 那种模型和训练逻辑写在同一个类里的写法，我更认可 Diffusers 这种将模型结构和训练、采样分离的设计。这套框架的训练器也只有训练的逻辑，不会掺杂其他逻辑。</p>
<p>本文的代码链接为 <a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DiffusersExample/tree/main/TrainingScript">https://github.com/SingleZombie/DiffusersExample/tree/main/TrainingScript</a></p>
<p><strong>注意</strong>，这份代码是我随手写的，只测试了简单的训练命令。如果发现 bug，欢迎提 issue。这份代码仅供本文教学使用，功能有限，以后我会在其他地方更新这份代码。另外，以后我写其他训练教程时也会复用这套代码。</p>
<h2 id="附录：训练器程序设计思路"><a href="#附录：训练器程序设计思路" class="headerlink" title="附录：训练器程序设计思路"></a>附录：训练器程序设计思路</h2><p>在设计训练器接口类的接口时，其实我没有做多少主观设计，基本上都是按照一些设计原则，机械地将原来的训练脚本进行重构。我也不知道这些原则是怎么想出来的，只是根据我多年写代码的经验，我感觉按照这些规则做可以保证训练脚本和训练器之间耦合度更低，易于拓展。这些原则有：</p>
<ol>
<li>如果在另一项任务里这行代码会变动，则这项代码应写入训练器类。</li>
<li>如果某一数据的调用<strong>全部</strong>都被放入了训练器类里，那么这个数据应该是训练器类的成员变量。如果该数据来自配置文件，则将该数据的定义从全局配置移入训练器配置。</li>
<li>如果某数据既要在训练脚本中使用，又要在训练器类里使用，则在训练脚本中初始化该数据，并以<strong>初始化参数</strong>或者<strong>接口参数</strong>两种方式将数据传入训练器。传入方式由数据被确定的时刻决定。比如脚本一开始就初始化好的日志对象应该作为初始化参数，而一些中途计算的当前 batch 数等参数应该作为接口参数。</li>
<li>原则上，训练脚本不从数据类里获取数据。</li>
</ol>
<p>根据这些原则，在设计训练器接口类时，我并没有一开始就定下有哪些接口、接口的参数分别是什么，而是一边搬运代码，一边根据代码的实际内容动态地编写接口类。比如一开始，我的接口类构造函数并没有加入日志库类型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Trainer</span>(<span class="params">metaclass=ABCMeta</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, cfg</span>):</span></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>后来写训练器验证方法时，我发现这里必须要获取日志类的类型，不得已在构造函数里多加了一个参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate</span>(<span class="params">self, epoch, global_step</span>):</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> self.logger == <span class="string">&quot;tensorboard&quot;</span>:</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight_dtype, accelerator, logger, cfg</span>):</span></span><br></pre></td></tr></table></figure></p>
<p>原则 3 和原则 4 本质上是将训练脚本也看成一个对象。所有数据要么属于训练脚本，要么属于训练类。原则 4 不从训练器里获取信息，某种程度上体现了面向对象中的封装性，不让训练器去改训练脚本里的数据。我尽可能地遵守了原则 4，但只有一处例外。在调用 <code>accelerate.prapare</code> 后，<code>train_dataloader</code> 在训练器里发生了更改。而 <code>train_dataloader</code> 其实是属于训练脚本的。没办法，这里只能去训练器里获取一次数据。我没来得及仔细研究，说不定 <code>accelerate.prapare</code> 可以多次调用，这样我就能让训练脚本自己维护 <code>train_dataloader</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer.prepare_modules()</span><br><span class="line">train_dataloader = trainer.train_dataloader</span><br></pre></td></tr></table></figure>
<p>这样看下来，这份代码框架在各种角度上都有很大的改进空间。以后我会来慢慢改进这份代码。就目前的设计，训练中整体逻辑、数据集、训练器三部分应该是相互独立的。数据集我还没有单独拿出来写。应该至少实现纯图像、带文本标注图像这两种数据集。</p>
<p>这次重构之后，我也有一些程序设计上的体会。重构代码比从头做程序设计要简单很多。重构只需要根据已有代码，设计出一套更合理的逻辑，像我这样按照某些原则，无脑地修改代码就行了。而程序设计需要考虑未知的情况，为未来可能加入的功能铺路。也正因为从头设计更难，有时会出现设计过度或者设计不足的情况。感觉更合理的开发方式是从头设计与重构交替进行。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/en/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/en/page/14/">14</a><a class="extend next" rel="next" href="/en/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">Designer, artist, philosopher, researcher.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/en/archives/">
        
          <span class="site-state-item-count">140</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/en/categories/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/en/tags/">
          
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/en/lib/anime.min.js"></script>
  <script src="/en/lib/velocity/velocity.min.js"></script>
  <script src="/en/lib/velocity/velocity.ui.min.js"></script>

<script src="/en/js/utils.js"></script>

<script src="/en/js/motion.js"></script>


<script src="/en/js/schemes/muse.js"></script>


<script src="/en/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
