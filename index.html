<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhouyifan.net","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:type" content="website">
<meta property="og:title" content="周弈帆的博客">
<meta property="og:url" content="https://zhouyifan.net/index.html">
<meta property="og:site_name" content="周弈帆的博客">
<meta property="og:description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Zhou Yifan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhouyifan.net/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>周弈帆的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">周弈帆的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/20220712-custom-op-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/20220712-custom-op-2/" class="post-title-link" itemprop="url">PyTorch 自定义算子：复现CPU和CUDA版的二维卷积</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:20:00" itemprop="dateCreated datePublished" datetime="2022-08-09T14:20:00+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/PyTorch/C/" itemprop="url" rel="index"><span itemprop="name">C++</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/PyTorch/C/%E7%BC%96%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">编程</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>我之前的<a href="https://zhouyifan.net/2022/03/18/20220315-custom-op/">一篇文章</a>介绍了如何给PyTorch添加CPU上的简单的加法算子。在这篇文章里，我将继续展示一个更具体的PyTorch自定义算子示例——自己动手复现二维卷积算子。这个示例是基于PyTorch Extension的，在迁移项目时，不需要自己生成动态库，只需要用<code>setup.py</code>重新编译一遍即可。我会同时介绍CPU版和CUDA版的实现。</p>
<p>许多前沿的神经网络都会对卷积进行一些修改。比如大名鼎鼎的可变形卷积(deformable convolution)。相信看完这篇文章后，大家能看懂PyTorch卷积的实现代码，并大概了解如何修改卷积的实现细节，并把新写好的卷积运用到自己的PyTorch项目中。</p>
<h1 id="PyTorch-Extension-实现二维卷积"><a href="#PyTorch-Extension-实现二维卷积" class="headerlink" title="PyTorch Extension 实现二维卷积"></a>PyTorch Extension 实现二维卷积</h1><h2 id="搭建项目"><a href="#搭建项目" class="headerlink" title="搭建项目"></a>搭建项目</h2><p>在开始写代码前，要准备一个崭新的目录，在这个文件夹里搭建项目。</p>
<p>在根目录下，先创建一个<code>setup.py</code>，之后要填写这份安装文件。</p>
<p>之后，创建一个文件夹，其名字是项目名。在这个文件夹里合适的地方新建一个子文件夹，专门用来放和算子相关的文件。我的项目名叫做<code>panoflow</code>，算子相关文件放在了<code>panoflow/core/op</code>子文件夹下。</p>
<p>接下来，和算子实现相关的文件都应该放在算子文件夹里。使用和测试算子的文件可以放在项目文件夹的其他地方。</p>
<p>由于在实现中我借用了MMCV的代码，还要提前准备好一些头文件。首先新建一个文件<code>pytorch_cpp_helper.hpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> PYTORCH_CPP_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTORCH_CPP_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/extension.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> at;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.device().is_cuda(), #x <span class="meta-string">&quot; must be a CUDA tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CPU(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(!x.device().is_cuda(), #x <span class="meta-string">&quot; must be a CPU tensor&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CONTIGUOUS(x) \</span></span><br><span class="line"><span class="meta">  TORCH_CHECK(x.is_contiguous(), #x <span class="meta-string">&quot; must be contiguous&quot;</span>)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CUDA_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CUDA(x);            \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CHECK_CPU_INPUT(x) \</span></span><br><span class="line"><span class="meta">  CHECK_CPU(x);            \</span></span><br><span class="line"><span class="meta">  CHECK_CONTIGUOUS(x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// PYTORCH_CPP_HELPER</span></span></span><br></pre></td></tr></table></figure>
<p>再创建一个文件<code>pytorch_cuda_helper.hpp</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> PYTORCH_CUDA_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PYTORCH_CUDA_HELPER</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/ATen.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/cuda/CUDAContext.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;c10/cuda/CUDAGuard.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ATen/cuda/CUDAApplyUtils.cuh&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;THC/THCAtomics.cuh&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;common_cuda_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> at::Half;</span><br><span class="line"><span class="keyword">using</span> at::Tensor;</span><br><span class="line"><span class="keyword">using</span> phalf = at::Half;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> __PHALF(x) (x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// PYTORCH_CUDA_HELPER</span></span></span><br></pre></td></tr></table></figure>
<p>还有一个<code>common_cuda_helper.hpp</code>：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> COMMON_CUDA_HELPER</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> COMMON_CUDA_HELPER</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_1D_KERNEL_LOOP(i, n)                              \</span></span><br><span class="line"><span class="meta">  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n); \</span></span><br><span class="line"><span class="meta">       i += blockDim.x * gridDim.x)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_2D_KERNEL_LOOP(i, n, j, m)                             \</span></span><br><span class="line"><span class="meta">  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; (n);   \</span></span><br><span class="line"><span class="meta">       i += blockDim.x * gridDim.x)                                 \</span></span><br><span class="line"><span class="meta">    for (size_t j = blockIdx.y * blockDim.y + threadIdx.y; j &lt; (m); \</span></span><br><span class="line"><span class="meta">         j += blockDim.y * gridDim.y)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> CUDA_2D_KERNEL_BLOCK_LOOP(i, n, j, m)          \</span></span><br><span class="line"><span class="meta">  for (size_t i = blockIdx.x; i &lt; (n); i += gridDim.x) \</span></span><br><span class="line"><span class="meta">    for (size_t j = blockIdx.y; j &lt; (m); j += gridDim.y)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> THREADS_PER_BLOCK 512</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">GET_BLOCKS</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> num_threads = THREADS_PER_BLOCK)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> optimal_block_num = (N + num_threads - <span class="number">1</span>) / num_threads;</span><br><span class="line">  <span class="keyword">int</span> max_block_num = <span class="number">4096</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">min</span>(optimal_block_num, max_block_num);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ T <span class="title">bilinear_interpolate</span><span class="params">(<span class="keyword">const</span> T* input, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="keyword">const</span> <span class="keyword">int</span> width, T y, T x,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="keyword">const</span> <span class="keyword">int</span> index <span class="comment">/* index for debug only*/</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// deal with cases that inverse elements are out of feature map boundary</span></span><br><span class="line">  <span class="keyword">if</span> (y &lt; <span class="number">-1.0</span> || y &gt; height || x &lt; <span class="number">-1.0</span> || x &gt; width) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y &lt;= <span class="number">0</span>) y = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (x &lt;= <span class="number">0</span>) x = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> y_low = (<span class="keyword">int</span>)y;</span><br><span class="line">  <span class="keyword">int</span> x_low = (<span class="keyword">int</span>)x;</span><br><span class="line">  <span class="keyword">int</span> y_high;</span><br><span class="line">  <span class="keyword">int</span> x_high;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y_low &gt;= height - <span class="number">1</span>) &#123;</span><br><span class="line">    y_high = y_low = height - <span class="number">1</span>;</span><br><span class="line">    y = (T)y_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    y_high = y_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (x_low &gt;= width - <span class="number">1</span>) &#123;</span><br><span class="line">    x_high = x_low = width - <span class="number">1</span>;</span><br><span class="line">    x = (T)x_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    x_high = x_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T ly = y - y_low;</span><br><span class="line">  T lx = x - x_low;</span><br><span class="line">  T hy = <span class="number">1.</span> - ly, hx = <span class="number">1.</span> - lx;</span><br><span class="line">  <span class="comment">// do bilinear interpolation</span></span><br><span class="line">  T v1 = input[y_low * width + x_low];</span><br><span class="line">  T v2 = input[y_low * width + x_high];</span><br><span class="line">  T v3 = input[y_high * width + x_low];</span><br><span class="line">  T v4 = input[y_high * width + x_high];</span><br><span class="line">  T w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;</span><br><span class="line"></span><br><span class="line">  T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> val;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">bilinear_interpolate_gradient</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> height, <span class="keyword">const</span> <span class="keyword">int</span> width, T y, T x, T&amp; w1, T&amp; w2, T&amp; w3, T&amp; w4,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span>&amp; x_low, <span class="keyword">int</span>&amp; x_high, <span class="keyword">int</span>&amp; y_low, <span class="keyword">int</span>&amp; y_high,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> index <span class="comment">/* index for debug only*/</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// deal with cases that inverse elements are out of feature map boundary</span></span><br><span class="line">  <span class="keyword">if</span> (y &lt; <span class="number">-1.0</span> || y &gt; height || x &lt; <span class="number">-1.0</span> || x &gt; width) &#123;</span><br><span class="line">    <span class="comment">// empty</span></span><br><span class="line">    w1 = w2 = w3 = w4 = <span class="number">0.</span>;</span><br><span class="line">    x_low = x_high = y_low = y_high = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y &lt;= <span class="number">0</span>) y = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (x &lt;= <span class="number">0</span>) x = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  y_low = (<span class="keyword">int</span>)y;</span><br><span class="line">  x_low = (<span class="keyword">int</span>)x;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (y_low &gt;= height - <span class="number">1</span>) &#123;</span><br><span class="line">    y_high = y_low = height - <span class="number">1</span>;</span><br><span class="line">    y = (T)y_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    y_high = y_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (x_low &gt;= width - <span class="number">1</span>) &#123;</span><br><span class="line">    x_high = x_low = width - <span class="number">1</span>;</span><br><span class="line">    x = (T)x_low;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    x_high = x_low + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  T ly = y - y_low;</span><br><span class="line">  T lx = x - x_low;</span><br><span class="line">  T hy = <span class="number">1.</span> - ly, hx = <span class="number">1.</span> - lx;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// reference in forward</span></span><br><span class="line">  <span class="comment">// T v1 = input[y_low * width + x_low];</span></span><br><span class="line">  <span class="comment">// T v2 = input[y_low * width + x_high];</span></span><br><span class="line">  <span class="comment">// T v3 = input[y_high * width + x_low];</span></span><br><span class="line">  <span class="comment">// T v4 = input[y_high * width + x_high];</span></span><br><span class="line">  <span class="comment">// T val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);</span></span><br><span class="line"></span><br><span class="line">  w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span>  <span class="comment">// COMMON_CUDA_HELPER</span></span></span><br></pre></td></tr></table></figure></p>
<p>这些文件添加了CPU和CUDA实现时需要的头文件和定义，后面的C++源码会用到它们。</p>
<h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="C-实现"><a href="#C-实现" class="headerlink" title="C++实现"></a>C++实现</h3><p>在用C++实现一个算子时，我们要编写一个形如这样的文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">my_add</span><span class="params">(torch::Tensor t1, torch::Tensor t2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">return</span> t1 + t2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TORCH_LIBRARY</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">	m.<span class="built_in">def</span>(<span class="string">&quot;my_add&quot;</span>, my_add);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个C++文件主要包含两部分内容：算子的实现函数和C++接口绑定。在实现卷积时，也是要实现这两部分内容。</p>
<p>在修改一个现有的算子时，最好的方法不是从头写一个，而是去开源库里找一份实现，并在这个基础上进行修改。</p>
<blockquote>
<p>我在MMCV的仓库里找到了<a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops">可变形卷积的实现</a>，并把它拆解回了普通的卷积。我参考了这篇教程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464492627">手把手教你如何高效地在 MMCV 中贡献算子</a>。另外，这份笔记还参考了<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension">PyTorch官方Extension教程</a>。</p>
</blockquote>
<p>找到了卷积的实现后，在算子文件夹下新建一个cpp源文件。比如我的文件路径就是<code>panoflow/core/op/my_conv.cpp</code>。这样一个普通卷积的实现如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;pytorch_cpp_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_shape_check</span><span class="params">(at::Tensor input,</span></span></span><br><span class="line"><span class="params"><span class="function">                         at::Tensor weight, <span class="keyword">int</span> kH,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> kW, <span class="keyword">int</span> dH, <span class="keyword">int</span> dW, <span class="keyword">int</span> padH, <span class="keyword">int</span> padW,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">int</span> dilationH, <span class="keyword">int</span> dilationW, <span class="keyword">int</span> group)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">        weight.<span class="built_in">ndimension</span>() == <span class="number">4</span>,</span><br><span class="line">        <span class="string">&quot;4D weight tensor (nOutputPlane,nInputPlane,kH,kW) expected, but got: %s&quot;</span>,</span><br><span class="line">        weight.<span class="built_in">ndimension</span>());</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(weight.<span class="built_in">is_contiguous</span>(), <span class="string">&quot;weight tensor has to be contiguous&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(kW &gt; <span class="number">0</span> &amp;&amp; kH &gt; <span class="number">0</span>,</span><br><span class="line">                <span class="string">&quot;kernel size should be greater than zero, but got kH: %d kW: %d&quot;</span>,</span><br><span class="line">                kH, kW);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>((weight.<span class="built_in">size</span>(<span class="number">2</span>) == kH &amp;&amp; weight.<span class="built_in">size</span>(<span class="number">3</span>) == kW),</span><br><span class="line">                <span class="string">&quot;kernel size should be consistent with weight, &quot;</span>,</span><br><span class="line">                <span class="string">&quot;but got kH: %d kW: %d weight.size(2): %d, weight.size(3): %d&quot;</span>,</span><br><span class="line">                kH, kW, weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(dW &gt; <span class="number">0</span> &amp;&amp; dH &gt; <span class="number">0</span>,</span><br><span class="line">                <span class="string">&quot;stride should be greater than zero, but got dH: %d dW: %d&quot;</span>, dH,</span><br><span class="line">                dW);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(</span><br><span class="line">        dilationW &gt; <span class="number">0</span> &amp;&amp; dilationH &gt; <span class="number">0</span>,</span><br><span class="line">        <span class="string">&quot;dilation should be greater than 0, but got dilationH: %d dilationW: %d&quot;</span>,</span><br><span class="line">        dilationH, dilationW);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> ndim = input.<span class="built_in">ndimension</span>();</span><br><span class="line">    <span class="keyword">int</span> dimf = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> dimh = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> dimw = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ndim == <span class="number">4</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        dimf++;</span><br><span class="line">        dimh++;</span><br><span class="line">        dimw++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(ndim == <span class="number">3</span> || ndim == <span class="number">4</span>,</span><br><span class="line">                <span class="string">&quot;3D or 4D input tensor expected but got: %s&quot;</span>, ndim);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> nInputPlane = weight.<span class="built_in">size</span>(<span class="number">1</span>) * group;</span><br><span class="line">    <span class="keyword">long</span> inputHeight = input.<span class="built_in">size</span>(dimh);</span><br><span class="line">    <span class="keyword">long</span> inputWidth = input.<span class="built_in">size</span>(dimw);</span><br><span class="line">    <span class="keyword">long</span> nOutputPlane = weight.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> outputHeight =</span><br><span class="line">        (inputHeight + <span class="number">2</span> * padH - (dilationH * (kH - <span class="number">1</span>) + <span class="number">1</span>)) / dH + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> outputWidth =</span><br><span class="line">        (inputWidth + <span class="number">2</span> * padW - (dilationW * (kW - <span class="number">1</span>) + <span class="number">1</span>)) / dW + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (outputWidth &lt; <span class="number">1</span> || outputHeight &lt; <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">AT_ERROR</span>(</span><br><span class="line">            <span class="string">&quot;Given input size: (%ld x %ld x %ld). &quot;</span></span><br><span class="line">            <span class="string">&quot;Calculated output size: (%ld x %ld x %ld). Output size is too small&quot;</span>,</span><br><span class="line">            nInputPlane, inputHeight, inputWidth, nOutputPlane, outputHeight,</span><br><span class="line">            outputWidth);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>(input.<span class="built_in">size</span>(<span class="number">1</span>) == nInputPlane,</span><br><span class="line">                <span class="string">&quot;invalid number of input planes, expected: %d, but got: %d&quot;</span>,</span><br><span class="line">                nInputPlane, input.<span class="built_in">size</span>(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">TORCH_CHECK</span>((inputHeight &gt;= kH &amp;&amp; inputWidth &gt;= kW),</span><br><span class="line">                <span class="string">&quot;input image is smaller than kernel&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_forward</span><span class="params">(Tensor input, Tensor weight, Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Tensor output, Tensor columns, <span class="keyword">int</span> kW,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> kH, <span class="keyword">int</span> dW, <span class="keyword">int</span> dH, <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> dilationW, <span class="keyword">int</span> dilationH, <span class="keyword">int</span> group,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> im2col_step)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> isCuda = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (input.<span class="built_in">device</span>().<span class="built_in">is_cuda</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(input);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(weight);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(bias);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(output);</span><br><span class="line">        <span class="built_in">CHECK_CUDA_INPUT</span>(columns);</span><br><span class="line">        isCuda = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(input);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(weight);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(bias);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(output);</span><br><span class="line">        <span class="built_in">CHECK_CPU_INPUT</span>(columns);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">my_conv_shape_check</span>(input, weight, kH, kW, dH, dW, padH,</span><br><span class="line">                        padW, dilationH, dilationW, group);</span><br><span class="line">    <span class="function">at::DeviceGuard <span class="title">guard</span><span class="params">(input.device())</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> batch = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (input.<span class="built_in">ndimension</span>() == <span class="number">3</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Force batch</span></span><br><span class="line">        batch = <span class="number">0</span>;</span><br><span class="line">        input.<span class="built_in">unsqueeze_</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> batchSize = input.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> nInputPlane = input.<span class="built_in">size</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">long</span> inputHeight = input.<span class="built_in">size</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">long</span> inputWidth = input.<span class="built_in">size</span>(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> nOutputPlane = weight.<span class="built_in">size</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> outputWidth =</span><br><span class="line">        (inputWidth + <span class="number">2</span> * padW - (dilationW * (kW - <span class="number">1</span>) + <span class="number">1</span>)) / dW + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> outputHeight =</span><br><span class="line">        (inputHeight + <span class="number">2</span> * padH - (dilationH * (kH - <span class="number">1</span>) + <span class="number">1</span>)) / dH + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    output = output.<span class="built_in">view</span>(&#123;batchSize / im2col_step, im2col_step, nOutputPlane,</span><br><span class="line">                          outputHeight, outputWidth&#125;);</span><br><span class="line">    columns = at::<span class="built_in">zeros</span>(</span><br><span class="line">        &#123;nInputPlane * kW * kH, im2col_step * outputHeight * outputWidth&#125;,</span><br><span class="line">        input.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    input = input.<span class="built_in">view</span>(&#123;batchSize / im2col_step, im2col_step, nInputPlane,</span><br><span class="line">                        inputHeight, inputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    Tensor output_buffer = at::<span class="built_in">zeros</span>(&#123;batchSize / im2col_step, nOutputPlane,</span><br><span class="line">                                      im2col_step * outputHeight, outputWidth&#125;,</span><br><span class="line">                                     output.<span class="built_in">options</span>());</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(</span><br><span class="line">        &#123;output_buffer.<span class="built_in">size</span>(<span class="number">0</span>), group, output_buffer.<span class="built_in">size</span>(<span class="number">1</span>) / group,</span><br><span class="line">         output_buffer.<span class="built_in">size</span>(<span class="number">2</span>), output_buffer.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize / im2col_step; elt++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (isCuda)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">my_conv_im2col_cuda</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                            inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                            dilationW, im2col_step, columns);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">my_conv_im2col_cpu</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                            inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                            dilationW, im2col_step, columns);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        columns = columns.<span class="built_in">view</span>(&#123;group, columns.<span class="built_in">size</span>(<span class="number">0</span>) / group, columns.<span class="built_in">size</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">        weight = weight.<span class="built_in">view</span>(&#123;group, weight.<span class="built_in">size</span>(<span class="number">0</span>) / group, weight.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">                              weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group; g++)</span><br><span class="line">        &#123;</span><br><span class="line">            output_buffer[elt][g] = output_buffer[elt][g]</span><br><span class="line">                                        .<span class="built_in">flatten</span>(<span class="number">1</span>)</span><br><span class="line">                                        .<span class="built_in">addmm_</span>(weight[g].<span class="built_in">flatten</span>(<span class="number">1</span>), columns[g])</span><br><span class="line">                                        .<span class="built_in">view_as</span>(output_buffer[elt][g]);</span><br><span class="line">        &#125;</span><br><span class="line">        columns =</span><br><span class="line">            columns.<span class="built_in">view</span>(&#123;columns.<span class="built_in">size</span>(<span class="number">0</span>) * columns.<span class="built_in">size</span>(<span class="number">1</span>), columns.<span class="built_in">size</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">        weight = weight.<span class="built_in">view</span>(&#123;weight.<span class="built_in">size</span>(<span class="number">0</span>) * weight.<span class="built_in">size</span>(<span class="number">1</span>), weight.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">                              weight.<span class="built_in">size</span>(<span class="number">3</span>), weight.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(</span><br><span class="line">        &#123;output_buffer.<span class="built_in">size</span>(<span class="number">0</span>), output_buffer.<span class="built_in">size</span>(<span class="number">1</span>) * output_buffer.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">         output_buffer.<span class="built_in">size</span>(<span class="number">3</span>), output_buffer.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    output_buffer = output_buffer.<span class="built_in">view</span>(&#123;batchSize / im2col_step, nOutputPlane,</span><br><span class="line">                                        im2col_step, outputHeight, outputWidth&#125;);</span><br><span class="line">    output_buffer.<span class="built_in">transpose_</span>(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    output.<span class="built_in">copy_</span>(output_buffer);</span><br><span class="line">    output = output.<span class="built_in">view</span>(&#123;batchSize, nOutputPlane, outputHeight, outputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    bias = bias.<span class="built_in">view</span>(&#123;<span class="number">1</span>, bias.<span class="built_in">size</span>(<span class="number">0</span>), <span class="number">1</span>, <span class="number">1</span>&#125;);</span><br><span class="line">    output.<span class="built_in">add_</span>(bias);</span><br><span class="line"></span><br><span class="line">    input = input.<span class="built_in">view</span>(&#123;batchSize, nInputPlane, inputHeight, inputWidth&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (batch == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        output = output.<span class="built_in">view</span>(&#123;nOutputPlane, outputHeight, outputWidth&#125;);</span><br><span class="line">        input = input.<span class="built_in">view</span>(&#123;nInputPlane, inputHeight, inputWidth&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; index++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_cpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">PYBIND11_MODULE</span>(my_ops, m)</span><br><span class="line">&#123;</span><br><span class="line">      m.<span class="built_in">def</span>(<span class="string">&quot;my_conv_forward&quot;</span>, my_conv_forward, <span class="string">&quot;my_conv_forward&quot;</span>,</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;input&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;weight&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;bias&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;output&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;columns&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;kW&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;kH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dW&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;padW&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;padH&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dilationW&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;dilationH&quot;</span>),</span><br><span class="line">            py::<span class="built_in">arg</span>(<span class="string">&quot;group&quot;</span>), py::<span class="built_in">arg</span>(<span class="string">&quot;im2col_step&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这份实现非常长，我挑一些重点的内容讲解。</p>
<p>从最下面的<code>PYBIND11_MODULE(my_ops, m)</code>看起。这里的<code>my_ops</code>是生成的库名，可以随便取名。待会要import这个库名。代码块里<code>m.def</code>用于定义C++函数的Python接口。<code>&quot;my_conv_forward&quot;</code>是Python调用时的函数名称，<code>my_conv_forward</code>是被Python代码调用的这份代码里的C++函数名称。也就是说，这份卷积实现的入口函数就是<code>my_conv_forward</code>。我们从这个函数看起。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_forward</span><span class="params">(Tensor input, Tensor weight, Tensor bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                     Tensor output, Tensor columns, <span class="keyword">int</span> kW,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> kH, <span class="keyword">int</span> dW, <span class="keyword">int</span> dH, <span class="keyword">int</span> padW, <span class="keyword">int</span> padH,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> dilationW, <span class="keyword">int</span> dilationH, <span class="keyword">int</span> group,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="keyword">int</span> im2col_step)</span></span></span><br></pre></td></tr></table></figure>
<p><code>my_conv_forward</code>就是卷积的主函数。它的参数除了PyTorch的<code>Conv2d</code>传入的参数外，还多了两个参数<code>output, columus</code>。这两个张量是保存中间结果的，在PyTorch侧是看不到的。<code>output</code>用于保存卷积输出，<code>columns</code>用于保存卷积时的列矩阵。底层实现卷积时，会先把图像转换成一个用列表示的矩阵，再把卷积操作当成一个矩阵乘法来完成。其中，第一步操作叫做”im2col”。对此原理不熟的话可以参考这篇文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63974249。">https://zhuanlan.zhihu.com/p/63974249。</a></p>
<p><code>my_conv_forward</code>函数的大部分内容都是在做类型检查和张量形状转换。在修改卷积实现时，这些东西都可以不用改。整个卷积操作的核心都在这一部分：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> elt = <span class="number">0</span>; elt &lt; batchSize / im2col_step; elt++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (isCuda)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">my_conv_im2col_cuda</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                        inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                        dilationW, im2col_step, columns);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">my_conv_im2col_cpu</span>(input[elt], nInputPlane, inputHeight,</span><br><span class="line">                        inputWidth, kH, kW, padH, padW, dH, dW, dilationH,</span><br><span class="line">                        dilationW, im2col_step, columns);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    columns = columns.<span class="built_in">view</span>(&#123;group, columns.<span class="built_in">size</span>(<span class="number">0</span>) / group, columns.<span class="built_in">size</span>(<span class="number">1</span>)&#125;);</span><br><span class="line">    weight = weight.<span class="built_in">view</span>(&#123;group, weight.<span class="built_in">size</span>(<span class="number">0</span>) / group, weight.<span class="built_in">size</span>(<span class="number">1</span>),</span><br><span class="line">                          weight.<span class="built_in">size</span>(<span class="number">2</span>), weight.<span class="built_in">size</span>(<span class="number">3</span>)&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> g = <span class="number">0</span>; g &lt; group; g++)</span><br><span class="line">    &#123;</span><br><span class="line">        output_buffer[elt][g] = output_buffer[elt][g]</span><br><span class="line">                                    .<span class="built_in">flatten</span>(<span class="number">1</span>)</span><br><span class="line">                                    .<span class="built_in">addmm_</span>(weight[g].<span class="built_in">flatten</span>(<span class="number">1</span>), columns[g])</span><br><span class="line">                                    .<span class="built_in">view_as</span>(output_buffer[elt][g]);</span><br><span class="line">    &#125;</span><br><span class="line">    columns =</span><br><span class="line">        columns.<span class="built_in">view</span>(&#123;columns.<span class="built_in">size</span>(<span class="number">0</span>) * columns.<span class="built_in">size</span>(<span class="number">1</span>), columns.<span class="built_in">size</span>(<span class="number">2</span>)&#125;);</span><br><span class="line">    weight = weight.<span class="built_in">view</span>(&#123;weight.<span class="built_in">size</span>(<span class="number">0</span>) * weight.<span class="built_in">size</span>(<span class="number">1</span>), weight.<span class="built_in">size</span>(<span class="number">2</span>),</span><br><span class="line">                          weight.<span class="built_in">size</span>(<span class="number">3</span>), weight.<span class="built_in">size</span>(<span class="number">4</span>)&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码先做了<code>im2col</code>操作，再做了矩阵乘法。其实，包括可变形卷积在内，各种稀奇古怪的卷积操作通过靠修改<code>im2col</code>来完成的。CPU和CUDA版卷积的主要区别，也体现在<code>im2col</code>中（后面的矩阵乘法在CPU和CUDA上都能用）。</p>
<p>由于是讲CPU实现，这里的CUDA实现我暂时放了一个空函数。<code>my_conv_im2col_cpu</code>的内容如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_cpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个函数其实只是处理了一下输入，真正的实现在<code>my_conv_im2col_cpu_kernel</code>里。<code>AT_DISPATCH_FLOATING_TYPES_AND_HALF</code>可以让实现兼容半精度和普通float，所以实现<code>my_conv_im2col_cpu_kernel</code>得写成一个模板函数。</p>
<p><code>my_conv_im2col_cpu_kernel</code>的实现如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; index++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>它的作用就是把图像里的数据搬到做卷积运算的<code>column</code>里。循环遍历每一次卷积的每一个位置，把待运算的量填入<code>column</code>。卷积里的所有参数(pad, stride, …)都是在这段函数里生效的。想实现可变形卷积等改进，也要修改这个函数。</p>
<h3 id="Python封装"><a href="#Python封装" class="headerlink" title="Python封装"></a>Python封装</h3><p>实现好了后，如果编译完了的话，刚刚的卷积接口可以通过以下方式在Python里调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> my_ops</span><br><span class="line">my_ops.my_conv_forward(...)</span><br></pre></td></tr></table></figure>
<p>这里的<code>my_ops</code>这个名称必须和开始<code>PYBIND11_MODULE(my_ops, m)</code>里面那个库名称对应。</p>
<p>基于这个接口，可以仿照PyTorch中<code>Conv2d</code>的接口，编写一个和<code>Conv2d</code>等价的<code>torch.nn.Module</code>出来。我的这个Python文件的路径是<code>panoflow/core/op/my_conv.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"><span class="keyword">from</span> torch.nn.modules.utils <span class="keyword">import</span> _pair</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> my_ops</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyConvF</span>(<span class="params">Function</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">ctx,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="built_in">input</span>: torch.Tensor,</span></span></span><br><span class="line"><span class="params"><span class="function">                weight,</span></span></span><br><span class="line"><span class="params"><span class="function">                bias,</span></span></span><br><span class="line"><span class="params"><span class="function">                stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                groups=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                im2col_step=<span class="number">32</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">input</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">input</span>.dim() != <span class="number">4</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">f&#x27;Expected 4D tensor as input, got <span class="subst">&#123;<span class="built_in">input</span>.dim()&#125;</span>D tensor \</span></span><br><span class="line"><span class="string">                  instead.&#x27;</span>)</span><br><span class="line">        ctx.stride = _pair(stride)</span><br><span class="line">        ctx.padding = _pair(padding)</span><br><span class="line">        ctx.dilation = _pair(dilation)</span><br><span class="line">        ctx.groups = groups</span><br><span class="line">        ctx.im2col_step = im2col_step</span><br><span class="line"></span><br><span class="line">        weight = weight.type_as(<span class="built_in">input</span>)</span><br><span class="line">        ctx.save_for_backward(<span class="built_in">input</span>, weight)</span><br><span class="line"></span><br><span class="line">        output = <span class="built_in">input</span>.new_empty(MyConvF._output_size(ctx, <span class="built_in">input</span>, weight))</span><br><span class="line"></span><br><span class="line">        ctx.bufs_ = [<span class="built_in">input</span>.new_empty(<span class="number">0</span>), <span class="built_in">input</span>.new_empty(<span class="number">0</span>)]  <span class="comment"># columns, ones</span></span><br><span class="line"></span><br><span class="line">        cur_im2col_step = <span class="built_in">min</span>(ctx.im2col_step, <span class="built_in">input</span>.size(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">assert</span> (<span class="built_in">input</span>.size(<span class="number">0</span>) % cur_im2col_step</span><br><span class="line">                ) == <span class="number">0</span>, <span class="string">&#x27;batch size must be divisible by im2col_step&#x27;</span></span><br><span class="line"></span><br><span class="line">        my_ops.my_conv_forward(</span><br><span class="line">            <span class="built_in">input</span>,</span><br><span class="line">            weight,</span><br><span class="line">            bias,</span><br><span class="line">            output,</span><br><span class="line">            ctx.bufs_[<span class="number">0</span>],</span><br><span class="line">            kW=weight.size(<span class="number">3</span>),</span><br><span class="line">            kH=weight.size(<span class="number">2</span>),</span><br><span class="line">            dW=ctx.stride[<span class="number">1</span>],</span><br><span class="line">            dH=ctx.stride[<span class="number">0</span>],</span><br><span class="line">            padW=ctx.padding[<span class="number">1</span>],</span><br><span class="line">            padH=ctx.padding[<span class="number">0</span>],</span><br><span class="line">            dilationW=ctx.dilation[<span class="number">1</span>],</span><br><span class="line">            dilationH=ctx.dilation[<span class="number">0</span>],</span><br><span class="line">            group=ctx.groups,</span><br><span class="line">            im2col_step=cur_im2col_step)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_output_size</span>(<span class="params">ctx, <span class="built_in">input</span>, weight</span>):</span></span><br><span class="line">        channels = weight.size(<span class="number">0</span>)</span><br><span class="line">        output_size = (<span class="built_in">input</span>.size(<span class="number">0</span>), channels)</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">input</span>.dim() - <span class="number">2</span>):</span><br><span class="line">            in_size = <span class="built_in">input</span>.size(d + <span class="number">2</span>)</span><br><span class="line">            pad = ctx.padding[d]</span><br><span class="line">            kernel = ctx.dilation[d] * (weight.size(d + <span class="number">2</span>) - <span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">            stride_ = ctx.stride[d]</span><br><span class="line">            output_size += ((in_size + (<span class="number">2</span> * pad) - kernel) // stride_ + <span class="number">1</span>, )</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">all</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> s: s &gt; <span class="number">0</span>, output_size)):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&#x27;convolution input is too small (output would be &#x27;</span> +</span><br><span class="line">                <span class="string">&#x27;x&#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, output_size)) + <span class="string">&#x27;)&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> output_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_conv = MyConvF.apply</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyConv2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 in_channels: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 out_channels: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 kernel_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 stride=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 groups: <span class="built_in">int</span> = <span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 bias: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        kernel_size_ = _pair(kernel_size)</span><br><span class="line">        stride_ = _pair(stride)</span><br><span class="line">        padding_ = _pair(padding)</span><br><span class="line">        dilation_ = _pair(dilation)</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        self.out_channels = out_channels</span><br><span class="line">        self.kernel_size = kernel_size_</span><br><span class="line">        self.stride = stride_</span><br><span class="line">        self.padding = padding_</span><br><span class="line">        self.dilation = dilation_</span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.weight = Parameter(</span><br><span class="line">            torch.Tensor(out_channels, in_channels // groups, *kernel_size_))</span><br><span class="line">        self.bias = Parameter(torch.Tensor(out_channels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Useless attributes</span></span><br><span class="line">        self.transposed = <span class="literal">None</span></span><br><span class="line">        self.output_padding = <span class="literal">None</span></span><br><span class="line">        self.padding_mode = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">return</span> my_conv(<span class="built_in">input</span>, self.weight, self.bias, self.stride,</span><br><span class="line">                       self.padding, self.dilation, self.groups)</span><br></pre></td></tr></table></figure>
<p>以后，用自己的卷积<code>MyConv2d</code>就和用普通的<code>Conv2d</code>一样了。</p>
<h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>打开外面的<code>setup.py</code>，填写以下内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">src_root = <span class="string">&#x27;panoflow/core/op&#x27;</span></span><br><span class="line">cpp_src = [<span class="string">&#x27;my_conv.cpp&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    include_dirs = [<span class="string">&#x27;panoflow/core/op&#x27;</span>]</span><br><span class="line">    cpp_path = [os.path.join(src_root, src) <span class="keyword">for</span> src <span class="keyword">in</span> cpp_src]</span><br><span class="line"></span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;panoflow&#x27;</span>,</span><br><span class="line">        ext_modules=[</span><br><span class="line">            cpp_extension.CppExtension(</span><br><span class="line">                <span class="string">&#x27;my_ops&#x27;</span>, cpp_path, include_dirs=include_dirs)</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>其中的路径要根据自己的实际情况修改。</p>
<p>和编译相关的内容都写在<code>cpp_extension.CppExtension</code>里。其中，源文件要写在第二个参数里，头文件目录要写在<code>include_dirs</code>。由于我的源文件放在<code>panoflow/core/op</code>里，我写了个源文件名数组<code>cpp_src</code>，在传参前把路径组合了一下。由于<code>include_dirs</code>和源文件在同一个目录下，我也填的是<code>panoflow/core/op</code>。</p>
<p>写完了<code>setup.py</code>后，运行<code>python setup.py develop</code>，就能一键编译和安装。如果运行后没有报编译错误，就可以把实现的卷积用起来了。</p>
<h3 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h3><p>用单元测试可以快速地验证卷积是否实现成功。我写了一个简单的单元测试文件，在任意一个文件夹下创建该文件即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> panoflow.core.op.my_conv <span class="keyword">import</span> MyConv2d</span><br><span class="line"></span><br><span class="line">inc = <span class="number">3</span></span><br><span class="line">outc = <span class="number">4</span></span><br><span class="line">img_shaspe = (<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># device_name = &#x27;cuda:0&#x27;</span></span><br><span class="line">device_name = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">open_bias = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one</span>():</span></span><br><span class="line">    ts = torch.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_two</span>():</span></span><br><span class="line">    ts = torch.rand([<span class="number">1</span>, inc, *img_shaspe]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test_one()</span><br><span class="line">    test_two()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其中，<code>panoflow.core.op.my_conv</code>是我刚刚放<code>MyConv2d</code>的Python模块。</p>
<p>直接运行这个Python文件，如果没有任何输出（报错信息），就说明卷积实现成功了。</p>
<h2 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h2><h3 id="C-实现-1"><a href="#C-实现-1" class="headerlink" title="C++实现"></a>C++实现</h3><p>在刚刚的实现中，有一个<code>my_conv_im2col_cuda</code>的实现是空着的。在CUDA版本中，我们要实现这个函数。不过，这个函数要放在一个用<code>nvcc</code>编译的<code>.cu</code>文件里。<strong>注意！注意！注意！</strong> 因此，<code>my_conv.cpp</code>里那个空的<code>my_conv_im2col_cuda</code>实现应该全部删掉。</p>
<p>新建一个文件<code>my_conv_cuda.cu</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Modify from https://github.com/open-mmlab/mmcv/blob/my_conv/mmcv/ops/csrc/common/cuda/deform_conv_cuda_kernel.cuh</span></span><br><span class="line"><span class="comment">// Copyright (c) OpenMMLab. All rights reserved.</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/types.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;pytorch_cuda_helper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">my_conv_im2col_gpu_kernel</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> T *data_im, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> kernel_h, <span class="keyword">const</span> <span class="keyword">int</span> kernel_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> pad_w, <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> num_channels, <span class="keyword">const</span> <span class="keyword">int</span> height_col,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">const</span> <span class="keyword">int</span> width_col, T *data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CUDA_1D_KERNEL_LOOP</span>(index, n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// index index of output matrix</span></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_col = index % width_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_col = (index / width_col) % height_col;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> b_col = (index / width_col / height_col) % batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_im = (index / width_col / height_col) / batch_size;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> c_col = c_im * kernel_h * kernel_w;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> h_in = h_col * stride_h - pad_h;</span><br><span class="line">        <span class="keyword">const</span> <span class="keyword">int</span> w_in = w_col * stride_w - pad_w;</span><br><span class="line">        T *data_col_ptr =</span><br><span class="line">            data_col +</span><br><span class="line">            ((c_col * batch_size + b_col) * height_col + h_col) * width_col + w_col;</span><br><span class="line">        <span class="keyword">const</span> T *data_im_ptr =</span><br><span class="line">            data_im + (b_col * num_channels + c_im) * height * width;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; kernel_h; ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; kernel_w; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                T val = <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>);</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> h_im = h_in + i * dilation_h;</span><br><span class="line">                <span class="keyword">const</span> <span class="keyword">int</span> w_im = w_in + j * dilation_w;</span><br><span class="line">                <span class="keyword">if</span> (h_im &gt; <span class="number">-1</span> &amp;&amp; w_im &gt; <span class="number">-1</span> &amp;&amp; h_im &lt; height &amp;&amp; w_im &lt; width)</span><br><span class="line">                &#123;</span><br><span class="line">                    val = data_im_ptr[h_im * width + w_im];</span><br><span class="line">                &#125;</span><br><span class="line">                *data_col_ptr = val;</span><br><span class="line">                data_col_ptr += batch_size * height_col * width_col;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">my_conv_im2col_cuda</span><span class="params">(Tensor data_im,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> channels, <span class="keyword">const</span> <span class="keyword">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> width, <span class="keyword">const</span> <span class="keyword">int</span> ksize_h,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> ksize_w, <span class="keyword">const</span> <span class="keyword">int</span> pad_h, <span class="keyword">const</span> <span class="keyword">int</span> pad_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> stride_h, <span class="keyword">const</span> <span class="keyword">int</span> stride_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> dilation_h, <span class="keyword">const</span> <span class="keyword">int</span> dilation_w,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="keyword">const</span> <span class="keyword">int</span> parallel_imgs, Tensor data_col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> height_col =</span><br><span class="line">        (height + <span class="number">2</span> * pad_h - (dilation_h * (ksize_h - <span class="number">1</span>) + <span class="number">1</span>)) / stride_h + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> width_col =</span><br><span class="line">        (width + <span class="number">2</span> * pad_w - (dilation_w * (ksize_w - <span class="number">1</span>) + <span class="number">1</span>)) / stride_w + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> num_kernels = channels * height_col * width_col * parallel_imgs;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_DISPATCH_FLOATING_TYPES_AND_HALF</span>(</span><br><span class="line">        data_im.<span class="built_in">scalar_type</span>(), <span class="string">&quot;my_conv_im2col_gpu&quot;</span>, [&amp;]</span><br><span class="line">        &#123; my_conv_im2col_gpu_kernel&lt;<span class="keyword">scalar_t</span>&gt;&lt;&lt;&lt;<span class="built_in">GET_BLOCKS</span>(num_kernels),</span><br><span class="line">                                                THREADS_PER_BLOCK, <span class="number">0</span>,</span><br><span class="line">                                                at::cuda::<span class="built_in">getCurrentCUDAStream</span>()&gt;&gt;&gt;(</span><br><span class="line">              num_kernels, data_im.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;(),</span><br><span class="line">              height, width, ksize_h, ksize_w,</span><br><span class="line">              pad_h, pad_w, stride_h, stride_w, dilation_h, dilation_w,</span><br><span class="line">              parallel_imgs, channels,</span><br><span class="line">              height_col, width_col, data_col.data_ptr&lt;<span class="keyword">scalar_t</span>&gt;()); &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">AT_CUDA_CHECK</span>(<span class="built_in">cudaGetLastError</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和CPU版的类似，<code>my_conv_im2col_cuda</code>也是预处理了输入，并调用核函数<code>my_conv_im2col_gpu_kernel</code>来实现<code>im2col</code>。</p>
<p>CUDA实现和CPU几乎一样，唯一的区别就是for循环变成了<code>CUDA_1D_KERNEL_LOOP(index, n)</code>。这个宏是头文件里帮我们定义的，它简化了CUDA的一维循环。</p>
<h3 id="编译-1"><a href="#编译-1" class="headerlink" title="编译"></a>编译</h3><p>修改<code>setup.py</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> setuptools <span class="keyword">import</span> setup</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> cpp_extension</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">src_root = <span class="string">&#x27;panoflow/core/op&#x27;</span></span><br><span class="line">cpp_src = [<span class="string">&#x27;my_conv.cpp&#x27;</span>, <span class="string">&#x27;my_conv_cuda.cu&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    include_dirs = [<span class="string">&#x27;panoflow/core/op&#x27;</span>]</span><br><span class="line">    cpp_path = [os.path.join(src_root, src) <span class="keyword">for</span> src <span class="keyword">in</span> cpp_src]</span><br><span class="line"></span><br><span class="line">    setup(</span><br><span class="line">        name=<span class="string">&#x27;panoflow&#x27;</span>,</span><br><span class="line">        ext_modules=[</span><br><span class="line">            cpp_extension.CUDAExtension(</span><br><span class="line">                <span class="string">&#x27;my_ops&#x27;</span>, cpp_path, include_dirs=include_dirs)</span><br><span class="line">        ],</span><br><span class="line">        cmdclass=&#123;<span class="string">&#x27;build_ext&#x27;</span>: cpp_extension.BuildExtension&#125;)</span><br></pre></td></tr></table></figure>
<p>首先，要把源文件加入<code>cpp_src</code>里。之后，把<code>CppExtension</code>改成<code>CUDAExtension</code>。这样，就能编译新写的CUDA文件了。</p>
<p>写完了之后，再次<code>python setup.py develop</code>编译即可。</p>
<blockquote>
<p>编译小技巧：不拿IDE直接写C++和CUDA源代码是很容易出错误的。但如果你想只用<code>setup.py</code>来验证代码的正确性，可以<code>python setup.py develop &gt; tmp.txt</code>把编译输出重定向到一个文件里来查看。由于编译时的信息过多，在命令行里很难从一堆编译warning里找到最重要的error。</p>
</blockquote>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>由于Python部分在之前都已经写好了，可以直接用刚刚的单元测试文件测试了。只要把刚刚那份文件的<code>device_name</code>改成<code>cuda:0</code>即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> panoflow.core.op.my_conv <span class="keyword">import</span> MyConv2d</span><br><span class="line"></span><br><span class="line">inc = <span class="number">3</span></span><br><span class="line">outc = <span class="number">4</span></span><br><span class="line">img_shaspe = (<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">device_name = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line"><span class="comment"># device_name = &#x27;cpu&#x27;</span></span><br><span class="line">open_bias = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one</span>():</span></span><br><span class="line">    ts = torch.ones([<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_two</span>():</span></span><br><span class="line">    ts = torch.rand([<span class="number">1</span>, inc, *img_shaspe]).to(device_name)</span><br><span class="line">    layer = nn.Conv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, bias=open_bias).to(device_name)</span><br><span class="line">    gt = layer(ts)</span><br><span class="line">    my_layer = MyConv2d(inc, outc, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>).to(device_name)</span><br><span class="line">    my_layer.load_state_dict(layer.state_dict(), strict=<span class="literal">False</span>)</span><br><span class="line">    res = my_layer(ts)</span><br><span class="line">    res = res.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    gt = gt.to(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    <span class="keyword">assert</span> torch.allclose(res, gt, <span class="number">1e-3</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test_one()</span><br><span class="line">    test_two()</span><br></pre></td></tr></table></figure>
<p>同样，没报错就说明写对了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/20220807-ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/20220807-ResNet/" class="post-title-link" itemprop="url">ResNet 论文概览与精读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:19:36" itemprop="dateCreated datePublished" datetime="2022-08-09T14:19:36+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>ResNet是CV中的经典网络。在这篇文章中，我将按照阅读论文的通用方法由粗至精地解读这篇文章。如果你对ResNet不熟，最好对着原论文阅读本文。如果你已经很熟悉ResNet了，也可以通过这篇文章查缺补漏。</p>
<h2 id="粗读"><a href="#粗读" class="headerlink" title="粗读"></a>粗读</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>摘要的前三句话开门见山地介绍本文要解决的问题以及解决的方法。</p>
<ul>
<li>问题：<strong>较深</strong>的神经网络很难训练。</li>
<li>本文的工作：提出了一种能够轻松训练比以往的网络要深得多的残差学习框架。</li>
<li>本文方法的进一步解释：神经网络的层将拟合一个基于输入的残差函数，而不是一个没有参考的函数。</li>
</ul>
<p>随后，摘要展示了这种方法取得的成就：</p>
<ul>
<li>经实验而非理论证明，增加深度后，模型的训练效果和测试效果都得到了提升。</li>
<li>精度打败了当时所有的分类模型。</li>
<li>深度高达152，大幅超过了当时较火的19层的VGG，同时并没有增加多少计算量。</li>
<li>精度打败了当时所有的检测、分割等任务的模型。这证明这种方法的泛化性强。</li>
</ul>
<p>这篇摘要没有罗列贡献，而是在提出问题后轻轻一点，介绍了本文的方法及其作用。随后，所有的篇幅都在秀这种方法的成效。看完这段话，我们可能还不知道“残差”是怎么算的，但我们知道了这种方法很厉害，测试精度、训练难易度、泛化性都十分优秀，成功解决了较深的模型难以训练的问题。</p>
<p>在这轮粗读中，我们可以带着以下问题去概览论文：</p>
<ul>
<li>文章要解决的具体是一个怎样的问题？为什么较深的神经网络很难训练？</li>
<li>文章提出的“残差”是什么？它为什么能解决深模型难训练的问题？</li>
<li>凭什么说深模型难训练的问题被解决了？实验是怎么做的？</li>
<li>这篇文章提出的模型比其他模型好在哪？</li>
</ul>
<h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>引言用十分清晰的逻辑阐明了本文的核心思想——<strong>深度残差学习</strong>框架。引言先介绍“训练更深的神经网络”这一问题的由来，再抽丝剥茧地讨论该问题的解决方法，最后自然过渡到了本文的核心思想上。看完引言，我们就应该看懂这篇文章。</p>
<p>深度卷积神经网络的有效性，得益于它的“深度”。那么，是不是层数更深的网络就更好呢？过去的实验显示：不是的。</p>
<p>更深的网络面临的第一个问题是梯度弥散/爆炸，这些问题会令网络难以收敛。但是，通过参数归一初始化和归一化层，这一问题以及得到了有效缓解。证据就是，较深的网络能够收敛。</p>
<p>这时，较深网络又暴露出了第二个问题——<strong>退化</strong>：增加网络的深度，反而会降低网络的精度。这种退化和过拟合还不同，因为退化不仅导致精度降低，还提升了模型的训练误差。</p>
<p>这种退化，是不是说明较深的网络本质上就比不过较浅的网络呢？其实并不是，退化只是因为较深的网络不是那么容易优化。考虑一个较浅的网络，我们往它的后面增加几层全等映射(y=x)。这个变深的网络与原来的网络完全等价。从这个角度看，更深的网络最少不会比更浅的网络更差才对。因此，我们要想办法让学习算法能够更好地优化更深的网络，最起码优化出一个不比较浅网络更差的网络。也就是说，我们要想办法让学习算法能够轻松学会恒等映射。</p>
<p>文章提出了一种深度残差学习框架。假设一层网络表示的映射是$H(x)$，则该层的非线性层要拟合另一个表示残差的映射$F(x) := H(x) - x$。换个角度看，就是一层网络的输出由原来的$F(x)$变成了$F(x)+x$，多加上了一个$x$。这样，只要令$F(x)=0$，网络就能轻松描述恒等映射，较深的网络最起码不比较浅的网络更差了。</p>
<p>多个数据集上的实验表明，这种方法不仅容易训练，还能得到更高的精度。在多项任务的CV竞赛中，这种方法都独占鳌头。</p>
<p>看完了引言，我们基本能知道文章在解决怎样的问题，也大致明白了残差学习的原理。接下来，我们来过一过这篇文章的实验，看看这种方法的效果究竟如何。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>我们主要看几组ImageNet上的实验。为了方便称呼，作者把不使用残差连接的普通网络叫做“平坦(plain)”网络。第一组实验对比了不同深度的平坦网络的训练误差（细线）和验证误差（粗线）。</p>
<p><img src="/2022/08/09/20220807-ResNet/1.jpg" alt></p>
<p>可以看出，更深的网络有更高的训练误差和验证误差。作者排除了梯度问题的影响。这些网络在训练时使用了Batch Normalization (BN)，经调试，它们的梯度值也一切正常。因此，这种问题不是梯度消失导致的。引言中对于退化问题的描述得到了验证。</p>
<p>第二组实验是对残差网络ResNet的实验。我们可以把它的结果和平坦网络的放在一起对比。</p>
<p><img src="/2022/08/09/20220807-ResNet/2.jpg" alt></p>
<p>从图中可以看出，使用残差学习后，更深的网络果然有了更低的训练误差和验证误差。同时，从表中可以看出，残差网络的测试误差也降低了一大截。这说明残差学习很好地解决了前文提到的退化问题，且这种有效性在测试数据上依然能保持。</p>
<p>由于这轮粗读我们没读方法部分，和方法有关的实验结果得跳过。可以直接翻页到和其他模型的对比表格处：</p>
<p><img src="/2022/08/09/20220807-ResNet/3.jpg" alt></p>
<p>ResNet 在各个评价指标上打败了当时的其他网络，确实很厉害。</p>
<p>后面的表格显示，不仅是图像分类，ResNet在检测和分割等任务中也取得了第一名。</p>
<p>一般看到这里，一轮粗读就差不多完成了。从这轮粗读中，我们看懂了残差学习的核心思想，基本上理解了本文的核心创新点。当然，粗读深度学习相关的论文时，还可以扫一眼网络的核心模块和模型结构。精读的时候，我们再仔细理解文章的方法。</p>
<h2 id="精读"><a href="#精读" class="headerlink" title="精读"></a>精读</h2><h3 id="残差公式"><a href="#残差公式" class="headerlink" title="残差公式"></a>残差公式</h3><p>设多个层构成的模块在拟合一个映射$H(x)$，其中第一层的输入是$x$，则我们希望网络的非线性层去拟合另一个残差函数$F(x) := H(x) - x$，或者说整个模块在拟合$F(x) + x$。由于神经网络的多个非线性层能拟合任意复杂的映射，拟合一个残差函数$H(x)-x$也是可以实现的。</p>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>有了整体的思路，我们来具体看看每一个带残差的模块是怎么搭建的。具体而言，一个模块可以由输入$x$和参数集合${W_i}$计算得到（为了简化表达，bias被省略了）:</p>
<script type="math/tex; mode=display">
y = F(x, \lbrace W_i \rbrace) +x</script><p>我们主要关心$F(x, {W_i})$是怎么搭建的。文章中给出了一种简单的双层残差块示例，其中，$x$是以短路的形式连接到输出上的：</p>
<script type="math/tex; mode=display">
F = W_2\sigma(W_1x), \sigma = ReLU</script><p><img src="/2022/08/09/20220807-ResNet/4.jpg" alt></p>
<p>注意这里的$y = W_2\sigma(W_1x)$是一个未经激活的结果。整个模块送入下一层的输出是$\sigma(y)$，还要加上激活函数。</p>
<p>残差学习是一个框架，每个残差块可以有更多层。比如本文在实验部分还测试了另一种三层残差块（下图是一个示例，实际上通道数可以任意改变）。</p>
<p><img src="/2022/08/09/20220807-ResNet/5.jpg" alt></p>
<p>多层的残差块都是可行的。但是，单层的残差块$y = W_1x + x$和线性层几乎一样，不能提升网络的有效性。</p>
<p>上述的输入$x$是直接加到激活前的输出上的。这个加法操作有一个前提：模块的输入输出通道数是一样的。在输入输出通道数不同时，可以在短路连接上加一个变换维度的线性运算$W_sx$。原来直接做加法的操作叫做全等映射，这里加上一个线性运算再做加法的操作叫做投影映射。</p>
<script type="math/tex; mode=display">
y = F(x, \lbrace W_i \rbrace) + W_sx</script><p><img src="/2022/08/09/20220807-ResNet/6.jpg" alt></p>
<p>这里的符号标记都是基于全连接层的。这些计算对卷积层来说也一样，把矩阵乘法替换成卷积即可。</p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>本文先借鉴VGG的思路，先搭建了一个图像大小逐渐减半，深度逐渐翻倍的平坦网络。之后，在平坦网络连续的3x3卷积层上添加残差连接。下图中的实线代表普通的短路连接，虚线表示需要变换张量形状的短路连接。</p>
<p><img src="/2022/08/09/20220807-ResNet/7.jpg" alt></p>
<p>在虚线表示的残差连接中，图像的大小和深度都发生了改变。对于深度的增加，即可以使用上一节提到的投影映射，也可以直接往多出来的通道数里填0（全等映射）。对于大小的减半，无论是用怎样的深度变化方法，都可以使用步幅为2的操作来实现大小减半。</p>
<blockquote>
<p>在投影时，要进行卷积操作，卷积的步幅为2很好理解。但是，文章没有详细介绍步幅为2的全等映射是怎么做的。直观上理解，就是以步幅2跳着去输入张量里取值。</p>
</blockquote>
<p>文章还提出了层数不同的ResNet架构。对于较深的网络，文章使用了3层残差块。</p>
<p><img src="/2022/08/09/20220807-ResNet/8.jpg" alt></p>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>训练的大部分配置都借鉴自AlexNet。如果你是刚入门图像识别且以后要从事这方面的研究，可以多关注这些细节。</p>
<p><strong>数据处理</strong>：</p>
<ul>
<li>随机缩放图像短边至 [256, 480] 中的均匀随机值</li>
<li>随机翻转</li>
<li>裁剪成 224x224</li>
<li>像素归一化（同AlexNet）</li>
<li>标准颜色增强（同AlexNet）</li>
</ul>
<p><strong>归一化</strong></p>
<ul>
<li>激活前使用BN</li>
<li>参数初始化使用 He Initialization</li>
</ul>
<p><strong>优化配置</strong></p>
<ul>
<li>batch size 256 的 mini-batch 梯度下降</li>
<li>学习率初始化为0.1，发现误差不动了就除以10</li>
<li>迭代$60 \times 10^4$轮</li>
<li>weight decay=0.0001, momentum=0.9</li>
<li>没有dropout</li>
</ul>
<p>此外，为了提高比赛中的精度，在测试时使用了10-crop（对图像裁剪10次，把输入分别输入网络，取结果的平均值）。同时，图像以不同的尺寸输入进了网络，结果取所有运算的平均值。</p>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>读懂了方法，我们再来详细读一遍实验部分。</p>
<p>再看一次平坦网络和残差网络的对比。</p>
<p><img src="/2022/08/09/20220807-ResNet/2.jpg" alt></p>
<p>在这份对比实验中，残差网络相对平坦网络没有添加任何参数。当通道数改变时，残差网络用的是0填充的全等映射。这样，平坦网络和残差网络的唯一区别就是是否有短路连接，整个对比实验非常公平。实验的结果证明了残差连接的有效性。</p>
<p>这篇工作还做了另一个比较平坦网络的实验：在CIFAR-10数据集上统计了平坦网络和残差网络的3x3卷积输出的标准差。标准差大，能说明输出的数值较大。</p>
<p><img src="/2022/08/09/20220807-ResNet/9.jpg" alt></p>
<p>从这份对比结果中，我们能看出残差网络的输出标准差小于平坦网络。这符合残差学习的设计初衷：残差块至少是一个不会使性能变差的全等映射，其效果能够在全等映射的基础上优化。也因此，残差网络的输出大小会比平坦网络更靠近0。</p>
<p>看完了和平坦网络的对比，再看一看不同配置的ResNet直接的对比。文章先比较了全等映射和投影映射的短路连接。本文探讨了短路连接的3种配置：A) 全部使用全等连接 B) 有通道数变动的地方才使用投影连接 C) 全部使用投影连接。它们的表现如下：</p>
<p><img src="/2022/08/09/20220807-ResNet/10.jpg" alt></p>
<p>可以看出，投影用得越多，效果越好。作者认为，B相对A更好，是因为通道数变化时0填充的部分没有残差学习（也就是没有做$+x$的操作）；C相对B更好，是因为参数更多了。这些实验结果说明，投影连接并不能解决退化问题，出于性能的考虑，本文没有在其他实验中使用C。</p>
<blockquote>
<p>后来，B是公认的ResNet标配。</p>
</blockquote>
<p>文章还讨论了构筑更深网络的bottleneck结构。如前文所述，50层以上的ResNet在每个残差块里使用了3个1x1, 3x3, 1x3的卷积。这种设计主要是为了缩短训练时间。ResNet-50和ResNet-34有着同样的时间复杂度，却因为深度更大，精度更高。</p>
<p>这篇论文的主要实验就是这些。论文后面还展示了一个比较有趣的实验：在CIFAR-10数据集上训练超过1000层的ResNet。实验结果显示，1000多层的ResNet仍能成功被优化，训练误差也降到了很低，但是测试误差没有110层的网络好。作者认为，这是因为训练数据太少，网络过拟合了。</p>
<p><img src="/2022/08/09/20220807-ResNet/11.jpg" alt></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>ResNet是基于深度学习的计算机视觉中里程碑式的工作。通过阅读这篇论文，我们发现，ResNet使用的残差结构本身并不十分复杂。这篇工作真正出彩之处，是发现了深度神经网络的一个普遍存在的问题，并用精彩的推理设计出了一套能够解决此问题的方案。这种方案确实很有效，基于残差学习的ResNet击败了同时代的所有网络。残差连接被用在了后续几乎所有网络架构中，使用ResNet为backbone也成了其他CV任务较为常用的初始配置。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/DLS-note-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/DLS-note-13/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十三）：CNN应用：人脸识别与风格迁移</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:13:53" itemprop="dateCreated datePublished" datetime="2022-08-09T14:13:53+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这堂课里，我们要学习两个具体的应用：人脸识别、风格迁移。</p>
<p>相信大家已经在很多地方见识过人脸识别应用了：在火车站，要通过身份证和人脸核实身份；办理业务时，可以用手机完成人脸识别以核实身份；进办公楼时，员工只要刷脸通过就可以打开门禁。通过这节课的学习，我们能够学会如何用CNN完成人脸识别。</p>
<p>神经网络风格迁移是一项有趣的应用。它利用了CNN捕捉图像抽象信息的特点，能够把一幅图像的风格转移到另一幅图像上，从而生成一幅新的图像。这项技术不需要从头训练网络，学完这门课后，我们能快速地用代码实现神经网络风格迁移。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><p>准确来说，在人脸识别(Face Recognition)任务中，会给定一个有$K$个人的数据库。之后，每一次识别都会输入一张图片，输出这张图片是$K$个人中的哪一个，或者没有检测到相关人士。</p>
<p>有一个与这个相关的任务叫做人脸验证(Face Verification)。这个任务稍微简单一些，输入是一张图片和一个标记身份的数据（比如身份证号），要求输出图片中的人是否符合该身份。</p>
<h3 id="单样本学习"><a href="#单样本学习" class="headerlink" title="单样本学习"></a>单样本学习</h3><p>按我们之前学的方法，假如我们在$K$个人的数据库上做识别（分类）任务，应该套用一个CNN模型，并在模型最后接一个$K+1$类的softmax，表示输入图片是K个人中的哪一个，或者都不是。</p>
<p>但是，这样的架构不适合人脸识别任务。以公司的门禁识别为例，这种方法有如下的缺点：</p>
<ol>
<li>每来一个新同事，模型就要重新训练一次。</li>
<li>每个人都得上传大量的个人照片供网络训练。</li>
</ol>
<p>理想情况下，我们希望模型能从一张人脸照片中学会分辨这张人脸，这样每个新同事只需要上传一张照片即可。这叫做单样本学习(One-shot Learning)。</p>
<p>为了完成单样本学习，我们可以从另一个角度来建模人脸识别问题：如果输入的人脸和数据库里某张人脸极为相似，我们就说识别出了这张人脸；否则，就说没有识别到有效的人脸。</p>
<p>这样，人脸识别问题就被转换为了一个求两张图片相似度的问题。我们可以让网络学习一个输入是两张图片，输出是二者相似度的一个映射。</p>
<h3 id="孪生网络"><a href="#孪生网络" class="headerlink" title="孪生网络"></a>孪生网络</h3><p>在完成和相似度有关的问题时，一种常见的做法是使用孪生网络（Siamese Network)。</p>
<p>假设网络的倒数第二层有128个神经元。在普通分类网络中，这128个神经元输出的长度为128的向量会被输入进最后的softmax层。而在孪生网络中，我们要去掉softmax层，并用这个没有sofrmax的网络$f$分别输出两张图片$x^{(1)}, x^{(2)}$对应的128维向量$f(x^{(1)}), f(x^{(2)})$。这样，每张图片有了唯一对应的一个128维向量，这个向量可以看成是该图片的编码(encoding)。而我们又知道，对向量求相似度是很方便的。我们可以利用两张图片的编码求出相似度。</p>
<p><img src="/2022/08/09/DLS-note-13/1.jpg" alt></p>
<p>说起向量的相似度，最容易想到是向量间的距离$||v - u||^2$。因此，我们可以i设法让网络学会这样一种关系：</p>
<ul>
<li>若$x^{(i)}, x^{(j)}$是同一人，则$||f(x^{(i)}) - f(x^{(j)})||^2$很小。</li>
<li>若$x^{(i)}, x^{(j)}$不是同一人，则$||f(x^{(i)}) - f(x^{(j)})||^2$很大。、</li>
</ul>
<p>为了达成这个目标，我们来看看应该用怎样的误差来指导网络的优化方向。</p>
<h3 id="三元组误差"><a href="#三元组误差" class="headerlink" title="三元组误差"></a>三元组误差</h3><p>在让网络学习基于距离的相似度时，一种常用的误差是三元组误差(Triplet Loss)。</p>
<p>在一轮训练中，我们要用到3张图片：一张基准图(anchor)$A$和与其相对的正负样本$P, N$各一张。设$d(A, B)$为图片$A, B$的编码的距离，则我们希望$d(A, P) \leq d(A, N)$。</p>
<blockquote>
<p>左边的人是吴恩达的妻子，这几张图片已经出现了好几次。</p>
</blockquote>
<p><img src="/2022/08/09/DLS-note-13/2.jpg" alt></p>
<p>移个项，即我们希望：</p>
<script type="math/tex; mode=display">
d(A, P) - d(A, N) \leq 0.</script><p>但这个条件太容易满足了，令$f(x)=0$恒成立的话无论怎样的输入都可以使上式左侧为0了。因此，我们要加一点小小的约束：</p>
<script type="math/tex; mode=display">
d(A, P) - d(A, N) + \alpha \leq 0,</script><p>这个$\alpha$是一个较小的数，比如可以令$\alpha=0.2$。为了达成这个目标，我们可以设置以下的误差函数</p>
<script type="math/tex; mode=display">
L(A, P, N)=max(d(A, P) - d(A, N) + \alpha, 0).</script><p>这里取max的直观解释是：只要满足开始那个不等式，让正样本和负样本能够分清楚就行了。至于两类样本要不要分辨得那么分明，我们并不关心。</p>
<p>为了利用这个误差训练网络，我们要在训练集里为同一个人准备多张照片。比如1000个人，每人100张照片，共100000张照片构成训练集。</p>
<p>在提出此设计的FaceNet中，还有一些小细节：为了加大训练难度，让模型效果更好，每次训练时应该选择较难的三元组$A, P, N$。详情请阅读原论文。</p>
<h3 id="人脸验证与二分类问题"><a href="#人脸验证与二分类问题" class="headerlink" title="人脸验证与二分类问题"></a>人脸验证与二分类问题</h3><p>其实，判断两张图片的编码是否相似的问题可以简单地转化成一个二分类问题：把两张图片的编码“拼起来”，输入进一个逻辑回归的单元，让网络输入两张图片是否相似。</p>
<p>这里把两张图片“拼起来”有很多的实现方式。一种简单的方式是求两个编码的绝对值（L1距离）。</p>
<p>另外，在部署时，由于比较的图像的编码是可以预处理的，只需要用神经网络跑一遍输入图片即可。</p>
<h2 id="神经网络风格迁移"><a href="#神经网络风格迁移" class="headerlink" title="神经网络风格迁移"></a>神经网络风格迁移</h2><blockquote>
<p>我之前写了一篇<a href="https://zhouyifan.net/2022/06/01/20220531-styletransfer/">详细介绍神经网络风格迁移的文章</a>，我认为那篇文章比这堂课更好理解，非常推荐大家阅读。因此，这部分笔记我会写得潦草一些。</p>
</blockquote>
<h3 id="什么是神经网络风格迁移？"><a href="#什么是神经网络风格迁移？" class="headerlink" title="什么是神经网络风格迁移？"></a>什么是神经网络风格迁移？</h3><p><img src="/2022/08/09/DLS-note-13/3.jpg" alt></p>
<p>如上图所示，在神经网络风格迁移中，输入一张表示内容的图(C)和一张表示画家风格的图(S)，我们可以借助神经网络生成一幅融合内容与风格的图片(G)。</p>
<p>接下来实现神经网络风格迁移时，我们会关注CNN浅层和深层提取出来的特征。因此，在具体介绍风格迁移的实现之前，我们先来看看CNN各层网络究竟学到了什么。</p>
<h3 id="深度卷积网络学到了什么？"><a href="#深度卷积网络学到了什么？" class="headerlink" title="深度卷积网络学到了什么？"></a>深度卷积网络学到了什么？</h3><p>为了设法可视化神经网络每一层的输出，我们可以考虑把整个训练集喂入网络，找出使某一层某一神经元激活输出最大的几个像素块。</p>
<p>以AlexNet为例，令第一层某几个神经元激活输出最大的像素块是：</p>
<p><img src="/2022/08/09/DLS-note-13/4.jpg" alt></p>
<p>可以看出，浅层的神经网络更关注不同方向轮廓信息。</p>
<p>用同样的方式可视化更深的层，可以看出网络关注的信息越来越抽象。</p>
<p><img src="/2022/08/09/DLS-note-13/5.jpg" alt></p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>和优化一个网络的参数不同，在神经网络风格迁移中，我们要把一幅图像当成可以优化的参数，通过修改图像的像素值来最小化一个损失函数。让我们看看这个损失函数是怎么定义的。</p>
<p>损失函数由两部分构成：生成图像(G)和内容图像(C)的<strong>内容损失</strong>与和风格图像(S)的<strong>风格损失</strong>。二者之间的比例靠比例系数$\alpha, \beta$控制。</p>
<script type="math/tex; mode=display">
J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G).</script><p>我们来看一个优化这个损失函数的步骤示例：</p>
<ol>
<li>生成一个随机图像$G: 100 \times 100 \times 3$</li>
<li>使用梯度下降更新$G$，最小化$J(G)$</li>
</ol>
<p><img src="/2022/08/09/DLS-note-13/6.jpg" alt></p>
<h3 id="内容损失"><a href="#内容损失" class="headerlink" title="内容损失"></a>内容损失</h3><p>内容损失的计算方式如下：</p>
<ol>
<li>使用一个预训练的网络（最常用的是VGG）。</li>
<li>选择神经网络中的某一层$l$，这一层相关的数据会用来计算内容损失。$l$越浅，表示越具体的图像；$l$越深，表示越抽象的图像。一般选取适中的$l$。</li>
<li>令$a^{<a href="C">l</a>}, a^{[l][G]}$为图像$C, G$网络第$l$层的激活输出。我们认为，如果这两个值很相似，则两幅图像的内容就很相似。</li>
<li>令$J_{content}(C, G) = ||a^{<a href="C">l</a>} - a^{[l][G]}||^2$</li>
</ol>
<h3 id="风格损失"><a href="#风格损失" class="headerlink" title="风格损失"></a>风格损失</h3><p>看完了内容损失，现在来看风格损失的计算方式。我们刚刚一直在用名词“风格”。这个词放在美术里，可以指画家的绘画风格。而对于神经网络来说，“风格”又是什么意思呢？</p>
<p>和刚刚的内容损失类似，计算风格损失时，我们也要选择CNN的某层$l$，计算这一层卷积激活输出的<strong>通道相关量</strong>。</p>
<p>这里“通道相关量”反映的是图像各个通道间两两的相关关系。这句话可能有点拗口，让我们看一个详细的通道相关量示例。</p>
<p><img src="/2022/08/09/DLS-note-13/7.jpg" alt></p>
<p>假设一个激活输出有5个通道，我们用颜色“红-黄-紫-绿-蓝”称呼它们。每个通道表示的是一类特征，比如红色的通道表示图像有没有竖着的条纹，黄色通道表示图像有没有橙色的色块。我们想计算红黄通道之间的相关量，其实就是计算图像每个像素处红色通道的值和绿色通道的值之间的相关关系，看看它们是会同时出现，还是一个出现了另一个就不出现。（两个数值的相关量就是它们的乘积，具体的数学表示会在后文中给出）</p>
<p>为什么这样的相关关系能够捕捉到风格信息呢？红色通道和黄色通道的相关关系，就是是不是有竖条纹的地方就有橙色色块。这样一种线条和颜色的关联，就可以代表图片的风格。（下图中红色的框和黄色的框分别圈出了两种特征）</p>
<p><img src="/2022/08/09/DLS-note-13/8.jpg" alt></p>
<p>从直觉上认识了风格，现在我们来看一看风格的具体计算方法。在计算两个向量的相关量时，可以直接求两个向量的积。因此，我们要用到一种叫做”风格矩阵“的中间量，它计算了所有通道向量的乘积：</p>
<script type="math/tex; mode=display">
G^{[l]}_{kk'}=\Sigma_i^{H} \Sigma_i^{W} a^{[l]}_{i, j, k} a^{[l]}_{i, j, k'}</script><p>其中，$a^{[l]}<em>{i, j, k}$是第$i$行$j$列第$k$个通道的激活输出，风格矩阵$G$的形状是$n_c^{[l]} \times n_c^{[l]}$，$G^{[l]}</em>{kk’}$描述的是第$l$层激活输出中，第$k, k’$个通道间的相关量。</p>
<blockquote>
<p>在数学中这个矩阵叫做gram矩阵。</p>
</blockquote>
<p>有了风格矩阵，就可以算风格损失了。为了简化表示，我们用$G^{[l]}(S), G^{[l]}(G)$分别表示风格图像S和生成图像G的风格矩阵。和刚刚一样，我们选择一层$l$，并计算风格误差：</p>
<script type="math/tex; mode=display">
J^{[l]}_{style}(S, G) = ||G^{[l]}(S)- G^{[l]}(G)||_F^2</script><p>这是一层上的风格误差。其实，我们还可以指定多个层上的风格误差，以使生成图像能够拟合风格图像在多个卷积层上（抽象程度不同）的风格。多个层的风格误差就是各层风格误差之和。</p>
<h2 id="1D和3D上的卷积"><a href="#1D和3D上的卷积" class="headerlink" title="1D和3D上的卷积"></a>1D和3D上的卷积</h2><p>在结束这门课之前，我们来学习最后一项内容——1D和3D数据上的卷积。之前，我们只关注2D的图像数据，当维度不是2时，卷积有怎样的变化呢？</p>
<p>1D数据可以是心电图。和可以用2D卷积捕捉2D图像的边缘一样，我们可以用下图中那个尖尖的1D卷积来捕获高频的数据。1D卷积前后的形状变化规律和2D一样，比如用16个长度为5的卷积卷一个$14 \times 1$的1D图像，可以得到$10 \times 16$的1D图像（第一维是图像长度，第二维是通道数）。 </p>
<p><img src="/2022/08/09/DLS-note-13/9.jpg" alt></p>
<p>3D数据也是类似的道理。用16个$5 \times 5 \times 5$的卷积核卷一个单通道$14 \times 14 \times 14$的单通道图像，可以得到一个$5 \times 5 \times 5 \times 16$的图像。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这节课主要介绍了人脸识别和神经网络风格迁移两项任务，最后顺便介绍了1D和3D上的卷积。</p>
<ul>
<li>人脸识别<ul>
<li>人脸验证与人脸识别任务的定义</li>
<li>如何对单样本学习建模</li>
<li>孪生网络</li>
<li>基于三元组误差和二分类误差的人脸识别网络</li>
</ul>
</li>
<li>神经网络风格迁移<ul>
<li>神经网络风格迁移的输入输出</li>
<li>利用小技巧可视化 CNN 学到的图像信息</li>
<li>生成风格迁移图像时的内容误差与风格误差</li>
</ul>
</li>
</ul>
<p>人脸识别任务依赖于数据集，项目搭建起来比较麻烦。对于这一周的内容，我就不展示其他的代码实战笔记了，欢迎阅读我之前写的<a href="https://zhouyifan.net/2022/06/01/20220531-styletransfer/">神经网络风格迁移的实现</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/08/09/DLS-note-12-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/09/DLS-note-12-2/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》代码实战（十二）：目标检测 NMS 的 Python 实现（附算法动图）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-08-09 14:09:41" itemprop="dateCreated datePublished" datetime="2022-08-09T14:09:41+08:00">2022-08-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这篇文章中，我将给出一份带运行示例的NMS Python脚本，并对算法和代码进行详细解说。相信大家看完这篇文章后，能够轻松地掌握NMS的底层原理。</p>
<p>如果你对目标检测的基础知识不太熟，欢迎先阅读我的上篇文章：<a href="https://zhouyifan.net/2022/07/26/DLS-note-12/">目标检测简介</a></p>
<p>示例脚本（包括可视化的代码）链接：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/nms">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/nms</a> </p>
<h2 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h2><p>在目标检测算法中，为了尽量不漏掉物体，会输出大量的检测结果（每一条结果由检测概率与检测框组成）。这些检测结果很可能有重复的，即会有多个框标出了同一个物体。下图就是一个例子，算法正确识别出了两辆车，但却有多个检测框标出了同一辆车。</p>
<p><img src="/2022/08/09/DLS-note-12-2/1.jpg" alt></p>
<p>我们需要一个算法来过滤多余的检测框。最常用的算法就是NMS(Non-Maximum Suppresion, 非极大值抑制)。该算法的思路很简单：只保留局部概率最大的检测框，与其重合的其他检测框都会被舍去。</p>
<p>算法的伪代码如下：</p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">输入所有检测概率、检测框</span><br><span class="line">当还有检测框没有处理：</span><br><span class="line">    从剩下的框里挑出检测概率最大的框 bbox_a</span><br><span class="line">    遍历所有没有处理的框bbox_i：</span><br><span class="line">        如果 bbox_i != bbox_a 且 bbox_i 与 bbox_a 重合：</span><br><span class="line">            舍去 bbox_i</span><br><span class="line">    把 bbox_a 输出成一个检测结果</span><br></pre></td></tr></table></figure>
<p>当然，这个算法的描述还不够准确：究竟该怎么定义两个检测框是“重合”呢？如果两个检测框有交集就说它们重合是行不通的，因为图片中很可能会有挨得很近的物体，它们的检测框就是相交的。因此，我们一般用IoU（交并比）来描述两个框的重合程度，如果IoU超出某个阈值，就说这两个框是“重合”的。</p>
<p>IoU的计算很简单，算出两个矩形的「交面积」，再用两个矩形面积之和减去「交面积」就可以得到「并面积」，「交面积」比「并面积」就是IoU。</p>
<p><img src="/2022/08/09/DLS-note-12-2/1.gif" alt></p>
<p>在NMS之前，一般还会先做一步预处理：对于预测概率小于某个阈值的检测框，我们不信任它们，会在进行NMS之前就把它们舍去。在代码实现时这部分逻辑常常会放到NMS的函数里。这样，整个NMS的流程是这样的：</p>
<p><img src="/2022/08/09/DLS-note-12-2/2.gif" alt></p>
<p>上述的NMS针对是识别一种物体的任务，在推广到多分类NMS时，只要把输入的检测概率换成有物体的概率乘上概率最大的类别的概率即可。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><blockquote>
<p>理论上这段代码是兼容任何格式的张量的。经测试，NumPy, PyTorch 的张量都可以正确运行。</p>
</blockquote>
<p>先看一下IoU的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span>(<span class="params">b1: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>], b2: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">    intersection = box_intersection(b1, b2)</span><br><span class="line">    inter_area = area(intersection)</span><br><span class="line">    union_area = area(b1) + area(b2) - inter_area</span><br><span class="line">    <span class="keyword">return</span> inter_area / union_area</span><br></pre></td></tr></table></figure>
<p>所有的检测框用一个长度为4的元组表示。<code>box_intersection</code>用于计算两个框的相交框，<code>area</code>用于计算框的面积。这段代码和之前描述得一样，先获取相交区域，计算交面积，再计算并面积，最后算除法。</p>
<p><code>box_intersection</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_intersection</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        b1: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">        b2: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]:</span></span><br><span class="line">    x11, y11, x12, y12 = b1</span><br><span class="line">    x21, y21, x22, y22 = b2</span><br><span class="line"></span><br><span class="line">    xl = <span class="built_in">max</span>(x11, x21)</span><br><span class="line">    xr = <span class="built_in">min</span>(x12, x22)</span><br><span class="line">    yt = <span class="built_in">max</span>(y11, y21)</span><br><span class="line">    yb = <span class="built_in">min</span>(y12, y22)</span><br><span class="line">    <span class="keyword">return</span> (xl, yt, xr, yb)</span><br></pre></td></tr></table></figure>
<p>算相交区域，可以理解成分别求出x, y方向上的相交部分，再把两部分合成一个框。而求直线的相交，就是取最大的左端点和最小的右端点。</p>
<p><code>area</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">area</span>(<span class="params">box: <span class="type">Tuple</span>[<span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>, <span class="built_in">int</span>]</span>) -&gt; <span class="built_in">float</span>:</span></span><br><span class="line">    x1, y1, x2, y2 = box</span><br><span class="line">    width = <span class="built_in">max</span>(x2 - x1, <span class="number">0</span>)</span><br><span class="line">    height = <span class="built_in">max</span>(y2 - y1, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> width * height</span><br></pre></td></tr></table></figure>
<p>如果两个框不相交，<code>x2 - x1</code>和<code>y2 - y1</code>会出现小于0的情况。因此，要保证它们最小为0，再算面积就不会有错了。</p>
<p>有了<code>iou</code>，就可以实现了NMS了。我的NMS函数定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">predicts: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">        score_thresh: <span class="built_in">float</span> = <span class="number">0.6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        iou_thresh: <span class="built_in">float</span> = <span class="number">0.3</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Non-Maximum Suppression</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        predicts (np.ndarray): Tensor of shape [n, 5]. The second demesion</span></span><br><span class="line"><span class="string">            includes 1 probability and 4 numbers x, y, w, h denoting a bounding</span></span><br><span class="line"><span class="string">            box.</span></span><br><span class="line"><span class="string">        score_thresh (float): The boxes with probability lower than</span></span><br><span class="line"><span class="string">            score_threash will be discarded.</span></span><br><span class="line"><span class="string">        iou_thresh (float): The threshold determining whether two boxes are</span></span><br><span class="line"><span class="string">            &quot;overlapped&quot;.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        (np.ndarray, List[int]): The filtered predictions and the indices of</span></span><br><span class="line"><span class="string">            remaining boxes.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<p>NMS算法需要输入检测结果<code>predicts</code>，输出过滤后的检测结果<code>filtered_predicts</code>。此外，NMS算法有两个输入属性<code>score_thresh</code>， <code>iou_thresh</code>，分别表示被选定的检测框最小需要的概率、判断两个框是否重合的IoU阈值。为了方便其他的计算（比如多分类NMS），我还输出了一个索引数组indices，表示被选中的框的索引。这方面的逻辑可以根据自己的项目要求进行优化，没有统一的规定。</p>
<p>NMS的实现和开始的伪代码几乎一模一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">predicts: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">        score_thresh: <span class="built_in">float</span> = <span class="number">0.6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        iou_thresh: <span class="built_in">float</span> = <span class="number">0.3</span></span>):</span></span><br><span class="line">    n_remainder = <span class="built_in">len</span>(predicts)</span><br><span class="line">    vis = [<span class="literal">False</span>] * n_remainder</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Filter predicts with low probability</span></span><br><span class="line">    <span class="keyword">for</span> i, predict <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> predict[<span class="number">0</span>] &lt; score_thresh:</span><br><span class="line">            vis[i] = <span class="literal">True</span></span><br><span class="line">            n_remainder -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># NMS</span></span><br><span class="line">    output_predicts = []</span><br><span class="line">    output_indices = []</span><br><span class="line">    <span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">        max_pro = -<span class="number">1</span></span><br><span class="line">        max_index = <span class="number">0</span></span><br><span class="line">        <span class="comment"># Find argmax</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">                <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                    max_index = i</span><br><span class="line">                    max_pro = p[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append output</span></span><br><span class="line">        max_p = predicts[max_index]</span><br><span class="line">        output_predicts.append(max_p)</span><br><span class="line">        output_indices.append(max_index)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Suppress</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">                <span class="keyword">if</span> iou(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; iou_thresh:</span><br><span class="line">                    vis[i] = <span class="literal">True</span></span><br><span class="line">                    n_remainder -= <span class="number">1</span></span><br><span class="line">        vis[max_index] = <span class="literal">True</span></span><br><span class="line">        n_remainder -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_predicts, output_indices</span><br></pre></td></tr></table></figure>
<p>一开始，先声明一些辅助的变量。<code>n_remainder</code>表示还有多少个框没被访问，<code>vis[i]</code>表示第<code>i</code>个框是否被访问。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_remainder = <span class="built_in">len</span>(predicts)</span><br><span class="line">vis = [<span class="literal">False</span>] * n_remainder</span><br></pre></td></tr></table></figure>
<p>一开始，先过滤那些概率过小的框：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, predict <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">    <span class="keyword">if</span> predict[<span class="number">0</span>] &lt; score_thresh:</span><br><span class="line">        vis[i] = <span class="literal">True</span></span><br><span class="line">        n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>之后，正式进入NMS，先准备好输出的列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NMS</span></span><br><span class="line">output_predicts = []</span><br><span class="line">output_indices = []</span><br><span class="line">    <span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br></pre></td></tr></table></figure>
<p>在还有没访问的框时，先找出剩下的框中概率最大的那个框：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    max_pro = -<span class="number">1</span></span><br><span class="line">    max_index = <span class="number">0</span></span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">            <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                max_index = i</span><br><span class="line">                max_pro = p[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>之后，抑制掉和概率最大框“重合”的框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    max_p = predicts[max_index]</span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">            <span class="keyword">if</span> iou(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; iou_thresh:</span><br><span class="line">                vis[i] = <span class="literal">True</span></span><br><span class="line">                n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>最后，把这个结果添加进输出，并维护好<code>vis</code>和<code>n_remainder</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append output</span></span><br><span class="line">    output_predicts.append(max_p)</span><br><span class="line">    output_indices.append(max_index)</span><br><span class="line">    vis[max_index] = <span class="literal">True</span></span><br><span class="line">    n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里可以把<code>vis[max_index] = True</code>那两行移到抑制操作的前面，这样判断里就不用加<code>i != max_index</code>了。我这样写是为了强调一下，判断重合的时候不需要判断自己。</p>
</blockquote>
<p>实现完了，返回结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> output_predicts, output_indices</span><br></pre></td></tr></table></figure>
<h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>做单元测试的时候，最好是找一份现有的实现用做对照。为了让整个测试过程更贴合实际一些，我用MMDetection的YOLOv3跑了一个NMS前和NMS后的检测框结果，并把NMS前的检测框输入进了自己实现的NMS，比较了两份NMS的输出。以下是具体的测试过程。</p>
<p>照着MMDetection的官方文档，下载好YOLOv3模型，用下面的代码即可对MMDetection的demo图片进行推理并保存结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mmdet.apis <span class="keyword">import</span> inference_detector, init_detector, show_result_pyplot</span><br><span class="line"><span class="keyword">from</span> mmdet.models.detectors <span class="keyword">import</span> BaseDetector</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose to use a config and initialize the detector</span></span><br><span class="line">config = <span class="string">&#x27;configs/yolo/yolov3_mobilenetv2_320_300e_coco.py&#x27;</span></span><br><span class="line"><span class="comment"># Setup a checkpoint file to load</span></span><br><span class="line">checkpoint = <span class="string">&#x27;checkpoints/yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth&#x27;</span></span><br><span class="line"><span class="comment"># initialize the detector</span></span><br><span class="line">model: BaseDetector = init_detector(config, checkpoint, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">img = <span class="string">&#x27;demo/demo.jpg&#x27;</span></span><br><span class="line">result = inference_detector(model, img)</span><br><span class="line"></span><br><span class="line">model.show_result(img, result, score_thr=<span class="number">0.3</span>, out_file=<span class="string">&#x27;work_dirs/1.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/08/09/DLS-note-12-2/d_1.jpg" alt></p>
<p>为了得到NMS前的输入，我在<code>mmdet/models/dense_head/YOLOV3Head.get_bboxes</code>里面插入了一段输出结果的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line">save_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;bboxes&#x27;</span>: bboxes,</span><br><span class="line">    <span class="string">&#x27;cls_scores&#x27;</span>: scores,</span><br><span class="line">    <span class="string">&#x27;prob&#x27;</span>: objectness</span><br><span class="line">&#125;</span><br><span class="line">torch.save(save_dict, <span class="string">&#x27;work_dirs/bboxes.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">det_bboxes, det_labels = multiclass_nms(</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>得到NMS前的输入是<br><img src="/2022/08/09/DLS-note-12-2/d_2.jpg" alt></p>
<p>把这份数据输入进我们的NMS中，得到的可视化结果如下：</p>
<p><img src="/2022/08/09/DLS-note-12-2/3.jpg" alt></p>
<p>这跟开始那份输出一模一样。看来我们实现的这份NMS完全正确。</p>
<p>如果你对测试过程、可视化、多分类NMS感兴趣，欢迎直接阅读我的项目源码。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我给出了一份NMS的简洁而靠近底层的Python实现，并对NMS算法进行了介绍。通过阅读这篇文章，相信大家已经完全理解了NMS的原理，并且能够用任何一种语言实现NMS。一般的NMS开源实现支持的参数更多，代码会更复杂一些，但它们的核心和我的这份实现是一样的。</p>
<p>这份NMS的实现还有很大的改进空间。比如每轮求概率最大的框时，可以先排好序，或者用优先队列，这样均摊下来每次获取概率最大的框的复杂度是<code>O(logn)</code>。但是后面判断重复的框一定有一个<code>O(n)</code>的计算，这部分的优化并不显著。大家有余力可以参考成熟的CV项目里NMS是怎么高效用C++实现的。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/26/DLS-note-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/26/DLS-note-12/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十二）：目标检测与语义分割简介 (YOLO, U-Net)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-26 21:00:00" itemprop="dateCreated datePublished" datetime="2022-07-26T21:00:00+08:00">2022-07-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这节课中，我们要学习计算机视觉中最重要的任务之一——目标检测任务。我们会先认识目标定位和关键点检测这两个比较简单的任务，慢慢过度到目标检测任务。之后，我们会详细学习目标检测的经典算法YOLO。最后，我们会稍微认识一下语义分割任务及适用于此问题的U-Net架构。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="目标定位"><a href="#目标定位" class="headerlink" title="目标定位"></a>目标定位</h2><p>在<strong>图像分类问题</strong>中，给定一幅图片，我们只要说出图片里的物体是什么就行了。在这堂课要讨论的任务中，我们还要多做一件事——定位。我们要先用边框圈出图中的物体，再说出框里的物体是什么。这叫做<strong>带定位(localization)的分类问题</strong>。更进一步，如果我们不再是只讨论一个物体，而是要把图片中所有物体都框出来，并标出每一个物体的类别，这就是<strong>目标检测</strong>问题，</p>
<p><img src="/2022/07/26/DLS-note-12/1.jpg" alt></p>
<p>我们对分类任务的神经网络结构已经很熟悉了。那么，带定位的分类该使用怎样的网络呢？实际上，一个边框可以用边框中心和边框宽高这四个量表示。除了softmax出来的分类结果外，我们只要让网络再多输出四个数就行了。如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/2.jpg" alt></p>
<p>这里，要统一一下对于边框的定义。我们用$b_x, b_y$表示边框的中心坐标，$b_h, b_w$表示边框的高、宽。</p>
<p>来看一下标签$y$的具体写法。假设一共有四类物体：行人、汽车、摩托车、背景（没有物体）。那么，标签$y$应该用$y=[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3]^T$表示。其中，$p_c$表示图中有没有物体。若$p_c=1$，则$c_1, c_2, c_3$分别表示物体属于除背景外的哪一类；若$p_c=0$，则其他值无意义。</p>
<p>这样，在算误差时，也需要分类讨论。若$p_c=1$，则算估计值与标签8个分量两两之间的均方误差；若$p_c=0$，只算$p_c$的均方误差，不用管其他量。</p>
<p><img src="/2022/07/26/DLS-note-12/3.jpg" alt></p>
<p>只要更换一下神经网络的输出格式，我们就能得到一个完成目标定位问题的网络。</p>
<h2 id="关键点检测"><a href="#关键点检测" class="headerlink" title="关键点检测"></a>关键点检测</h2><p>我们刚刚学了用2个点表示一个边框。其实，拓展一下边框检测，就是一个关键点（英文有时叫做”landmark”，是“地标”的意思）检测问题。</p>
<p>比如，在人脸关键点检测中，我们可以定义一堆关键点，分别表示眼睛、鼻子、嘴巴……的位置。我们还是让网络先输出一个数，表示图中有没有人脸；再输出2n个数，表示n个人脸关键点。这样，网络就能学习怎么标出人脸关键点了。</p>
<p><img src="/2022/07/26/DLS-note-12/4.jpg" alt></p>
<p>很多应用都基于人脸关键点检测技术。比如我们检测到了眼睛周围的关键点后，就可以给人“戴上”墨镜。</p>
<p>总之，通过这一节的学习，我们要知道，目标定位中输出2个坐标只是关键点检测的一个特例。只要训练数据按照某种规律标出了关键点，不管这些关键点是表示一个框，还是人脸上各器官的位置，网络都能学习这种规律。</p>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><p>有了之前的知识储备，现在我们来正式学习目标检测。目标检测可以用一种叫做“滑动窗口”的技术实现。</p>
<p>假设我们要构建一个汽车的目标检测系统。我们可以先构造一个汽车分类数据集——数据集的x是一些等大的图片，y表示图片里是不是有汽车。如果图片里有汽车，汽车应该占据图片的大部分位置。</p>
<p><img src="/2022/07/26/DLS-note-12/5.jpg" alt></p>
<p>通过学习，网络就能够判断一个框里的物体是不是汽车了。这样，我们可以用一个边框框出图片的一部分，裁剪下来，让网络看看图片的这一部分是不是汽车。只要我们尝试的次数够多，总能找出图中的汽车。</p>
<p><img src="/2022/07/26/DLS-note-12/6.jpg" alt></p>
<p>在遍历边框时，我们是通过“滑动”的方法：遍历边框的大小，选择好大小后把框放到左上角，先往右移，再往下移。所以这种方法叫做“滑动窗口”。</p>
<p>滑动窗口算法有一个缺点：如果我们移动窗口的步伐过小，则运行分类器的次数会很多；如果移动窗口的步伐过大，则算法的精度会受到影响。在深度学习时代之前，分类器都是一些简单的线性函数，能够快速算完，多遍历一些滑动窗口没有问题。而使用了深度CNN后，遍历滑动窗口的代价就很大了。</p>
<p>幸好，滑动窗口也可以通过卷积来生成，而不一定要遍历出来。让我们看下去。</p>
<h2 id="基于卷积的滑动窗口"><a href="#基于卷积的滑动窗口" class="headerlink" title="基于卷积的滑动窗口"></a>基于卷积的滑动窗口</h2><p>滑动窗口其实可以通过执行巧妙的卷积来生成。在那之前，我们先学一门前置技能：怎么把全连接层变成卷积层。</p>
<p>前两周学习CNN时，我们学过，卷积结束后，卷积的输出会被喂入全连接层中。实际上，我们可以用卷积来等价实现全连接层。比如下图中，一个$5 \times 5 \times 16$的体积块想变成一个长度为400的向量，可以通过执行400个$5 \times 5$的卷积来实现。</p>
<p><img src="/2022/07/26/DLS-note-12/7.jpg" alt></p>
<p>知道了这一点后，我们就可以利用卷积来快速实现滑动窗口了。</p>
<p>假设我们按照上一节的算法，先实现了对$14 \times 14$的小图片进行分类的分类器。之后，我们输入了一张$16 \times 16$的大图片。我们遍历滑动窗口，令步幅为2。这样，理论上，有4个合法的滑动窗口，应该执行4次分类器的运算，如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/8-2.jpg" alt></p>
<p>可是，仔细一想，在执行4次分类器的过程中，有很多重复的运算。比如，对于4个滑动窗口中间那共有的$12 \times 12$个像素，它们的卷积结果被算了4次。理想情况下，只需要对它们做一次卷积就行了。这该怎么优化呢？</p>
<p>其实，很简单，我们可以利用卷积本身的特性来优化。卷积层只定义了卷积核，而没有规定输入图像的大小。我们可以拿出之前在$14 \times 14$的图像上训练好的卷积层，把它们用在$16 \times 16$的图片的卷积上。经过相同的网络，$16 \times 16$的图片会生成一个$2 \times 2$大小的分类结果，而不是$1 \times 1$的。这$2 \times 2$个分类结果，恰好就是那4个滑动窗口的分类结果。通过这样巧妙地利用卷积操作，我们规避了遍历滑动窗口带来的重复计算。</p>
<p><img src="/2022/07/26/DLS-note-12/8.jpg" alt></p>
<p>不过，这个方法还是有一些缺陷的。在刚才那个例子中，$16 \times 16$的图片其实可以放下9个$14 \times 14$大小的边框。但是，由于分类网络中max pool的存在，我们只能生成4个分类结果，也就是步幅为2的滑动窗口的分类结果。同时，最准确的检测框也不一定是正方形的，而可能是长方形的。为了让生成的滑动窗口更准确一些，我们要用到其他方法。</p>
<h2 id="预测边框"><a href="#预测边框" class="headerlink" title="预测边框"></a>预测边框</h2><p>在这一节，我们要使用YOLO(You Only Look Once)算法解决上一节中碰到的问题。还记得这周课开头学的目标定位问题吗？我们可以把滑动窗口和目标定位结合一下。</p>
<p>给定一幅图像，我们可以把图像分成$3 \times 3$个格子。训练模型前，我们要对训练数据做预处理。根据每个训练样本中物体的中心点所在的格子，我们把物体分配到每一个格子中。也就是说，不管一个物体的边框跨越了几个格子，它的中心点在哪，它就属于哪个格子。比如对于下图的训练样本，右边那辆车就属于橙色的格子。之后，我们给每个格子标上标签$y$。这个标签$y$就是目标定位中那个表示图片中是否有物体、物体的边框、物体的类别的标签向量。对于这个$3 \times 3$的格子，有9个标签向量，整个标签张量的形状是$3 \times 3 \times 8$。</p>
<p><img src="/2022/07/26/DLS-note-12/9.jpg" alt></p>
<p>这样，每一幅图像的输出和标签一样，也是一个$3 \times 3 \times 8$的张量了。输入一幅图片后，我们利用上一节学的卷积滑动窗口，同时预测出每个格子里的物体边框。</p>
<p>另外，这里要详细讨论一下$b_x, b_y, b_h, b_w$的表示方法。由于我们只关心框相对于格子的位置，因此我们可以把规定一个格子的边长为1。这样，就满足$0 \leq b_x, b_y \leq 1$了。不过，由于物体的边框可以超出小框，$b_h, b_w &gt; 1$是很有可能的。</p>
<p>看到这，大家可能会有一些疑问：如果一个格子里有多个物体呢？的确，这个算法无法输出一个格子里的多个物体。一种解决方法是，我们可以把格子分得更细一点，比如$19 \times 19$个格子。这样，可以被检测到物体会多一些。但是，增加格子数又会引入一个新的问题——多个格子检测到了同一个物体。下面的两节里我们会尝试解决这个新的问题。</p>
<blockquote>
<p>吴恩达老师说，YOLO这篇论文很难读懂，他和其他几个资深研究者都花了很大的功夫才读懂这篇论文。</p>
</blockquote>
<h2 id="IoU-交并比"><a href="#IoU-交并比" class="headerlink" title="IoU(交并比)"></a>IoU(交并比)</h2><p>在目标检测中，有一个微妙的问题：框出一个物体的边框有无数个，想精确框出标签的边框是不可能的。怎么判定一个输出结果和标签里的边框“差不多”呢？这就要用到<strong>IoU(Intersection over Union，交并比)</strong> 这个概念。</p>
<p>IoU，顾名思义，二者的交集比上二者的并集，很好理解。比如下图中，网络的输出是紫框，真值是红框。二者的并集是绿色区域，交集是橙色区域。则IoU就是橙色比绿色。</p>
<p><img src="/2022/07/26/DLS-note-12/10.jpg" alt></p>
<p>依照惯例，如果IoU$\geq 0.5$，我们就认为网络的输出是正确的。当然，想更严格一点，0.6,0.7也是可以的。</p>
<blockquote>
<p>IOU 也是 “I owe you(我欠了你的钱)”的缩写，哈哈哈。</p>
</blockquote>
<h2 id="NMS-非极大值抑制"><a href="#NMS-非极大值抑制" class="headerlink" title="NMS(非极大值抑制)"></a>NMS(非极大值抑制)</h2><p>假设在YOLO中，我们用$19 \times 19$个小格来检测物体。可是，由于小格子太多了，算法得到了多个重复的检测框（以及每个框中有物体的概率）。这该怎么办呢？</p>
<p><img src="/2022/07/26/DLS-note-12/11.jpg" alt></p>
<p>NMS(Non-Maximum Suppresion，非极大值抑制)就是解决这个问题的算法。这个算法的名字听起来很奇怪，但大家理解了这个算法的实现后，就知道这个“抑制”是什么意思了。</p>
<blockquote>
<p>讲起算法我就不困了。我会抛弃视频中的讲解思路，用我自己的逻辑讲一遍。讲算法，千万不能一上来就讲算法的步骤，一定要先讲清楚算法的思路。</p>
</blockquote>
<p>在学NMS之前，我们先动动脑，看看在去掉重复的框时，我们期望得到怎样的去重输出结果。</p>
<p>首先，既然是去重，那么就不能出现两个框过度重合的情况。其次，我们希望留下来的框的预测概率尽可能大。</p>
<p>在这两个要求下，我们来看看上面那幅图的输出应该是怎样的。<br>我们一眼就能看出，对于左边那辆车，我们应该保留0.8的框；对于右边那辆车，我们应该保留0.9的框。</p>
<p>为什么我们能“一眼看出”呢？这是因为左边两个框、右边三个框恰好都分别表示了一辆车。我们能够快速地把这些框分成两类。但是，在情况比较复杂时，我们就难以快速找出最好的框了。比如下面这种情况中，两辆车很近，有些框甚至同时标出了两辆车：</p>
<p><img src="/2022/07/26/DLS-note-12/11-2.jpg" alt></p>
<p>为了处理这种复杂的情况，我们必须想出一种万全的算法，以筛选出那些概率比较大的框。</p>
<p>稍微思考一下，其实这样的算法非常简单：找出最大的框，去掉所有和它过度重合的框；在剩下的框中，找出最大的框，去掉所有和它过度重合的框；……。一直重复直到没有未处理的框为止。这就是NMS算法。</p>
<p>还是让我们来看看刚刚那个例子。使用NMS时，我们会先找到0.9这个框，“抑制”掉右边0.6和0.7的框。在剩下的框中，最大的是0.8这个框，它会“抑制”掉左边那个0.7的框。</p>
<p><img src="/2022/07/26/DLS-note-12/11-3.jpg" alt></p>
<p>接下来，让我们来严格描述一下这个算法。假设我们有$19 \times 19=361$个输出结果，每个输出结果是一个长度为5的向量$[p_c, b_x, b_y, b_h, b_w]$，分别表示有物体的概率、边框的中心和高宽（我们先不管检测多个物体的情况。事实上，当推广到多个物体时，只要往这个输出结果里多加几个概率就行了）。我们要用NMS输出应该保留的检测结果。“过度重合”，就是两个框的IoU大于0.5。</p>
<p>首先，先做一步初筛，扔掉概率$p_c$小于0.6的结果。</p>
<p>之后，对于没有遍历的框，重复执行：找出概率最大的框，把它加入输出结果；去掉所有和它IoU大于0.5的框。</p>
<p>这个过程用伪代码表示如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Input and preprocessing</span></span><br><span class="line"><span class="built_in">input</span> predicts of size [<span class="number">19</span>, <span class="number">19</span>, <span class="number">5</span>]</span><br><span class="line">resize predicts to [<span class="number">361</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Filter predicts with low probability</span></span><br><span class="line">filtered_predicts = []</span><br><span class="line"><span class="keyword">for</span> predict <span class="keyword">in</span> predicts:</span><br><span class="line">    <span class="comment"># drop p_c &lt; 0.6</span></span><br><span class="line">    <span class="keyword">if</span> predict[<span class="number">0</span>] &gt;= <span class="number">0.6</span>:  </span><br><span class="line">        filtered_predicts.append(predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># NMS</span></span><br><span class="line">n_remainder  = <span class="built_in">len</span>(filtered_predicts)</span><br><span class="line">vis = [<span class="literal">False</span>] * n_remainder <span class="comment"># False for unvisited item</span></span><br><span class="line">output_predicts = []</span><br><span class="line"><span class="keyword">while</span> n_remainder &gt; <span class="number">0</span>:</span><br><span class="line">    max_pro = -<span class="number">1</span></span><br><span class="line">    max_index = <span class="number">0</span></span><br><span class="line">    <span class="comment"># Find argmax</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(filtered_predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i]:</span><br><span class="line">            <span class="keyword">if</span> max_pro &lt; p[<span class="number">0</span>]:</span><br><span class="line">                max_index = i</span><br><span class="line">                max_pro = p[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append output</span></span><br><span class="line">    max_p = filtered_predicts[max_index]</span><br><span class="line">    output_predicts.append(max_p)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Suppress</span></span><br><span class="line">    <span class="keyword">for</span> i, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(filtered_predicts):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> vis[i] <span class="keyword">and</span> i != max_index:</span><br><span class="line">            <span class="keyword">if</span> get_IoU(p[<span class="number">1</span>:<span class="number">5</span>], max_p[<span class="number">1</span>:<span class="number">5</span>]) &gt; <span class="number">0.5</span>:</span><br><span class="line">                vis[i] = <span class="literal">True</span></span><br><span class="line">                n_remainder -= <span class="number">1</span></span><br><span class="line">    vis[max_index] = <span class="literal">True</span></span><br><span class="line">    n_remainder -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>假设进NMS的框有$N$个。算法里求当前最大框那一步可以用优先队列来优化，这一步复杂度是$O(logN)$。但是“抑制”那一步必须要遍历一遍剩下的框，还是有一个$O(N)$复杂度（我暂时想不出朴素的低复杂度的算法）。因此，不用优先队列优化也差不多。算上外层的循环，整个算法的复杂度是$O(N^2)$。在实际的应用中，送入NMS的结果没那么多，不会超过10000个。而且，随着框被不断过滤，外层循环的次数会减少不少。这个算法的性能瓶颈不在输入数$N$上，而在于求IoU实现的细节上。</p>
</blockquote>
<h2 id="锚框（Anchor-Boxes"><a href="#锚框（Anchor-Boxes" class="headerlink" title="锚框（Anchor Boxes)"></a>锚框（Anchor Boxes)</h2><p>为了让一个格子能够检测到多个物体，YOLO论文还提出了一种叫做锚框(Anchor Boxes)的技术。</p>
<p>假设一个格子里同时包含了两个物体：一个“竖着”的人和一个“横着”的车。那么，我们可以以这个格子的中心点为“锚”，画一个竖的框和横的框，让每个格子可以检测到两个物体。这样，人和车都能被检测了。</p>
<p><img src="/2022/07/26/DLS-note-12/12.jpg" alt></p>
<p>严谨地描述，锚框技术是这样做改进的：</p>
<ul>
<li>之前，每一个格子只能包含<strong>一个</strong>样本。训练数据中每一个标签框会被分配到<strong>它中点所在</strong>的<strong>格子</strong>。</li>
<li>现在，每一个格子能包含<strong>多个</strong>样本。每个格子都会预定义几个不同形状的锚框，有几个锚框，就最多能检测到几个物体。训练数据的每一个标签框会被分配到<strong>和它交并比最大的</strong>的<strong>锚框</strong>。</li>
</ul>
<p>注意，之前的最小单元是格子，现在是锚框，所以说现在每个样本被分配到锚框上而不是格子上。可以看下面这两个样本的例子，第一个例子是两个物体都检测到了，第二个是只有锚框2里有物体。和之前一样，如果有某个锚框里没有物体，则除了$p_c$外全填问号即可。</p>
<p><img src="/2022/07/26/DLS-note-12/13.jpg" alt></p>
<p>锚框技术实际上只是对训练数据做了一个约束，改变了训练数据的格式。检测算法本身没有什么改变。</p>
<h2 id="YOLO-算法总结"><a href="#YOLO-算法总结" class="headerlink" title="YOLO 算法总结"></a>YOLO 算法总结</h2><p>让我们把前几节的内容总结一下，看一下YOLO算法的全貌。</p>
<p>在训练前，我们要对数据做预处理。首先，我们要指定以下超参数：图片切分成多大的格子、每个格子里有多少个锚框。之后，根据这些信息，我们可以得到每一个训练标签张量的形状。比如$3 \times 3 \times 2 \times 8$的一个训练标签，表示图片被切成了$3 \times 3$的格子，每个格子有两个锚框。这是一个三分类问题，对于每一个检测出来的物体，都可以用一个长度为$8$的向量表示。其中，$p_c$表示这个锚框里有没有物体, $(b_x, b_y), (b_h, b_w)$分别表示中心点坐标、框的高宽，$c_1, c_2, c_3$分别表示是否该类物体。</p>
<p>有了预处理好的训练数据，就可以训练一个CNN了。</p>
<p><img src="/2022/07/26/DLS-note-12/14.jpg" alt></p>
<p>在网络给出了输出后，由于输出的框往往多于标签中的框，还要对输出结果进行筛选。筛选的过程如前文所述，先去掉概率过小的框，再分别对每一类物体的框做NMS。</p>
<h2 id="区域提案"><a href="#区域提案" class="headerlink" title="区域提案"></a>区域提案</h2><p>YOLO算法是在一堆固定的框里找物体。实际上，我们还可以用神经网络来找出候选框，再在这些框里详细检测。这种技术就叫做区域提案(region proposal)，相关的网络叫做R-CNN(Region with CNN)。</p>
<p>R-CNN 系列网络有多个改进版本：</p>
<ul>
<li>R-CNN: 使用区域提案，但是每次只对一个区域里的物体做分类。</li>
<li>Fast R-CNN: 使用区域提案，并使用基于卷积的滑动窗口加速各区域里物体的分类。</li>
<li>Faster R-CNN: 前两个算法都是用传统方法提案区域，Faster R-CNN用CNN来提案区域，进一步令算法加速。</li>
</ul>
<p>吴恩达老师认为，虽然区域提案的方法很酷，但把目标检测分两步来完成还是太麻烦了，一步到位的YOLO系列算法已经挺方便了。</p>
<h2 id="基于U-Net的语义分割"><a href="#基于U-Net的语义分割" class="headerlink" title="基于U-Net的语义分割"></a>基于U-Net的语义分割</h2><blockquote>
<p>最早这门课是没有这一节的，估计U-Net的架构太常用了，吴恩达老师把基于U-Net的语义分割加入了这周的课中。</p>
</blockquote>
<p>语义分割也是应用非常广泛的一项CV任务。相较于只把物体框出来的目标检测，语义分割会把每一类物体的每个像素都精确地标出来。如下图的示例所示，输入一张图片，语义分割会把每一类物体准确地用同一种颜色表示。</p>
<p><img src="/2022/07/26/DLS-note-12/15.jpg" alt></p>
<p>具体来说，语义分割的输出是一个单通道图片。图片的数字表示此处像素的类别。</p>
<p><img src="/2022/07/26/DLS-note-12/16.jpg" alt></p>
<p>在分类模型中，图像会越卷越小，最后压平放进全连接层并输出多个类别的分类概率。而在语义分割模型中，由于模型的输出也是一幅图像，在输入图像被卷小了以后，应该还有一个放大的过程。</p>
<p><img src="/2022/07/26/DLS-note-12/17.jpg" alt></p>
<p>目前，我们还没有学过带学习参数的可以放大图像分辨率的结构。下一节介绍的反卷积能够完成这件事。</p>
<h2 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h2><p><img src="/2022/07/26/DLS-note-12/1.gif" alt></p>
<p>反卷积和卷积的输入输出大小彻底相反。让我们看看反卷积的形状是怎么计算的。</p>
<p><img src="/2022/07/26/DLS-note-12/18.jpg" alt></p>
<p>如上图所示，反卷积也有卷积核大小、步幅、填充这些参数。不过这些参数都是在输出图像上做的。也就是说，我们会在输出图像上做填充，并且每次在输出图像上一步一步移动。我们把正卷积的输出大小计算公式套到反卷积上的输出上，就能算出反卷积的输入的大小。</p>
<p>在卷积时，我们是把卷积核与图像对应位置的数字乘起来，再求和，算出一个输出值；反卷积则是反了过来，把一个输入值乘到卷积核的每个位置上，再把乘法结果放再输出的对应位置上。一趟反卷积计算如下图所示：</p>
<p><img src="/2022/07/26/DLS-note-12/19.jpg" alt></p>
<p>这里我们只需要知道反卷积可以做上采样就行了，不需要纠结底层实现细节。</p>
<blockquote>
<p>本课对反卷积的介绍甚少。实际上，反卷积可以通过正卷积来实现。我扫了一圈没看到讲解得比较好的相关文章，有兴趣的可以自行查找资料。</p>
</blockquote>
<h2 id="U-Net-架构"><a href="#U-Net-架构" class="headerlink" title="U-Net 架构"></a>U-Net 架构</h2><p>学完了反卷积，可以来看U-Net的结构了。</p>
<p>U-Net除了对图像使用了先缩小再放大的卷积外，还使用了一种跳连（不是ResNet中残差连接的跳连，而是把两份输入拼接在了一起）。这样，在反卷积层中，不仅有来自上一层的输入，还有来自前面相同大小的正卷积的结果。这样做的好处是，后半部分的网络既能获得前一个卷积的抽象、高级（比如类别）的输入，又能获得前半部分网络中具体，低级的特征（比如形状）。这样，后面的层能够更好地生成输出。</p>
<p><img src="/2022/07/26/DLS-note-12/20.jpg" alt></p>
<p>U-Net具体的结构如下：</p>
<p><img src="/2022/07/26/DLS-note-12/21.jpg" alt></p>
<p>这幅图中，做运算的图像张量被表示成了一个二维矩形，矩形的高度是图像的宽高，矩形的宽度是通道数。U-Net的前半部分和常见的CNN一样，缩小图像大小，增大图像通道数。而在后半部分中，每次上采样时，一半的通道来自上一层的输出，另一半的通道来自于网络前半部分。</p>
<p>从图中能看出，U-Net的结构图是一个“U”型，因此它才被叫做U-Net。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我们主要学习了以下内容：</p>
<ul>
<li>任务定义与输出格式<ul>
<li>目标定位</li>
<li>关键点检测</li>
<li>目标检测</li>
<li>语义分割</li>
</ul>
</li>
<li>YOLO<ul>
<li>用卷积实现全连接</li>
<li>用卷积实现滑动窗口</li>
<li>锚框</li>
<li>IoU</li>
<li>NMS</li>
<li>YOLO算法</li>
</ul>
</li>
<li>U-Net<ul>
<li>反卷积</li>
<li>U-Net架构</li>
</ul>
</li>
</ul>
<p>这周的代码实战中，我会详细讲解NMS的实现。时间允许的话，我还会展示一下如何在COCO上训练YOLO。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/20220717-chinese-internet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/20220717-chinese-internet/" class="post-title-link" itemprop="url">从我的公众号被诬告抄袭想到的：中国互联网不配有未来</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-24 00:33:06" itemprop="dateCreated datePublished" datetime="2022-07-24T00:33:06+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/" itemprop="url" rel="index"><span itemprop="name">杂谈</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E8%B0%88/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>今天，刚收到公众号官方的两条通知，说我的文章涉嫌整合他人内容，被暂时取消原创声明功能。</p>
<p><img src="/2022/07/24/20220717-chinese-internet/1.jpg" alt></p>
<p>一开始，我还以为是系统的检测算法出了故障，赶紧低声下气地提交了申诉，说明全网上我的文章都是由我自己完成的。</p>
<p>当我回过头来阅读通知时，愕然发现了“经用户投诉且经平台审核”这几个字，一时间怒火中烧：哦，原来我是被诬告了。</p>
<p>我是两三个月前才在开始在公开媒体上创作的，对一些规则可能不熟。有些长期做自媒体的人或许会劝我：唉，这种事正常啊。只要去申诉，给你恢复就行了。而且你看，就封了两天，之后不就正常了？忍一忍就过去了。你不要玻璃心了。</p>
<p>是啊，从利益的角度来看，这件事不就是让我两天发不了“原创”的文章？两天过后一切损失都抹平了。</p>
<p>才怪呢。</p>
<p>这件事对我真正的影响，是损害了我的名誉。</p>
<p>说实话，你可以说我水平差劲，说我没钱没势，说我狂妄自大。背后说，当面讲，拿着个大喇叭对全国人民喊。我都会不以为然。</p>
<p>问题是，对于我的作品，对于我辛辛苦苦创作出来的受到了客观认可的作品，你不能诋毁它。甚至不是去挖苦文章的内容，而是拿最恶劣的抄袭来指控我。这是对我名声的侮辱，对所有有尊严的创作者的侮辱，也是你们创作平台自己的耻辱。</p>
<p>通知里说“有用户举报”。我不知道是不是真的有人举报。如果是真的，那我也奈何不了那个人。在这件事上，我是弱势的一方。我也不知道是谁干的，也没有受到什么严重的经济损失，没有任何追责的可能。可能别人就是觉得好玩，顺手按了个举报按钮呢？我除了骂一句“此人卑鄙无耻”以外，也做不了什么。</p>
<p>真正有问题的是微信公众号的官方。你们的审核人员心慵意懒，玩忽职守。手握审核大权而不知善用，身着公正之衣而不辩是非。不察之下竟把抄袭之罪强加于光明磊落的原创作者，以至于颠倒是非，污人清白，真是岂有此理！</p>
<p>你以为你们平台做起来靠的是什么？靠的是你们掌握的数以亿计的流量？别开玩笑了。给你们带来价值的，是会下金蛋的鸡。看着满棚的金鸡，几位手持饲料的奴仆倒好像也长出了翅膀，以为自己也能下出金蛋一般，觉得随手杀掉一两只鸡也无所谓。真是可笑至极。</p>
<p>我这里还要好心奉劝一下所有的创作平台，烦请你们给审核人员的评估指标中加一个错审率，加大造成冤情的惩罚。同时，在认定冤情的申诉通过后，把“对不起”三个大字好好地打在私信里。</p>
<p>仔细一想，这事也怪不了公众号平台，整个环境毕竟就是这样的。</p>
<p>每天在平台上发送的内容那么多，审核员能够把每篇文章都过一遍都实属不易，出几个纰漏也是情理之中。这些道理我肯定都懂，也可以理解。</p>
<p>但趁着这口气，我还要发表一下对于中国内容创作平台的看法。</p>
<p>以前，去网上查编程知识的时候，查出来的全是低劣的复制粘贴文章。想要搜个教程，还要跳过那万年不变的前几个网站，去后面几个搜索结果的跟帖中翻出学习资源来。想在网络中找精品资源，可谓是沙里淘金，海底捞针。</p>
<p>现在，我学有小成，想在网上分享一些学习的心得。可是，又关注者寥寥。</p>
<p>是我不会用搜索引擎吗？是我写不出好的文章吗？</p>
<p>我看，是这个互联网的运行机制有问题啊。</p>
<p>在“后来者居上”的论坛中，优秀的帖子还是会被顶起，随后贴上“精品”的标签，供后人赏读。</p>
<p>而在以推荐机制为主的封闭创作平台当道之后，本来就稀有的精品内容便沉入了泥潭之下。只推荐自己喜欢的内容，有谁不乐意呢？坐揽着源源不断的流量，那哪平台不开心呢？这就是大势所趋啊。</p>
<p>平台只知道流量，只知道赚钱。但这也没有办法。很多平台看似规模宏大，实际上，他们还烧着投资人的钱，他们自己还身陷囹圄，入不敷出。因此，他们只能想尽一切办法，赶快扩大规模，赶快收割流量，赶快盈利。然而，哪怕真有一日，他们开始盈利了，也只会在只知道赚钱的道路上转不过弯，忘记了当年平台是怎么火起来的。</p>
<p>公益性地维护一个优质的内容平台。这种看上去吃力不讨好的事情，小平台不会做，大平台也不会做。</p>
<p>按他们这样下去，中国互联网上优质的文章只会越来越少见，看不到更好的未来啊。</p>
<p>质量和金钱，真的就是互斥的关系吗？</p>
<p>我看，只是运营这些平台的人太菜了吧。</p>
<p>一来，他们过于浮躁。在指定最优化目标时，只想到了赚钱，却不知道往里面加一点点的“情怀”。</p>
<p>二来，他们水平低下。但凡掺入了一些不赚钱的因素，就觉得要运营不下去了。</p>
<p>三来，他们目光短浅。以为创造没有利润的精品是在浪费时间，实际上有内涵的事物在多年后能够带来超出金钱的价值。</p>
<p>等我有钱了，我能够把这一切都做好。</p>
<p>我知道，十多年的寒窗苦读，对多数人来讲并不是什么愉快的经历。很多时候，并不是自己没有学好，而是教育的方法有问题。这一问题在大学之后尤为突出。倘若当年能够收获一些优质的知识，也不至于会走那么多的弯路。</p>
<p>等我有钱了，我会设法建立一个吸引优秀创作者的平台，把优质的内容结合并组织起来，把名声打响，让大家都能来这里学习。我不仅要做一些“公益”的事情，我还要赚钱，我要把平台持久地运营下去。我会扶正互联网的创作风气，还互联网一个蓬勃发展的未来。</p>
<p>在这篇文章里，我也只能随口嚷嚷。诸君把这些话当作笑谈即可。不过，在当下，我还是会慢慢地行动着，创作着。</p>
<p>如果未来优秀的中文内容越来越多，说不定不再是我们计算机学生抱着一堆机械工业出版社的黑书，而是美国的教授拿着一本本从中文英化过去的参考书。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/DLS-note-11-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/DLS-note-11-2/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》代码实战（十一）：用 TensorFlow 实现 ResNet 并验证残差连接的有效性</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-24 00:31:40" itemprop="dateCreated datePublished" datetime="2022-07-24T00:31:40+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在这篇文章中，我会介绍如何用TensorFlow实现下面4个模型：</p>
<ol>
<li>ResNet-18</li>
<li>ResNet-18 无跳连</li>
<li>ResNet-50</li>
<li>ResNet-50 无跳连</li>
</ol>
<p>实现结束后，我会在一个简单的数据集上训练这4个模型。从实验结果中，我们能直观地看出ResNet中残差连接的作用。</p>
<p>项目链接：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos">https://github.com/SingleZombie/DL-Demos</a></p>
<p>主要代码在<code>dldemos/ResNet/tf_main.py</code>这个文件里。</p>
<h2 id="模型实现"><a href="#模型实现" class="headerlink" title="模型实现"></a>模型实现</h2><h3 id="主要结构"><a href="#主要结构" class="headerlink" title="主要结构"></a>主要结构</h3><p>ResNet中有跳连的结构，直接用<code>tf.keras.Sequenctial</code>串行模型不太方便。因此，我们要自己把模型的各模块连起来，对应的TensorFlow写法是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Initialize input</span></span><br><span class="line"><span class="built_in">input</span> = layers.Input(input_shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get output</span></span><br><span class="line">output = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build model</span></span><br><span class="line">model = models.Model(inputs=<span class="built_in">input</span>, outputs=output)</span><br><span class="line"><span class="built_in">print</span>(model.summary())</span><br><span class="line"><span class="keyword">return</span> model</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>用<code>layers.Input</code>创建一个输入张量后，就可以对这个张量进行计算，并在最后用<code>tf.keras.models.Model</code>把和该张量相关的计算图搭起来。</p>
<p>接下来，我们看看这个<code>output</code>具体是怎么算出来的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_model</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        input_shape=(<span class="params"><span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span></span>), model_name=<span class="string">&#x27;ResNet18&#x27;</span>, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="comment"># Initialize input</span></span><br><span class="line">    <span class="built_in">input</span> = layers.Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get output</span></span><br><span class="line">    x = layers.Conv2D(<span class="number">64</span>, <span class="number">7</span>, (<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">&#x27;same&#x27;</span>)(<span class="built_in">input</span>)</span><br><span class="line">    x = layers.MaxPool2D((<span class="number">3</span>, <span class="number">3</span>), (<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> model_name == <span class="string">&#x27;ResNet18&#x27;</span>:</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">        x = convolution_block_2(x, <span class="number">3</span>, <span class="number">128</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">        x = convolution_block_2(x, <span class="number">3</span>, <span class="number">256</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">        x = convolution_block_2(x, <span class="number">3</span>, <span class="number">512</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">        x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    <span class="keyword">elif</span> model_name == <span class="string">&#x27;ResNet50&#x27;</span>:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">block_group</span>(<span class="params">x, fs1, fs2, count</span>):</span></span><br><span class="line">            x = convolution_block_3(x, <span class="number">3</span>, fs1, fs2, <span class="number">2</span>, use_shortcut)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count - <span class="number">1</span>):</span><br><span class="line">                x = identity_block_3(x, <span class="number">3</span>, fs1, fs2, use_shortcut)</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">        x = block_group(x, <span class="number">64</span>, <span class="number">256</span>, <span class="number">3</span>)</span><br><span class="line">        x = block_group(x, <span class="number">128</span>, <span class="number">512</span>, <span class="number">4</span>)</span><br><span class="line">        x = block_group(x, <span class="number">256</span>, <span class="number">1024</span>, <span class="number">6</span>)</span><br><span class="line">        x = block_group(x, <span class="number">512</span>, <span class="number">2048</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(<span class="string">f&#x27;No such model <span class="subst">&#123;model_name&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    x = layers.AveragePooling2D((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line">    x = layers.Flatten()(x)</span><br><span class="line">    output = layers.Dense(<span class="number">1</span>, <span class="string">&#x27;sigmoid&#x27;</span>)(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build model</span></span><br><span class="line">    model = models.Model(inputs=<span class="built_in">input</span>, outputs=output)</span><br><span class="line">    <span class="built_in">print</span>(model.summary())</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>构建模型时，我们需要给出输入张量的形状。同时，这个函数用<code>model_name</code>控制模型的结构，<code>use_shortcut</code>控制是否使用跳连。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_model</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        input_shape=(<span class="params"><span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span></span>), model_name=<span class="string">&#x27;ResNet18&#x27;</span>, use_shortcut=<span class="literal">True</span></span>):</span></span><br></pre></td></tr></table></figure>
<p>在ResNet中，主要有两种残差块。</p>
<p><img src="/2022/07/24/DLS-note-11-2/1.jpg" alt></p>
<p>第一种是上图中实线连接的，这种残差块的输入输出形状相同，输入可以直接加到激活函数之前的输出上；第二种是上图中虚线连接的，这种残差块输入输出形状不同，需要用一个1x1卷积调整宽高和通道数。</p>
<p>此外，每种残差块用两种实现方式。</p>
<p><img src="/2022/07/24/DLS-note-11-2/2.jpg" alt></p>
<p>第一种实现方式如上图左半部分所示，这样的残差块由两个通道数相同的3x3卷积构成，只有一个需要决定的通道数；第二种实现方式采用了瓶颈(bottlenect)结构，先用1x1卷积降低了通道数，再进行3x3卷积，共有两个要决定的通道数（第1, 2个卷积和第3个卷积的通道数），如上图右半部分所示。</p>
<p>代码中，我用<code>identity_block_2</code>, <code>identity_block_3</code>分别表示输入输出相同的残差块的两种实现，<code>convolution_block_2</code>, <code>convolution_block_3</code>分别表示输入输出不同的残差块的两种实现。这些代码会在下一小节里给出。</p>
<p>现在，我们来看看该如何用这些模块构成ResNet-18和ResNet-50。首先，我们看一看原论文中这几个ResNet的结构图。</p>
<p><img src="/2022/07/24/DLS-note-11-2/3.jpg" alt></p>
<p>对于这两种架构，它们一开始都要经过一个大卷积层和一个池化层，最后都要做一次平均池化并输入全连接层。不同之处在于中间的卷积层。ResNet-18和ResNet-50使用了实现方式不同且个数不同的卷积层组。</p>
<p>在代码中，开始的大卷积及池化是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = layers.Conv2D(<span class="number">64</span>, <span class="number">7</span>, (<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">&#x27;same&#x27;</span>)(<span class="built_in">input</span>)</span><br><span class="line">x = layers.MaxPool2D((<span class="number">3</span>, <span class="number">3</span>), (<span class="number">2</span>, <span class="number">2</span>))(x)</span><br></pre></td></tr></table></figure>
<p>ResNet-18的实现是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> model_name == <span class="string">&#x27;ResNet18&#x27;</span>:</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    x = convolution_block_2(x, <span class="number">3</span>, <span class="number">128</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    x = convolution_block_2(x, <span class="number">3</span>, <span class="number">256</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br><span class="line">    x = convolution_block_2(x, <span class="number">3</span>, <span class="number">512</span>, <span class="number">2</span>, use_shortcut)</span><br><span class="line">    x = identity_block_2(x, <span class="number">3</span>, use_shortcut)</span><br></pre></td></tr></table></figure>
<p>其中，<code>identity_block_2</code>的参数分别为输入张量、卷积核边长、是否使用短路。<code>convolution_block_2</code>的参数分别为输入张量、卷积核边长、输出通道数、步幅、是否使用短路。</p>
<p>ResNet-50的实现是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> model_name == <span class="string">&#x27;ResNet50&#x27;</span>:</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">block_group</span>(<span class="params">x, fs1, fs2, count</span>):</span></span><br><span class="line">        x = convolution_block_3(x, <span class="number">3</span>, fs1, fs2, <span class="number">2</span>, use_shortcut)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(count - <span class="number">1</span>):</span><br><span class="line">            x = identity_block_3(x, <span class="number">3</span>, fs1, fs2, use_shortcut)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    x = block_group(x, <span class="number">64</span>, <span class="number">256</span>, <span class="number">3</span>)</span><br><span class="line">    x = block_group(x, <span class="number">128</span>, <span class="number">512</span>, <span class="number">4</span>)</span><br><span class="line">    x = block_group(x, <span class="number">256</span>, <span class="number">1024</span>, <span class="number">6</span>)</span><br><span class="line">    x = block_group(x, <span class="number">512</span>, <span class="number">2048</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>其中，<code>identity_block_3</code>的参数分别为输入张量、卷积核边长、中间和输出通道数、是否使用短路。<code>convolution_block_3</code>的参数分别为输入张量、卷积核边长、中间和输出通道数、步幅、是否使用短路。</p>
<p>最后是计算分类输出的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = layers.AveragePooling2D((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>))(x)</span><br><span class="line">x = layers.Flatten()(x)</span><br><span class="line">output = layers.Dense(<span class="number">1</span>, <span class="string">&#x27;sigmoid&#x27;</span>)(x)</span><br></pre></td></tr></table></figure>
<h3 id="残差块实现"><a href="#残差块实现" class="headerlink" title="残差块实现"></a>残差块实现</h3><p><img src="/2022/07/24/DLS-note-11-2/4.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block_2</span>(<span class="params">x, f, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    _, _, _, C = x.shape</span><br><span class="line">    x_shortcut = x</span><br><span class="line">    x = layers.Conv2D(C, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.Conv2D(C, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> use_shortcut:</span><br><span class="line">        x = x + x_shortcut</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/24/DLS-note-11-2/5.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution_block_2</span>(<span class="params">x, f, filters, s: <span class="built_in">int</span>, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    x_shortcut = x</span><br><span class="line">    x = layers.Conv2D(filters, f, strides=(s, s), padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.Conv2D(filters, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> use_shortcut:</span><br><span class="line">        x_shortcut = layers.Conv2D(filters, <span class="number">1</span>, strides=(s, s),</span><br><span class="line">                                   padding=<span class="string">&#x27;valid&#x27;</span>)(x_shortcut)</span><br><span class="line">        x_shortcut = layers.BatchNormalization(axis=<span class="number">3</span>)(x_shortcut)</span><br><span class="line">        x = x + x_shortcut</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/24/DLS-note-11-2/6.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block_3</span>(<span class="params">x, f, filters1, filters2, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    x_shortcut = x</span><br><span class="line">    x = layers.Conv2D(filters1, <span class="number">1</span>, padding=<span class="string">&#x27;valid&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.Conv2D(filters1, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.Conv2D(filters2, <span class="number">1</span>, padding=<span class="string">&#x27;valid&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> use_shortcut:</span><br><span class="line">        x = x + x_shortcut</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><img src="/2022/07/24/DLS-note-11-2/7.jpg" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution_block_3</span>(<span class="params">x, f, filters1, filters2, s: <span class="built_in">int</span>, use_shortcut=<span class="literal">True</span></span>):</span></span><br><span class="line">    x_shortcut = x</span><br><span class="line">    x = layers.Conv2D(filters1, <span class="number">1</span>, strides=(s, s), padding=<span class="string">&#x27;valid&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.Conv2D(filters1, f, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    x = layers.Conv2D(filters2, <span class="number">1</span>, padding=<span class="string">&#x27;valid&#x27;</span>)(x)</span><br><span class="line">    x = layers.BatchNormalization(axis=<span class="number">3</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> use_shortcut:</span><br><span class="line">        x_shortcut = layers.Conv2D(filters2, <span class="number">1</span>, strides=(s, s),</span><br><span class="line">                                   padding=<span class="string">&#x27;same&#x27;</span>)(x_shortcut)</span><br><span class="line">        x_shortcut = layers.BatchNormalization(axis=<span class="number">3</span>)(x_shortcut)</span><br><span class="line">        x = x + x_shortcut</span><br><span class="line">    x = layers.ReLU()(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这些代码中有一个细节要注意：在<code>convolution_block_3</code>中，<code>stride=2</code>是放在第一个还是第二个卷积层中没有定论。不同框架似乎对此有不同的实现方式。这里是把它放到了第一个1x1卷积里。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>在这个项目中，我已经准备好了数据集预处理的代码。可以轻松地生成数据集并用TensorFlow训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    train_X, train_Y, test_X, test_Y = get_cat_set(</span><br><span class="line">        <span class="string">&#x27;dldemos/LogisticRegression/data/archive/dataset&#x27;</span>,</span><br><span class="line">        train_size=<span class="number">500</span>,</span><br><span class="line">        test_size=<span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(train_X.shape)  <span class="comment"># (m, 224, 224, 3)</span></span><br><span class="line">    <span class="built_in">print</span>(train_Y.shape)  <span class="comment"># (m , 1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#model = init_model()</span></span><br><span class="line">    <span class="comment">#model = init_model(use_shortcut=False)</span></span><br><span class="line">    model = init_model(model_name=<span class="string">&#x27;ResNet50&#x27;</span>)</span><br><span class="line">    <span class="comment"># model = init_model(model_name=&#x27;ResNet50&#x27;, use_shortcut=False)</span></span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                  loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">                  metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    model.fit(train_X, train_Y, epochs=<span class="number">20</span>, batch_size=<span class="number">16</span>)</span><br><span class="line">    model.evaluate(test_X, test_Y)</span><br></pre></td></tr></table></figure>
<p>为了让训练尽快结束，我只训了20个epoch，且使用的数据集比较小。我在ResNet-18中使用了3000个训练样本，ResNet-50中使用了1000个训练样本。数据的多少不影响对比结果，我们只需要知道模型的训练误差，便足以比较这四个模型了。</p>
<p>以下是我在四个实验中得到的结果。</p>
<p><strong>ResNet-18</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">63/63 [==============================] - 75s 1s/step - loss: 1.9463 - accuracy: 0.5485</span><br><span class="line">Epoch 2/20</span><br><span class="line">63/63 [==============================] - 71s 1s/step - loss: 0.9758 - accuracy: 0.5423</span><br><span class="line">Epoch 3/20</span><br><span class="line">63/63 [==============================] - 81s 1s/step - loss: 0.8490 - accuracy: 0.5941</span><br><span class="line">Epoch 4/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.8309 - accuracy: 0.6188</span><br><span class="line">Epoch 5/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 0.7375 - accuracy: 0.6402</span><br><span class="line">Epoch 6/20</span><br><span class="line">63/63 [==============================] - 77s 1s/step - loss: 0.7932 - accuracy: 0.6769</span><br><span class="line">Epoch 7/20</span><br><span class="line">63/63 [==============================] - 78s 1s/step - loss: 0.7782 - accuracy: 0.6713</span><br><span class="line">Epoch 8/20</span><br><span class="line">63/63 [==============================] - 76s 1s/step - loss: 0.6272 - accuracy: 0.7147</span><br><span class="line">Epoch 9/20</span><br><span class="line">63/63 [==============================] - 77s 1s/step - loss: 0.6303 - accuracy: 0.7059</span><br><span class="line">Epoch 10/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.6250 - accuracy: 0.7108</span><br><span class="line">Epoch 11/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.6065 - accuracy: 0.7142</span><br><span class="line">Epoch 12/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.5289 - accuracy: 0.7754</span><br><span class="line">Epoch 13/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.5005 - accuracy: 0.7506</span><br><span class="line">Epoch 14/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.3961 - accuracy: 0.8141</span><br><span class="line">Epoch 15/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.4417 - accuracy: 0.8121</span><br><span class="line">Epoch 16/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.3761 - accuracy: 0.8136</span><br><span class="line">Epoch 17/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.2764 - accuracy: 0.8809</span><br><span class="line">Epoch 18/20</span><br><span class="line">63/63 [==============================] - 71s 1s/step - loss: 0.2698 - accuracy: 0.8878</span><br><span class="line">Epoch 19/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 0.1483 - accuracy: 0.9457</span><br><span class="line">Epoch 20/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 0.2495 - accuracy: 0.9079</span><br></pre></td></tr></table></figure></p>
<p><strong>ResNet-18 无跳连</strong></p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">63/63 [==============================] - 63s 963ms/step - loss: 1.4874 - accuracy: 0.5111</span><br><span class="line">Epoch 2/20</span><br><span class="line">63/63 [==============================] - 62s 990ms/step - loss: 0.7654 - accuracy: 0.5386</span><br><span class="line">Epoch 3/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.6799 - accuracy: 0.6210</span><br><span class="line">Epoch 4/20</span><br><span class="line">63/63 [==============================] - 62s 990ms/step - loss: 0.6891 - accuracy: 0.6086</span><br><span class="line">Epoch 5/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.7921 - accuracy: 0.5182</span><br><span class="line">Epoch 6/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.7123 - accuracy: 0.5643</span><br><span class="line">Epoch 7/20</span><br><span class="line">63/63 [==============================] - 64s 1s/step - loss: 0.7071 - accuracy: 0.5173</span><br><span class="line">Epoch 8/20</span><br><span class="line">63/63 [==============================] - 64s 1s/step - loss: 0.6653 - accuracy: 0.6227</span><br><span class="line">Epoch 9/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.6675 - accuracy: 0.6249</span><br><span class="line">Epoch 10/20</span><br><span class="line">63/63 [==============================] - 64s 1s/step - loss: 0.6959 - accuracy: 0.6130</span><br><span class="line">Epoch 11/20</span><br><span class="line">63/63 [==============================] - 66s 1s/step - loss: 0.6730 - accuracy: 0.6182</span><br><span class="line">Epoch 12/20</span><br><span class="line">63/63 [==============================] - 63s 1s/step - loss: 0.6321 - accuracy: 0.6491</span><br><span class="line">Epoch 13/20</span><br><span class="line">63/63 [==============================] - 63s 992ms/step - loss: 0.6413 - accuracy: 0.6569</span><br><span class="line">Epoch 14/20</span><br><span class="line">63/63 [==============================] - 63s 1s/step - loss: 0.6130 - accuracy: 0.6885</span><br><span class="line">Epoch 15/20</span><br><span class="line">63/63 [==============================] - 62s 988ms/step - loss: 0.6750 - accuracy: 0.6056</span><br><span class="line">Epoch 16/20</span><br><span class="line">63/63 [==============================] - 66s 1s/step - loss: 0.6341 - accuracy: 0.6526</span><br><span class="line">Epoch 17/20</span><br><span class="line">63/63 [==============================] - 68s 1s/step - loss: 0.6384 - accuracy: 0.6676</span><br><span class="line">Epoch 18/20</span><br><span class="line">63/63 [==============================] - 65s 1s/step - loss: 0.5750 - accuracy: 0.6997</span><br><span class="line">Epoch 19/20</span><br><span class="line">63/63 [==============================] - 63s 997ms/step - loss: 0.5932 - accuracy: 0.7094</span><br><span class="line">Epoch 20/20</span><br><span class="line">63/63 [==============================] - 62s 990ms/step - loss: 0.6133 - accuracy: 0.6420</span><br></pre></td></tr></table></figure>
<p><strong>ResNet-50</strong></p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 3.4660 - accuracy: 0.4970</span><br><span class="line">Epoch 2/20</span><br><span class="line">63/63 [==============================] - 67s 1s/step - loss: 1.3429 - accuracy: 0.5686</span><br><span class="line">Epoch 3/20</span><br><span class="line">63/63 [==============================] - 68s 1s/step - loss: 1.0294 - accuracy: 0.5616</span><br><span class="line">Epoch 4/20</span><br><span class="line">63/63 [==============================] - 68s 1s/step - loss: 0.7920 - accuracy: 0.6186</span><br><span class="line">Epoch 5/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.6698 - accuracy: 0.6773</span><br><span class="line">Epoch 6/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.6884 - accuracy: 0.7289</span><br><span class="line">Epoch 7/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.7144 - accuracy: 0.6399</span><br><span class="line">Epoch 8/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.7088 - accuracy: 0.6698</span><br><span class="line">Epoch 9/20</span><br><span class="line">63/63 [==============================] - 68s 1s/step - loss: 0.6385 - accuracy: 0.6446</span><br><span class="line">Epoch 10/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.5389 - accuracy: 0.7417</span><br><span class="line">Epoch 11/20</span><br><span class="line">63/63 [==============================] - 71s 1s/step - loss: 0.4954 - accuracy: 0.7832</span><br><span class="line">Epoch 12/20</span><br><span class="line">63/63 [==============================] - 73s 1s/step - loss: 0.4489 - accuracy: 0.7782</span><br><span class="line">Epoch 13/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.3987 - accuracy: 0.8257</span><br><span class="line">Epoch 14/20</span><br><span class="line">63/63 [==============================] - 72s 1s/step - loss: 0.3228 - accuracy: 0.8519</span><br><span class="line">Epoch 15/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.2089 - accuracy: 0.9235</span><br><span class="line">Epoch 16/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.4766 - accuracy: 0.7756</span><br><span class="line">Epoch 17/20</span><br><span class="line">63/63 [==============================] - 75s 1s/step - loss: 0.2148 - accuracy: 0.9181</span><br><span class="line">Epoch 18/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.3086 - accuracy: 0.8623</span><br><span class="line">Epoch 19/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.3544 - accuracy: 0.8732</span><br><span class="line">Epoch 20/20</span><br><span class="line">63/63 [==============================] - 70s 1s/step - loss: 0.0796 - accuracy: 0.9704</span><br></pre></td></tr></table></figure>
<p><strong>ResNet-50 无跳连</strong></p>
<figure class="highlight plaintext"><figcaption><span>text</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/20</span><br><span class="line">63/63 [==============================] - 60s 882ms/step - loss: 1.2093 - accuracy: 0.5034</span><br><span class="line">Epoch 2/20</span><br><span class="line">63/63 [==============================] - 56s 892ms/step - loss: 0.8433 - accuracy: 0.4861</span><br><span class="line">Epoch 3/20</span><br><span class="line">63/63 [==============================] - 59s 931ms/step - loss: 0.7512 - accuracy: 0.5235</span><br><span class="line">Epoch 4/20</span><br><span class="line">63/63 [==============================] - 62s 991ms/step - loss: 0.7395 - accuracy: 0.4887</span><br><span class="line">Epoch 5/20</span><br><span class="line">63/63 [==============================] - 62s 990ms/step - loss: 0.7770 - accuracy: 0.5316</span><br><span class="line">Epoch 6/20</span><br><span class="line">63/63 [==============================] - 60s 945ms/step - loss: 0.7408 - accuracy: 0.4947</span><br><span class="line">Epoch 7/20</span><br><span class="line">63/63 [==============================] - 67s 1s/step - loss: 0.7345 - accuracy: 0.5434</span><br><span class="line">Epoch 8/20</span><br><span class="line">63/63 [==============================] - 62s 984ms/step - loss: 0.7214 - accuracy: 0.5605</span><br><span class="line">Epoch 9/20</span><br><span class="line">63/63 [==============================] - 60s 950ms/step - loss: 0.7770 - accuracy: 0.4784</span><br><span class="line">Epoch 10/20</span><br><span class="line">63/63 [==============================] - 60s 956ms/step - loss: 0.7171 - accuracy: 0.5203</span><br><span class="line">Epoch 11/20</span><br><span class="line">63/63 [==============================] - 63s 994ms/step - loss: 0.7045 - accuracy: 0.4921</span><br><span class="line">Epoch 12/20</span><br><span class="line">63/63 [==============================] - 63s 1s/step - loss: 0.6884 - accuracy: 0.5430</span><br><span class="line">Epoch 13/20</span><br><span class="line">63/63 [==============================] - 60s 958ms/step - loss: 0.7333 - accuracy: 0.5278</span><br><span class="line">Epoch 14/20</span><br><span class="line">63/63 [==============================] - 61s 966ms/step - loss: 0.7050 - accuracy: 0.5106</span><br><span class="line">Epoch 15/20</span><br><span class="line">63/63 [==============================] - 59s 943ms/step - loss: 0.6958 - accuracy: 0.5622</span><br><span class="line">Epoch 16/20</span><br><span class="line">63/63 [==============================] - 60s 954ms/step - loss: 0.7398 - accuracy: 0.5172</span><br><span class="line">Epoch 17/20</span><br><span class="line">63/63 [==============================] - 69s 1s/step - loss: 0.7104 - accuracy: 0.5023</span><br><span class="line">Epoch 18/20</span><br><span class="line">63/63 [==============================] - 74s 1s/step - loss: 0.7411 - accuracy: 0.4747</span><br><span class="line">Epoch 19/20</span><br><span class="line">63/63 [==============================] - 67s 1s/step - loss: 0.7056 - accuracy: 0.4706</span><br><span class="line">Epoch 20/20</span><br><span class="line">63/63 [==============================] - 81s 1s/step - loss: 0.7901 - accuracy: 0.4898</span><br></pre></td></tr></table></figure>
<p>对比ResNet-18和ResNet-50，可以看出，ResNet-50的拟合能力确实更强一些。</p>
<p>对比无跳连的ResNet-18和ResNet-50，可以看出，ResNet-50的拟合能力反而逊于ResNet-18。这符合ResNet的初衷，如果不加残差连接的话，过深的网络反而会因为梯度问题而有更高的训练误差。</p>
<p>此外，不同模型的训练速度也值得一讲。在训练数据量减少到原来的1/3后，ResNet-50和ResNet-18的训练速度差不多。ResNet-50看上去比ResNet-18多了很多层，网络中间也使用了通道数很大的卷积，但整体的参数量并没有增大多少，这多亏了能降低运算量的瓶颈结构。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我展示了ResNet-18和ResNet-50的TensorFlow实现。这份代码包括了经典ResNet中两种残差块的两种实现，完整地复现了原论文的模型模块。同时，经实验分析，我验证了ResNet残差连接的有效性。</p>
<p>未来我还会写一篇ResNet的PyTorch实现，并附上论文的详细解读。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/DLS-note-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/DLS-note-11/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》笔记（十一）：深度卷积模型——从示例中学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-24 00:31:30" itemprop="dateCreated datePublished" datetime="2022-07-24T00:31:30+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="学习提示"><a href="#学习提示" class="headerlink" title="学习提示"></a>学习提示</h1><p>上周，我们学完了CNN的基础组成模块。而从这周开始，我们要换一种学习方式：我们会认识一些经典的CNN架构，从示例中学习。一方面来说，通过了解他人的网络，阅读他人的代码，我们能够更快地掌握如何整合CNN的基础模块；另一方面，CNN架构往往泛化能力较强，学会了其他任务中成熟的架构，可以把这些架构直接用到我们自己的任务中。</p>
<p>接下来，我们会按照CNN的发展历史，认识许多CNN架构。首先是经典网络：</p>
<ul>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>VGG</li>
</ul>
<p>之后是近年来的一些网络：</p>
<ul>
<li>ResNet</li>
<li>Inception</li>
<li>MobileNet</li>
</ul>
<p>我们不会把这些研究的论文详细过一遍，而只会学习各研究中最精华的部分。学有余力的话，最好能在课后把论文自己过一遍。</p>
<h1 id="课堂笔记"><a href="#课堂笔记" class="headerlink" title="课堂笔记"></a>课堂笔记</h1><h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>LeNet-5是用于手写数字识别（识别0~9的阿拉伯数字）的网络。它的结构如下：</p>
<p><img src="/2022/07/24/DLS-note-11/1.jpg" alt></p>
<p>网络是输入是一张[32, 32, 1]的灰度图像，输入经过4个卷积+池化层，再经过两个全连接层，输出一个0~9的数字。这个网络和我们上周见过的网络十分相似，数据体的宽和高在不断变小，而通道数在不断变多。</p>
<p>这篇工作是1998年发表的，当时的神经网络架构和现在我们学的有不少区别：</p>
<ul>
<li>当时padding还没有得到广泛使用，数据体的分辨率会越降越小。</li>
<li>当时主要使用平均池化，而现在最大池化更常见。</li>
<li>网络只输出一个值，表示识别出来的数字。而现在的多分类任务一般会输出10个值并使用softmax激活函数。</li>
<li>当时激活函数只用sigmoid和tanh，没有人用ReLU。</li>
<li>当时的算力没有现在这么强，原工作在计算每个通道卷积时使用了很多复杂的小技巧。而现在我们直接算就行了。</li>
</ul>
<p>LeNet-5只有6万个参数。随着算力的增长，后来的网络越来越大了。</p>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>AlexNet是2012年发表的有关图像分类的CNN结构。它的输入是[227, 227, 3]的图像，输出是一个1000类的分类结果。</p>
<blockquote>
<p>原论文里写的是输入形状是[224, 224, 3]，但实际上这个分辨率是有问题的，按照这个分辨率是算不出后续结果的分辨率的。但现在一些框架对AlexNet的复现中，还是会令输入的分辨率是224。这是因为框架在第一层卷积中加了一个padding的操作，强行让后续数据的分辨率和原论文对上了。 </p>
</blockquote>
<p><img src="/2022/07/24/DLS-note-11/2.jpg" alt></p>
<p>AlexNet和LeNet-5在架构上十分接近。但是，AlexNet做出了以下改进：</p>
<ul>
<li>AlexNet用了更多的参数，一共有约6000万个参数。</li>
<li>使用ReLU作为激活函数。</li>
</ul>
<p>AlexNet还提出了其他一些创新，但与我们要学的知识没有那么多关系：</p>
<ul>
<li>当时算力还是比较紧张，AlexNet用了双GPU训练。论文里写了很多相关的工程细节。</li>
<li>使用了Local Response Normalization这种归一化层。现在几乎没人用这种归一化。</li>
</ul>
<p>AlexNet中的一些技术在今天看来，已经是常识般的存在。而在那个年代，尽管深度学习在语音识别等任务上已经初露锋芒，人们还没有开始重视深度学习这项技术。正是由于AlexNet这一篇工作的出现，计算机视觉的研究者开始关注起了深度学习。甚至在后来，这篇工作的影响力已经远超出了计算机视觉社区。</p>
<h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p>VGG-16也是一个图像分类网络。VGG的出发点是：为了简化网络结构，只用3x3等长(same)卷积和2x2最大池化。</p>
<p><img src="/2022/07/24/DLS-note-11/3.jpg" alt></p>
<p>可以看出，VGG也是经过了一系列的卷积和池化层，最后使用全连接层和softmax输出结果。顺带一提，VGG-16里的16表示有16个带参数的层。</p>
<p>VGG非常庞大，有138M(1.38亿)个参数。但是它简洁的结构吸引了很多人的关注。</p>
<p>吴恩达老师鼓励大家去读一读这三篇论文。可以先看AlexNet，再看VGG。LeNet有点难读，可以放到最后去读。</p>
<h2 id="ResNets（基于残差的网络）"><a href="#ResNets（基于残差的网络）" class="headerlink" title="ResNets（基于残差的网络）"></a>ResNets（基于残差的网络）</h2><p>非常非常深的神经网络是很难训练的，这主要是由梯度爆炸/弥散问题导致的。在这一节中，我们要学一种叫做“跳连(skip connection)”的网络模块连接方式。使用跳连，我们能让浅层模块的输出直接对接到深层模块的输入上，进而搭建基于残差的网络，解决梯度爆炸/弥散问题，训练深达100层的网络。</p>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>回忆一下，在全连接网络中，假如我们有中间层的输出$a^{[l]}, a^{[l+2]}$，$a^{[l+2]}$是怎么由$a^{[l]}$算出来的呢？我们之前用的公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
z^{[l+1]}&=W^{[l+1]}a^{[l]}+b^{[l+1]} \\
a^{[l+1]}&=g(z^{[l+1]}) \\
z^{[l+2]}&=W^{[l+2]}a^{[l+1]}+b^{[l+2]} \\
a^{[l+2]}&=g(z^{[l+2]}) \\
\end{aligned}</script><p>也就是说，$a^{[l]}$要经过一个线性层、一个激活函数、一个线性层、一个激活函数，才能传递到$a^{[l+2]}$，这条路径非常长：</p>
<p><img src="/2022/07/24/DLS-note-11/4.jpg" alt></p>
<p>而在残差块(Residual block)中，我们使用了一种新的连接方法：</p>
<p><img src="/2022/07/24/DLS-note-11/5.jpg" alt></p>
<p>$a^{[l]}$的值被直接加到了第二个ReLU层之前的线性输出上，这是一种类似电路中短路的连接方法（又称跳连）。这样，浅层的信息能更好地传到深层了。</p>
<p>使用这种方法后，计算公式变更为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
z^{[l+1]}&=W^{[l+1]}a^{[l]}+b^{[l+1]} \\
a^{[l+1]}&=g(z^{[l+1]}) \\
z^{[l+2]}&=W^{[l+2]}a^{[l+1]}+b^{[l+2]} \\
a^{[l+2]}&=g(z^{[l+2]}+a^{[l]}) \\
\end{aligned}</script><p>残差块中还有一个要注意的细节。$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$这个式子能够成立，实际上是默认了$a^{[l+2]}, a^{[l]}$的维度相同。而一旦$a^{[l+2]}$的维度发生了变化，就需要用下面这种方式来调整了。</p>
<p>$a^{[l+2]}=g(z^{[l+2]}+W’a^{[l]})$</p>
<p>我们可以用一个$W’$来完成维度的转换。为了方便理解，我们先让所有$a$都是一维向量，$W’$是矩阵。这样，假设$a^{[l+2]}$的长度是256，$a^{[l]}$的长度是128，则$W’$的形状就是$256 \times 128$。</p>
<p>但实际上，$a$是一个三维的图像张量，三个维度的长度都可能发生变化。因此，对于图像，上式中的$W’$应该表示的是一个卷积操作。通过卷积操作，我们能够减小图像的宽高，调整图像的通道数，使得$a^{[l]}$和$a^{[l+2]}$的维度完全相同。</p>
<h3 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h3><p>在构建残差网络ResNet时，只要把这种残差块一个一个拼接起来即可。或者从另一个角度来看，对于一个“平坦网络”（”plain network”, ResNet论文中用的词，用于表示非残差网络），我们只要把线性层两两打包，添加跳连即可。</p>
<p><img src="/2022/07/24/DLS-note-11/6.jpg" alt></p>
<p>残差块起到了什么作用呢？让我们看看在网络层数变多时，平坦网络和残差网络训练误差的变化趋势：</p>
<p><img src="/2022/07/24/DLS-note-11/7.jpg" alt></p>
<p>理论上来说，层数越深，训练误差应该越低。但在实际中，对平坦网络增加深度，反而会让误差变高。而使用ResNet后，随着深度增加，训练误差起码不会降低了。</p>
<p>正是有这样的特性，我们可以用ResNet架构去训练非常深的网络。</p>
<p>为什么ResNet是有这样的特性呢？我们还是从刚刚那个ResNet的公式里找答案。</p>
<p>假设我们设计好了一个网络，又给它新加了一个残差块，即多加了两个卷积层，那么最后的输出可以写成：</p>
<script type="math/tex; mode=display">
a^{[l+2]}=g(z^{[l+2]}+a^{[l]}),</script><p>即</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{[l+2]}&=g(z^{[l+2]}+a^{[l]}) \\
a^{[l+2]}&=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}) \\
\end{aligned}</script><p>由于正则化的存在，所有$W$和$b$都倾向于变得更小。极端情况下，$W, b$都变为0了。那么，</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{[l+2]}&=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]}), \\
a^{[l+2]}&=g(a^{[l]}). \\
\end{aligned}</script><p>再不妨设$g=ReLU$。则因为$a^{[l]}$也是ReLU的输出，有</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{[l+2]}&=g(a^{[l]}), \\
a^{[l+2]}&=a^{[l]}. \\
\end{aligned}</script><p>这其实是一个恒等映射，也就是说，新加的残差块对之前的输出没有任何影响。网络非常容易学习到恒等映射。这样，最起码能够保证较深的网络不比浅的网络差。</p>
<p>准备好了所有基础知识，我们来看看完整的ResNet长什么样。</p>
<p><img src="/2022/07/24/DLS-note-11/7_2.jpg" alt></p>
<p>ResNet有几个参数量不同的版本。这里展示的叫做ResNet-34。完整的网络很长，我们只用关注其中一小部分就行了。</p>
<p>一开始，网络还是用一个大卷积核大步幅的卷积以及一个池化操作快速降低图像的宽度，再把数据传入残差块中。和我们刚刚学的一样，残差块有两种，一种是维度相同可以直接相加的（实线），一种是要调整维度的（虚线）。整个网络就是由这若干个这样的残差块组构成。经过所有残差块后，还是和经典的网络一样，用全连接层输出结果。</p>
<p>这里，我们只学习了残差连接的基本原理。ResNet的论文里还有更多有关网络结构、实验的细节。最好能读一读论文。当然，这周的编程实战里我们也会复现ResNet，以加深对其的理解。</p>
<h2 id="Inception-网络"><a href="#Inception-网络" class="headerlink" title="Inception 网络"></a>Inception 网络</h2><p>我们已经见过不少CNN的示例了。当我们仿照它们设计自己的网络时，或许会感到迷茫：有3x3, 5x5卷积，有池化，该怎么选择每一个模块呢？Inception网络给了一个解决此问题的答案：我全都要。</p>
<p>Inception网络用到了一种特殊的1x1卷积。我们会先学习1x1卷积，再学习Inception网络的知识。</p>
<h3 id="1x1卷积"><a href="#1x1卷积" class="headerlink" title="1x1卷积"></a>1x1卷积</h3><p>用1x1的卷积核去卷一幅图像，似乎是一件很滑稽的事情。假设一幅图像的数字是[1, 2, 3]，卷积核是[2]，那么卷出来的图像就是[2, 4, 6]。这不就是把每个数都做了一次乘法吗？</p>
<p>对于通道数为1的图像，1x1卷积确实没什么大用。而当通道数多起来后，1x1卷积的意义就逐渐显现出来了。思考一下，对多通道的图像做1x1卷积，就是把某像素所有通道的数字各乘一个数，求和，加一个bias，再通过激活函数。这是计算一个输出结果的过程，而如果有多个卷积核，就可以计算出多个结果。（下图中，蓝色的数据体是输入图像，黄色的数据体是1x1的卷积核。两个数据体重合部分的数据会先做乘法，再求和，加bias，经过激活函数。）</p>
<p><img src="/2022/07/24/DLS-note-11/7_3.jpg" alt></p>
<p>这个过程让你想起了什么？没错，正是最早学习的全连接网络。1x1卷积，实际上就是在各通道上做了一次全连接的计算。1x1卷积的输入通道数，就是全连接层上一层神经元的数量；1x1卷积核的数量，就是这一层神经元的数量。</p>
<p>1x1卷积主要用于变换图像的通道数。比如要把一个192通道数的图像变成32通道的，就应该用32个1x1卷积去卷原图像。</p>
<h3 id="Inception块的原理"><a href="#Inception块的原理" class="headerlink" title="Inception块的原理"></a>Inception块的原理</h3><p>在Inception网络中，我们会使用这样一种混合模块：对原数据做1x1, 3x3, 5x5卷积以及最大池化，得到通道数不同的数据体。这些数据体会被拼接起来，作为整个模块的输出。</p>
<p><img src="/2022/07/24/DLS-note-11/7_4.jpg" alt></p>
<p>值得注意的是，这里的池化操作和我们之前见过的不太一样。为了保持输出张量的宽高，这个池化的步幅为1，且使用了等长填充。另外，为了调整池化操作输出的通道数，这条数据处理路线上还有一个用1x1卷积变换通道数的操作。这份图省略了很多这样的细节，下一节我们会见到这幅图的完整版。</p>
<p>在实现这样一种模块时，会碰到计算量过大的问题。比如把上面$28 \times 28 \times 192$的数据体用$5 \times 5$卷积卷成$28 \times 28 \times 32$的数据体，需要多少次乘法计算呢？对每个像素单独考虑，一个通道上的卷积要做$5 \times 5$此乘法，192个通道的卷积要做$192 \times 5 \times 5$次乘法。32个这样的卷积在$28 \times 28$的图片上要做$28 \times 28 \times 32 \times 192 \times 5 \times 5 \approx 120M$次乘法。这个计算量太大了。</p>
<p>为此，我们可以巧妙地先利用1x1卷积减少通道数，再做5x5卷积。这样，计算量就少得多了。</p>
<p><img src="/2022/07/24/DLS-note-11/7_5.jpg" alt></p>
<p>这样一种两头大，中间小的结构被形象地称为瓶颈(bottlenect)。这种结构被广泛用在许多典型网络中。</p>
<h3 id="Inception网络"><a href="#Inception网络" class="headerlink" title="Inception网络"></a>Inception网络</h3><p>有了之前的知识，我们可以看Inception模块的完整结构了。1x1卷积没有什么特别的。为了减少3x3卷积和5x5卷积的计算量，做这两种卷积之前都会用1x1卷积减少通道数。而为了改变池化结果的通道数，池化后接了一个1x1卷积操作。</p>
<p><img src="/2022/07/24/DLS-note-11/7_6.jpg" alt></p>
<p>实际上，理解了Inception块，也就能看懂Inception网络了。如下图所示，红框内的模块都是Inception块。而这个网络还有一些小细节：除了和普通网络一样在网络的最后使用softmax输出结果外，这个网络还根据中间结果也输出了几个结果。当然，这些都是早期网络的设计技巧了。</p>
<p><img src="/2022/07/24/DLS-note-11/7_7.jpg" alt></p>
<h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><p>MobileNet，顾名思义，这是一种适用于移动(mobile)设备的神经网络。移动设备的计算资源通常十分紧缺，因此，MobileNet对网络的计算量进行了极致的压缩。</p>
<h3 id="减少卷积运算量"><a href="#减少卷积运算量" class="headerlink" title="减少卷积运算量"></a>减少卷积运算量</h3><p>再回顾一遍，一次卷积操作中主要的计算量如下：</p>
<p><img src="/2022/07/24/DLS-note-11/7_8.jpg" alt></p>
<p>计算量这么大，主要问题出在每一个输出通道都要与每一个输入通道“全连接”上。为此，我们可以考虑让输出通道只由部分的输入通道决定。这样一种卷积的策略叫逐深度可分卷积(Depthwise Separable Convolution)。</p>
<blockquote>
<p>这里的depthwise是“逐深度”的意思，但我觉得“逐通道”这个称呼会更容易理解一点。</p>
</blockquote>
<p>逐深度可分卷积分为两步：逐深度卷积(depthwise convolution)，逐点卷积(pointwise convolution)。逐深度卷积生成新的通道，逐点卷积把各通道的信息关联起来。</p>
<p><img src="/2022/07/24/DLS-note-11/7_9.jpg" alt></p>
<p>之前，要对下图中的三通道图片做卷积，需要3个卷积核分别处理3个通道。而在逐深度卷积中，我们只要1个卷积核。这个卷积核会把输入图像当成三个单通道图像来看待，分别对原图像的各个通道进行卷积，并生成3个单通道图像，最后把3个单通道图像拼回一个三通道图像。也就是说，逐深度卷积只能生成一幅通道数相同的新图像。</p>
<blockquote>
<p>逐深度卷积可以通过设置卷积在编程框架中的<code>groups</code>参数来实现。参见我<a href>讲解卷积的文章</a>。</p>
</blockquote>
<p><img src="/2022/07/24/DLS-note-11/7_10.jpg" alt></p>
<p>下一步，是逐点卷积，也就是1x1卷积。它用来改变图片的通道数。</p>
<p><img src="/2022/07/24/DLS-note-11/7_11.jpg" alt></p>
<p>之前的卷积有2160次乘法，现在只有432+240=672次，计算量确实减少了不少。实际上，优化后计算量占原计算量的比例是：</p>
<script type="math/tex; mode=display">
\frac{1}{n_c'} + \frac{1}{f^2}</script><p>其中$n_c’$是输出通道数，$f$是卷积核边长。一般来说计算量都会少10倍。</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>知道了MobileNet的基本思想，我们来看几个不同版本的MobileNet。</p>
<h4 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h4><p>13个逐深度可分卷积模块，之后接通常的池化、全连接、softmax。</p>
<p><img src="/2022/07/24/DLS-note-11/8.jpg" alt></p>
<h4 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet v2"></a>MobileNet v2</h4><p>两个改进：</p>
<ol>
<li>残差连接</li>
<li>扩张(expansion)操作</li>
</ol>
<p><img src="/2022/07/24/DLS-note-11/9.jpg" alt></p>
<p>残差连接和ResNet一样。这里我们关注一下第二个改进。</p>
<p>在MobileNet v2中，先做一个扩张维度的1x1卷积，再做逐深度卷积，最后做之前的逐点1x1卷积。由于最后的逐点卷积起到的是减小维度的作用，所以最后一步操作也叫做投影。</p>
<p><img src="/2022/07/24/DLS-note-11/10.jpg" alt></p>
<p>这种架构很好地解决了性能和效果之间的矛盾：在模块之间，数据的通道数只有3，占用内存少；在模块之内，更高通道的数据能拟合更复杂的函数。</p>
<h2 id="EfficientNet"><a href="#EfficientNet" class="headerlink" title="EfficientNet"></a>EfficientNet</h2><p>EfficientNet能根据设备的计算能力，自动调整网络占用的资源。</p>
<p>让我们想想，哪些因素决定了一个网络占用的运算资源？我们很快能想到下面这些因素：</p>
<ul>
<li>图像分辨率</li>
<li>网络深度</li>
<li>特征的长度（即卷积核数量或神经元数量）</li>
</ul>
<p>在EfficientNet中，我们可以在这三个维度上缩放网络，动态改变网络的计算量。EfficientNet的开源实现中，一般会提供各设备下的最优参数。</p>
<h2 id="卷积网络实现细节"><a href="#卷积网络实现细节" class="headerlink" title="卷积网络实现细节"></a>卷积网络实现细节</h2><h3 id="使用开源实现"><a href="#使用开源实现" class="headerlink" title="使用开源实现"></a>使用开源实现</h3><p>由于深度学习项目涉及很多训练上的细节，想复现一个前人的工作是很耗时的。最好的学习方法是找到别人的开源代码，在现有代码的基础上学习。</p>
<p>深度学习的开源代码一般在GitHub上都能找到。如果是想看PyTorch实现，可以直接去GitHub上搜索OpenMMLab。</p>
<h3 id="使用迁移学习"><a href="#使用迁移学习" class="headerlink" title="使用迁移学习"></a>使用迁移学习</h3><p>如第三门课第二周所学，我们可以用迁移学习，导入别人训练好的模型里的权重为初始权重，加速我们自己模型的训练。</p>
<p>还是以多分类任务的迁移学习为例（比如把一个1000分类的分类器迁移到一个猫、狗、其他的三分类模型上）。迁移后，新的网络至少要删除输出层，并按照新的多分类个数，重新初始化一个输出层。之后，根据新任务的数据集大小，冻结网络的部分参数，从导入的权重开始重新训练网络的其他部分：</p>
<p><img src="/2022/07/24/DLS-note-11/11.jpg" alt></p>
<p>当然，可以多删除几个较深的层，也可以多加入几个除了输出层以外的隐藏层。</p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>由于CV任务总是缺少数据，数据增强是一种常见的提升网络性能的手段。</p>
<p>常见的改变形状的数据增强手段有：</p>
<ul>
<li>镜像</li>
<li>裁剪</li>
<li>旋转</li>
<li>扭曲</li>
</ul>
<p>此外，还可以改变图像的颜色。比如对三个颜色通道都随机加一个偏移量。</p>
<p>数据增强有一些实现上的细节：数据的读取及增强是放在CPU上运行的，训练是放在CPU或GPU上运行的。这两步其实是独立的，可以并行完成。最常见的做法是，在CPU上用多进程（发挥多核的优势）读取数据并进行数据增强，之后把数据搬到GPU上训练。</p>
<h3 id="计算机视觉的现状与相关建议"><a href="#计算机视觉的现状与相关建议" class="headerlink" title="计算机视觉的现状与相关建议"></a>计算机视觉的现状与相关建议</h3><p>一般来说，算法从两个来源获取知识：标注的数据，人工设计的特征。这二者是互补的关系。对于人工智能任务来说，如果有足够的数据，设计一个很简单的网络就行了；而如果数据量不足，则需要去精心设计网络结构。</p>
<p>像语音识别这种任务就数据充足，用简单的网络就行了。而大部分计算机视觉任务都处于数据不足的状态。哪怕计算机视觉中比较基础的图像分类任务，都需要设计结构复杂的网络，更不用说目标检测这些更难的任务了。</p>
<p>如果你想用深度学习模型参加刷精度的比赛，可以使用以下几个小技巧：</p>
<ul>
<li>同时开始训练多个网络，算结果时取它们的平均值。</li>
<li>对图像分类任务，可以把图像随机裁剪一部分并输入网络，多次执行这一步骤并取平均分类结果。</li>
</ul>
<p>也就是说，只是为了提高精度的话，可以想办法对同一份输入执行多次条件不同的推理，并对结果求平均。当然，实际应用中是不可能用性能这么低的方法。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这节课是CNN中最重要的一节课。通过学习一些经典的CNN架构，我们掌握了很多有关搭建CNN的知识。总结一下：</p>
<ul>
<li>早期CNN<ul>
<li>卷积、池化、全连接</li>
<li>边长减小，通道数增加</li>
</ul>
</li>
<li>ResNet<ul>
<li>为什么使用ResNet？</li>
<li>梯度问题是怎么被解决的？</li>
<li>残差块的一般结构</li>
<li>输入输出通道数不同的残差块</li>
<li>了解ResNet的结构(ResNet-18, ResNet-50)</li>
</ul>
</li>
<li>Incpetion 网络<ul>
<li>1x1卷积</li>
<li>用1x1卷积减少计算量</li>
<li>Inception网络的基本模块</li>
</ul>
</li>
<li>MobileNet<ul>
<li>逐深度可分卷积</li>
<li>MobileNet v2中的瓶颈结构</li>
</ul>
</li>
</ul>
<p>这节课介绍的都是比较前沿的CNN架构。在深度学习技术日新月异的今天，最好的学习方式是读论文，尽快一步一步跟上最新的技术。这堂课中提及的比较新的几篇论文，都有很高的阅读价值。</p>
<p>我打算在学完CNN的四周课后，暂时不去学第五门课，而是去阅读这些经典CNN论文并分享一下笔记。</p>
<p>在这周的代码实战里，我会分享一下如何用TensorFlow和PyTorch编写ResNet，同时介绍两种框架的进阶用法。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/DLS-note-10-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/DLS-note-10-5/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》代码实战（十）：4. 算子反向传播的实现思路（以NumPy版卷积为例）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-24 00:31:03" itemprop="dateCreated datePublished" datetime="2022-07-24T00:31:03+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在之前的文章中，我介绍了<a href>如何用NumPy实现卷积正向传播</a>。<br>在这篇文章里，我会继续介绍如何用NumPy复现二维卷积的反向传播，并用PyTorch来验证结果的正确性。通过阅读这篇文章，大家不仅能进一步理解卷积的实现原理，更能领悟到一般算子的反向传播实现是怎么推导、编写出来的。</p>
<p>项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicCNN">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicCNN</a></p>
<p>本文代码在<code>dldemos/BasicCNN/np_conv_backward.py</code>这个文件里。</p>
<h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>回忆一下，在正向传播中，我们是这样做卷积运算的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i_h <span class="keyword">in</span> <span class="built_in">range</span>(h_o):</span><br><span class="line">    <span class="keyword">for</span> i_w <span class="keyword">in</span> <span class="built_in">range</span>(w_o):</span><br><span class="line">        <span class="keyword">for</span> i_c <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br><span class="line">            h_lower = i_h * stride</span><br><span class="line">            h_upper = i_h * stride + f</span><br><span class="line">            w_lower = i_w * stride</span><br><span class="line">            w_upper = i_w * stride + f</span><br><span class="line">            input_slice = input_pad[h_lower:h_upper, w_lower:w_upper, :]</span><br><span class="line">            kernel_slice = weight[i_c]</span><br><span class="line">            output[i_h, i_w, i_c] = np.<span class="built_in">sum</span>(input_slice * kernel_slice)</span><br><span class="line">            output[i_h, i_w, i_c] += bias[i_c]</span><br></pre></td></tr></table></figure>
<p>我们遍历输出图像的每一个位置，选择该位置对应的输入图像切片和卷积核，做一遍乘法，再加上bias。</p>
<p>其实，一轮运算写成数学公式的话，就是一个线性函数<code>y=wx+b</code>。对<code>w, x, b</code>求导非常简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dw_i = x * dy</span><br><span class="line">dx_i = w * dy</span><br><span class="line">db_i = dy</span><br></pre></td></tr></table></figure>
<p>在反向传播中，我们只需要遍历所有这样的线性运算，计算这轮运算对各参数的导数的贡献即可。最后，累加所有的贡献，就能得到各参数的导数。当然，在用代码实现这段逻辑时，可以不用最后再把所有贡献加起来，而是一算出来就加上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dw += x * dy</span><br><span class="line">dx += w * dy</span><br><span class="line">db += dy</span><br></pre></td></tr></table></figure>
<p>这里要稍微补充一点。在前向传播的实现中，我加入了<code>dilation, groups</code>这两个参数。为了简化反向传播的实现代码，只展示反向传播中最精华的部分，我在这份卷积实现中没有使用这两个参数。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在开始实现反向传播之前，我们先思考一个问题：反向传播的函数应该有哪些参数？从数学上来讲，反向传播和正向传播的参数是相反的。设正向传播的输入是<code>A_prev, W, b</code>（输入图像、卷积核组、偏差），则应该输出<code>Z</code>（输出图像）。那么，在反向传播中，应该输入<code>dZ</code>，输出<code>dA_prev, dW, db</code>。可是，在写代码时，我们还需要一些其他的输入参数。</p>
<p>我的反向传播函数的函数定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_backward</span>(<span class="params">dZ: np.ndarray, cache: <span class="type">Dict</span>[<span class="built_in">str</span>, np.ndarray], stride: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                    padding: <span class="built_in">int</span></span>) -&gt; <span class="type">Tuple</span>[np.ndarray, np.ndarray, np.ndarray]:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;2D Convolution Backward Implemented with NumPy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dZ: (np.ndarray): The derivative of the output of conv.</span></span><br><span class="line"><span class="string">        cache (Dict[str, np.ndarray]): Record output &#x27;Z&#x27;, weight &#x27;W&#x27;, bias &#x27;b&#x27;</span></span><br><span class="line"><span class="string">            and input &#x27;A_prev&#x27; of forward function.</span></span><br><span class="line"><span class="string">        stride (int): Stride for convolution.</span></span><br><span class="line"><span class="string">        padding (int): The count of zeros to pad on both sides.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        Tuple[np.ndarray, np.ndarray, np.ndarray]: The derivative of W, b,</span></span><br><span class="line"><span class="string">            A_prev.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>虽然我这里把所有参数都写在了一起，但从逻辑上来看，这些参数应该分成三个类别。在编程框架中，这三类参数会储存在不同的地方。</p>
<ul>
<li><code>dZ</code>: 反向传播函数真正的输入。</li>
<li><code>cache</code>: 正向传播中的一些中间变量<code>Z, W, b</code>。由于我们必须在一个独立的函数里完成反向传播，这些中间变量得以输入参数的形式供函数访问。</li>
<li><code>stride, padding</code>: 这两个参数是卷积的属性。如果卷积层是用一个类表示的话，这些参数应该放在类属性里，而不应该放在反向传播的输入里。</li>
</ul>
<p>给定这三类参数，就足以完成反向传播计算了。下面我来介绍<code>conv2d_backward</code>的具体实现。</p>
<p>首先，获取<code>cache</code>中的参数，并且新建储存梯度的张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">W = cache[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">b = cache[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">A_prev = cache[<span class="string">&#x27;A_prev&#x27;</span>]</span><br><span class="line">dW = np.zeros(W.shape)</span><br><span class="line">db = np.zeros(b.shape)</span><br><span class="line">dA_prev = np.zeros(A_prev.shape)</span><br><span class="line"></span><br><span class="line">_, _, c_i = A_prev.shape</span><br><span class="line">c_o, f, f_2, c_k = W.shape</span><br><span class="line">h_o, w_o, c_o_2 = dZ.shape</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> (f == f_2)</span><br><span class="line"><span class="keyword">assert</span> (c_i == c_k)</span><br><span class="line"><span class="keyword">assert</span> (c_o == c_o_2)</span><br></pre></td></tr></table></figure>
<p>之后，为了实现填充操作，我们要把<code>A_prev</code>和<code>dA_prev</code>都填充一下。注意，算完了所有梯度后，别忘了要重新把<code>dA_prev</code>从<code>dA_prev_pad</code>里抠出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A_prev_pad = np.pad(A_prev, [(padding, padding), (padding, padding),</span><br><span class="line">                                (<span class="number">0</span>, <span class="number">0</span>)])</span><br><span class="line">dA_prev_pad = np.pad(dA_prev, [(padding, padding), (padding, padding),</span><br><span class="line">                                (<span class="number">0</span>, <span class="number">0</span>)])</span><br></pre></td></tr></table></figure>
<p>接下来，就是梯度的计算了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i_h <span class="keyword">in</span> <span class="built_in">range</span>(h_o):</span><br><span class="line">    <span class="keyword">for</span> i_w <span class="keyword">in</span> <span class="built_in">range</span>(w_o):</span><br><span class="line">        <span class="keyword">for</span> i_c <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br><span class="line">            h_lower = i_h * stride</span><br><span class="line">            h_upper = i_h * stride + f</span><br><span class="line">            w_lower = i_w * stride</span><br><span class="line">            w_upper = i_w * stride + f</span><br><span class="line"></span><br><span class="line">            input_slice = A_prev_pad[h_lower:h_upper, w_lower:w_upper, :]</span><br><span class="line">            <span class="comment"># forward</span></span><br><span class="line">            <span class="comment"># kernel_slice = W[i_c]</span></span><br><span class="line">            <span class="comment"># Z[i_h, i_w, i_c] = np.sum(input_slice * kernel_slice)</span></span><br><span class="line">            <span class="comment"># Z[i_h, i_w, i_c] += b[i_c]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># backward</span></span><br><span class="line">            dW[i_c] += input_slice * dZ[i_h, i_w, i_c]</span><br><span class="line">            dA_prev_pad[h_lower:h_upper,</span><br><span class="line">                        w_lower:w_upper, :] += W[i_c] * dZ[i_h, i_w, i_c]</span><br><span class="line">            db[i_c] += dZ[i_h, i_w, i_c]</span><br></pre></td></tr></table></figure>
<p>在算导数时，我们应该对照着正向传播的计算，算出每一条计算对导数的贡献。如前文所述，卷积操作只是一个简单的<code>y=wx+b</code>，把对应的<code>w, x, b</code>从变量里正确地取出来并做运算即可。</p>
<p>最后，要把这些导数返回。别忘了把填充后的<code>dA_prev</code>恢复一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">    dA_prev = dA_prev_pad[padding:-padding, padding:-padding, :]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    dA_prev = dA_prev_pad</span><br><span class="line"><span class="keyword">return</span> dW, db, dA_prev</span><br></pre></td></tr></table></figure>
<p>这里有一个细节：如果<code>padding==0</code>，则在取切片时范围会变成<code>[0:-0]</code>，这样会取出一个长度为<code>0</code>的切片，而不是我们期望的原长度的切片。因此，要特判一下<code>padding&lt;=0</code>的情况。</p>
<h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>为了方便地进行单元测试，我使用了pytest这个单元测试库。可以直接pip一键安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pytest</span><br></pre></td></tr></table></figure>
<p>之后就可以用pytest执行我的这份代码，代码里所有以<code>test_</code>开头的函数会被认为是单元测试的主函数。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pytest dldemos/BasicCNN/np_conv_backward.py</span><br></pre></td></tr></table></figure>
<p>单元测试函数的定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;c_i, c_o&#x27;</span>, [(<span class="params"><span class="number">3</span>, <span class="number">6</span></span>), (<span class="params"><span class="number">2</span>, <span class="number">2</span></span>)]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;kernel_size&#x27;</span>, [<span class="number">3</span>, <span class="number">5</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;stride&#x27;</span>, [<span class="number">1</span>, <span class="number">2</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;padding&#x27;</span>, [<span class="number">0</span>, <span class="number">1</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_conv</span>(<span class="params">c_i: <span class="built_in">int</span>, c_o: <span class="built_in">int</span>, kernel_size: <span class="built_in">int</span>, stride: <span class="built_in">int</span>, padding: <span class="built_in">str</span></span>):</span></span><br></pre></td></tr></table></figure>
<p><code>@pytest.mark.parametrize</code>用于设置单元测试参数的可选值。我设置了4组参数，每组参数有2个可选值，经过排列组合后可以生成<code>2^4=16</code>个单元测试，pytest会自动帮我们执行不同的测试。</p>
<p>在单元测试中，我打算测试<code>conv2d</code>在各种输入通道数、输出通道数、卷积核大小、步幅、填充数的情况。</p>
<p>测试函数是这样写的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_conv</span>(<span class="params">c_i: <span class="built_in">int</span>, c_o: <span class="built_in">int</span>, kernel_size: <span class="built_in">int</span>, stride: <span class="built_in">int</span>, padding: <span class="built_in">str</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Preprocess</span></span><br><span class="line">    <span class="built_in">input</span> = np.random.randn(<span class="number">20</span>, <span class="number">20</span>, c_i)</span><br><span class="line">    weight = np.random.randn(c_o, kernel_size, kernel_size, c_i)</span><br><span class="line">    bias = np.random.randn(c_o)</span><br><span class="line"></span><br><span class="line">    torch_input = torch.from_numpy(np.transpose(</span><br><span class="line">        <span class="built_in">input</span>, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))).unsqueeze(<span class="number">0</span>).requires_grad_()</span><br><span class="line">    torch_weight = torch.from_numpy(np.transpose(</span><br><span class="line">        weight, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>))).requires_grad_()</span><br><span class="line">    torch_bias = torch.from_numpy(bias).requires_grad_()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    torch_output_tensor = torch.conv2d(torch_input, torch_weight, torch_bias,</span><br><span class="line">                                       stride, padding)</span><br><span class="line">    torch_output = np.transpose(</span><br><span class="line">        torch_output_tensor.detach().numpy().squeeze(<span class="number">0</span>), (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    cache = conv2d_forward(<span class="built_in">input</span>, weight, bias, stride, padding)</span><br><span class="line">    numpy_output = cache[<span class="string">&#x27;Z&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> np.allclose(torch_output, numpy_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    torch_sum = torch.<span class="built_in">sum</span>(torch_output_tensor)</span><br><span class="line">    torch_sum.backward()</span><br><span class="line">    torch_dW = np.transpose(torch_weight.grad.numpy(), (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">    torch_db = torch_bias.grad.numpy()</span><br><span class="line">    torch_dA_prev = np.transpose(torch_input.grad.numpy().squeeze(<span class="number">0</span>),</span><br><span class="line">                                 (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    dZ = np.ones(numpy_output.shape)</span><br><span class="line">    dW, db, dA_prev = conv2d_backward(dZ, cache, stride, padding)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> np.allclose(dW, torch_dW)</span><br><span class="line">    <span class="keyword">assert</span> np.allclose(db, torch_db)</span><br><span class="line">    <span class="keyword">assert</span> np.allclose(dA_prev, torch_dA_prev)</span><br></pre></td></tr></table></figure>
<p>整个测试函数可以分成三部分：变量预处理、前向传播、反向传播。在前向传播和反向传播中，我们要分别用刚编写的卷积核PyTorch中的卷积进行计算，并比较两个运算结果是否相同。</p>
<p>预处理时，我们要创建NumPy和PyTorch的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess</span></span><br><span class="line"><span class="built_in">input</span> = np.random.randn(<span class="number">20</span>, <span class="number">20</span>, c_i)</span><br><span class="line">weight = np.random.randn(c_o, kernel_size, kernel_size, c_i)</span><br><span class="line">bias = np.random.randn(c_o)</span><br><span class="line"></span><br><span class="line">torch_input = torch.from_numpy(np.transpose(</span><br><span class="line">    <span class="built_in">input</span>, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))).unsqueeze(<span class="number">0</span>).requires_grad_()</span><br><span class="line">torch_weight = torch.from_numpy(np.transpose(</span><br><span class="line">    weight, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>))).requires_grad_()</span><br><span class="line">torch_bias = torch.from_numpy(bias).requires_grad_()</span><br></pre></td></tr></table></figure>
<p>之后是正向传播。计算结果和中间变量会被存入<code>cache</code>中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># forward</span></span><br><span class="line">torch_output_tensor = torch.conv2d(torch_input, torch_weight, torch_bias,</span><br><span class="line">                                    stride, padding)</span><br><span class="line">torch_output = np.transpose(</span><br><span class="line">    torch_output_tensor.detach().numpy().squeeze(<span class="number">0</span>), (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">cache = conv2d_forward(<span class="built_in">input</span>, weight, bias, stride, padding)</span><br><span class="line">numpy_output = cache[<span class="string">&#x27;Z&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.allclose(torch_output, numpy_output)</span><br></pre></td></tr></table></figure>
<p>最后是反向传播。在那之前，要补充说明一下如何在PyTorch里手动求一些数据的导数。在PyTorch中，各个张量默认是不可训练的。为了让框架知道我们想求哪几个参数的导数，我们要执行张量的<code>required_grad_()</code>方法，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch_input = torch.from_numpy(np.transpose(</span><br><span class="line">        <span class="built_in">input</span>, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))).unsqueeze(<span class="number">0</span>).requires_grad_()</span><br></pre></td></tr></table></figure>
<p>这样，在正向传播时，PyTorch就会自动把对可训练参数的运算搭成计算图了。</p>
<p>正向传播后，对结果张量调用<code>backward()</code>即可执行反向传播。但是，PyTorch要求调用<code>backward()</code>的张量必须是一个标量，也就是它不能是矩阵，不能是任何长度大于1的数据。而这里PyTorch的卷积结果又是一个四维张量。因此，我把PyTorch卷积结果做了求和，得到了一个标量，用它来调用<code>backward()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch_sum = torch.<span class="built_in">sum</span>(torch_output_tensor)</span><br><span class="line">torch_sum.backward()</span><br></pre></td></tr></table></figure>
<p>这样，就可以用<code>tensor.grad</code>获取<code>tensor</code>的导数了，如</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch_weight.grad</span><br><span class="line">torch_bias.grad</span><br><span class="line">torch_input.grad</span><br></pre></td></tr></table></figure>
<p>整个反向传播测试的代码如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># backward</span></span><br><span class="line">torch_sum = torch.<span class="built_in">sum</span>(torch_output_tensor)</span><br><span class="line">torch_sum.backward()</span><br><span class="line">torch_dW = np.transpose(torch_weight.grad.numpy(), (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">torch_db = torch_bias.grad.numpy()</span><br><span class="line">torch_dA_prev = np.transpose(torch_input.grad.numpy().squeeze(<span class="number">0</span>),</span><br><span class="line">                                (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">dZ = np.ones(numpy_output.shape)</span><br><span class="line">dW, db, dA_prev = conv2d_backward(dZ, cache, stride, padding)</span><br></pre></td></tr></table></figure>
<p>再补充一下，在求导时，运算结果的导数是1。因此，新建<code>dZ</code>时，我用的是<code>np.ones</code>（全1张量）。同理，PyTorch也会默认运算结果的导数为1，即这里<code>torch_sum.grad==1</code>。而执行加法运算不会改变导数，所以<code>torch_output_tensor.grad</code>也是一个全是1的张量，和NumPy的<code>dZ</code>的值是一模一样的。</p>
<p>写完单元测试函数后，运行前面提到的单元测试命令，pytest就会输出很多测试的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pytest dldemos/BasicCNN/np_conv_backward.py</span><br></pre></td></tr></table></figure>
<p>如果看到了类似的输出，就说明我们的代码是正确的。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">==== 16 passed in 1.04s ====</span><br></pre></td></tr></table></figure>
<h2 id="反向传播的编写思路"><a href="#反向传播的编写思路" class="headerlink" title="反向传播的编写思路"></a>反向传播的编写思路</h2><p>通过阅读上面的实现过程，相信大家已经明白如何编写卷积的反向传播了。接下来，我将总结一下实现一般算子的正向、反向传播的思路。无论是用NumPy，还是PyTorch等编程框架，甚至是纯C++，这种思路都是适用的。</p>
<p>一开始，我们要明白，一个算子总共会涉及到这些参数：</p>
<ul>
<li>输入与输出：算子的输入张量和输出张量。正向传播和反向传播的输入输出恰好是相反的。</li>
<li>属性：算子的超参数。比如卷积的<code>stride, padding</code>。</li>
<li>中间变量：前向传播传递给反向传播的变量。</li>
</ul>
<p>一般情况下，我们应该编写一个算子类。在初始化算子类时，算子的属性就以类属性的形式存储下来了。</p>
<p>在正向传播时，我们按照算子定义直接顺着写下去就行。这个时候，可以先准备好<code>cache</code>变量，但先不去管它，等写到反向传播的时候再处理。</p>
<p>接着，编写反向传播。由于反向传播和正向传播的运算步骤相似，我们可以直接把正向传播的代码复制一份。在这个基础上，思考每一步正向传播运算产生了哪些导数，对照着写出导数计算的代码即可。这时，我们会用到一些正向传播的中间结果，这下就可以去正向传播代码里填写<code>cache</code>，在反向传播里取出来了。</p>
<p>最后，写完了算子，一定要做单元测试。如果该算子有现成的实现，用现成的实现来对齐运算结果是最简单的一种实现单元测试的方式。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我介绍了以下内容：</p>
<ul>
<li>卷积反向传播的NumPy实现</li>
<li>如何用PyTorch手动求导</li>
<li>如何编写完整的算子单元测试</li>
<li>实现算子正向传播、反向传播的思路</li>
</ul>
<p>如果你也想把代码基础打牢，一定一定要像这样自己动手从头写一份代码。在写代码，调bug的过程中，一定会有很多收获。</p>
<p>由于现在的编程框架都比较成熟，搞科研时基本不会碰到自己动手写底层算子的情况。但是，如果你想出了一个特别棒的idea，想出了一个全新的神经网络模块，却在写代码时碰到了阻碍，那可就太可惜了。学一学反向传播的实现还是很有用的。</p>
<p>在模型部署中，反向传播可能完全派不上用场。但是，一般框架在实现算子的正向传播时，是会照顾反向传播的。也就是说，如果抛掉反向传播，正向传播的实现或许可以写得更加高效。这样看来，了解反向传播的实现也是很有帮助的。我们可以用这些知识看懂别人的正向传播、反向传播的实现，进而优化代码的性能。</p>
<h2 id="附录：完整代码"><a href="#附录：完整代码" class="headerlink" title="附录：完整代码"></a>附录：完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pytest</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_forward</span>(<span class="params"><span class="built_in">input</span>: np.ndarray, weight: np.ndarray, bias: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">                   stride: <span class="built_in">int</span>, padding: <span class="built_in">int</span></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, np.ndarray]:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;2D Convolution Forward Implemented with NumPy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input (np.ndarray): The input NumPy array of shape (H, W, C).</span></span><br><span class="line"><span class="string">        weight (np.ndarray): The weight NumPy array of shape</span></span><br><span class="line"><span class="string">            (C&#x27;, F, F, C).</span></span><br><span class="line"><span class="string">        bias (np.ndarray | None): The bias NumPy array of shape (C&#x27;).</span></span><br><span class="line"><span class="string">            Default: None.</span></span><br><span class="line"><span class="string">        stride (int): Stride for convolution.</span></span><br><span class="line"><span class="string">        padding (int): The count of zeros to pad on both sides.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        Dict[str, np.ndarray]: Cached data for backward prop.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    h_i, w_i, c_i = <span class="built_in">input</span>.shape</span><br><span class="line">    c_o, f, f_2, c_k = weight.shape</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (f == f_2)</span><br><span class="line">    <span class="keyword">assert</span> (c_i == c_k)</span><br><span class="line">    <span class="keyword">assert</span> (bias.shape[<span class="number">0</span>] == c_o)</span><br><span class="line"></span><br><span class="line">    input_pad = np.pad(<span class="built_in">input</span>, [(padding, padding), (padding, padding), (<span class="number">0</span>, <span class="number">0</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_new_sidelngth</span>(<span class="params">sl, s, f, p</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (sl + <span class="number">2</span> * p - f) // s + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    h_o = cal_new_sidelngth(h_i, stride, f, padding)</span><br><span class="line">    w_o = cal_new_sidelngth(w_i, stride, f, padding)</span><br><span class="line"></span><br><span class="line">    output = np.empty((h_o, w_o, c_o), dtype=<span class="built_in">input</span>.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i_h <span class="keyword">in</span> <span class="built_in">range</span>(h_o):</span><br><span class="line">        <span class="keyword">for</span> i_w <span class="keyword">in</span> <span class="built_in">range</span>(w_o):</span><br><span class="line">            <span class="keyword">for</span> i_c <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br><span class="line">                h_lower = i_h * stride</span><br><span class="line">                h_upper = i_h * stride + f</span><br><span class="line">                w_lower = i_w * stride</span><br><span class="line">                w_upper = i_w * stride + f</span><br><span class="line">                input_slice = input_pad[h_lower:h_upper, w_lower:w_upper, :]</span><br><span class="line">                kernel_slice = weight[i_c]</span><br><span class="line">                output[i_h, i_w, i_c] = np.<span class="built_in">sum</span>(input_slice * kernel_slice)</span><br><span class="line">                output[i_h, i_w, i_c] += bias[i_c]</span><br><span class="line"></span><br><span class="line">    cache = <span class="built_in">dict</span>()</span><br><span class="line">    cache[<span class="string">&#x27;Z&#x27;</span>] = output</span><br><span class="line">    cache[<span class="string">&#x27;W&#x27;</span>] = weight</span><br><span class="line">    cache[<span class="string">&#x27;b&#x27;</span>] = bias</span><br><span class="line">    cache[<span class="string">&#x27;A_prev&#x27;</span>] = <span class="built_in">input</span></span><br><span class="line">    <span class="keyword">return</span> cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d_backward</span>(<span class="params">dZ: np.ndarray, cache: <span class="type">Dict</span>[<span class="built_in">str</span>, np.ndarray], stride: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                    padding: <span class="built_in">int</span></span>) -&gt; <span class="type">Tuple</span>[np.ndarray, np.ndarray, np.ndarray]:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;2D Convolution Backward Implemented with NumPy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dZ: (np.ndarray): The derivative of the output of conv.</span></span><br><span class="line"><span class="string">        cache (Dict[str, np.ndarray]): Record output &#x27;Z&#x27;, weight &#x27;W&#x27;, bias &#x27;b&#x27;</span></span><br><span class="line"><span class="string">            and input &#x27;A_prev&#x27; of forward function.</span></span><br><span class="line"><span class="string">        stride (int): Stride for convolution.</span></span><br><span class="line"><span class="string">        padding (int): The count of zeros to pad on both sides.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        Tuple[np.ndarray, np.ndarray, np.ndarray]: The derivative of W, b,</span></span><br><span class="line"><span class="string">            A_prev.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    W = cache[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">    b = cache[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">    A_prev = cache[<span class="string">&#x27;A_prev&#x27;</span>]</span><br><span class="line">    dW = np.zeros(W.shape)</span><br><span class="line">    db = np.zeros(b.shape)</span><br><span class="line">    dA_prev = np.zeros(A_prev.shape)</span><br><span class="line"></span><br><span class="line">    _, _, c_i = A_prev.shape</span><br><span class="line">    c_o, f, f_2, c_k = W.shape</span><br><span class="line">    h_o, w_o, c_o_2 = dZ.shape</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (f == f_2)</span><br><span class="line">    <span class="keyword">assert</span> (c_i == c_k)</span><br><span class="line">    <span class="keyword">assert</span> (c_o == c_o_2)</span><br><span class="line"></span><br><span class="line">    A_prev_pad = np.pad(A_prev, [(padding, padding), (padding, padding),</span><br><span class="line">                                 (<span class="number">0</span>, <span class="number">0</span>)])</span><br><span class="line">    dA_prev_pad = np.pad(dA_prev, [(padding, padding), (padding, padding),</span><br><span class="line">                                   (<span class="number">0</span>, <span class="number">0</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i_h <span class="keyword">in</span> <span class="built_in">range</span>(h_o):</span><br><span class="line">        <span class="keyword">for</span> i_w <span class="keyword">in</span> <span class="built_in">range</span>(w_o):</span><br><span class="line">            <span class="keyword">for</span> i_c <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br><span class="line">                h_lower = i_h * stride</span><br><span class="line">                h_upper = i_h * stride + f</span><br><span class="line">                w_lower = i_w * stride</span><br><span class="line">                w_upper = i_w * stride + f</span><br><span class="line"></span><br><span class="line">                input_slice = A_prev_pad[h_lower:h_upper, w_lower:w_upper, :]</span><br><span class="line">                <span class="comment"># forward</span></span><br><span class="line">                <span class="comment"># kernel_slice = W[i_c]</span></span><br><span class="line">                <span class="comment"># Z[i_h, i_w, i_c] = np.sum(input_slice * kernel_slice)</span></span><br><span class="line">                <span class="comment"># Z[i_h, i_w, i_c] += b[i_c]</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># backward</span></span><br><span class="line">                dW[i_c] += input_slice * dZ[i_h, i_w, i_c]</span><br><span class="line">                dA_prev_pad[h_lower:h_upper,</span><br><span class="line">                            w_lower:w_upper, :] += W[i_c] * dZ[i_h, i_w, i_c]</span><br><span class="line">                db[i_c] += dZ[i_h, i_w, i_c]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> padding &gt; <span class="number">0</span>:</span><br><span class="line">        dA_prev = dA_prev_pad[padding:-padding, padding:-padding, :]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dA_prev = dA_prev_pad</span><br><span class="line">    <span class="keyword">return</span> dW, db, dA_prev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;c_i, c_o&#x27;</span>, [(<span class="params"><span class="number">3</span>, <span class="number">6</span></span>), (<span class="params"><span class="number">2</span>, <span class="number">2</span></span>)]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;kernel_size&#x27;</span>, [<span class="number">3</span>, <span class="number">5</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;stride&#x27;</span>, [<span class="number">1</span>, <span class="number">2</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;padding&#x27;</span>, [<span class="number">0</span>, <span class="number">1</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_conv</span>(<span class="params">c_i: <span class="built_in">int</span>, c_o: <span class="built_in">int</span>, kernel_size: <span class="built_in">int</span>, stride: <span class="built_in">int</span>, padding: <span class="built_in">str</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Preprocess</span></span><br><span class="line">    <span class="built_in">input</span> = np.random.randn(<span class="number">20</span>, <span class="number">20</span>, c_i)</span><br><span class="line">    weight = np.random.randn(c_o, kernel_size, kernel_size, c_i)</span><br><span class="line">    bias = np.random.randn(c_o)</span><br><span class="line"></span><br><span class="line">    torch_input = torch.from_numpy(np.transpose(</span><br><span class="line">        <span class="built_in">input</span>, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))).unsqueeze(<span class="number">0</span>).requires_grad_()</span><br><span class="line">    torch_weight = torch.from_numpy(np.transpose(</span><br><span class="line">        weight, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>))).requires_grad_()</span><br><span class="line">    torch_bias = torch.from_numpy(bias).requires_grad_()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    torch_output_tensor = torch.conv2d(torch_input, torch_weight, torch_bias,</span><br><span class="line">                                       stride, padding)</span><br><span class="line">    torch_output = np.transpose(</span><br><span class="line">        torch_output_tensor.detach().numpy().squeeze(<span class="number">0</span>), (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    cache = conv2d_forward(<span class="built_in">input</span>, weight, bias, stride, padding)</span><br><span class="line">    numpy_output = cache[<span class="string">&#x27;Z&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> np.allclose(torch_output, numpy_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    torch_sum = torch.<span class="built_in">sum</span>(torch_output_tensor)</span><br><span class="line">    torch_sum.backward()</span><br><span class="line">    torch_dW = np.transpose(torch_weight.grad.numpy(), (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">    torch_db = torch_bias.grad.numpy()</span><br><span class="line">    torch_dA_prev = np.transpose(torch_input.grad.numpy().squeeze(<span class="number">0</span>),</span><br><span class="line">                                 (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    dZ = np.ones(numpy_output.shape)</span><br><span class="line">    dW, db, dA_prev = conv2d_backward(dZ, cache, stride, padding)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> np.allclose(dW, torch_dW)</span><br><span class="line">    <span class="keyword">assert</span> np.allclose(db, torch_db)</span><br><span class="line">    <span class="keyword">assert</span> np.allclose(dA_prev, torch_dA_prev)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://zhouyifan.net/2022/07/24/DLS-note-10-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Zhou Yifan">
      <meta itemprop="description" content="A foresighted strategist with big-picture thinking. 大局观选手。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="周弈帆的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/24/DLS-note-10-4/" class="post-title-link" itemprop="url">吴恩达《深度学习专项》代码实战（十）：3.用 NumPy 复现参数一致的 torch.conv2d 前向传播</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-24 00:30:59" itemprop="dateCreated datePublished" datetime="2022-07-24T00:30:59+08:00">2022-07-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">知识记录</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>《深度学习专项》只介绍了卷积的stride, padding这两个参数。实际上，编程框架中常用的卷积还有其他几个参数。在这篇文章里，我会介绍如何用NumPy复现PyTorch中的二维卷积<code>torch.conv2d</code>的前向传播。如果大家也想多学一点的话，建议看完本文后也<strong>自己动手</strong>写一遍卷积，彻底理解卷积中常见的参数。</p>
<p>项目网址：<a target="_blank" rel="noopener" href="https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicCNN">https://github.com/SingleZombie/DL-Demos/tree/master/dldemos/BasicCNN</a></p>
<p>本文代码在<code>dldemos/BasicCNN/np_conv.py</code>这个文件里。</p>
<h2 id="卷积参数介绍"><a href="#卷积参数介绍" class="headerlink" title="卷积参数介绍"></a>卷积参数介绍</h2><p>与<code>torch.conv2d</code>类似，在这份实现中，我们的卷积应该有类似如下的函数定义（张量的形状写在docstring中）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params"><span class="built_in">input</span>: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">           weight: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">           stride: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           padding: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           dilation: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           groups: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           bias: np.ndarray = <span class="literal">None</span></span>) -&gt; np.ndarray:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;2D Convolution Implemented with NumPy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input (np.ndarray): The input NumPy array of shape (H, W, C).</span></span><br><span class="line"><span class="string">        weight (np.ndarray): The weight NumPy array of shape</span></span><br><span class="line"><span class="string">            (C&#x27;, F, F, C / groups).</span></span><br><span class="line"><span class="string">        stride (int): Stride for convolution.</span></span><br><span class="line"><span class="string">        padding (int): The count of zeros to pad on both sides.</span></span><br><span class="line"><span class="string">        dilation (int): The space between kernel elements.</span></span><br><span class="line"><span class="string">        groups (int): Split the input to groups.</span></span><br><span class="line"><span class="string">        bias (np.ndarray | None): The bias NumPy array of shape (C&#x27;).</span></span><br><span class="line"><span class="string">            Default: None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        np.ndarray: The output NumPy array of shape (H&#x27;, W&#x27;, C&#x27;)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></p>
<p>我们知道，对于不加任何参数的卷积，其计算方式如下：</p>
<p><img src="/2022/07/24/DLS-note-10-4/1.gif" alt></p>
<p>此图中，下面蓝色的区域是一张$4 \times 4$的输入图片，输入图片上深蓝色的区域是一个$3 \times 3$的卷积核。这样，会生成上面那个$2 \times 2$的绿色的输出图片。每轮计算输出图片上一个深绿色的元素时，卷积核所在位置会标出来。</p>
<p>接下来，使用类似图例，我们来看看卷积各参数的详细解释。</p>
<h3 id="stride（步幅）"><a href="#stride（步幅）" class="headerlink" title="stride（步幅）"></a>stride（步幅）</h3><p><img src="/2022/07/24/DLS-note-10-4/2.gif" alt></p>
<p>每轮计算后，卷积核向右或向下移动多格，而不仅仅是1格。每轮移动的格子数用stride表示。上图是stride=2的情况。</p>
<h3 id="padding（填充数）"><a href="#padding（填充数）" class="headerlink" title="padding（填充数）"></a>padding（填充数）</h3><p><img src="/2022/07/24/DLS-note-10-4/3.gif" alt></p>
<p>卷积开始前，向输入图片四周填充数字（最常见的情况是填充0），填充的数字个数用padding表示。这样，输出图片的边长会更大一些。一般我们会为了让输出图片和输入图片一样大而调整padding，比如上图那种padding=1的情况。</p>
<h3 id="dilation（扩充数）"><a href="#dilation（扩充数）" class="headerlink" title="dilation（扩充数）"></a>dilation（扩充数）</h3><p><img src="/2022/07/24/DLS-note-10-4/4.gif" alt></p>
<p>被卷积的相邻像素之间有间隔，这个间隔等于dilation。等价于在卷积核相邻位置之间填0，再做普通的卷积。上图是dilation=2的情况。</p>
<blockquote>
<p>dliated convolution 被翻译成空洞卷积。</p>
</blockquote>
<h3 id="groups（分组数）"><a href="#groups（分组数）" class="headerlink" title="groups（分组数）"></a>groups（分组数）</h3><p>下图展示了输入通道数12，输出通道数6的卷积在两种不同groups下的情况。左边是group=1的普通卷积，右边是groups=3的分组卷积。在具体看分组卷积的介绍前，大家可以先仔细观察这张图，看看能不能猜出分组卷积是怎么运算的。</p>
<p><img src="/2022/07/24/DLS-note-10-4/1.png" alt></p>
<p>当输入图片有多个通道时，卷积核也应该有相同数量的通道。输入图片的形状是(H, W, C)的话，卷积核的形状就应该是(f, f, C)。</p>
<p>但是，这样一轮运算只能算出一张单通道的图片。为了算多通道的图片，应该使用多个卷积核。因此，如果输入图片的形状是(H, W, C)，想要生成(H, W, C’)的输出图片，则应该有C’个形状为(f, f, C)的卷积核，或者说卷积核组的形状是(C’, f, f, C)。</p>
<p>如分组卷积示意图的左图所示，对于普通卷积，每一个输出通道都需要用到所有输入通道的数据。为了减少计算量，我们可以把输入通道和输出通道分组。每组的输出通道仅由该组的输入通道决定。如示意图的右图所示，我们令分组数groups=3，这样，一共有6个卷积核，每组的输入通道有4个，输出通道有2个（即使用2个卷积核）。这时候，卷积核组的形状应该是(C’=6, f, f, C=4)。</p>
<blockquote>
<p>groups最常见的应用是令groups=C，即depth-wise convolution。《深度学习专项》第四门课第二周会介绍有关的知识。</p>
</blockquote>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>理解了所有参数，下面让我们来用NumPy实现这样一个卷积。</p>
<p>完整的代码是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params"><span class="built_in">input</span>: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">           weight: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">           stride: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           padding: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           dilation: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           groups: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           bias: np.ndarray = <span class="literal">None</span></span>) -&gt; np.ndarray:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;2D Convolution Implemented with NumPy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        input (np.ndarray): The input NumPy array of shape (H, W, C).</span></span><br><span class="line"><span class="string">        weight (np.ndarray): The weight NumPy array of shape</span></span><br><span class="line"><span class="string">            (C&#x27;, F, F, C / groups).</span></span><br><span class="line"><span class="string">        stride (int): Stride for convolution.</span></span><br><span class="line"><span class="string">        padding (int): The count of zeros to pad on both sides.</span></span><br><span class="line"><span class="string">        dilation (int): The space between kernel elements.</span></span><br><span class="line"><span class="string">        groups (int): Split the input to groups.</span></span><br><span class="line"><span class="string">        bias (np.ndarray | None): The bias NumPy array of shape (C&#x27;).</span></span><br><span class="line"><span class="string">            Default: None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">        np.ndarray: The output NumPy array of shape (H&#x27;, W&#x27;, C&#x27;)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    h_i, w_i, c_i = <span class="built_in">input</span>.shape</span><br><span class="line">    c_o, f, f_2, c_k = weight.shape</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (f == f_2)</span><br><span class="line">    <span class="keyword">assert</span> (c_i % groups == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span> (c_o % groups == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">assert</span> (c_i // groups == c_k)</span><br><span class="line">    <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">assert</span> (bias.shape[<span class="number">0</span>] == c_o)</span><br><span class="line"></span><br><span class="line">    f_new = f + (f - <span class="number">1</span>) * (dilation - <span class="number">1</span>)</span><br><span class="line">    weight_new = np.zeros((c_o, f_new, f_new, c_k), dtype=weight.dtype)</span><br><span class="line">    <span class="keyword">for</span> i_c_o <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br><span class="line">        <span class="keyword">for</span> i_c_k <span class="keyword">in</span> <span class="built_in">range</span>(c_k):</span><br><span class="line">            <span class="keyword">for</span> i_f <span class="keyword">in</span> <span class="built_in">range</span>(f):</span><br><span class="line">                <span class="keyword">for</span> j_f <span class="keyword">in</span> <span class="built_in">range</span>(f):</span><br><span class="line">                    i_f_new = i_f * dilation</span><br><span class="line">                    j_f_new = j_f * dilation</span><br><span class="line">                    weight_new[i_c_o, i_f_new, j_f_new, i_c_k] = \</span><br><span class="line">                        weight[i_c_o, i_f, j_f, i_c_k]</span><br><span class="line"></span><br><span class="line">    input_pad = np.pad(<span class="built_in">input</span>, [(padding, padding), (padding, padding), (<span class="number">0</span>, <span class="number">0</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cal_new_sidelngth</span>(<span class="params">sl, s, f, p</span>):</span></span><br><span class="line">        <span class="keyword">return</span> (sl + <span class="number">2</span> * p - f) // s + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    h_o = cal_new_sidelngth(h_i, stride, f_new, padding)</span><br><span class="line">    w_o = cal_new_sidelngth(w_i, stride, f_new, padding)</span><br><span class="line"></span><br><span class="line">    output = np.empty((h_o, w_o, c_o), dtype=<span class="built_in">input</span>.dtype)</span><br><span class="line"></span><br><span class="line">    c_o_per_group = c_o // groups</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i_h <span class="keyword">in</span> <span class="built_in">range</span>(h_o):</span><br><span class="line">        <span class="keyword">for</span> i_w <span class="keyword">in</span> <span class="built_in">range</span>(w_o):</span><br><span class="line">            <span class="keyword">for</span> i_c <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br><span class="line">                i_g = i_c // c_o_per_group</span><br><span class="line">                h_lower = i_h * stride</span><br><span class="line">                h_upper = i_h * stride + f_new</span><br><span class="line">                w_lower = i_w * stride</span><br><span class="line">                w_upper = i_w * stride + f_new</span><br><span class="line">                c_lower = i_g * c_k</span><br><span class="line">                c_upper = (i_g + <span class="number">1</span>) * c_k</span><br><span class="line">                input_slice = input_pad[h_lower:h_upper, w_lower:w_upper,</span><br><span class="line">                                        c_lower:c_upper]</span><br><span class="line">                kernel_slice = weight_new[i_c]</span><br><span class="line">                output[i_h, i_w, i_c] = np.<span class="built_in">sum</span>(input_slice * kernel_slice)</span><br><span class="line">                <span class="keyword">if</span> bias:</span><br><span class="line">                    output[i_h, i_w, i_c] += bias[i_c]</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>先回顾一下我们要用到的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params"><span class="built_in">input</span>: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">           weight: np.ndarray,</span></span></span><br><span class="line"><span class="params"><span class="function">           stride: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           padding: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           dilation: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           groups: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           bias: np.ndarray = <span class="literal">None</span></span>) -&gt; np.ndarray:</span></span><br></pre></td></tr></table></figure>
<p>再次提醒，<code>input</code>的形状是<code>(H, W, C)</code>，卷积核组<code>weight</code>的形状是<code>(C&#39;, H, W, C_k)</code>。其中<code>C_k = C / groups</code>。同时<code>C&#39;</code>也必须能够被<code>groups</code>整除。<code>bias</code>的形状是<code>(C&#39;)</code>。</p>
<p>一开始，把要用到的形状从<code>shape</code>里取出来，并检查一下形状是否满足要求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">h_i, w_i, c_i = <span class="built_in">input</span>.shape</span><br><span class="line">c_o, f, f_2, c_k = weight.shape</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> (f == f_2)</span><br><span class="line"><span class="keyword">assert</span> (c_i % groups == <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> (c_o % groups == <span class="number">0</span>)</span><br><span class="line"><span class="keyword">assert</span> (c_i // groups == c_k)</span><br><span class="line"><span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">assert</span> (bias.shape[<span class="number">0</span>] == c_o)</span><br></pre></td></tr></table></figure>
<p>回忆一下，空洞卷积可以用卷积核扩充实现。因此，在开始卷积前，可以先预处理好扩充后的卷积核。我们先算好扩充后卷积核的形状，并创建好新的卷积核，最后用多重循环给新卷积核赋值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">f_new = f + (f - <span class="number">1</span>) * (dilation - <span class="number">1</span>)</span><br><span class="line">    weight_new = np.zeros((c_o, f_new, f_new, c_k), dtype=weight.dtype)</span><br><span class="line">    <span class="keyword">for</span> i_c_o <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br><span class="line">        <span class="keyword">for</span> i_c_k <span class="keyword">in</span> <span class="built_in">range</span>(c_k):</span><br><span class="line">            <span class="keyword">for</span> i_f <span class="keyword">in</span> <span class="built_in">range</span>(f):</span><br><span class="line">                <span class="keyword">for</span> j_f <span class="keyword">in</span> <span class="built_in">range</span>(f):</span><br><span class="line">                    i_f_new = i_f * dilation</span><br><span class="line">                    j_f_new = j_f * dilation</span><br><span class="line">                    weight_new[i_c_o, i_f_new, j_f_new, i_c_k] = \</span><br><span class="line">                        weight[i_c_o, i_f, j_f, i_c_k]</span><br></pre></td></tr></table></figure>
<p>接下来，我们要考虑padding。<code>np.pad</code>就是填充操作使用的函数。该函数第一个参数是输入，第二个参数是填充数量，要分别写出每个维度上左上和右下的填充数量。我们只填充图片的前两维，并且左上和右下填的数量一样多。因此，填充的写法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_pad = np.pad(<span class="built_in">input</span>, [(padding, padding), (padding, padding), (<span class="number">0</span>, <span class="number">0</span>)])</span><br></pre></td></tr></table></figure>
<p>预处理都做好了，马上要开始卷积计算了。在计算开始前，我们还要把算出输出张量的形状并将其初始化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_new_sidelngth</span>(<span class="params">sl, s, f, p</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (sl + <span class="number">2</span> * p - f) // s + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">h_o = cal_new_sidelngth(h_i, stride, f_new, padding)</span><br><span class="line">w_o = cal_new_sidelngth(w_i, stride, f_new, padding)</span><br><span class="line"></span><br><span class="line">output = np.empty((h_o, w_o, c_o), dtype=<span class="built_in">input</span>.dtype)</span><br></pre></td></tr></table></figure>
<p>为严谨起见，我这里用统一的函数计算了卷积后的宽高。不考虑dilation的边长公式由<code>cal_new_sidelngth</code>表示。如果对这个公式不理解，可以自己推一推。而考虑dilation时，只需要把原来的卷积核长度<code>f</code>换成新卷积核长度<code>f_new</code>即可。</p>
<blockquote>
<p>初始化<code>output</code>时，我没有像前面初始化<code>weight_new</code>一样使用<code>np.zeros</code>，而是用了<code>np.empty</code>。这是因为<code>weight_new</code>会有一些地方不被访问到，这些地方都应该填0。而<code>output</code>每一个元素都会被访问到并赋值，可以不用令它们初值为0。理论上，<code>np.empty</code>这种不限制初值的初始化方式是最快的，只是使用时一定别忘了要先给每个元素赋值。这种严谨的算法实现思维还是挺重要的，尤其是在用C++实现高性能的底层算法时。</p>
</blockquote>
<p>终于，可以进行卷积计算了。这部分的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">c_o_per_group = c_o // groups</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i_h <span class="keyword">in</span> <span class="built_in">range</span>(h_o):</span><br><span class="line">    <span class="keyword">for</span> i_w <span class="keyword">in</span> <span class="built_in">range</span>(w_o):</span><br><span class="line">        <span class="keyword">for</span> i_c <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br><span class="line">            i_g = i_c // c_o_per_group</span><br><span class="line">            h_lower = i_h * stride</span><br><span class="line">            h_upper = i_h * stride + f_new</span><br><span class="line">            w_lower = i_w * stride</span><br><span class="line">            w_upper = i_w * stride + f_new</span><br><span class="line">            c_lower = i_g * c_k</span><br><span class="line">            c_upper = (i_g + <span class="number">1</span>) * c_k</span><br><span class="line">            input_slice = input_pad[h_lower:h_upper, w_lower:w_upper,</span><br><span class="line">                                    c_lower:c_upper]</span><br><span class="line">            kernel_slice = weight_new[i_c]</span><br><span class="line">            output[i_h, i_w, i_c] = np.<span class="built_in">sum</span>(input_slice * kernel_slice)</span><br><span class="line">            <span class="keyword">if</span> bias:</span><br><span class="line">                output[i_h, i_w, i_c] += bias[i_c]</span><br></pre></td></tr></table></figure>
<p>来一点一点看这段代码。</p>
<p><code>c_o_per_group = c_o // groups</code>预处理了每组的输出通道数，后面会用到这个数。</p>
<p>为了填入输出张量每一处的值，我们应该遍历输出张量的每一个元素的下标：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i_h <span class="keyword">in</span> <span class="built_in">range</span>(h_o):</span><br><span class="line">    <span class="keyword">for</span> i_w <span class="keyword">in</span> <span class="built_in">range</span>(w_o):</span><br><span class="line">        <span class="keyword">for</span> i_c <span class="keyword">in</span> <span class="built_in">range</span>(c_o):</span><br></pre></td></tr></table></figure>
<p>做卷积时，我们要获取两个东西：被卷积的原图像上的数据、卷积用的卷积核。所以，下一步应该去获取原图像上的数据切片。这个切片可以这样表示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_slice = input_pad[h_lower:h_upper, w_lower:w_upper,</span><br><span class="line">                                    c_lower:c_upper]</span><br></pre></td></tr></table></figure>
<p>宽和高上的截取范围很好计算。只要根据<code>stride</code>确认截取起点，再加上<code>f_new</code>就得到了截取终点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">h_lower = i_h * stride</span><br><span class="line">h_upper = i_h * stride + f_new</span><br><span class="line">w_lower = i_w * stride</span><br><span class="line">w_upper = i_w * stride + f_new</span><br></pre></td></tr></table></figure>
<p>比较难想的是考虑groups后，通道上的截取范围该怎么获得。这里，不妨再看一次分组卷积的示意图：</p>
<p><img src="/2022/07/24/DLS-note-10-4/1.png" alt></p>
<p>获取通道上的截取范围，就是获取右边那幅图中的输入通道组。究竟是红色的1-4，还是绿色的5-8，还是黄色的9-12。为了知道是哪一个范围，我们要算出当前输出通道对应的组号（颜色），这个组号由下面的算式获得：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i_g = i_c // c_o_per_group</span><br></pre></td></tr></table></figure>
<p>有了组号，就可以方便地计算通道上的截取范围了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c_lower = i_g * c_k</span><br><span class="line">c_upper = (i_g + <span class="number">1</span>) * c_k</span><br></pre></td></tr></table></figure>
<p>整个获取输入切片的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">i_g = i_c // c_o_per_group</span><br><span class="line">h_lower = i_h * stride</span><br><span class="line">h_upper = i_h * stride + f_new</span><br><span class="line">w_lower = i_w * stride</span><br><span class="line">w_upper = i_w * stride + f_new</span><br><span class="line">c_lower = i_g * c_k</span><br><span class="line">c_upper = (i_g + <span class="number">1</span>) * c_k</span><br><span class="line">input_slice = input_pad[h_lower:h_upper, w_lower:w_upper,</span><br><span class="line">                        c_lower:c_upper]</span><br></pre></td></tr></table></figure>
<p>而卷积核就很容易获取了，直接选中第<code>i_c</code>个卷积核即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_slice = weight_new[i_c]</span><br></pre></td></tr></table></figure>
<p>最后是卷积运算，别忘了加上bias。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output[i_h, i_w, i_c] = np.<span class="built_in">sum</span>(input_slice * kernel_slice)</span><br><span class="line"><span class="keyword">if</span> bias:</span><br><span class="line">    output[i_h, i_w, i_c] += bias[i_c]</span><br></pre></td></tr></table></figure>
<p>写完了所有东西，返回输出结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>为了方便地进行单元测试，我使用了pytest这个单元测试库。可以直接pip一键安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pytest</span><br></pre></td></tr></table></figure>
<p>之后就可以用pytest执行我的这份代码，代码里所有以<code>test_</code>开头的函数会被认为是单元测试的主函数。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pytest dldemos/BasicCNN/np_conv.py</span><br></pre></td></tr></table></figure>
<p>完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;c_i, c_o&#x27;</span>, [(<span class="params"><span class="number">3</span>, <span class="number">6</span></span>), (<span class="params"><span class="number">2</span>, <span class="number">2</span></span>)]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;kernel_size&#x27;</span>, [<span class="number">3</span>, <span class="number">5</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;stride&#x27;</span>, [<span class="number">1</span>, <span class="number">2</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;padding&#x27;</span>, [<span class="number">0</span>, <span class="number">1</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;dilation&#x27;</span>, [<span class="number">1</span>, <span class="number">2</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;groups&#x27;</span>, [<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;all&#x27;</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;bias&#x27;</span>, [<span class="literal">False</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_conv</span>(<span class="params">c_i: <span class="built_in">int</span>, c_o: <span class="built_in">int</span>, kernel_size: <span class="built_in">int</span>, stride: <span class="built_in">int</span>, padding: <span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">              dilation: <span class="built_in">int</span>, groups: <span class="built_in">str</span>, bias: <span class="built_in">bool</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> groups == <span class="string">&#x27;1&#x27;</span>:</span><br><span class="line">        groups = <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> groups == <span class="string">&#x27;all&#x27;</span>:</span><br><span class="line">        groups = c_i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> bias:</span><br><span class="line">        bias = np.random.randn(c_o)</span><br><span class="line">        torch_bias = torch.from_numpy(bias)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bias = <span class="literal">None</span></span><br><span class="line">        torch_bias = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">input</span> = np.random.randn(<span class="number">20</span>, <span class="number">20</span>, c_i)</span><br><span class="line">    weight = np.random.randn(c_o, kernel_size, kernel_size, c_i // groups)</span><br><span class="line"></span><br><span class="line">    torch_input = torch.from_numpy(np.transpose(<span class="built_in">input</span>, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    torch_weight = torch.from_numpy(np.transpose(weight, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line">    torch_output = torch.conv2d(torch_input, torch_weight, torch_bias, stride,</span><br><span class="line">                                padding, dilation, groups).numpy()</span><br><span class="line">    torch_output = np.transpose(torch_output.squeeze(<span class="number">0</span>), (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    numpy_output = conv2d(<span class="built_in">input</span>, weight, stride, padding, dilation, groups,</span><br><span class="line">                          bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> np.allclose(torch_output, numpy_output)</span><br></pre></td></tr></table></figure>
<p>其中，单元测试函数的定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;c_i, c_o&#x27;</span>, [(<span class="params"><span class="number">3</span>, <span class="number">6</span></span>), (<span class="params"><span class="number">2</span>, <span class="number">2</span></span>)]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;kernel_size&#x27;</span>, [<span class="number">3</span>, <span class="number">5</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;stride&#x27;</span>, [<span class="number">1</span>, <span class="number">2</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;padding&#x27;</span>, [<span class="number">0</span>, <span class="number">1</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;dilation&#x27;</span>, [<span class="number">1</span>, <span class="number">2</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;groups&#x27;</span>, [<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;all&#x27;</span>]</span>)</span></span><br><span class="line"><span class="meta">@pytest.mark.parametrize(<span class="params"><span class="string">&#x27;bias&#x27;</span>, [<span class="literal">False</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_conv</span>(<span class="params">c_i: <span class="built_in">int</span>, c_o: <span class="built_in">int</span>, kernel_size: <span class="built_in">int</span>, stride: <span class="built_in">int</span>, padding: <span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">              dilation: <span class="built_in">int</span>, groups: <span class="built_in">str</span>, bias: <span class="built_in">bool</span></span>):</span></span><br></pre></td></tr></table></figure>
<p>先别管上面那一堆装饰器，先看一下单元测试中的输入参数。在对某个函数进行单元测试时，要测试该函数的参数在不同取值下的表现。我打算测试我们的<code>conv2d</code>在各种输入通道数、输出通道数、卷积核大小、步幅、填充数、扩充数、分组数、是否加入bias的情况。</p>
<p><code>@pytest.mark.parametrize</code>用于设置单元测试参数的可选值。我设置了6组参数，每组参数有2个可选值，经过排列组合后可以生成<code>2^6=64</code>个单元测试，pytest会自动帮我们执行不同的测试。</p>
<p>在测试函数内，我先预处理了一下输入的参数，并生成了随机的输入张量，使这些参数和<code>conv2d</code>的参数一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_conv</span>(<span class="params">c_i: <span class="built_in">int</span>, c_o: <span class="built_in">int</span>, kernel_size: <span class="built_in">int</span>, stride: <span class="built_in">int</span>, padding: <span class="built_in">str</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">              dilation: <span class="built_in">int</span>, groups: <span class="built_in">str</span>, bias: <span class="built_in">bool</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> groups == <span class="string">&#x27;1&#x27;</span>:</span><br><span class="line">        groups = <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> groups == <span class="string">&#x27;all&#x27;</span>:</span><br><span class="line">        groups = c_i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> bias:</span><br><span class="line">        bias = np.random.randn(c_o)</span><br><span class="line">        torch_bias = torch.from_numpy(bias)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        bias = <span class="literal">None</span></span><br><span class="line">        torch_bias = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">input</span> = np.random.randn(<span class="number">20</span>, <span class="number">20</span>, c_i)</span><br><span class="line">    weight = np.random.randn(c_o, kernel_size, kernel_size, c_i // groups)</span><br></pre></td></tr></table></figure>
<p>为了确保我们实现的卷积和<code>torch.conv2d</code>是对齐的，我们要用<code>torch.conv2d</code>算一个结果，作为正确的参考值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch_input = torch.from_numpy(np.transpose(<span class="built_in">input</span>, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))).unsqueeze(<span class="number">0</span>)</span><br><span class="line">torch_weight = torch.from_numpy(np.transpose(weight, (<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)))</span><br><span class="line">torch_output = torch.conv2d(torch_input, torch_weight, torch_bias, stride,</span><br><span class="line">                            padding, dilation, groups).numpy()</span><br><span class="line">torch_output = np.transpose(torch_output.squeeze(<span class="number">0</span>), (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>由于<code>torch</code>里张量的形状格式是NCHW，weight的形状是C’Cff，我这里做了一些形状上的转换。</p>
<p>之后，调用我们自己的卷积函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numpy_output = conv2d(<span class="built_in">input</span>, weight, stride, padding, dilation, groups,</span><br><span class="line">                          bias)</span><br></pre></td></tr></table></figure>
<p>最后，验证一下两个结果是否对齐：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span> np.allclose(torch_output, numpy_output)</span><br></pre></td></tr></table></figure>
<p>运行前面提到的单元测试命令，pytest会输出很多测试的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pytest dldemos/BasicCNN/np_conv.py</span><br></pre></td></tr></table></figure>
<p>如果看到了类似的输出，就说明我们的代码是正确的。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">========== 64 passed in 1.20s ===============</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章中，我介绍了<code>torch.conv2d</code>的等价NumPy实现。同时，我还详细说明了卷积各参数(stride, padding, dilation, groups)的意义。通过阅读本文，相信大家能够深刻地理解一轮卷积是怎么完成的。</p>
<p>如果你也想把这方面的基础打牢，一定一定要自己动手从头写一份代码。在写代码，调bug的过程中，一定会有很多收获。</p>
<p>相比torch里的卷积，这份卷积实现还不够灵活。torch里可以自由输入卷积核的宽高、stride的宽高。而我们默认卷积核是正方形，宽度和高度上的stride是一样的。不过，要让卷积更灵活一点，只需要稍微修改一些预处理数据的代码即可，卷积的核心实现代码是不变的。</p>
<p>其实，在编程框架中，卷积的实现都是很高效的，不可能像我们这样先扩充卷积核，再填充输入图像。这些操作都会引入很多冗余的计算量。为了尽可能利用并行加速卷积的运算，卷积的GPU实现使用了一种叫做im2col的算法。这种算法会把每次卷积乘加用到的输入图像上的数据都放进列向量中，把卷积乘加转换成一次矩阵乘法。有兴趣的话欢迎搜索这方面的知识。</p>
<p>这篇文章仅介绍了卷积操作的正向传播。有了正向传播，反向传播倒没那么了难了。之后有时间的话我会再分享一篇用NumPy实现卷积反向传播的文章。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>本文中的动图来自于 <a target="_blank" rel="noopener" href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a></p>
<p>本文中分组卷积的图来自于论文 <a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/321325862_CondenseNet_An_Efficient_DenseNet_using_Learned_Group_Convolutions">https://www.researchgate.net/publication/321325862_CondenseNet_An_Efficient_DenseNet_using_Learned_Group_Convolutions</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Zhou Yifan</p>
  <div class="site-description" itemprop="description">A foresighted strategist with big-picture thinking. 大局观选手。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">91</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">53</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhou Yifan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
